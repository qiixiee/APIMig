{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5945", "change_description": ": All file handling converted to NIO.2 apis.", "change_title": "Full cutover to Path api from java.io.File", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Using NIO2 has a lot of benefits: We already banned File.delete and switched to Files.delete, I think we should ban File completely (except for some sugar methods that just forward with .toPath, like FSDirectory.open) For tests, ideally we go a little further and ban methods like FileSystems.getDefault(). Instead we could exempt LuceneTestCase and ensure all Paths are created via one protected method. This leaves open the possibility to mock up filesystem behavior at a lower level in tests in the future.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12668587/LUCENE-5945.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5946", "change_description": ": SimpleFSDirectory now uses Files.newByteChannel, for\nportability with custom FileSystemProviders. If you want the old\nnon-interruptible behavior of RandomAccessFile, use RAFDirectory\nin the misc/ module.", "change_title": "Change SimpleFSDirectory to use newByteChannel", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Currently our javadocs point to using SimpleFSDirectory \"to avoid ClosedByInterruptException\". But this is really bogus. If you interrupt() a thread doing i/o, then you need to be prepared for the consequences. Its just that RAF is broken and not interruptible at all, but that shouldnt justify us continuing to use it. SimpleFSDirectory should be \"the most portable\", so it should use Files.newByteChannel. And we should remove the javadocs/warnings about ClosedByInterrupt", "patch_link": "https://issues.apache.org/jira/secure/attachment/12668629/LUCENE-5946.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "SOLR-3359", "change_description": ": Added analyzer attribute/property to SynonymFilterFactory.", "change_title": "SynonymFilterFactory should accept analyzer attribute", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I've not been realized that CJKTokenizer and its factory classes was marked deprecated in 3.6/4.0 (the ticket is LUCENE-2906) until someone talked to me. I agree with the idea of using the chain of the Tokenizer and TokenFilters instead of CJKTokenizer, but it could be a problem for the existing users of SynonymFilterFactory with CJKTokenizerFactory. So this ticket comes to my mind again.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12592141/0001-Make-SynonymFilterFactory-accept-analyzer-attr.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5648", "change_description": ": Index and search date ranges, particularly multi-valued ones. It's\nimplemented in the spatial module as DateRangePrefixTree used with\nNumberRangePrefixTreeStrategy.", "change_title": "Index/search multi-valued time durations", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "If you need to index a date/time duration, then the way to do that is to have a pair of date fields; one for the start and one for the end – pretty straight-forward. But if you need to index a variable number of durations per document, then the options aren't pretty, ranging from denormalization, to joins, to using Lucene spatial with 2D as described here.  Ideally it would be easier to index durations, and work in a more optimal way. This issue implements the aforementioned feature using Lucene-spatial with a new single-dimensional SpatialPrefixTree implementation. Unlike the other two SPT implementations, it's not based on floating point numbers. It will have a Date based customization that indexes levels at meaningful quantities like seconds, minutes, hours, etc.  The point of that alignment is to make it faster to query across meaningful ranges (i.e. [2000 TO 2014]) and to enable a follow-on issue to facet on the data in a really fast way. I'll expect to have a working patch up this week.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646103/LUCENE-5648.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5895", "change_description": ": Lucene now stores a unique id per-segment and per-commit to aid\nin accurate replication of index files", "change_title": "Add per-segment and per-commit id to help replication", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "It would be useful if Lucene recorded a unique id for each segment written and each commit point.  This way, file-based replicators can use this to \"know\" whether the segment/commit they are looking at on a source machine and dest machine are in fact that same. I know this would have been very useful when I was playing with NRT replication (LUCENE-5438).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12663148/LUCENE-5895.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5889", "change_description": ": Add commit method to AnalyzingInfixSuggester, and allow just using .add\nto build up the suggester.", "change_title": "AnalyzingInfixSuggester should expose commit()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "There is no way short of close() for a user of AnalyzingInfixSuggester to cause it to commit() its underlying index: only refresh() is provided.  But callers might want to ensure the index is flushed to disk without closing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12663370/LUCENE-5889.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5123", "change_description": ": Add a \"pull\" option to the postings writing API, so\nthat a PostingsFormat now receives a Fields instance and it is\nresponsible for iterating through all fields, terms, documents and\npositions.", "change_title": "invert the codec postings API", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Currently FieldsConsumer/PostingsConsumer/etc is a \"push\" oriented api, e.g. FreqProxTermsWriter streams the postings at flush, and the default merge() takes the incoming codec api and filters out deleted docs and \"pushes\" via same api (but that can be overridden). It could be cleaner if we allowed for a \"pull\" model instead (like DocValues). For example, maybe FreqProxTermsWriter could expose a Terms of itself and just passed this to the codec consumer. This would give the codec more flexibility to e.g. do multiple passes if it wanted to do things like encode high-frequency terms more efficiently with a bitset-like encoding or other things... A codec can try to do things like this to some extent today, but its very difficult (look at buffering in Pulsing). We made this change with DV and it made a lot of interesting optimizations easy to implement...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603029/LUCENE-5123.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5268", "change_description": ": Full cutover of all postings formats to the \"pull\"\nFieldsConsumer API, removing PushFieldsConsumer.  Added new\nPushPostingsWriterBase for single-pass push of docs/positions to the\npostings format.", "change_title": "Cutover more postings formats to the inverted \"pull\" API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "In LUCENE-5123, we added a new, more flexible, \"pull\" API for writing postings.  This API allows the postings format to iterate the fields/terms/postings more than once, and mirrors the API for writing doc values. But that was just the first step (only SimpleText was cutover to the new API).  I want to cutover more components, so we can (finally) e.g. play with different encodings depending on the term's postings, such as using a bitset for high freq DOCS_ONLY terms (LUCENE-5052).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607863/LUCENE-5268.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5906", "change_description": ": Use Files.delete everywhere instead of File.delete, so that\nwhen things go wrong, you get a real exception message why.", "change_title": "Use Files.delete instead of File.delete + made up exception", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We are on java7, if we cannot delete a file, this one returns a real exception as to why.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12666489/LUCENE-5906.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5933", "change_description": ": Added FilterSpans for easier wrapping of Spans instance.", "change_title": "Add FilterSpans to allow easily wrapping a Spans impl", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I found this useful while working with spans recently. It's simple and straightforward. I'll add a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12667636/LUCNE-5933.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5925", "change_description": ": Remove fallback logic from opening commits, instead use\nDirectory.renameFile so that in-progress commits are never visible.", "change_title": "Use rename instead of segments_N fallback / segments.gen etc", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Our commit logic is strange, we write corrupted commit points and only on the last phase of commit do we \"correct them\". This means the logic to get the latest commit is always scary and awkward, since it must deal with partial commits, and try to determine if it should fall back to segments_N-1 or actually relay an exception. This logic is incomplete/sheisty as we, e.g. i think we only fall back so far (at most one). If we somehow screw up in all this logic do the wrong thing, then we lose data (e.g. LUCENE-4870 wiped entire index because of TooManyOpenFiles). We now require java 7, i think we should expore instead writing pending_segments_N and then in finishCommit() doing an atomic rename to segments_N. We could then remove all the complex fallback logic completely, since we no longer have to deal with \"ignoring partial commits\", instead simply delivering any exception we get when trying to read the commit and sleep better at night. In java 7, we have the apis for this (ATOMIC_MOVE).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12667180/LUCENE-5925.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5820", "change_description": ": SuggestStopFilter should have a factory.", "change_title": "SuggestStopFilter should have a factory", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "While trying to use the new Suggester in Solr I realized that SuggestStopFilter did not have a factory. We should add one.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12666958/LUCENE-5820.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5949", "change_description": ": Add Accountable.getChildResources().", "change_title": "Add Accountable.getChildResources()", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Since Lucene 4.5, you can see how much memory lucene is using at a basic level by looking at SegmentReader.ramBytesUsed() In 4.11 its already improved, you can pull the codec producers and get ram usage split out by postings, norms, docvalues, stored fields, term vectors, etc. Unfortunately most toString's are fairly useless, so you don't have any insight further than that, even though behind the scenes its mostly just adding up other Accountables. So instead if we can improve the toString's, and if an Accountable can return its children, we can connect all the dots and you can easily diagnose/debug issues and see what is going on. I know i've been frustrated with having to hack up tons of System.out.printlns during development to see this stuff. So I think we should add this method to Accountable: We can also add a simple helper method for quick debugging Accountables.toString(Accountable) to print the \"tree\", example output for a lucene segment: Note this works for any accountable, so also e.g. NRTCachingDirectory, OrdinalMap, Suggesters, FSTs, and so on. You can also e.g. traverse the graph yourself and output whatever you want. To be safe, I define that the graph returned is \"point in time snapshot\" and free of race conditions, and the Accountable helper methods provide this and also prevent access (even via cast) to datastructures you shouldn't be able to get to, just provide information. Since we aren't on java 8 yet (and cannot provide a simple default method), instead I think we should just add the method to Accountable, but add default emptyList() implementations to impacted datastructures such as DocIDSet and Suggester. For codec APIs, these are lower level, and there I think its best to leave the method abstract since they should really be providing useful information.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12668793/LUCENE-5949.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "SOLR-5986", "change_description": ": Added ExitableDirectoryReader that extends FilterDirectoryReader and enables\nexiting requests that take too long to enumerate over terms.", "change_title": "Don't allow runaway queries from harming Solr cluster health or search performance", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "The intent of this ticket is to have all distributed search requests stop wasting CPU cycles on requests that have already timed out or are so complicated that they won't be able to execute. We have come across a case where a nasty wildcard query within a proximity clause was causing the cluster to enumerate terms for hours even though the query timeout was set to minutes. This caused a noticeable slowdown within the system which made us restart the replicas that happened to service that one request, the worst case scenario are users with a relatively low zk timeout value will have nodes start dropping from the cluster due to long GC pauses. amccurry Built a mechanism into Apache Blur to help with the issue in BLUR-142 (see commit comment for code, though look at the latest code on the trunk for newer bug fixes). Solr should be able to either prevent these problematic queries from running by some heuristic (possibly estimated size of heap usage) or be able to execute a thread interrupt on all query threads once the time threshold is met. This issue mirrors what others have discussed on the mailing list: http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200903.mbox/%3C856ac15f0903272054q2dbdbd19kea3c5ba9e105b9d8@mail.gmail.com%3E", "patch_link": "https://issues.apache.org/jira/secure/attachment/12671285/SOLR-5986.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5911", "change_description": ": Add MemoryIndex.freeze() to allow thread-safe searching over a\nMemoryIndex.", "change_title": "Make MemoryIndex thread-safe for queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We want to be able to run multiple queries at once over a MemoryIndex in luwak (see https://github.com/flaxsearch/luwak/commit/49a8fba5764020c2f0e4dc29d80d93abb0231191), but this isn't possible with the current implementation.  However, looking at the code, it seems that it would be relatively simple to make MemoryIndex thread-safe for reads/queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12670422/LUCENE-5911.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5969", "change_description": ": Lucene 5.0 has a new index format with mismatched file detection,\nimproved exception handling, and indirect norms encoding for sparse fields.", "change_title": "Add Lucene50Codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Spinoff from LUCENE-5952: It would also be nice if we had a \"bumpCodecVersion\" script so rolling a new codec is not so daunting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12671465/LUCENE-5969.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6053", "change_description": ": Add Serbian analyzer.", "change_title": "Serbian Analyzer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This is analyzer for Serbian language, so far consisting only of a normalizer. Serbian language uses both Cyrillic and Latin alphabet, so the normalizer works with both alphabets. In the future, I'll see to add stopwords, stemmer and so on.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12680156/LUCENE-Serbian-1.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-4400", "change_description": ": Add support for new NYSIIS Apache commons phonetic\ncodec", "change_title": "add support for new commons-codec encoder (nysiis)", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "From Thomas on LUCENE-3720: btw. the next release will happen quite soon afaik and will also include a new phonetic encoder called Nysiis, which should perform slightly better than Soundex (see https://issues.apache.org/jira/browse/CODEC-63). Any feedback is very welcome! I didn't do this in LUCENE-3720 because I wanted to fix the bug separately, but this should be pretty easy to add.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12680646/LUCENE-4400.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6059", "change_description": ": Add Daitch-Mokotoff Soundex phonetic Apache commons\nphonetic codec, and upgrade to Apache commons codec 1.10.", "change_title": "Add Daitch-Mokotoff Soundex phonetic filter from latest commons-codec 1.10 release", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The latest commons-codec release (1.10) has added a new phonetic encoder: Daitch-Mokotoff Soundex.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12680656/LUCENE-6059.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6058", "change_description": ": With the upgrade to Apache commons codec 1.10, the\nexperimental BeiderMorseFilter has changed its behavior, so any\nindex using it will need to be rebuilt.", "change_title": "Changes to Beider-Morse Encoder in latest commons-codec 1.10 release", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We have just recently released commons-codec 1.10 and I wanted to inform you that there have been slight changes to the Beider Morse encoder. This might result in slightly different encodings (see CODEC-187). At least one unit test has to be adjusted when updating. The results of the Beider Morse encoder are now identical to the reference implementation of the algorithm (v3.4, http://stevemorse.org/census/soundex.html), it might be necessary to inform users before upgrading as they might have to re-index to get consistent results.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6050", "change_description": ": Accept MUST and MUST_NOT (in addition to SHOULD) for\neach context passed to Analyzing/BlendedInfixSuggester", "change_title": "Add possibility to specify SHOUD or MUST for each context for AnalyzingInfixSuggester.loockup()", "detail_type": "Bug", "detail_affect_versions": "4.10.2", "detail_fix_versions": "5.0,6.0", "detail_description": "Currently as shown at  https://github.com/apache/lucene-solr/blob/lucene_solr_4_9_0/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L362  , we have: and SHOULD is being applied to all contexts. We need the ability to specify whether it's a SHOULD or a MUST on each individual context. Thanks.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12680876/LUCENE-6050.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5929", "change_description": ": Also extract terms to highlight from block join\nqueries.", "change_title": "Standard highlighting doesn't work for ToParentBlockJoinQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Because WeightedSpanTermExtractor#extract doesn't check for ToParentBlockJoinQuery, the Highlighter class fails to produce highlights for this type of query. At first it may seem like there's no issue, because ToParentBlockJoinQuery only returns parent documents, while the highlighting applies to children. But if a client can directly supply the text from child documents (as elasticsearch does if _source is enabled), then highlighting will unexpectedly fail. A test case that triggers the bug is attached. The same issue exists for ToChildBlockJoinQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12681316/LUCENE-5929.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6063", "change_description": ": Allow overriding whether/how ConcurrentMergeScheduler\nstalls incoming threads when merges are falling behind", "change_title": "Allow overriding ConcurrentMergeScheduler's denial-of-service protection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "In LUCENE-5310 we explored improving CMS/SMS sharing/concurrency, but the issue never \"converged\", so I want to break out one small part of it here: the ability to override CMS's default \"aggressive\" denial-of-service protection where it forcefully stalls the incoming threads that are responsible for creating too many segments. More advanced applications can more gracefully handle the \"too many merges\" by e.g. slowing down the incoming indexing rate at a higher level.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12681511/LUCENE-6063.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5833", "change_description": ": DocumentDictionary now enumerates each value separately\nin a multi-valued field (not just the first value), so you can build\nsuggesters from multi-valued fields.", "change_title": "Suggestor Version 2 doesn't support multiValued fields", "detail_type": "Bug", "detail_affect_versions": "4.8.1", "detail_fix_versions": "5.0,6.0", "detail_description": "So if you use a multiValued field in the new suggestor it will not pick up terms for any term after the first one. So it treats the first term as the only term it will make it's dictionary from. This is the suggestor I'm talking about: https://issues.apache.org/jira/browse/SOLR-5378", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701504/LUCENE-5833_branch4_10.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6077", "change_description": ": Added a filter cache.", "change_title": "Add a filter cache", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "Lucene already has filter caching abilities through CachingWrapperFilter, but CachingWrapperFilter requires you to know which filters you want to cache up-front. Caching filters is not trivial. If you cache too aggressively, then you slow things down since you need to iterate over all documents that match the filter in order to load it into an in-memory cacheable DocIdSet. On the other hand, if you don't cache at all, you are potentially missing interesting speed-ups on frequently-used filters. Something that would be nice would be to have a generic filter cache that would track usage for individual filters and make the decision to cache or not a filter on a given segments based on usage statistics and various heuristics, such as:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684024/LUCENE-6077.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6088", "change_description": ": TermsFilter implements Accountable.", "change_title": "Make TermsFilter implement Accountable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Terms filters can sometimes be massive. Having their memory usage exposed can be useful eg. for the FilterCache.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684679/LUCENE-6088.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6034", "change_description": ": The default highlighter when used with QueryScorer will highlight payload-sensitive\nqueries provided that term vectors with positions, offsets, and payloads are present. This is the\nonly highlighter that can highlight such queries accurately.", "change_title": "MemoryIndex should be able to wrap TermVector Terms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The default highlighter has a \"WeightedSpanTermExtractor\" that uses MemoryIndex for certain queries – basically phrases, SpanQueries, and the like.  For lots of text, this aspect of highlighting is time consuming and consumes a fair amount of memory.  What also consumes memory is that it wraps the tokenStream in CachingTokenFilter in this case.  But if the underlying TokenStream is actually from TokenSources (wrapping TermVector Terms), this is all needless!  Furthermore, MemoryIndex doesn't support payloads. The patch here has 3 aspects to it:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685239/LUCENE-6034.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-5914", "change_description": ": Add an option to Lucene50Codec to support either BEST_SPEED\nor BEST_COMPRESSION for stored fields.", "change_title": "More options for stored fields compression", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Since we added codec-level compression in Lucene 4.1 I think I got about the same amount of users complaining that compression was too aggressive and that compression was too light. I think it is due to the fact that we have users that are doing very different things with Lucene. For example if you have a small index that fits in the filesystem cache (or is close to), then you might never pay for actual disk seeks and in such a case the fact that the current stored fields format needs to over-decompress data can sensibly slow search down on cheap queries. On the other hand, it is more and more common to use Lucene for things like log analytics, and in that case you have huge amounts of data for which you don't care much about stored fields performance. However it is very frustrating to notice that the data that you store takes several times less space when you gzip it compared to your index although Lucene claims to compress stored fields. For that reason, I think it would be nice to have some kind of options that would allow to trade speed for compression in the default codec.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685224/LUCENE-5914.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6119", "change_description": ": Add auto-IO-throttling to ConcurrentMergeScheduler, to\nrate limit IO writes for each merge depending on incoming merge\nrate.", "change_title": "Add auto-io-throttle to ConcurrentMergeScheduler", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This method returns number of \"incoming\" bytes IW has written since it was opened, excluding merging. It tracks flushed segments, new commits (segments_N), incoming files/segments by addIndexes, newly written live docs / doc values updates files. It's an easy statistic for IW to track and should be useful to help applications more intelligently set defaults for IO throttling (RateLimiter). For example, an application that does hardly any indexing but finally triggered a large merge can afford to heavily throttle that large merge so it won't interfere with ongoing searches. But an application that's causing IW to write new bytes at 50 MB/sec must set a correspondingly higher IO throttling otherwise merges will clearly fall behind.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12690056/LUCENE-6119.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6155", "change_description": ": Add payload support to MemoryIndex. The default highlighter's\nQueryScorer and WeighedSpanTermExtractor now have setUsePayloads(bool).", "change_title": "Payload support for MemoryIndex", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "MemoryIndex could be enhanced to support payloads.  It should be optional, defaulting to false.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689703/LUCENE-6155_Payloads_in_MemoryIndex.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6166", "change_description": ": Deletions (alone) can now trigger new merges.", "change_title": "Deletions alone never trigger merges", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "If an app has an old index and only does deletions against it, we seem to never trigger a merge, so deletions are never reclaimed in this case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12690677/LUCENE-6166.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "New Features", "change_id": "LUCENE-6177", "change_description": ": Add CustomAnalyzer that allows to configure analyzers\nlike you do in Solr's index schema. This class has a builder API to configure\nTokenizers, TokenFilters, and CharFilters based on their SPI names\nand parameters as documented by the corresponding factories.", "change_title": "Add CustomAnalyzer - a builder that creates Analyzers from the factory classes", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I prepared some \"generic Analyzer class CustomAnalyzer, that makes it easy to build analyzers like in Solr or Elasticsearch. Under the hood it uses the factory classes. The class is made like a builder: It is possible to give the resource loader (used by stopwords and similar). By default it tries to load stuff from context classloader (without any class as reference so paths must be absolute - this is the behaviour ClasspathResourseLoader defaults to). In addition you can give a Lucene MatchVersion, by default it would use Version.LATEST (once LUCENE-5900 is completely fixed).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12692229/LUCENE-6177.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5960", "change_description": ": Use a more efficient bitset, not a Set<Integer>, to\ntrack visited states.", "change_title": "Avoid grow of Set in AnalyzingSuggester.topoSortStates(Automaton)", "detail_type": "Improvement", "detail_affect_versions": "4.10", "detail_fix_versions": "5.0,6.0", "detail_description": "Converted \"visited\" to a BitSet and sized it correctly in AnalyzingSuggester.topoSortStates(Automaton). This avoids dynamic resizing of the set.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5959", "change_description": ": Don't allocate excess memory when building automaton in\nfinish.", "change_title": "Optimized memory management in Automaton.Builder.finish()", "detail_type": "Improvement", "detail_affect_versions": "4.10", "detail_fix_versions": "5.0,6.0", "detail_description": "Reworked Automaton.Builder.finish() to not allocate memory stepwise. Added growTransitions(int numTransitions) to be able to resize the transistions array just once.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12669427/finish.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5963", "change_description": ": Reduce memory allocations in\nAnalyzingSuggester.", "change_title": "Improved AnalyzingSuggester.replaceSep()", "detail_type": "Improvement", "detail_affect_versions": "4.10", "detail_fix_versions": "5.0,6.0", "detail_description": "Reworked AnalyzingSuggester.replaceSep() to use Automaton.Builder instead of Automaton. This avoids most of the unnecessary allocation of memory via grow*().", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5938", "change_description": ": MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE is now faster on\nqueries that match few documents by using a sparse bit set implementation.", "change_title": "New DocIdSet implementation with random write access", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "We have a great cost API that is supposed to help make decisions about how to best execute queries. However, due to the fact that several of our filter implementations (eg. TermsFilter and BooleanFilter) return FixedBitSets, either we use the cost API and make bad decisions, or need to fall back to heuristics which are not as good such as RandomAccessFilterStrategy.useRandomAccess which decides that random access should be used if the first doc in the set is less than 100. On the other hand, we also have some nice compressed and cacheable DocIdSet implementation but we cannot make use of them because TermsFilter requires a DocIdSet that has random write access, and FixedBitSet is the only DocIdSet that we have that supports random access. I think it would be nice to replace FixedBitSet in those filters with another DocIdSet that would also support random write access but would have a better cost?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12671898/LUCENE-5938.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5969", "change_description": ": Refactor merging to be more efficient, checksum calculation is\nper-segment/per-producer, and norms and doc values merging no longer cause\nRAM spikes for latent fields.", "change_title": "Add Lucene50Codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Spinoff from LUCENE-5952: It would also be nice if we had a \"bumpCodecVersion\" script so rolling a new codec is not so daunting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12671465/LUCENE-5969.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5983", "change_description": ": CachingWrapperFilter now uses a new DocIdSet implementation\ncalled RoaringDocIdSet instead of WAH8DocIdSet.", "change_title": "RoaringDocIdSet", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "Robert pointed me to this paper: http://arxiv.org/pdf/1402.6407v4 that describes an interesting way to build doc id sets: The bit space is divided into blocks of 2^16 bits so that you can store the bits which are set either in a short[] (2 bytes per doc ID) or in a FixedBitSet. The choice is easy, if less than 2^12 bits are set, then the short[] representation is more compact otherwise a FixedBitSet would be more compact. It's quite similar to the way that Solr builds DocSets in SolrIndexSearcher.getDocSet(DocsEnumState).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12672330/LUCENE-5983.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6022", "change_description": ": DocValuesDocIdSet checks live docs before doc values.", "change_title": "DocValuesDocIdSet: check deleted docs before doc values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "When live documents are not null, DocValuesDocIdSet checks if doc values match the document before the live docs. Given that checking if doc values match could involve a heavy computation (eg. geo distance) and that the default codec has live docs in memory but doc values on disk, I think it makes more sense to check live docs first?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12676607/LUCENE-6022.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6030", "change_description": ": Add norms patched compression for a small number of common values", "change_title": "Add norms patched compression which uses table for most common values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We have added the PATCHED norms sub format in lucene 50, which uses a bitset to mark documents that have the most common value (when >97% of the documents have that value).  This works well for fields that have a predominant value length, and then a small number of docs with some other random values.  But another common case is having a handful of very common value lengths, like with a title field. We can use a table (see TABLE_COMPRESSION) to store the most common values, and save an oridinal for the \"other\" case, at which point we can lookup in the secondary patch table.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12677654/LUCENE-6030.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6040", "change_description": ": Speed up EliasFanoDocIdSet through broadword bit selection.", "change_title": "Speedup broadword bit selection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "Use table lookup instead of some broadword manipulations", "patch_link": "https://issues.apache.org/jira/secure/attachment/12678802/LUCENE-6040.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6033", "change_description": ": CachingTokenFilter now uses ArrayList not LinkedList, and has new\nisCached() method.", "change_title": "Add CachingTokenFilter.isCached and switch LinkedList to ArrayList", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "CachingTokenFilter could use a simple boolean isCached() method implemented as-such: It's useful for the highlighting code to remove its wrapping of CachingTokenFilter if after handing-off to parts of its framework it turns out that it wasn't used. Furthermore, use an ArrayList, not a LinkedList.  ArrayList is leaner when the token count is high, and this class doesn't manipulate the list in a way that might favor LL. A separate patch will come that actually uses this method.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12677993/LUCENE-6033.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6031", "change_description": ": TokenSources (in the default highlighter) converts term vectors into a\nTokenStream much faster in linear time (not N*log(N) using less memory, and with reset()\nimplemented.  Only one of offsets or positions are required of the term vector.", "change_title": "TokenSources optimization, avoid sort", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "TokenSources.java, in the highlight module, is a facade that returns a TokenStream for a field by either un-inverting & converting the TermVector Terms, or by text re-analysis if TermVectors are unavailable or don't have the right options.  TokenSources is used by the default highlighter, which is the most accurate highlighter we've got.  When documents are large (say hundreds of kilobytes on up), I found that most of the highlighter's activity was up-front spent un-inverting & converting the term vector to a TokenStream, not on the actual/real highlighting that follows.  Much of that time was on a huge sort of hundreds of thousands of Tokens.  Time was also spent doing lots of String conversion and char copying, and it used a lot of memory, too. In this patch, I overhauled TokenStreamFromTermPositionVector.java, and I removed similar logic in TokenSources that was used in circumstances when positions weren't available but offsets were.  This class can un-invert term vectors that have positions and/or offsets (at least one).  It doesn't sort.  It places Tokens directly into an array of tokens directly indexed by position.  When positions aren't available, the startOffset/8 is a substitute.  I've got a more light-weight Token inner class used in place of the former and deprecated Token that ultimately forms a linked-list when the process is done.  There is no string conversion; character copying is minimized.  The Token array is GC'ed after initialization, it's only needed during construction. Misc: A key assumption is that the position increment gap or first position isn't gigantic, as that will create wasted space and the linked-list formation ultimately has to visit all the slots.  We also assume that there aren't a ton of tokens at the same position, since inserting new tokens in sorted order is O(N^2) where 'N' is the average co-occurring token length. My performance testing using Lucene's benchmark module on a megabyte document showed >5x speedup, in conjunction with some other patches to be posted separately. This patch made the most difference.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12683765/LUCENE-6031.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6089", "change_description": ",", "change_title": "Tune CompressionMode.HIGH_COMPRESSION", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Patch to apply the parameters Adrien proposed on LUCENE-5914. These make this option a lot less costly on CPU and actually compress better too. This only impacts tests.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684687/LUCENE-6089.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6090", "change_description": ",", "change_title": "don't wrap Deflater in CompressionMode", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This outputs zlib header/footer and computes adler32 for each block. The space is nothing, but the adler32 computation on encode/decode has a cost, and we already have our own checksum. Since we currently compress/decompress at merge, this reduces the overall time of merging stored fields with deflate vs lz4, from 1.8x to 1.5x, reducing some of the pain.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684854/LUCENE-6090.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6034", "change_description": ": QueryScorer, used by the default highlighter, needn't re-index the provided\nTokenStream with MemoryIndex when it comes from TokenSources (term vectors) with offsets and\npositions.", "change_title": "MemoryIndex should be able to wrap TermVector Terms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The default highlighter has a \"WeightedSpanTermExtractor\" that uses MemoryIndex for certain queries – basically phrases, SpanQueries, and the like.  For lots of text, this aspect of highlighting is time consuming and consumes a fair amount of memory.  What also consumes memory is that it wraps the tokenStream in CachingTokenFilter in this case.  But if the underlying TokenStream is actually from TokenSources (wrapping TermVector Terms), this is all needless!  Furthermore, MemoryIndex doesn't support payloads. The patch here has 3 aspects to it:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685239/LUCENE-6034.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-5951", "change_description": ": ConcurrentMergeScheduler detects whether the index is on SSD or not\nand does a better job defaulting its settings.  This only works on Linux for now;\nother OS's will continue to use the previous defaults (tuned for spinning disks).", "change_title": "Detect when index is on SSD and set dynamic defaults", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "E.g. ConcurrentMergeScheduler should default maxMergeThreads to 3 if it's on SSD and 1 if it's on spinning disks. I think the new NIO2 APIs can let us figure out which device we are mounted on, and from there maybe we can do os-specific stuff e.g. look at  /sys/block/dev/queue/rotational to see if it's spinning storage or not ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12688286/LUCENE-5951.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6131", "change_description": ": Optimize SortingMergePolicy.", "change_title": "optimize SortingMergePolicy", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This has a number of performance problems today: We can fix these two problems without completely refactoring LeafReader... we won't get a \"bulk byte merge\", checksum computation will still be suboptimal, and its not a general solution to \"merging with filterreaders\" but that stuff can be for another day.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12688676/LUCENE-6131.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6133", "change_description": ": Improve default StoredFieldsWriter.merge() to be more efficient.", "change_title": "improve default stored fields merge algorithm", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This is not very efficient today. We should try to improve the default one and not rely so much upon bulk merges. At the least we can use a specialized visitor, save some cpu, and avoid creating so much garbage. Another pain point in default merge is all the string creation and unicode encode/decoding back and forth. I don't tackle that here... but that would be fantastic to fix in another issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12688929/LUCENE-6133.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6145", "change_description": ": Make EarlyTerminatingSortingCollector able to early-terminate\nwhen the sort order is a prefix of the index-time order.", "change_title": "Make EarlyTerminatingSortingCollector smarter about when it can early terminate", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Today EarlyTerminatingSortingCollector only early-terminates if the sort order matches exactly the index-time sort order. It should also early-terminate when the sort order is a prefix of the index-time sort order.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689527/LUCENE-6145.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6178", "change_description": ": Score boolean queries containing MUST_NOT clauses with BooleanScorer2,\nto use skip list data and avoid unnecessary scoring.", "change_title": "don't score MUST_NOT clauses with BooleanScorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Its similar to the conjunction case: we should just use BS2 since it has advance(). Even in the dense case I think its currently better since it avoids calling score() in cases where BS1 calls it redundantly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12691732/LUCENE-6178.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5900", "change_description": ": Deprecated more constructors taking Version in *InfixSuggester and\nICUCollationKeyAnalyzer, and removed TEST_VERSION_CURRENT from the test framework.", "change_title": "Version cleanup", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "There are still a couple things taking Version in their constructor (AnalyzingInfixSuggester/BlendedInfixSuggester), TEST_VERSION_CURRENT isn't needed anymore, and there are a number of places with :Post-Release-Update-Version:, which should be possible to remove completely.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12663762/LUCENE-5900.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-4535", "change_description": ": oal.util.FilterIterator is now an internal API.", "change_title": "Make FilterIterator @lucene.internal", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "FilterIterator has been release without the @lucene.internal tag, we should fix it for 5.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552042/LUCENE-4535.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-4924", "change_description": ": DocIdSetIterator.docID() must now return -1 when the iterator is\nnot positioned. This change affects all classes that inherit from\nDocIdSetIterator, including DocsEnum and DocsAndPositionsEnum.", "change_title": "Make DocIdSetIterator.docID() return -1 when not positioned", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Today DocIdSetIterator.docID() can either return -1 or NO_MORE_DOCS when the enum is not positioned. I would like to only allow it to return -1 so that we can have better assertions. (This proposal is for trunk only.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578443/LUCENE-4924.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5127", "change_description": ": Reduce RAM usage of FixedGapTermsIndex. Remove\nIndexWriterConfig.setTermIndexInterval, IndexWriterConfig.setReaderTermsIndexDivisor,\nand termsIndexDivisor from StandardDirectoryReader. These options have been no-ops\nwith the default codec since Lucene 4.0. If you want to configure the interval for\nthis term index, pass it directly in your codec, where it can also be configured\nper-field.", "change_title": "FixedGapTermsIndex should use monotonic compression", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "for the addresses in the big in-memory byte[] and disk blocks, we could save a good deal of RAM here. I think this codec just never got upgraded when we added these new packed improvements, but it might be interesting to try to use for the terms data of sorted/sortedset DV implementations. patch works, but has nocommits and currently ignores the divisor. The annoying problem there being that we have the shared interface with \"get(int)\" for PackedInts.Mutable/Reader, but no equivalent base class for monotonics get(long)... Still its enough that we could benchmark/compare for now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594733/LUCENE-5127.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5388", "change_description": ": Remove Reader from Tokenizer's constructor and from\nAnalyzer's createComponents. TokenStreams now always get their input\nvia setReader.", "change_title": "Eliminate construction over readers for Tokenizer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "In the modern world, Tokenizers are intended to be reusable, with input supplied via #setReader. The constructors that take Reader are a vestige. Worse yet, they invite people to make mistakes in handling the reader that tangle them up with the state machine in Tokenizer. The sensible thing is to eliminate these ctors, and force setReader usage.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12622106/LUCENE-5388.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "pull request #16", "change_description": ": Remove Reader from Tokenizer's constructor and from\nAnalyzer's createComponents. TokenStreams now always get their input\nvia setReader.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Added solr & lucene version to the response of the query handlers for debug purpsoses.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5527", "change_description": ": The Collector API has been refactored to use a dedicated Collector\nper leaf.", "change_title": "Make the Collector API work per-segment", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Spin-off of LUCENE-5299. LUCENE-5229 proposes different changes, some of them being controversial, but there is one of them that I really really like that consists in refactoring the Collector API in order to have a different Collector per segment. The idea is, instead of having a single Collector object that needs to be able to take care of all segments, to have a top-level Collector: and a per-AtomicReaderContext collector: I think it makes the API clearer since it is now obious setScorer and acceptDocsOutOfOrder need to be called after setNextReader which is otherwise unclear. It also makes things more flexible. For example, a collector could much more easily decide to use different strategies on different segments. In particular, it makes the early-termination collector much cleaner since it can return different atomic collectors implementations depending on whether the current segment is sorted or not. Even if we have lots of collectors all over the place, we could make it easier to migrate by having a Collector that would implement both Collector and AtomicCollector, return this in setNextReader and make current concrete Collector implementations extend this class instead of directly extending Collector.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638588/LUCENE-5527.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5702", "change_description": ": The FieldComparator API has been refactor to a per-leaf API, just\nlike Collectors.", "change_title": "Per-segment comparator API", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "As a next step of LUCENE-5527, it would be nice to have per-segment comparators, and maybe even change the default behavior of our top* comparators so that they merge top hits in the end.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689795/LUCENE-5702.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-4246", "change_description": ": IndexWriter.close now always closes, even if it throws\nan exception.  The new IndexWriterConfig.setCommitOnClose (default\ntrue) determines whether close() should commit before closing.", "change_title": "Fix IndexWriter.close() to not commit or wait for pending merges", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638789/LUCENE-4246.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5608", "change_description": ",", "change_title": "SpatialPrefixTree API refactor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This is a refactor of the SpatialPrefixTree spatial API, in preparation for more SPT implementations on the near horizon.  These are fairly internal APIs; SpatialExample.java didn't have to change, nor the Solr adapters, and I doubt ES would have to either. API changes: Future: I wish I had done this as a series of commits on a GitHub branch; ah well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640489/LUCENE-5608__SpatialPrefixTree_API_refactor.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5565", "change_description": ",", "change_title": "Remove String based encoding from SpatialPrefixTree/Cell API; just use bytes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "The SpatialPrefixTree/Cell API supports bytes and String encoding/decoding dually.  I want to remove the String side to keep the API simpler.  Included in this issue, I'd like to make some small refactorings to reduce assumptions the filters make of the underlying encoding such that future encodings can work a in more different ways with less impact on the filters. String encode/decode will exist for the Geohash one for now since GeohashUtils works off of Strings, but Quad could change more easily.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638431/LUCENE-5565_SPT_remove_String.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5666", "change_description": ": Change uninverted access (sorting, faceting, grouping, etc)\nto use the DocValues API instead of FieldCache. For FieldCache functionality,\nuse UninvertingReader in lucene/misc (or implement your own FilterReader).\nUninvertingReader is more efficient: supports multi-valued numeric fields,\ndetects when a multi-valued field is single-valued, reuses caches\nof compatible types (e.g. SORTED also supports BINARY and SORTED_SET access\nwithout insanity).  \"Insanity\" is no longer possible unless you explicitly want it.\nRename FieldCache* and DocTermOrds* classes in the search package to DocValues*.\nMove SortedSetSortField to core and add SortedSetFieldSource to queries/, which\ntakes the same selectors. Add helper methods to DocValues.java that are better\nsuited for search code (never return null, etc).", "change_title": "Add UninvertingReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Currently the fieldcache is not pluggable at all. It would be better if everything used the docvalues apis. This would allow people to customize the implementation, extend the classes with custom subclasses with additional stuff, etc etc. FieldCache can be accessed via the docvalues apis, using the FilterReader api.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12644786/LUCENE-5666.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5871", "change_description": ": Remove Version from IndexWriterConfig. Use\nIndexWriterConfig.setCommitOnClose to change the behavior of IndexWriter.close().\nThe default has been changed to match that of 4.x.", "change_title": "Simplify or remove use of Version in IndexWriterConfig", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "IndexWriter currently uses Version from IndexWriterConfig to determine the semantics of close().  This is a trapdoor for users, as they often default to just sending Version.LUCENE_CURRENT since they don't understand what it will be used for.  Instead, we should make the semantics of close a direction option in IWC.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12661689/LUCENE-5871.iwclose.4x.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5965", "change_description": ": CorruptIndexException requires a String or DataInput resource.", "change_title": "Make CorruptIndexException require 'resource' like TooOld/TooNew do", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "and review all of these to ensure other pertinent information is included.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12670084/LUCENE-5965.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5972", "change_description": ": IndexFormatTooOldException and IndexFormatTooNewException now\n             extend from IOException.", "change_title": "Index too old/new is not a corruption", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "IndexFormatTooOldException and IndexFormatTooNewException both extend from CorruptIndexException.  But this is not a corruption, it is simply an unsupported version of an index.  They should just extend IOException.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12670520/LUCENE-5972.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5569", "change_description": ": *AtomicReader/AtomicReaderContext have been renamed to *LeafReader/LeafReaderContext.", "change_title": "Rename AtomicReader to LeafReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "See LUCENE-5527 for more context: several of us seem to prefer Leaf to Atomic. Talking from my experience, I was a bit confused in the beginning that this thing is named AtomicReader, since Atomic is otherwise used in Java in the context of concurrency. So maybe renaming it to Leaf would help remove this confusion and also carry the information that these readers are used as leaves of top-level readers?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12670454/LUCENE-5569.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5938", "change_description": ": Removed MultiTermQuery.ConstantScoreAutoRewrite as\nMultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE is usually better.", "change_title": "New DocIdSet implementation with random write access", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "We have a great cost API that is supposed to help make decisions about how to best execute queries. However, due to the fact that several of our filter implementations (eg. TermsFilter and BooleanFilter) return FixedBitSets, either we use the cost API and make bad decisions, or need to fall back to heuristics which are not as good such as RandomAccessFilterStrategy.useRandomAccess which decides that random access should be used if the first doc in the set is less than 100. On the other hand, we also have some nice compressed and cacheable DocIdSet implementation but we cannot make use of them because TermsFilter requires a DocIdSet that has random write access, and FixedBitSet is the only DocIdSet that we have that supports random access. I think it would be nice to replace FixedBitSet in those filters with another DocIdSet that would also support random write access but would have a better cost?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12671898/LUCENE-5938.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5924", "change_description": ": Rename CheckIndex -fix option to -exorcise. This option does not\nactually fix the index, it just drops data.", "change_title": "Rename checkindex's -fix option", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This option is dangerous. It sounds so good though that people are quick to use it, but it definitely drops entire segments. The commandline flag should be someting other than -fix (e.g. -exorcise). I dont agree with the current description of the option either. True, it does have *WARNING* but I think it should read more like the scary options in 'man hdparm'. Like hdparm, we could fail if you provide it with an even more ridiculous warning, and make you run again with --yes-i-really-know-what-i-am-doing as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12667020/LUCENE-5924.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5969", "change_description": ": Add Codec.compoundFormat, which handles the encoding of compound\nfiles. Add getMergeInstance() to codec producer APIs, which can be overridden\nto return an instance optimized for merging instead of searching. Add\nTerms.getStats() which can return additional codec-specific statistics about a field.\nChange instance method SegmentInfos.read() to two static methods: SegmentInfos.readCommit()\nand SegmentInfos.readLatestCommit().", "change_title": "Add Lucene50Codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Spinoff from LUCENE-5952: It would also be nice if we had a \"bumpCodecVersion\" script so rolling a new codec is not so daunting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12671465/LUCENE-5969.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5992", "change_description": ": Remove FieldInfos from SegmentInfosWriter.write API.", "change_title": "Version should not be encoded as a String in the index", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The version is really \"just\" 3 (maybe 4) ints under-the-hood, but today we write it as a String which then requires spooky string tokenization/parsing when we open the index.  I think it should be encoded directly as ints. In LUCENE-5952 I had tried to make this change, but it was controversial, and got booted. Then in LUCENE-5969, I tried again, but that issue has morphed (nicely!) into fixing all sorts of things except these three ints. Maybe 3rd time's a charm", "patch_link": "https://issues.apache.org/jira/secure/attachment/12673207/LUCENE-5992_tests.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5998", "change_description": ": Simplify Field/SegmentInfoFormat to read+write methods.", "change_title": "Remove unnecessary codec *infos abstractions", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "SI and FIS formats have no state, e.g. they dont hold on to file handles or anything. neither their Readers or Writers are closeable, they each both just contain one simple read() or write() method respectively. We should remove these unnecessary abstractions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12673447/LUCENE-5998.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6000", "change_description": ": Removed StandardTokenizerInterface.  Tokenizers now use\ntheir jflex impl directly.", "change_title": "Remove StandardTokenizerInterface", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This interface existed for backcompat, so that each impl had at least some common minimal interface, and could be used by StandardTokenizer.  However, in LUCENE-5999 backcompat for standard tokenizer was implemented using separate named classes.  We should remove this interface, as it no longer serves a purpose.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12673703/LUCENE-6000.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6006", "change_description": ": Removed FieldInfo.normType since it's redundant: it\nwill be DocValuesType.NUMERIC if the field indexed and does not omit\nnorms, else null.", "change_title": "Replace FieldInfo.normsType with FieldInfo.hasNorms boolean", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I came across this precursor while working on LUCENE-6005: I think FieldInfo.normsType can only be null (field did not index norms) or DocValuesType.NUMERIC (it did).  I'd like to simplify to just boolean hasNorms. This is a strange boolean, though: in theory it should be derived from indexed && omitNorms == false, but we have it for the exceptions case where every document in a segment hit an exception and never added norms.  I think this is the only reason it exists?  (In theory, such cases should result in 100% deleted segments, which IW should then drop ... but seems dangerous to \"rely\" on that). So I changed the indexing chain to just fill in the default (0) norms for all documents in such exceptional cases; this way going forward (starting with 5.0 indices) we really don't need this hasNorms.  But we still need it for pre-5.0 indices...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12675030/LUCENE-6006.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6013", "change_description": ": Removed indexed boolean from IndexableFieldType and\nFieldInfo, since it's redundant with IndexOptions != null.", "change_title": "Remove IndexableFieldType.indexed()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Like LUCENE-6006, here's another pre-cursor for LUCENE-6005 ... because I think it's important to nail down Lucene's low-schema (FieldType/FieldInfos) semantics before adding a high-schema. IndexableFieldType.indexed() is redundant with IndexableFieldType.indexOptions() != null, so we should remove it, codecs shouldn't have to write/read it, high-schema should not configure it, etc. Similarly, the FieldInfo.indexed bit is redundant, so I removed it, but I left the sugar API (FieldInfo.isIndexed) and implement it as just checking IndexOptions != null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12675885/LUCENE-6013.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6021", "change_description": ": FixedBitSet.nextSetBit now returns DocIdSetIterator.NO_MORE_DOCS\ninstead of -1 when there are no more bits which are set.", "change_title": "Make FixedBitSet and SparseFixedBitSet share a wider common interface", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "Today, the only common interfaces that these two classes share are Bits and Accountable. I would like to add a BitSet base class that would be both extended by FixedBitSet and SparseFixedBitSet. The idea is to share more code between these two impls and make them interchangeable for more use-cases so that we could just use one or the other based on the density of the data that we are working on.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12676602/LUCENE-6021.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-5953", "change_description": ": Directory and LockFactory APIs were restructured: Locking is\nnow under the responsibility of the Directory implementation. LockFactory is\nonly used by subclasses of BaseDirectory to delegate locking to an impl\nclass. LockFactories are now singletons and are responsible to create a Lock\ninstance based on a Directory implementation passed to the factory method.\nSee MIGRATE.txt for more details.", "change_title": "Refactor LockFactory usage in Directory", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We should remove the setters for the LockFactory from Directory and make the field final. It is a bug to change the LockFactory after creating a directory, because you may break locking (if locks are currently held). The LockFactory should be passed on ctor only. The other suggestion: Should LockFactory have a directory at all? We moved away from having the lock separately from the index directory. This is no longer a supported configuration (since approx Lucene 2.9 or 3.0). I would like to remove the directory from LockFactory and make it part of the Directory only.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12680432/LUCENE-5953.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6062", "change_description": ": Throw exception instead of silently doing nothing if you try to\nsort/group/etc on a misconfigured field (e.g. no docvalues, no UninvertingReader, etc).", "change_title": "Index corruption from numeric DV updates", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I hit this while working on on LUCENE-6005: when cutting over TestNumericDocValuesUpdates to the new Document2 API, I accidentally enabled additional docValues in the test, and this this: A one-line change to the existing test (on trunk) causes this corruption: For some reason, the base doc values for the 2nd segment is not being written, but clearly should have (to hold field \"foo\")... I'm not sure why.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12682194/LUCENE-6062.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6068", "change_description": ": LeafReader.fields() never returns null.", "change_title": "Remove reader.fields() == null checks everywhere", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I don't know how this got this way, but it never returns null. SegmentReader even asserts this. But the api requires consumers to do a bunch of useless null checks. This is a bug.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12683056/LUCENE-6068.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6082", "change_description": ": Remove abort() from codec apis.", "change_title": "only IndexFileDeleter should delete files", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Currently the codec API has SI writers, stored fields writers, and term vectors writers all deleting files, Stored fields and term vectors codec APIs have abort() methods, etc. This is outdated and scary, since TrackingDirectoryWrapper is used to the list of files the codec used, and this is ultimately passed to IndexFileDeleter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684287/LUCENE-6082.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6084", "change_description": ": IndexOutput's constructor now requires a String\nresourceDescription so its toString is sane", "change_title": "Add reasonable IndexOutput.toString", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "In LUCENE-3539 we fixed IndexInput.toString to always include the resourceDescription. I think we should do the same for IndexOutput? I don't think Lucene currently uses/relies on IndexOutput.toString, but e.g. at least Elasticsearch does, and likely others, so I think it can only help if you can see which path is open by this IndexOutput.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684431/LUCENE-6084.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6087", "change_description": ": Allow passing custom DirectoryReader to SearcherManager", "change_title": "SearcherManager should accept foreign DirectoryReader on init", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Today you init SearcherManager either with a dir or an IndexWriter, but since we have a useful FilterDirectoryReader class to wrap the sub-readers, it's useful for apps to also pass their own wrapped DirectoryReader and have SearcherManager reopen from that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684610/LUCENE-6087.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6085", "change_description": ": Undeprecate SegmentInfo attributes, but add safety so they\nwon't be trappy if codec tries to use them during docvalues updates.", "change_title": "Add back SI.attributes (safely)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We removed this for two reasons: But Adrien has a real use case (LUCENE-5914), and I think we can just add some safety for the updates case (e.g. if the map is unmodifiable then the trap will not exist, any put() will throw exception). In general, we should have more safety in SI anyway (diagnostics map, too).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12684429/LUCENE-6085.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6097", "change_description": ": Remove dangerous / overly expert\nIndexWriter.abortMerges and waitForMerges methods.", "change_title": "Make IW.abortMerges and waitForMerges package private", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4,5.0,6.0", "detail_description": "These are crazy expert methods; must they be public?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685535/LUCENE-6097.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6099", "change_description": ": Add FilterDirectory.unwrap and\nFilterDirectoryReader.unwrap", "change_title": "Add FilterDirectory.unwrap and FilterDirectoryReader.unwrap", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "We already have the useful FilterLeafReader.unwrap, but not for these other filter classes....", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685576/LUCENE-6099.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6121", "change_description": ": CachingTokenFilter.reset() now propagates to its input if called before\nincrementToken().  You must call reset() now on this filter instead of doing it a-priori on the\ninput(), which previously didn't work.", "change_title": "Fix CachingTokenFilter to propagate reset() the first time", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "CachingTokenFilter should have been propagating reset() but only the first time and thus you would then use CachingTokenFilter in a more normal way – wrap it and call reset() then increment in a loop, etc., instead of knowing you need to reset() on what it wraps but not this token filter itself. That's weird. It's ab-normal for a TokenFilter to never propagate reset, so every user of CachingTokenFilter to date has worked around this by calling reset() on the underlying input instead of the final wrapping token filter (CachingTokenFilter in this case).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12688212/LUCENE-6121_CachingTokenFilter_reset_propagates_reset_if_not_cached.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6147", "change_description": ": Make the core Accountables.namedAccountable function public", "change_title": "Make the core Accountables.namedAccountable function public", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Accountables has a number of methods named namedAccountable.  The core one of these works by taking a snapshot with an anonymous Accountable.  This method is currently private due to concerns over safety.  However, I think we should make it public, and document the how safety can be achieved (which is by only using that and the other namedAccountable methods).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689641/LUCENE-6147.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6150", "change_description": ": Remove staleFiles set and onIndexOutputClosed() from FSDirectory.", "change_title": "Remove staleFiles set and onIndexOutputClosed from FSDirectory", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Hi, the \"hack\" to keep track of files written to in FSDirectory, to filter them when calling sync is heavily broken. mikemccand already opened an issue, which was abandoned then. In fact handling this in FSDirectory is a hack and broken! If IndexWriter is itsself not able to correctly handle tracking the files, it is also his repsonsibilty to do this. We already have a class that can do this: TrackingDirectoryWrapper. IndexWriter should use an instance of this class to track those stale files (until the problem is solved). I would like to keep FSDirectory clean from this, especially, as this is broken anyways: If somebody has another directory impl like HDFS or Infinispan, the problem still persists. The base directory should throw an IOException if trying to sync a file that does not exist!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689683/LUCENE-6150.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6146", "change_description": ": Replaced Directory.copy() with Directory.copyFrom().", "change_title": "Directory.copy -> Directory.copyFrom", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Spinoff of LUCENE-4746. This method is currently: But it would be better to restructure this so the destination directory is the one actually being changed by the operation: Besides fixing the order to make sense, adding it to the name might help prevent bugs like the current TrackingDirectoryWrapper impl (used by IndexWriter to track what files are used):", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689603/LUCENE-6146.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6149", "change_description": ": Infix suggesters' highlighting and allTermsRequired can\nbe set at the constructor for non-contextual lookup.", "change_title": "Infix suggesters' highlighting, allTermsRequired options are hardwired and not configurable for non-contextual lookup", "detail_type": "Improvement", "detail_affect_versions": "4.9,4.10.1,4.10.2,4.10.3", "detail_fix_versions": "5.0,6.0", "detail_description": "Highlighting and allTermsRequired are hardwired in AnalyzingInfixSuggester for non-contextual lookup (via Lookup) see true, true below: The above means the majority of the current infix suggester lookup always return highlighted results with allTermsRequired in effect. There is no way to change this despite the options and improvement of LUCENE-6050, made to incorporate Boolean lookup clauses (MUST/SHOULD). This shortcoming has also been reported in SOLR-6648. The suggesters (AnalyzingInfixSuggester, BlendedInfixSuggester) should provide a proper mechanism to set defaults for highlighting and \"allTermsRequired\", e.g. in constructors (and in Solr factories, thus configurable via solrconfig.xml).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12690572/LUCENE-6149-v4.10.3.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6158", "change_description": ",", "change_title": "IW.addIndexes(IndexReader...) -> IW.addIndexes(LeafReader...)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "addIndexes(IndexReader...) is useful to force a single merge that transforms data: you wrap the readers with some logic that alters them. But for any use case doing this, they need to work on leaves (LeafReader) to actually do anything. Otherwise, for simply merging indexes, allowing addIndexes(IndexReader) is unnecessary and maybe a slight trap, its way faster to call addIndexes(Directory), and it won't force a single slow merge, but will just copy in the relevant files and call maybeMerge(). Part of the confusion is the two methods have such different behavior that i don't think they should be both be named addIndexes. But lets do that separately, first i want to fix the parameters. Long term taking LeafReader here is a simple step towards a more performant api for \"merging with filterreader\", since its horribly inefficient today.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689929/LUCENE-6158.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6165", "change_description": ",", "change_title": "Change merging APIs to work on CodecReader instead of LeafReader", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Patch factors out \"reader based on codec apis\" and changes all merge policy/addIndexes apis to use this. If you want to do slow wrapping, you can still do it, just use SlowCodecReaderWrapper.wrap(LeafReader) yourself (versus SegmentMerger doing it always if its not a SegmentReader). Also adds FilterCodecReader, to make it easier to start efficiently filtering on merge. I cutover all the index splitters to this. This means they should be much much faster with this patch, they just change the deletes as you expect, and the merge is as optimal as a normal one. In other places, for now I think we should just do a rote conversion with SlowCodecReaderWrapper.wrap. Its no slower than today, just explicit, and we can incrementally fix them to do the right thing in the future rather than all at once.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12690519/LUCENE-6165.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6179", "change_description": ": Out-of-order scoring is not allowed anymore, so\nWeight.scoresDocsOutOfOrder and LeafCollector.acceptsDocsOutOfOrder have been\nremoved and boolean queries now always score in order.", "change_title": "Remove out-of-order scoring", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Out-of-order currently adds complexity that I would like to remove. Here is a selection of issues that come from out-of-order scoring. I initially wanted to keep it and improve the decision process in LUCENE-6172 but I'm not sure it's the right approach as it would require to make the API even more complicated... hence the suggestion to remove out-of-order scoring completely.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12692258/LUCENE-6179.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "API Changes", "change_id": "LUCENE-6212", "change_description": ": IndexWriter no longer accepts per-document Analyzer to\nadd/updateDocument.  These methods were trappy as they made it\neasy to accidentally index tokens that were not easily\nsearchable.", "change_title": "Remove IndexWriter's per-document analyzer add/updateDocument APIs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,5.1,6.0", "detail_description": "IndexWriter already takes an analyzer up-front (via IndexWriterConfig), but it also allows you to specify a different one for each add/updateDocument. I think this is quite dangerous/trappy since it means you can easily index tokens for that document that don't match at search-time based on the search-time analyzer. I think we should remove this trap in 5.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695530/LUCENE-6212.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5650", "change_description": ": Enforce read-only access to any path outside the temporary\nfolder via security manager, and make test temp dirs absolute.", "change_title": "Enforce read-only access to any path outside the temporary folder via security manager", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The recent refactoring to all the create temp file/dir functions (which is great!) has a minor regression from what existed before.  With the old LuceneTestCase.TEMP_DIR, the directory was created if it did not exist.  So, if you set java.io.tmpdir to \"./temp\", then it would create that dir within the per jvm working dir.  However, getBaseTempDirForClass() now does asserts that check the dir exists, is a dir, and is writeable. Lucene uses \".\" as java.io.tmpdir.  Then in the test security manager, the per jvm cwd has read/write/execute permissions.  However, this allows tests to write to their cwd, which I'm trying to protect against (by setting cwd to read/execute in my test security manager).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645807/dih.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5948", "change_description": ": RateLimiter now fully inits itself on init.", "change_title": "Improve RateLimiters Initialization semantics", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "I was working on SOLR-6485 when I realized that the first time pause is called even if we write a lot of bytes pause doesn't work correctly because in SimpleRateLimiter.pause() lastNS is 0 and startNS is always more than targetNS. If we remove the following line from TestRateLimiter.testPause() then the test fails - Should we do one of the following ?  1. Initialize lastNS in the ctor 2. Add a method saying start() which does the same", "patch_link": "https://issues.apache.org/jira/secure/attachment/12669032/LUCENE-5948.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5981", "change_description": ": CheckIndex obtains write.lock, since with some parameters it\nmay modify the index, and to prevent false corruption reports, as it does\nnot have the regular \"spinlock\" of DirectoryReader.open. It now implements\nCloseable and you must close it to release the lock.", "change_title": "CheckIndex modifies index without write.lock", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Instead it asks you nicely to \"not do that\". Due to the way this is implemented, if you choose to drop corrupt segments, it should obtain the lock before actually doing any reads too, or it might lose more than you think or do other strange stuff.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12672299/LUCENE-5981.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6004", "change_description": ": Don't highlight the LookupResult.key returned from\nAnalyzingInfixSuggester", "change_title": "Highlighting AnalyzingInfixSuggester skips non-highlighted key", "detail_type": "Bug", "detail_affect_versions": "4.10", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "when setting 'doHighlight' to true at AnalyzingInfixSuggester.lookup(..), both the key and the highlightKey inside the returned lookupresult are set to the highlighted string. See at AnalyzingInfixSuggester.createResults, line 530: if (doHighlight) else As I understand, the key should'nt be highlighted in any case, only the highlightKey.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12681554/LUCENE-6004.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5980", "change_description": ": Don't let document length overflow.", "change_title": "IW positions check not quite right", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "I noticed this when working on LUCENE-5977. We only check that position doesn't overflow, not length. So a buggy analyzer can happily write a corrupt index (negative freq)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12672193/LUCENE-5980.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5999", "change_description": ": Fix backcompat support for StandardTokenizer", "change_title": "Add backcompat support for StandardTokenizer before 4.7", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "The merge from trunk for branch_5x remove the backcompat support in StandardTokenizer for previous unicode versions.  With 5x, we only need to add support back for 4.0-4.6 (unicode 6.1).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12673620/LUCENE-5999.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5961", "change_description": ": Fix the exists() method for FunctionValues returned by many ValueSources to\nbehave properly when wrapping other ValueSources which do not exist for the specified document", "change_title": "FunctionValues.exist(int) isn't returning false in cases where it should for many \"math\" based value sources", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The FunctionValues class contains an exist(int doc) method with a default implementation that returns true - field based DocValues override this method as appropriate, but most of the \"function\" based subclasses in the code (typically anonymous subclasses of \"FloatDocValues\") don't override this method when wrapping other ValueSources. So for example: the FunctionValues returned by ProductFloatFunction.getValues() will say that a value exists for any doc, even if that ProductFloatFunction wraps two FloatFieldSources that don't exist for any docs", "patch_link": "https://issues.apache.org/jira/secure/attachment/12675311/LUCENE-5961.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6039", "change_description": ": Add IndexOptions.NONE and DocValuesType.NONE instead of\nusing null to mean not index and no doc values, renamed\nIndexOptions.DOCS_ONLY to DOCS, and pulled IndexOptions and\nDocValues out of FieldInfo into their own classes in\norg.apache.lucene.index", "change_title": "Add IndexOptions.NO and DocValuesType.NO, instead of null", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Idea from Simon: it seems dangerous for IndexOptions and DocValuesType via Indexable/FieldType and FieldInfo that we use null to mean it's not indexed or has no doc values. We should instead have an explicit choice (IndexOptions.NO, DocValuesType.NO) in the enum?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12678453/LUCENE-6039.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6043", "change_description": ": Fix backcompat support for UAX29URLEmailTokenizer", "change_title": "Add backcompat support for UAX29URLEmailTokenizer before 4.7", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "In LUCENE-5999 backcompat support was added for StandardTokenizer with unicode 6.1, but UAX29URLEmailTokenizer was overlooked.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12678710/LUCENE-6043.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6041", "change_description": ": Remove sugar methods FieldInfo.isIndexed and\nFieldInfo.hasDocValues.", "change_title": "remove sugar FieldInfo.isIndexed and .hasDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Follow-on from LUCENE-6039; these two booleans don't really exist: they are just sugar to check for IndexOptions.NO and DocValuesType.NO.  I think for the low-level schema API in Lucene we should not expose such sugar: callers should have to be explicit.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12678518/LUCENE-6041.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6044", "change_description": ": Fix backcompat support for token filters with enablePositionIncrements=false.\nAlso fixed backcompat for TrimFilter with updateOffsets=true.  These options\nare supported with a match version before 4.4, and no longer valid at all with 5.0.", "change_title": "Add backcompat for TokenFilters with posInc=false before 4.4", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "In Lucene 4.4, a number of token filters supporting the enablePositionIncrements=false setting were changed to default to true.  However, with Lucene 5.0, the setting was removed altogether.  We should have backcompat for this setting, as well as work when used with a TokenFilterFactory and match version < 4.4.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697231/LUCENE-6044-JapanesePartOfSpeechStopFilter.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6042", "change_description": ": CustomScoreQuery explain was incorrect in some cases,\nsuch as when nested inside a boolean query.", "change_title": "CustomScoreQuery Explain differs from the actual score when topLevelBoost is used.", "detail_type": "Bug", "detail_affect_versions": "4.8", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "CustomScoreQuery.java, doExplain has the following line: This multiplies the custom score query by just the boost of the current query, and not by which is the value that's actually used during scoring. This leads to drastically different scores in the debug info, relative to the actual score, when the query is a subquery of another one, like a BooleanQuery clause, with a non-1 boost.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12679141/LUCENE-6042.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6046", "change_description": ": Add maxDeterminizedStates safety to determinize (which has\nan exponential worst case) so that if it would create too many states, it\nnow throws an exception instead of exhausting CPU/RAM.", "change_title": "RegExp.toAutomaton high memory use", "detail_type": "Bug", "detail_affect_versions": "4.10.1", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "When creating an automaton from an org.apache.lucene.util.automaton.RegExp, it's possible for the automaton to use so much memory it exceeds the maximum array size for java. The following caused an OutOfMemoryError with a 32gb heap: When increased to a 60gb heap, the following exception is thrown:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12679128/LUCENE-6046.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6054", "change_description": ": Allow repeating the empty automaton", "change_title": "RegExp.toAutomaton fails on #*", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "This throws an assertion error: new RegExp(\"#*\").toAutomaton(1000);", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6049", "change_description": ": Don't throw cryptic exception writing a segment when\nthe only docs in it had fields that hit non-aborting exceptions\nduring indexing but also had doc values.", "change_title": "Cryptic exception if all docs in a segment hit non-aborting exceptions before adding their doc values", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "I hit this while working on LUCENE-6005: If you add a document with a single field that's both indexed and has doc values, and during inversion it hits a non-aborting exception, and all docs for a given segment had this happen, then you'll hit this confusing exception: The good news here is that exception is new from LUCENE-6019 and it prevents this case from causing index corruption, but the bad news is, you shouldn't even get an exception writing the segment in the first place.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12679656/LUCENE-6049.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6055", "change_description": ": PayloadAttribute.clone() now does a deep clone of the underlying\nbytes.", "change_title": "PayloadAttribute.clone() should deep clone its BytesRef", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "PayloadAttribute.clone() does a shallow clone, unlike e.g. CharTermAttribute. Attributes should deep clone, otherwise capturing state isn't correct. In addition, both PA's and CTA's .clone() falsely documents that they do shallow cloning on purposes, so need to fix that too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12680567/LUCENE-6055.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6060", "change_description": ": Remove dangerous IndexWriter.unlock method", "change_title": "Remove IndexWriter.unLock", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,5.0,6.0", "detail_description": "This method used to be necessary, when our locking impls were buggy, but it's a godawful dangerous method: it invites index corruption. I think we should remove it. Apps that for some scary reason really need it can do their own thing...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12681047/LUCENE-6060.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6062", "change_description": ": Pass correct fieldinfos to docvalues producer when the\nsegment has updates.", "change_title": "Index corruption from numeric DV updates", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "I hit this while working on on LUCENE-6005: when cutting over TestNumericDocValuesUpdates to the new Document2 API, I accidentally enabled additional docValues in the test, and this this: A one-line change to the existing test (on trunk) causes this corruption: For some reason, the base doc values for the 2nd segment is not being written, but clearly should have (to hold field \"foo\")... I'm not sure why.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12682194/LUCENE-6062.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6075", "change_description": ": Don't overflow int in SimpleRateLimiter", "change_title": "SimpleRateLimiter cast overflow results in Thread.sleep exception", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "SimpleRateLimiter.pause() uses an uncheck cast of longs to ints: Thread.sleep((int) (pauseNS/1000000), (int) (pauseNS % 1000000)); Although we check that pauseNS is positive, however if it's large enough the cast to int produces a negative value, causing Thread.sleep to throw an exception. We should protect for it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12683346/LUCENE-6075.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5987", "change_description": ": IndexWriter will now forcefully close itself on\naborting exception (an exception that would otherwise cause silent\ndata loss).", "change_title": "Make indexwriter a mere mortal when exceptions strike", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "IndexWriter's exception handling is overly complicated. Every method in general reads like this: Part of the problem is it acts like its an invincible superhero, e.g. can take a disk full on merge or flush to the face and just keep on trucking, and you can somehow fix the root cause and then just go about making commits on the same instance. But we have a hard enough time ensuring exceptions dont do the wrong thing (e.g. cause corruption), and I don't think we really test this crazy behavior anywhere: e.g. making commits AFTER hitting disk full and so on. It would probably be simpler if when such things happen, IW just considered them \"tragic\" just like OOM and rolled itself back, instead of doing all kinds of really scary stuff to try to \"keep itself healthy\" (like the little dance it plays with IFD in mergeMiddle manually deleting CFS files). Besides, without something like a WAL, Indexwriter isn't really fit to be a superhero anyway: it can't prevent you from losing data in such situations. It just doesn't have the right tools for the job. edit: just to be clear I am referring to abort (low level exception during flush) and exceptions during merge. For simple non-aborting cases like analyzer errors, of course we can deal with this. We already made great progress on turning a lot of BS exceptions that would cause aborts into non-aborting ones recently.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685183/LUCENE-5987.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6094", "change_description": ": Allow IW.rollback to stop ConcurrentMergeScheduler even\nwhen it's stalling because there are too many merges.", "change_title": "IW.rollback can take forever when CMS has stalled threads", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.3,5.0,6.0", "detail_description": "CMS hard-stalls incoming threads for denial-of-service protection when merging cannot keep up with whatever is producing new segments. When you call IW.rollback, it asks all merges to abort, and a running merge will periodically check to see if it should abort. However, a stalled merge fails to check, which means rollback can take indefinitely long; I've seen this in Elasticsearch causing shutdown to take > 10 sec.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12685490/LUCENE-6094.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6105", "change_description": ": Don't cache FST root arcs if the number of root arcs is\nsmall, or if the cache would be > 20% of the size of the FST.", "change_title": "Don't create root arc cache for tiny FSTs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4,5.0,6.0", "detail_description": "The purpose of the root arc cache is to speed up lookups for ASCII terms, but it adds high overhead if the FST is already tiny.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12686245/LUCENE-6105.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6124", "change_description": ": Fix double-close() problems in codec and store APIs.", "change_title": "Fix broken close() methods", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Closeable.close() says \"If the stream is already closed then invoking this method has no effect.\". But a lot of our code does not really respect that. If i add an \"extra\" close() call in assertingcodec, it finds all kinds of bugs in codec code, for example:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12688163/LUCENE-6124.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6152", "change_description": ": Fix double close problems in OutputStreamIndexOutput.", "change_title": "Fix double close bug in OutputStreamIndexOutput", "detail_type": "Bug", "detail_affect_versions": "4.10.3,5.0,6.0", "detail_fix_versions": "5.0,6.0", "detail_description": "As discovered in LUCENE-6151, we shouldprevent calling flush() twice when closing OutputStreamIndexOutput two times.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689695/LUCENE-6152.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6139", "change_description": ": Highlighter: TokenGroup start & end offset getters should have\nbeen returning the offsets of just the matching tokens in the group when\nthere's a distinction.", "change_title": "TokenGroup.getStart|EndOffset should return matchStart|EndOffset not start|endOffset", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The default highlighter has a TokenGroup class that is passed to Formatter.highlightTerm().  TokenGroup also has getStartOffset() and getEndOffset() methods that ostensibly return the start and end offsets into the original text of the current term.  These getters aren't called by Lucene or Solr but they are made available and are useful to me.  The problem is that they return the wrong offsets when there are tokens at the same position.  I believe this was an oversight of LUCENE-627 in which these getters should have been updated but weren't.  The fix is simple: return matchStartOffset and matchEndOffset from these getters, not startOffset and endOffset.  I think this oversight would not have occurred if Highlighter didn't have package-access to TokenGroup's fields.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12689722/LUCENE-6139_TokenGroup_offsets.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6173", "change_description": ": NumericTermAttribute and spatial/CellTokenStream do not clone\ntheir BytesRef(Builder)s. Also equals/hashCode was missing.", "change_title": "NumericTermAttribute does not implement deep clone since cutover to BytesRefBuilder", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This was an oversight. We should fix it, although NumericTermAttribute is not really a public class (it is an implementation detail).", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6205", "change_description": ": Fixed intermittent concurrency issue that could cause\nFileNotFoundException when writing doc values updates at the same\ntime that a merge kicks off.", "change_title": "DV updates can hit FileNotFoundException due to concurrency bug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4,5.0,5.1,6.0", "detail_description": "Jenkins has hit this a few times recently, e.g.: It repros only after substantial beasting. It's a concurrency issue between one thread kicking off a merge, and another thread resolving doc values updates.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695032/LUCENE-6205.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6192", "change_description": ": Fix int overflow corruption case in skip data for\nhigh frequency terms in extremely large indices", "change_title": "Long overflow in LuceneXXSkipWriter can corrupt skip data", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.5,5.0,6.0", "detail_description": "I've been iterating with Tom on this corruption that CheckIndex detects in his rather large index (720 GB in a single segment): And Rob spotted long -> int casts in our skip list writers that look like they could cause such corruption if a single high-freq term with many positions required > 2.1 GB to write its positions into .pos.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693399/LUCENE-6192.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6093", "change_description": ": Don't throw NullPointerException from\nBlendedInfixSuggester for lookups that do not end in a prefix\ntoken.", "change_title": "BlendedInfixSuggester throws NullPointerException if there were discarded trailing characters in the query", "detail_type": "Bug", "detail_affect_versions": "4.10.2", "detail_fix_versions": "4.10.4,5.0,5.1,6.0", "detail_description": "BlendedInfixSuggester throws NullPointerException if there were discarded trailing characters (e.g. whitespace or special character) in the query. The problem seems to be in the createCoefficient method that fails to check if prefixToken parameter is null. AnalyzingInfixSuggester sets prefixToken to null in the described case and passes it to BlendedInfixSuggester. On the side not even if BlendedInfixSuggester is changed to handle this creates a problem to calculate the weights as prefixToken is null and cannot be used. I would be better to have AnalyzingInfixSuggester to always set prefixToken to lastToken.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695304/LUCENE-6093.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6214", "change_description": ": Fixed IndexWriter deadlock when one thread is\ncommitting while another opens a near-real-time reader and an\nunrecoverable (tragic) exception is hit.", "change_title": "IW deadlocks if commit and reopen happens concurrently while exception is hit", "detail_type": "Bug", "detail_affect_versions": "5.0", "detail_fix_versions": "4.10.4,5.0,5.1,6.0", "detail_description": "I just hit this while working on an elasticseach test using a lucene 5.1 snapshot (5.1.0-snapshot-1654549). The test throws random exceptions via MockDirWrapper and deadlocks, jstack says:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695800/LUCENE-6214.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Documentation", "change_id": "LUCENE-5392", "change_description": ": Add/improve analysis package documentation to reflect\nanalysis API changes.", "change_title": "Documentation for modified token / analysis pipeline", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "5.0,6.0", "detail_description": "The changes to the tokenizer and analyzer need to be reflected in the package overview for core analysis.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Documentation", "change_id": "pull request #17", "change_description": ": Add/improve analysis package documentation to reflect\nanalysis API changes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "add asserts and Improve Javadoc comments (patch from gerlowskija)", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Documentation", "change_id": "LUCENE-6057", "change_description": ": Improve Sort(SortField) docs", "change_title": "Clarify the Sort(SortField...) constructor)", "detail_type": "Improvement", "detail_affect_versions": "4.10.2,6.0", "detail_fix_versions": "4.10.2,5.0,6.0", "detail_description": "I don't really know which version this affects, but I clarified the documentation of the Sort(SortField...) constructor to ease the understanding for new users. Pull Request: https://github.com/apache/lucene-solr/pull/20", "patch_link": "https://issues.apache.org/jira/secure/attachment/12681314/LUCENE-6057.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Documentation", "change_id": "LUCENE-6112", "change_description": ": Fix compile error in FST package example code", "change_title": "Compile error with FST package example code", "detail_type": "Task", "detail_affect_versions": "4.10.2", "detail_fix_versions": "5.0,6.0", "detail_description": "I run the FST construction example guided package.html with lucene 4.10, and found a compile error. http://lucene.apache.org/core/4_10_2/core/index.html?org/apache/lucene/util/fst/package-summary.html javac claimed as below. \"FSTTest\" is my test class, just copied from javadoc's example. I modified scratchInts variable type from IntsRef to IntsRefBuilder, it worked fine. (I checked o.a.l.u.fst.TestFSTs.java TestCase and my modification seems to be correct.) Util.toIntsRef() method takes IntsRefBuilder as 2nd argument instead of IntsRef since 4.10, so Javadocs also should be fixed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12687103/LUCENE-6112.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Tests", "change_id": "LUCENE-5957", "change_description": ": Add option for tests to not randomize codec", "change_title": "Add option for tests to not randomize codec", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This is particularly useful when creating the backcompat indexes, since it is a pain to figure out which codec you need to specify to avoid being randomized.  Something like -Dtests.codec=default could simply bypass the randomization of the codec.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12669179/LUCENE-5957.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Tests", "change_id": "LUCENE-5974", "change_description": ": Add check that backcompat indexes use default codecs", "change_title": "add smoketester or test check that indexes are created with the correct codecs.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "This wasted a bunch of my time this morning. Tests should fail.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12670750/LUCENE-5974.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Tests", "change_id": "LUCENE-5971", "change_description": ": Create addBackcompatIndexes.py script to build and add\nbackcompat test indexes for a given lucene version. Also renamed backcompat\nindex files to use Version.toString() in filename.", "change_title": "Separate backcompat creation script from adding version", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "The recently created bumpVersion.py attempts to create a new backcompat index if the default codec has changed.  However, we now want to create a backcompat index for every released version, instead of just when there is a change to the default codec. We should have a separate script which creates the backcompat indexes.  It can even work directly on the released artifacts (by pulling down from mirrors once released), so that there is no possibility for generating the index from an incorrect svn/git checkout.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12670448/LUCENE-5971.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Tests", "change_id": "LUCENE-6002", "change_description": ": Monster tests no longer fail.  Most of them now have an 80 hour\ntimeout, effectively removing the timeout.  The tests that operate near the 2\nbillion limit now use IndexWriter.MAX_DOCS instead of Integer.MAX_VALUE.\nSome of the slow Monster tests now explicitly choose the default codec.", "change_title": "Fix monster tests so they pass", "detail_type": "Bug", "detail_affect_versions": "5.0", "detail_fix_versions": "5.0,6.0", "detail_description": "The tests labelled @Monster are having a very hard time passing.  Initially, there were some real failures on tests that push the 2 billion envelope, as well as some test bugs regarding suite timeouts and the quantity of date sent to sysout.  mikemccand committed a fix for the real failures and the initial sysout problems. Trying to move @SuppressSysoutChecks to the definition of @Monster didn't work.  I'm also having trouble defining the suite timeouts.  One test that says it takes about 45 minutes hit the six hour timeout that I had configured for the suite, and failed. What sort of machine should we use as a \"normal\" machine for deciding on suite timeouts?  My machine is over four years old, but it's got three 2.5Ghz cores and 12GB of RAM.  That's certainly not a completely modern machine, but it's not exactly slow.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12674428/LUCENE-6002.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Tests", "change_id": "LUCENE-5968", "change_description": ": Improve error message when 'ant beast' is run on top-level\nmodules.", "change_title": "Improve error message when 'ant beast' is run on top-level modules", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "As discussed at http://markmail.org/thread/c5y63pmvpgyrmct5 'ant beast' currently gives confusing error messages when run on top-level modules, this makes it clear that it should only be run within a module.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Tests", "change_id": "LUCENE-6120", "change_description": ": Fix MockDirectoryWrapper's close() handling.", "change_title": "how should MockIndexOutputWrapper.close handle exceptions in delegate.close", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Chasing a tricking Elasticsearch test failure, it came down to the delegate.close throwing an exception (ClosedByInterruptException, disturbingly, in this case), causing MockIndexOutputWrapper.close to fail to remove that IO from MDW's map. The question is, what should we do here, when delegate.close throws an exception?  Is the delegate in fact closed, even when it throws an exception? Java8's docs on java.io.Closeable say this: As noted in AutoCloseable.close(), cases where the close may fail require careful attention. It is strongly advised to relinquish the underlying resources and to internally mark the Closeable as closed, prior to throwing the IOException. And our OutputStreamIndexOutput is careful about this (flushes, then closes in a try-with-resources). So, I think MDW should be fixed to mark the IO as closed even if delegate.close throws an exception...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12688704/LUCENE-6120.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Build", "change_id": "LUCENE-5909", "change_description": ": Smoke tester now has better command line parsing and\noptionally also runs on Java 8.", "change_title": "Run smoketester on Java 8", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "In the past, when we were on Java 6, we ran the Smoketester on Java 6 and Java 7. As Java 8 is now officially released and supported, smoketester should now use and require JAVA8_HOME. For the nightly-smoke tests I have to install the openjdk8 FreeBSD package, but that should not be a problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12665558/LUCENE-5909.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Build", "change_id": "LUCENE-5902", "change_description": ": Add bumpVersion.py script to manage version increase after release branch is cut.", "change_title": "Add bumpVersion script to increment version after release branch creation", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Thanks to LUCENE-5898 there are many less places to increment version.  However, I still think this script can be useful in automating the entire process (minus the commit).  This would:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12665551/LUCENE-5902.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Build", "change_id": "LUCENE-5962", "change_description": ": Rename diffSources.py to createPatch.py and make it work with all text file types.", "change_title": "patch creation tool (rename and improve diffSources.py)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0", "detail_description": "The script diffSources.py is used for creating patches for feature branches.  I think the name createPatch.py would be more apt.  It also only works with certain file types and is one of the only python scripts written for python 2. I'd like to rename this script, upgrade it to python 3, and fix it to work with all files that git/svn would not ignore.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12669916/LUCENE-5962.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Build", "change_id": "LUCENE-5995", "change_description": ": Upgrade ICU to 54.1", "change_title": "upgrade ICU library", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Its a little out of date, we've hit wierd fails in jenkins that look like ICU bugs and so on.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12673270/LUCENE-5995.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Build", "change_id": "LUCENE-6070", "change_description": ": Upgrade forbidden-apis to 1.7", "change_title": "forbidden-apis fails on Java 9 while parsing deprecated signatures", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "5.0,6.0", "detail_description": "This is a really new thing: The method java.util.jar.Pack200$Packer#addPropertyChangeListener(java.beans.PropertyChangeListener) part of the JDK 8 deprecation list. But this method was actually removed in Java 9 completely (the first deprecation ever that was actually removed!). This method was deprecated in Java 8 for the first time, with the following text: \"Deprecated.  The dependency on PropertyChangeListener creates a significant impediment to future modularization of the Java platform. This method will be removed in a future release. Applications that need to monitor progress of the packer can poll the value of the PROGRESS property instead.\" So I am not sure how to handle that, it seems that in JDK9, Oracle may need to remove more deprecated stuff to make modularization possible. In branch_5x builds this is not a problem, because it was not deprecated in Java 8, so the java 7 signatures file does not have that method listed. A workaround would be to enable the forbidden feature to not fail on missing signatures.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12683235/LUCENE-6070.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Other", "change_id": "LUCENE-5563", "change_description": ": Removed sep layout: which has fallen behind on features and doesn't\nperform as well as other options.", "change_title": "Remove sep layout", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This has fallen behind feature wise, and isn't really performant (the 4.1 block format is a great improvement). I think we should remove it, its served its purpose but its time to retire...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638306/LUCENE-5563.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Other", "change_id": "LUCENE-4086", "change_description": ": Removed support for Lucene 3.x indexes. See migration guide for\nmore information.", "change_title": "Remove 3.x index support from trunk", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "Once we fix LUCENE-4084, we should generate a 4.0 back compat index, plug it into TestBackwardsCompatibility, and remove preflex-r/rw codecs completely. we should also nuke all junit assumes, suppressCodecs, etc that dodge around 3.x-specific issues, remove the segmentinfos legacy write method, etc etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12530129/LUCENE-4086.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Other", "change_id": "LUCENE-5858", "change_description": ": Moved Lucene 4 compatibility codecs to 'lucene-backward-codecs.jar'.", "change_title": "Move back compat codecs out of core/ into codecs/ jar", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "These take significant space and bloat core lucene. Not everyone needs the ability to read ancient indexes (especially building a new app). We should move this cruft out of the core/ jar. codecs/ is the obvious place, its already setup in the build system for tests and everything else.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Other", "change_id": "LUCENE-5915", "change_description": ": Remove Pulsing postings format.", "change_title": "remove pulsing", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "This optimization has been folded into the default postings format for a long time. I'd like to remove \"separate pulsing\", its really quite hairy (especially docsenum reuse logic and stuff like that)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12665765/LUCENE-5915.patch", "patch_content": "none"}
{"library_version": "5.0.0", "change_type": "Other", "change_id": "LUCENE-6213", "change_description": ": Add useful exception message when commit contains segments from legacy codecs.", "change_title": "Add test for IndexFormatTooOldException if a commit has a 3.x segment", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "We should add a 4.x index (4.x commit) with some 3.x segment(s) to our backwards tests. I don't think we throw IndexFormatTooOldException correctly in this case. I think instead the user will get a confusing SPI error about a missing codec \"Lucene3x\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695942/LUCENE-6213.patch", "patch_content": "none"}

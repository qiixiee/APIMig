{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6019", "change_description": ",", "change_title": "IndexWriter allows to add same field with different docvlaues type", "detail_type": "Bug", "detail_affect_versions": "4.10.1", "detail_fix_versions": "4.10.2,5.0,6.0", "detail_description": "IndexWriter checks if the DV types are consitent in multiple places but if due to some problems in Elasticsearch users where able to add the same field with different DV types causing merges to fail. Yet I was able to reduce this to a lucene testcase but I was puzzled since it always failed. Yet, I had to run it without assertions and that cause the bug to happen. I can add field foo with BINARY and SORTED_SET causing a merge to fail. Here is a gist https://gist.github.com/s1monw/8707f924b76ba40ee5f3 / https://github.com/elasticsearch/elasticsearch/issues/8009 While this is certainly a problem in Elasticsearch Lucene also allows to corrupt an index due to user error which I think should be prevented. NOTE: this only fails if you run without assertions which I think lucene should do in CI once in a while too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12676451/LUCENE-6019.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6117", "change_description": ",", "change_title": "infostream is currently unusable out of box", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.0,6.0", "detail_description": "testpoints used to only be emitted by assertions (still sketchy), but now are emitted always. I assume this is due to the change to support running tests with assertions disabled. we should try to clean this up, simple stuff like this is now useless: I hit this several times today just trying to do benchmarks and debugging.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12687676/LUCENE-6117.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6161", "change_description": ": Resolving deletes was failing to reuse DocsEnum likely\ncausing substantial performance cost for use cases that frequently\ndelete old documents", "change_title": "Applying deletes is sometimes dog slow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "I hit this while testing various use cases for LUCENE-6119 (adding auto-throttle to ConcurrentMergeScheduler). When I tested \"always call updateDocument\" (each add buffers a delete term), with many indexing threads, opening an NRT reader once per second (forcing all deleted terms to be applied), I see that BufferedUpdatesStream.applyDeletes sometimes seems to take a loooong time, e.g.: What this means is even though I want an NRT reader every second, often I don't get one for up to ~7 or more seconds. This is on an SSD, machine has 48 GB RAM, heap size is only 2 GB.  12 indexing threads. As hideously complex as this code is, I think there are some inefficiencies, but fixing them could be hard / make code even hairier ... Also, this code is mega-locked: holds IW's lock, holds BD's lock.  It blocks things like merges kicking off or finishing... E.g., we pull the MergedIterator many times on the same set of sub-iterators.  Maybe we can create the sorted terms up front and reuse that? Maybe we should go \"term stride\" (one term visits all N segments) not \"segment stride\" (visit each segment, iterating all deleted terms for it).  Just iterating the terms to be deleted takes a sizable part of the time, and we now do that once for every segment in the index. Also, the \"isUnique\" bit in LUCENE-6005 should help here, since if we know the field is unique, we can stop seekExact once we found a segment that has the deleted term, we can maybe pass false for removeDuplicates to MergedIterator...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693596/LUCENE-6161.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6192", "change_description": ": Fix int overflow corruption case in skip data for\nhigh frequency terms in extremely large indices", "change_title": "Long overflow in LuceneXXSkipWriter can corrupt skip data", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.5,5.0,6.0", "detail_description": "I've been iterating with Tom on this corruption that CheckIndex detects in his rather large index (720 GB in a single segment): And Rob spotted long -> int casts in our skip list writers that look like they could cause such corruption if a single high-freq term with many positions required > 2.1 GB to write its positions into .pos.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693399/LUCENE-6192.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6207", "change_description": ": Fixed consumption of several terms enums on the same\nsorted (set) doc values instance at the same time.", "change_title": "Multiple filtered subsets of the same underlying index passed to IW.addIndexes() can produce an index with bad SortedDocValues", "detail_type": "Bug", "detail_affect_versions": "4.9,4.9.1,4.10,4.10.1,4.10.2,4.10.3", "detail_fix_versions": "4.10.4,5.0,6.0", "detail_description": "Were hit by this in a custom index splitter implementation that showed no problems with Lucene 4.8. After upgrading to 4.10 documents started having wrong SortedDocValues after splitting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695241/LUCENE-6207.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6093", "change_description": ": Don't throw NullPointerException from\nBlendedInfixSuggester for lookups that do not end in a prefix\ntoken.", "change_title": "BlendedInfixSuggester throws NullPointerException if there were discarded trailing characters in the query", "detail_type": "Bug", "detail_affect_versions": "4.10.2", "detail_fix_versions": "4.10.4,5.0,5.1,6.0", "detail_description": "BlendedInfixSuggester throws NullPointerException if there were discarded trailing characters (e.g. whitespace or special character) in the query. The problem seems to be in the createCoefficient method that fails to check if prefixToken parameter is null. AnalyzingInfixSuggester sets prefixToken to null in the described case and passes it to BlendedInfixSuggester. On the side not even if BlendedInfixSuggester is changed to handle this creates a problem to calculate the weights as prefixToken is null and cannot be used. I would be better to have AnalyzingInfixSuggester to always set prefixToken to lastToken.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695304/LUCENE-6093.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6279", "change_description": ": Don't let an abusive leftover _N_upgraded.si in the\nindex directory cause index corruption on upgrade", "change_title": "3.x -> 4.x .si upgrade should not be tricked by leftover upgrade marker file", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.5", "detail_description": "Today when you do the first IW.commit to a 3.x index from Lucene 4.x, we go through a per-segment upgrade process when writing the next segments_N file, writing .si files for each segment if we didn't already do so. However, this process can be fooled by a leftover _N_upgraded.si file, in case the app above Lucene wasn't careful and reused a directory that had leftover files... I think we can make this more robust.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700288/LUCENE-6279.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6287", "change_description": ": Fix concurrency bug in IndexWriter that could cause\nindex corruption (missing _N.si files) the first time 4.x kisses a\n3.x index if merges are also running.", "change_title": "Corrupt index (missing .si file) on first 4.x commit to a 3.x index", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4", "detail_description": "If you have a 3.x index, and you open it with a 4.x IndexWriter for the first time, and you do something that kicks of merges while concurrently committing, it's possible the index will corrupt itself with exceptions like this: Back compat tests in Elasticsearch hit this, and at first I thought maybe LUCENE-6279 was the cause (I still think we should fix that) but after further debugging there is a different concurrency bug lurking here. I have a test case which after substantial beasting is able to reproduce the bug, but I don't yet have a fix.  I think IW is missing a checkpoint after writing a new commit...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700567/LUCENE-6287.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6205", "change_description": ": Fixed intermittent concurrency issue that could cause\nFileNotFoundException when writing doc values updates at the same\ntime that a merge kicks off.", "change_title": "DV updates can hit FileNotFoundException due to concurrency bug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4,5.0,5.1,6.0", "detail_description": "Jenkins has hit this a few times recently, e.g.: It repros only after substantial beasting. It's a concurrency issue between one thread kicking off a merge, and another thread resolving doc values updates.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695032/LUCENE-6205.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6214", "change_description": ": Fixed IndexWriter deadlock when one thread is\ncommitting while another opens a near-real-time reader and an\nunrecoverable (tragic) exception is hit.", "change_title": "IW deadlocks if commit and reopen happens concurrently while exception is hit", "detail_type": "Bug", "detail_affect_versions": "5.0", "detail_fix_versions": "4.10.4,5.0,5.1,6.0", "detail_description": "I just hit this while working on an elasticseach test using a lucene 5.1 snapshot (5.1.0-snapshot-1654549). The test throws random exceptions via MockDirWrapper and deadlocks, jstack says:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695800/LUCENE-6214.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6105", "change_description": ": Don't cache FST root arcs if the number of root arcs is\nsmall, or if the cache would be > 20% of the size of the FST.", "change_title": "Don't create root arc cache for tiny FSTs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4,5.0,6.0", "detail_description": "The purpose of the root arc cache is to speed up lookups for ASCII terms, but it adds high overhead if the FST is already tiny.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12686245/LUCENE-6105.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6001", "change_description": ": DrillSideways hits NullPointerException for certain\nBooleanQuery searches.", "change_title": "DrillSideways throws NullPointerException for some searches", "detail_type": "Bug", "detail_affect_versions": "4.10.1", "detail_fix_versions": "4.10.4,5.1,6.0", "detail_description": "For some DrillSideways searches I get NullPointerException. I have tracked the problem to DrillSidewaysScorer class, on line 126 in DrillSidewaysScorer.java: long baseQueryCost = baseScorer.cost(); On some of my index segments, this call throws NullPoinerException.  \"baseScorer\" is instance of ReqExclScorer.  In ReqExclScorer.java: public long cost() throws NullPointerException because reqScorer is null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701148/LUCENE-6001.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "Bug fixes", "change_id": "LUCENE-6306", "change_description": ": Merging of doc values and norms now checks whether the\nmerge was aborted so IndexWriter.rollback can more promptly abort a\nrunning merge.", "change_title": "Merging of doc values, norms is not abortable", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10.4", "detail_description": "When you call IW.rollback, IW asks all running merges to abort, and the merges should periodically check their abort flags (it's a \"cooperative\" mechanism, like thread interrupting in Java). In 5.x/trunk we have a nice clean solution where the Directory checks the abort bit during writes, so the codec doesn't have to bother with this. But in 4.x, we have to call MergeState.checkAbort.work, and I noticed that neither DVs nor norms call this. Typically this is not a problem since merging DVs and norms is usually fast, but for a very large merge / very many DVs and norm'd fields, it could take non-trivial time to merge.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701311/LUCENE-6306.patch", "patch_content": "none"}
{"library_version": "4.10.4", "change_type": "API Changes", "change_id": "LUCENE-6212", "change_description": ": Deprecate IndexWriter APIs that accept per-document Analyzer.\nThese methods were trappy as they made it easy to accidentally index\ntokens that were not easily searchable and will be removed in 5.0.0.", "change_title": "Remove IndexWriter's per-document analyzer add/updateDocument APIs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.0,5.1,6.0", "detail_description": "IndexWriter already takes an analyzer up-front (via IndexWriterConfig), but it also allows you to specify a different one for each add/updateDocument. I think this is quite dangerous/trappy since it means you can easily index tokens for that document that don't match at search-time based on the search-time analyzer. I think we should remove this trap in 5.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695530/LUCENE-6212.patch", "patch_content": "none"}

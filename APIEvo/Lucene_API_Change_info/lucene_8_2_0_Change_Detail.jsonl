{"library_version": "8.2.0", "change_type": "API Changes", "change_id": "LUCENE-8865", "change_description": ": IndexSearcher now uses Executor instead of ExecutorSerivce.\nThis change is fully backwards compatible since ExecutorService directly\nimplements Executor.", "change_title": "Use incoming thread for execution if IndexSearcher has an executor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Today we don't utilize the incoming thread for a search when IndexSearcher     has an executor. This thread is only idleing but can be used to execute a search     once all other collectors are dispatched.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "API Changes", "change_id": "LUCENE-8856", "change_description": ": Intervals queries have moved from the sandbox to the queries\nmodule.", "change_title": "Move Intervals query from sandbox to queries module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "Interval queries are stable now and being used in production.  I think it's time to move them out of the sandbox, and I propose putting them in the queries submodule", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "API Changes", "change_id": "LUCENE-8893", "change_description": ": Intervals.wildcard() and Intervals.prefix() methods now take\nBytesRef rather than String.", "change_title": "Intervals.wildcard() and Intervals.prefix() should take BytesRef", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "Generally speaking, when parsers are creating wildcard or prefix IntervalsSources, they will first want to pass the user input through Analyzer.normalize().  This returns a BytesRef rather than a String, so the factory methods for these sources should take BytesRef as well.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8632", "change_description": ": New XYShape Field and Queries for indexing and searching general cartesian\ngeometries.", "change_title": "XYShape: Adapt LatLonShape tessellator, field type, and queries to non-geo shapes", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Currently the tessellator is tightly coupled with latitude and longitude (WGS84) geospatial coordinates. This issue will explore generalizing the tessellator, LatLonShape field and LatLonShapeQuery to non geospatial (cartesian) coordinate systems so lucene can provide the index & search capability for general geometry / non GIS type use cases.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8891", "change_description": ": Snowball stemmer/analyzer for the Estonian language.", "change_title": "Snowball stemmer/analyzer for the Estonian language", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Currently there is no Estonian specific stemmer for SnowballFilter. I would like to add a Snowball stemmer for the Estonian language and also add a new Language analyzer for the Estonian language based on the snowball stemmer. https://github.com/gpaimla/lucene-solr fork of master branch with the analyzer implemented", "patch_link": "https://issues.apache.org/jira/secure/attachment/12973249/LUCENE-8891.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8815", "change_description": ": Provide a DoubleValues implementation for retrieving the value of features without\nrequiring a separate numeric field. Note that as feature values are stored with only 8 bits of\nmantissa the values returned may have a delta from the original values indexed.", "change_title": "Enable retrieving of feature values through a DoubleValues implementation", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Currently getting the values of a feature in a feature field requires another numeric field to be indexed containing the values. It would be useful to be able to obtain the indexed values of features from the feature field directly using a `DoubleValues` implementation. The feature sort implemented in https://issues.apache.org/jira/browse/LUCENE-8803 could also be moved to using this `DoubleValue` implementation to avoid duplicating logic.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8803", "change_description": ": Provide a FeatureSortfield to allow sorting search hits by descending value of a\nfeature. This is exposed via the factory method FeatureField#newFeatureSort.", "change_title": "Provide a FieldComparator to allow sorting by a feature from a FeatureField", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "It would be useful to be able to sort search hits by the value of a feature from a feature field (e.g. pagerank). A FieldComparatorSource implementation that enables this would create a convenient generic way to sort using values from feature fields.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8784", "change_description": ": The KoreanTokenizer now preserves punctuations if discardPunctuation is set\nto false (defaults to true).", "change_title": "Nori(Korean) tokenizer removes the decimal point.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "This is the same issue that I mentioned to https://github.com/elastic/elasticsearch/issues/41401#event-2293189367 unlike standard analyzer, nori analyzer removes the decimal point. nori tokenizer removes \".\" character by default.  In this case, it is difficult to index the keywords including the decimal point. It would be nice if there had the option whether add a decimal point or not. Like Japanese tokenizer does,  Nori need an option to preserve decimal point. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969655/LUCENE-8784.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8812", "change_description": ": Add new KoreanNumberFilter that can change Hangul character to number\nand process decimal point. It is similar to the JapaneseNumberFilter.", "change_title": "add KoreanNumberFilter to Nori(Korean) Analyzer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "This is a follow-up issue to LUCENE-8784. The KoreanNumberFilter is a TokenFilter that normalizes Korean numbers to regular Arabic decimal numbers in half-width characters. Logic is similar to JapaneseNumberFilter. It should be able to cover the following test cases. 1) Korean Word to Number 십만이천오백 => 102500 2) 1 character conversion 일영영영 => 1000 3) Decimal Point Calculation 3.2천 => 3200 4) Comma between three digits 4,647.0010 => 4647.001", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969657/LUCENE-8812.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8362", "change_description": ": Add doc-value support to range fields.", "change_title": "Add DocValue support for RangeFields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "I'm opening this issue to discuss adding DocValue support to {Int|Long|Float|Double}Range field types. Since existing numeric range fields already provide the methods for encoding ranges as a byte array I think this could be as simple as adding syntactic sugar to existing range fields that simply build an instance of BinaryDocValues using that same encoding. I'm envisioning something like doc.add(IntRange.newDocValuesField(\"intDV\", 100) But I'd like to solicit other ideas or potential drawbacks to this approach.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969896/LUCENE-8362-approach2.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8766", "change_description": ": Add monitor subproject (previously Luwak monitoring library). This\nallows a stream of documents to be matched against a set of registered queries\nin an efficient manner, for use as a monitoring or classification tool.", "change_title": "Add Luwak as a lucene module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "Luwak https://github.com/flaxsearch/luwak is a stored query engine, allowing users to efficiently match a stream of documents against a large set of queries.  It's only dependency is lucene, and most recent updates have just been upgrading the version of lucene against which it can run. It's a generally useful piece of software, and already licensed as Apache 2.  The maintainers would like to donate it to the ASF, and merge it in to the lucene-solr project.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-7714", "change_description": ": Add a numeric range query in sandbox that takes advantage of index sorting.", "change_title": "Optimize range queries for the sorted case", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "It feels like we could make range queries faster when the index is sorted, maybe by running on doc values, figuring ou the first and last matching documents with a binary search and returning a doc id set iterator that iterates through this range of documents?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "New Features", "change_id": "LUCENE-8859", "change_description": ": The completion suggester's postings format now have an option to\nload its internal FST off-heap.", "change_title": "Add an option to load the completion suggester's FST off-heap", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Now that FSTs can be loaded off-heap (https://issues.apache.org/jira/browse/LUCENE-8635) it would be nice to expose this option in the completion suggester postings format. I didn't ran any benchmark yet so I can't say it this really makes sense or not but I wanted to get some opinion whether this could be a good trade-off.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12971773/LUCENE-8859.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8831", "change_description": ": Fixed LatLonShapeBoundingBoxQuery .hashCode methods.", "change_title": "LatLonShapeBoundingBoxQuery hashcode is wrong", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2,8.1.2", "detail_description": "Currently the hashcode implementation for LatLonShapeBoundingBoxQuery returns always a different value. Therefore the query cannot be cached.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8775", "change_description": ": Improve tessellator to handle better cases where a hole share a vertex\nwith the polygon.", "change_title": "Tessellator: Improve the election of diagonals when splitting the polygon", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2,8.1.2", "detail_description": "There are some cases when polygon tessellation fails and it seems it is due to a bad election of the diagonal when splitting the polygon. Here I propose a patch that make sure when splitting a polygon that the resulting polygons are valid CW polygons. In addition this patch adds few test to check the functionality of the tessellator and throws an error if the polygon cannot be splitted instead of just empty the current tessellation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8785", "change_description": ": Ensure new threadstates are locked before retrieving the number of active threadstates.\nThis causes assertion errors and potentially broken field attributes in the IndexWriter when\nIndexWriter#deleteAll is called while actively indexing.", "change_title": "TestIndexWriterDelete.testDeleteAllNoDeadlock failure", "detail_type": "Bug", "detail_affect_versions": "7.6", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "I was running Lucene's core tests on an i3.16xlarge EC2 instance (64 cores), and hit this random yet spooky failure: It does not reproduce unfortunately ... but maybe there is some subtle thread safety issue in this code ... this is a hairy part of Lucene", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8804", "change_description": ": Forbid calls to putAttribute on frozen FieldType instances.", "change_title": "FieldType attribute map should not be modifiable after freeze", "detail_type": "Bug", "detail_affect_versions": "8.0", "detail_fix_versions": "9.0,8.2", "detail_description": "Today FieldType attribute map can be modifiable even after freeze. For all other properties of FieldType, we do \"checkIfFrozen()\" before making the update to the property but for attribute map, we does not seem to make such check.  https://github.com/apache/lucene-solr/blob/releases/lucene-solr/8.0.0/lucene/core/src/java/org/apache/lucene/document/FieldType.java#L363 we may need to add check at the beginning of the function similar to other properties setters. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969191/LUCENE-8804.01.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8828", "change_description": ": Removes the buggy 'disallow overlaps' boolean from Intervals.unordered(),\nand replaces it with a new Intervals.unorderedNoOverlaps() method", "change_title": "Fix Intervals.unordered() without overlaps", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "LUCENE-8300 added an option to Intervals.unordered() which would attempt to find intervals that contained all of a set of subintervals where none of the subintervals overlapped.  Unfortunately, this implementation was buggy, and could miss documents depending on the order in which the subintervals were passed to the factory method. After some digging around, I think that it is not in fact possible to implement this in anything other than n! time, because of the need to minimize the resulting intervals.  My proposal is to remove the boolean flag, and instead implement an Intervals.unorderedNoOverlaps() method that takes only two subsources, and rewrites NO_OVERLAPS(a, b) to OR(ORDERED(a, b), ORDERED(b, a)).  The usual simplifications will apply here, so NO_OVERLAPS(a, a) will end up as ORDERED(a, a)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12970855/LUCENE-8828.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8843", "change_description": ": Don't ignore exceptions that are thrown when trying to open a\nfile in IOUtils#fsync.", "change_title": "Only ignore IOException on dirs when invoking force", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "I think the IOException suppression in IOUtils#fsync when fsyncing directories is too broad, it suppresses any IOException that occurs in this method if isDir is true. For example, it causes exceptions opening non-existent directories, if access is denied to the directory, or general filesystem errors to be suppressed. To me that feels like it could be suppressing application bugs, or at least exceptions an application would want to know about. Instead, the IOException suppression should only apply after we have successfully opened the directory. I submitted a PR to propose a change along these lines: https://github.com/apache/lucene-solr/pull/706. Please let me know what you think.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8835", "change_description": ": FileSwitchDirectory now respects the file extension when listing directory\ncontents to ensure we don't expose pending deletes if both directory point to the same\nunderlying filesystem directory.", "change_title": "Respect file extension when listing files form FileSwitchDirectory", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "FileSwitchDirectory splits file actions between 2 directories based on file extensions. The extensions are respected on write operations like delete or create but ignored when we list the content of the directories. Until now we only deduplicated the contents on Directory#listAll which can cause inconsistencies and hard to debug errors due to double deletions in IndexWriter is a file is pending delete in one of the directories but still shows up in the directory listing form the other directory. This case can happen if both directories point to the same underlying FS directory which is a common usecase to split between mmap and noifs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8853", "change_description": ": FileSwitchDirectory now applies best effort to place tmp files in the same\ndirectory as the target files.", "change_title": "FileSwitchDirectory is broken if temp outputs are used", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "FileSwitchDirectory basically doesn't work if tmp output are used for files that are explicitly mapped with extensions. here is a failing test:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8892", "change_description": ": Add missing closing parentheses in MultiBoolFunction's description()", "change_title": "Missing closing parens in string representation of MultiBoolFunction", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "The description function of MultiBoolFunction includes an open parenthesis, but doesn't close it. This makes score explanations more confusing than necessary sometimes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12973176/LUCENE-8892.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-7840", "change_description": ": Non-scoring BooleanQuery now removes SHOULD clauses before building the scorer supplier\nas opposed to eliminating them during scoring construction.", "change_title": "BooleanQuery.rewriteNoScoring - optimize away any SHOULD clauses if at least 1 MUST/FILTER clause and 0==minShouldMatch", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "I haven't thought this through completely, let alone write up a patch / test case, but IIUC... We should be able to optimize  {{ BooleanQuery rewriteNoScoring() }} so that (after converting MUST clauses to FILTER clauses) we can check for the common case of 0==getMinimumNumberShouldMatch() and throw away any SHOULD clauses as long as there is is at least one FILTER clause.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12968073/LUCENE-7840.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8770", "change_description": ": BlockMaxConjunctionScorer now leverages two-phase iterators in order to avoid\nexecuting the second phase when scorers don't intersect.", "change_title": "BlockMaxConjunctionScorer should support two-phase scorers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "The support for two-phase scorers in BlockMaxConjunctionScorer is missing. This can slow down some queries that need to execute costly second phase on more documents.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969240/LUCENE-8770.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8818", "change_description": ": Fix smokeTestRelease.py encoding bug", "change_title": "smokeTestRelease.py encoding bug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2,8.1.2", "detail_description": "Smoke tester may crash while parsing log file created from gpg stdout, but only in certain conditions. Error trace is The failing line is https://github.com/apache/lucene-solr/blob/faaee86efb01fa6e431fcb129cfb956c7d62d514/dev-tools/scripts/smokeTestRelease.py#L378 Found by ab", "patch_link": "https://issues.apache.org/jira/secure/attachment/12970622/LUCENE-8818.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8845", "change_description": ": Allow Intervals.prefix() and Intervals.wildcard() to specify\ntheir maximum allowed expansions", "change_title": "Allow maxExpansions to be set on multi-term Intervals", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "MultiTermIntervalsSource has a maxExpansions parameter which is always set to 128 by the factory methods Intervals.prefix() and Intervals.wildcard().  We should keep 128 as the default, but also add additional methods that take a configurable maximum.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12971335/LUCENE-8845.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8875", "change_description": ": Introduce a Collector optimized for use cases when large\nnumber of hits are requested", "change_title": "Should TopScoreDocCollector Always Populate Sentinel Values?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "TopScoreDocCollector always initializes HitQueue as the PQ implementation, and instruct HitQueue to populate with sentinels. While this is a great safety mechanism, for very large datasets where the query's selectivity is high, the sentinel population can be redundant and can become a large enough bottleneck in itself. Does it make sense to introduce a new parameter in TopScoreDocCollector which uses a heuristic (say number of hits > 10k) and does not populate sentinels?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8848", "change_description": "", "change_title": "UnifiedHighlighter should highlight all Query types that implement Weight.matches", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "The UnifiedHighlighter internally extracts terms and automata from the query.  Usually this works perfectly but it's possible a Query might be of a type it doesn't know – a leaf query that is perhaps in effect similar to a MultiTermQuery yet it might not even be a subclass of this or it does but the UH doesn't know how to extract an automata from it.  The UH is oblivious to this and probably won't highlight this query.  If re-analysis of the text is necessary, the UH will pre-filter all terms to only those it thinks are pertinent.  Or if offsets are in the postings then the UH could perform very poorly by unleashing this query on the index for each highlighted document without recognizing re-analysis is a more appropriate path. I think to solve this, the UnifiedHighlighter.getFieldHighlighter needs to inspect the query (using a QueryVisitor) to see if it can find a leaf query that is not one it knows how to pull automata from, and is otherwise not in a special list (like MatchAllDocsQuery).  If we find one, we avoid choosing OffsetSource.POSTINGS or OffsetSource.NONE_NEEDED since we might in effect have an MTQ like query.  If a MemoryIndex is needed then we don't pre-filter the terms since we can't assume we know precisely which terms are pertinent. We needn't bother extracting terms & automata in this case either; it's wasted effort which can involve building a CharacterRunAutomaton (see MultiTermHighlighting.binaryToCharRunAutomaton).  Speaking of which, it'd be nice to avoid that in other cases as well, like for WEIGHT_MATCHES when we aren't using MemoryIndex (thus no term pre-filtering).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12971349/LUCENE-8848.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-7757", "change_description": "", "change_title": "Unified highlighter does not highlight wildcard phrases correctly when ComplexPhraseQueryParser is used", "detail_type": "Bug", "detail_affect_versions": "6.4", "detail_fix_versions": "8.2", "detail_description": "Given the text: \"Kontraktsproget vil være dansk og arbejdssproget kan være dansk, svensk, norsk og engelsk\" and the query: {!complexphrase df=content_da}(\"sve* no*\") the unified highlighter (hl.method=unified) does not return any highlights. For reference, the original highlighter returns a snippet with the expected highlights: Kontraktsproget vil være dansk og arbejdssproget kan være dansk, <em>svensk</em>, <em>norsk</em> og Is this expected behaviour with the unified highlighter?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8492", "change_description": "", "change_title": "UnifiedHighlighter does not work with Surround query parser (SurroundQParser)", "detail_type": "Bug", "detail_affect_versions": "7.2.1", "detail_fix_versions": "8.2", "detail_description": "I'm attempting to use the UnifiedHighlighter in conjunction with queries parsed by Solr's SurroundQParserPlugin. When doing so, the response yields empty arrays for documents that should contain highlighted snippets. I've attached a test for UnifiedHighlighter that uses the surround's QueryParser and preprocesses the query in a similar fashion as SurroundQParser, which results in test failure.  When creating a SpanQuery directly (rather via surround's QueryParser), the test passes. The problem can be isolated to the code path initiated by UnifiedHighlighter.extractTerms(), which uses EMPTY_INDEXSEARCHER to extract terms from the query. After a series of method calls, we end up at DistanceQuery.getSpanNearQuery(), where ((DistanceSubQuery)sqi.next()).addSpanQueries(sncf) fails silently and doesn't add any span queries. Another data point: If I hack UnifiedHighlighter and pass in a live IndexSearcher to extractTerms(), highlighting works.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8793", "change_description": ": Luke enhanced UI for CustomAnalyzer: show detailed analysis steps.", "change_title": "Enhanced UI for CustomAnalyzer : show analysis steps", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "This is a migrated issue from previous Luke project in GitHub: https://github.com/DmitryKey/luke/issues/134  For on-the-fly inspection / debugging, it is desirable to show the more detailed step by step information in the Custom Analyzer UI. This will be just like Solr's Analysis screen, https://lucene.apache.org/solr/guide/7_5/analysis-screen.html or Elasticsearch's _analyze API and Kibana's Analyzer UI. https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html https://github.com/johtani/analyze-api-ui-plugin", "patch_link": "https://issues.apache.org/jira/secure/attachment/12972546/LUCENE-8793.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Improvements", "change_id": "LUCENE-8855", "change_description": ": Add Accountable to some Query implementations", "change_title": "Add Accountable to some Query implementations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "Query implementations should also support Accountable API in order to monitor the memory consumption e.g. in caches where either keys or values are Query instances.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12972898/LUCENE-8855.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8796", "change_description": ": Use exponential search instead of binary search in\nIntArrayDocIdSet#advance method", "change_title": "Use exponential search in IntArrayDocIdSet advance method", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Chatting with jpountz , he suggested to improve IntArrayDocIdSet by making its advance method use exponential search instead of binary search. This should help performance of queries including conjunctions: given that ConjunctionDISI uses leap frog, it advances through doc ids in small steps, hence exponential search should be faster when advancing on average compared to binary search. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8865", "change_description": ": Use incoming thread for execution if IndexSearcher has an executor.\nNow caller threads execute at least one search on an index even if there is\nan executor provided to minimize thread context switching.", "change_title": "Use incoming thread for execution if IndexSearcher has an executor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Today we don't utilize the incoming thread for a search when IndexSearcher     has an executor. This thread is only idleing but can be used to execute a search     once all other collectors are dispatched.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8868", "change_description": ": New storing strategy for BKD tree leaves with low cardinality.\nIt stores the distinct values once with the cardinality value reducing the\nstorage cost.", "change_title": "New storing strategy for BKD tree leaves with low cardinality", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Currently if a leaf on the BKD tree contains only few values, then the leaf is treated the same way as it all values are different. It many cases it can be much more efficient to store the distinct values with the cardinality. The strategy is the following: 1.  When writing a leaf block the cardinality is computed. 2. Perform some naive calculation to compute if it is better to store the leaf as a low cardinality leaf. The storage cost are calculated as follows: 3. If the tree has low cardinality then we set the compressed dim to -2. Note that -1 is when all values are equal.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8885", "change_description": ": Optimise BKD reader by exploiting cardinality information stored\non leaves.", "change_title": "Optimise BKD reader by exploiting cardinality information stored on leaves", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "In LUCENE-8688 it was introduce a new storing strategy for leaves contains duplicated points. In such case the points are stored together with the cardinality. We still call the IntersectVisitor once per document therefore we are checking many times the same point agains the query. The idea is to check the point once and then add all the documents. The API of the IntersectVisitor does not allow that, and therefore to exploit that property we need to either change the API or extend it. Here are the possibilities I can think of: 1) Modify the API by replacing the method IntersectVisitor#visit(byte[], int) by the following method: This will allow the BKD reader to check if a point matches the query and if true then Coll the method IntersectVisitor#visit(int) for all documents associated with that point. The drawback of this approach is backwards compatibility and the need to update all classes implement this interface. 2) Extends the API by adding a new default method in the IntersectVisitor interface: The merit of this approach is that is backwards compatible and it is up to the implementors to override this method and get the benefits for this optimisation.The biggest downside is that it assumes that the codec has doc IDs available in an int[] slice as opposed to streaming them from disk directly to the IntersectVisitor for instance as jpountz noted. Maybe there are more options I did not think about so looking forward to hearing opining if we should do this change at all and if so, how to approach it. My +1 goes to 1).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8896", "change_description": ": Override default implementation of IntersectVisitor#visit(DocIDSetBuilder, byte[])\nfor several queries.", "change_title": "Override default implementation of IntersectVisitor#visit(DocIDSetBuilder, byte[]) for several queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "In LUCENE-8885, it was introduced a new method on the IntersectsVisitor interface. It contains a default implementation but queries can override it and therefore benefit when there are several documents on a leaf associated to the same point. In this issue the following queries are proposed to override the default implementation", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8901", "change_description": ": Load frequencies lazily only when needed in BlockDocsEnum and\nBlockImpactsEverythingEnum", "change_title": "Load frequencies lazily for postings and impacts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "Allow frequencies blocks to be loaded lazily when they are not needed", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8888", "change_description": ": Optimize distribution of points with data dimensions in\nBKD tree leaves.", "change_title": "Improve distribution of points with data dimension in BKD tree leaves", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "In LUCENE-8688 it was introduce a new storing strategy for leaves contains duplicated points. This works well with indexed dimension as the process of partition the space and the final sorting of leaves groups points with equal indexed dimensions. This is not the case all the time if the point contain data dimensions. It might happen that if two points have the same indexed dimensions but different data dimensions, the distribution on the leaves is not the most optimal. A good example is if a user tries to index a bounding box using LatLonShape. The resulting tessellation of a bounding box is two triangles with the same indexed dimensions but different data dimensions. If there are two documents indexing the same bounding box, the result in the leaf is the triangles from one document followed by the triangles of the second document. This is  because the current sorting/selection algorithms  use one indexed dimension and tie-break on the  docID. The most optimal distribution in the case above is two group together the equal triangles. Therefore what it is propose here is to update the selection/ sorting algorithms to use the data dimensions when they exist as tie-breakers before using the docID.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8311", "change_description": ": Phrase queries now leverage impacts.", "change_title": "Leverage impacts for phrase queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "Now that we expose raw impacts, we could leverage them for phrase queries. For instance for exact phrases, we could take the minimum term frequency for each unique norm value in order to get upper bounds of the score for the phrase.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924509/LUCENE-8311.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Test Framework", "change_id": "LUCENE-8825", "change_description": ": CheckHits now display the shard index in case of mismatch\nbetween top hits.", "change_title": "Improve Print Info Of CheckHits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "CheckHits should publish the shardIndex of the two involved ScoreDoc instances when there is a mismatch. Since shardIndex can be involved in the ordering of result ScoreDocs (due to it being considered during tie break when no sort order is specified), this can be useful for understanding test failures involving CheckHits.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12970819/LUCENE-8825.patch", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Other", "change_id": "LUCENE-8847", "change_description": ": Code Cleanup: Remove StringBuilder.append with concatenated\nstrings.", "change_title": "Code Cleanup: Rewrite StringBuilder.append with concatted strings", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "EDIT: Again, to clarify, please don't bother yourself with this ticket on company time, on personal time you could be working on something that makes you money or improves the product for your feature personally.  This entire ticket is an afterthough. A look back at the code base that most people don't have the time for. ---------  Code cleanup as suggested by static analysis tools. Will be done in my spare time. If someone reviews this, please also do not take up actual time from your work to do that. I do not wish to take away from your working hours.  These are simple, trivial things, that were probably overlooked or not even considered(which isn't an accusation or something negative). But also stuff that the Java compiler/JIT won't optimize on its own.  That's what static analysis tool are good for: picking stuff like that up.  I'm talking about Intellij's static code analysis. Facebook's \"Infer\" for Java. Google's \"errorprone\", etc... These are the kinds of things that, frankly, for the people actually working on real features, are very time consuming, not even part of the feature, and have a very low chance of actually turning up a real performance issue. So I'm opting to have a look at the results of these tools and implementing the sensible stuff and if something bigger pops up I'll make a separate ticket for those things individually.  Creating this ticket so I can name a branch after it.  The only questions I have are: since the code base is so large, do I apply each subject to all parts of it? Or only core? How do I split it up? Do I make multiple PRs with this one ticket? Or do I make multiple tickets and give each their own PR?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Other", "change_id": "LUCENE-8861", "change_description": ": Script to find open Github PRs that needs attention", "change_title": "Script to find open PRs that needs attention", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Other", "change_id": "LUCENE-8852", "change_description": ": ReleaseWizard tool for release managers", "change_title": "ReleaseWizard tool", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.2", "detail_description": "The Release Wizard guides the Release Manager through the release process step by step, helping you to to run the right commands in the right order, generating e-mail templates with the correct texts, versions, paths etc, obeying the voting rules and much more. It also serves as a documentation of all the steps, with timestamps, preserving log files from each command etc, showing only the steps and commands required for a major/minor/bugfix release. It also lets you generate a full Asciidoc guide for the release. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Other", "change_id": "LUCENE-8838", "change_description": ": Remove support for Steiner points on Tessellator.", "change_title": "Tessellator: Remove support for Steiner points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Tessellator has support from Steiner points which come from the original porting of the MapBox's earcut algorithm to Java. We are not using such points and therefore it would be better to remove it. In addition, it actually introduces a bug when a polygon hole is a line with al coplanar points.  In some cases it can be reduced to a point and then treated as a Steiner points. This looks to be wrong and on those cases we should throw an error.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Other", "change_id": "LUCENE-8879", "change_description": ": Improve BKDRadixSelector tests.", "change_title": "Add tests for BKDRadixSelector#sort", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "This issue just add some test specifically for the sorting capability of class BKDRadixSelector and improves the existing ones for the selection capabilities.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.2.0", "change_type": "Other", "change_id": "LUCENE-8886", "change_description": ": Fix TestMutablePointsReaderUtils tests.", "change_title": "TestMutablePointsReaderUtils not doing what it is expected", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "The  TestMutablePointsReaderUtils is actually not doing what it is expected. The problem is that we are constructing Point objects but not copying the bytes provided so is always working with arrays with 0 values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12972990/LUCENE-8886.patch", "patch_content": "none"}

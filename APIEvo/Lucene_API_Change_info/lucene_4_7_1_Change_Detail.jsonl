{"library_version": "4.7.1", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5532", "change_description": ": AutomatonQuery.equals is no longer implemented as \"accepts same language\".\nThis was inconsistent with hashCode, and unnecessary for any subclasses in Lucene.\nIf you desire this in a custom subclass, minimize the automaton.", "change_title": "AutomatonQuery.hashCode is not thread safe", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "This hashCode is implemented based on  #states and #transitions. These methods use getNumberedStates() though, which may oversize itself during construction and then \"size down\" when its done. But numberedStates is prematurely set (before its \"ready\"), which can cause a hashCode call from another thread to see a corrupt state... causing things like NPEs from null states and other strangeness. I don't think we should set this variable until its \"finished\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635037/LUCENE-5532.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5450", "change_description": ": Fix getField() NPE issues with SpanOr/SpanNear when they have an\nempty list of clauses. This can happen for example,  when a wildcard matches\nno terms.", "change_title": "NPE and Illegal Argument Exception in wrapped SpanMultiTerms with no matches", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "There are problems with the handling of wrapped span multiterms that don't have any matches. 1) In the test framework, when AssertingIndexSearcher does a rewrite and then checks for equality, there's an NPE for SpanNear and SpanOr: 2) The same issue is causing a \"Clauses must have same field\" illegal argument exception in a SpanNotQuery. The basic problem is that an empty SpanQuery (SpanOrQuery with zero clauses) does not have a field, and much of our code assumes that the field is not null. Test case and patch on way.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629594/LUCENE-5450.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5473", "change_description": ": Throw IllegalArgumentException, not\nNullPointerException, if the synonym map is empty when creating\nSynonymFilter", "change_title": "the contructor function of SynonymFilter can not report exception correctly", "detail_type": "Bug", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "when use code  SynonymFilter filter=new SynonymFilter(new WhitespaceTokenizer(Version.LUCENE_42, new StringReader(\"aa bb\")), new SynonymMap.Builder(true).build(), true); create a filter,it throw NullPointerException but not IllegalArgumentException(\"fst must be non-null\");", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5432", "change_description": ": EliasFanoDocIdSet: Fix number of index entry bits when the maximum\nentry is a power of 2.", "change_title": "EliasFanoEncoder number of index entry bits is off by 1 for powers of 2 of max index entry", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5466", "change_description": ": query is always null in countDocsWithClass() of SimpleNaiveBayesClassifier.", "change_title": "query is always null in countDocsWithClass() of SimpleNaiveBayesClassifier", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12630524/LUCENE-5466.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5502", "change_description": ": Fixed TermsFilter.equals that could return true for different\nfilters.", "change_title": "equals method of TermsFilter might equate two different filters", "detail_type": "Bug", "detail_affect_versions": "4.7", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "If two terms filters have 1) the same number of terms, 2) use the same field in all these terms and 3) term values happened to have the same hash codes, these two filter are considered to be equal as long as the first term is the same in both filters.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633733/LUCENE-5502.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5522", "change_description": ": FacetsConfig didn't add drill-down terms for association facet\nfields labels.", "change_title": "FacetConfig doesn't add drill-down terms for facet associations", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "I bumped into this while updating my examples code. Will attach a patch which fixes this shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634186/LUCENE-5522.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5520", "change_description": ": ToChildBlockJoinQuery would hit\nArrayIndexOutOfBoundsException if a parent document had no children", "change_title": "ArrayIndexOutOfBoundException in ToChildBlockJoinQuery when there's a deleted parent without any children", "detail_type": "Bug", "detail_affect_versions": "4.2,4.7", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "This problem is found in lucene 4.2.0 and reproduced in 4.7.0 In our app when we delete a document we always delete all the children.  But not all parents have children. The exception happen for us when the parent without children is deleted.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634139/LUCENE-5220.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5532", "change_description": ": AutomatonQuery.hashCode was not thread-safe.", "change_title": "AutomatonQuery.hashCode is not thread safe", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "This hashCode is implemented based on  #states and #transitions. These methods use getNumberedStates() though, which may oversize itself during construction and then \"size down\" when its done. But numberedStates is prematurely set (before its \"ready\"), which can cause a hashCode call from another thread to see a corrupt state... causing things like NPEs from null states and other strangeness. I don't think we should set this variable until its \"finished\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635037/LUCENE-5532.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5525", "change_description": ": Implement MultiFacets.getAllDims, so you can do sparse\nfacets through DrillSideways, for example.", "change_title": "Implement MultiFacets.getAllDims", "detail_type": "Bug", "detail_affect_versions": "4.7", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "DrillSideways.DrillSidewaysResult uses Facets when the query does not filter by a facet, but it uses MultiFacets when it does, and MultiFacets implementation is not complete. See: https://github.com/apache/lucene-solr/blob/0b0bc89932622f5bc2c4d74f978178b9ae15c700/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java#L67 See http://pastebin.com/5eDbTM2v This code works when DrillDownQuery.add is not called (when there is no facets selected), but it fails with an UnsupportedOperationException. Perhaps I'm not using Facets correctly, but I'm trying to figure it out to upgrade from 4.6.1 by my self as I could not find a documentation other than javadocs for facets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634968/LUCENE-5525.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5481", "change_description": ": IndexWriter.forceMerge used to run a merge even if there was a\nsingle segment in the index.", "change_title": "IndexWriter.forceMerge may run unneeded merges", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "I was running some tests and was surprised that IndexWriter.forceMerge caused the index to be merged even when the index contains a single segment with no deletions. This is due to MergePolicy.isMerged which always returns false with the default configuration although merge policies rely on it to know whether a single-segment index should be merged.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631730/LUCENE-5481.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5538", "change_description": ": Fix FastVectorHighlighter bug with index-time synonyms when the\nquery is more complex than a single phrase.", "change_title": "FastVectorHighlighter fails with booleans of phrases", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "in some situations a query of (P1 OR PQ) returns no results, even though individually, both P1 or P2 by themselves will highlight correctly..", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635459/LUCENE-5538.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5544", "change_description": ": Exceptions during IndexWriter.rollback could leak file handles\nand the write lock.", "change_title": "exceptions during IW.rollback can leak files and locks", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "Today, rollback() doesn't always succeed: if it does, it closes the writer nicely. otherwise, if it hits exception, it leaves you with a half-broken writer, still potentially holding file handles and write lock. This is especially bad if you use Native locks, because you are kind of hosed, the static map prevents you from forcefully unlocking (e.g. IndexWriter.unlock) so you have no real course of action to try to recover. If rollback() hits exception, it should still deliver the exception, but release things (e.g. like IOUtils.close).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636042/LUCENE-5544.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4978", "change_description": ": Spatial RecursivePrefixTree queries could result in false-negatives for\nindexed shapes within 1/2 maxDistErr from the edge of the query shape.  This meant\nsearching for a point by the same point as a query rarely worked.", "change_title": "Spatial search with point query won't find identical indexed point", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "Given a document with indexed POINT (10 20), when a search for INTERSECTS( POINT (10 20)) is issued, no results are returned. The work-around is to not search with a point shape, use a very small-radius circle or rectangle.  (I'm marking this issue as \"minor\" because it's easy to do this). An unstated objective of the PrefixTree/grid approximation is that no matter what precision you use, an intersects query will find all true-positives.  Due to approximations, it may also find some close false-positives.  But in the case above, that unstated promise is violated.  But it can also happen for query shapes other than points which do in fact barely enclose the point given at index time yet the indexed point is in-effect shifted to the center point of a cell which could be outside the query shape, and ultimately leading to a false-negative.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634953/LUCENE-4978_fix_small_grid_false_negatives.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5553", "change_description": ": IndexReader#ReaderClosedListener is not always invoked when\nIndexReader#close() is called or if refCount is 0. If an exception is\nthrown during interal close or on any of the close listerns some or all\nlisterners might be missed. This can cause memory leaks if the core listeners\nare used to clear caches.", "change_title": "IndexReader#ReaderClosedListener is not always called on IndexReader#close()", "detail_type": "Bug", "detail_affect_versions": "4.7,6.0", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "Today IndexReader#ReaderClosedListener might not be called if the last IndexReader#decRef() call runs into an exception on IndexReader#doClose(). Today we just reset the refCount and never go and call the listeners. There seem to be a bunch of problems here along the same lines but IMO if we close a reader it should close all resources no matter what exception it runs into. What this should do is call the close listeners in a finally block and then rethrow the exception. The real problem here for apps relying on the listener to release resources is that you might leak memory or file handles or whatnot which I think is a bug how we handle closing the IR. As a side-note I think we should never reset the reference here to be honest. Along the same lines I think we need to fix the loop in IndexReader#notifyReaderClosedListeners() to make sure we call all of them in the case any of them throws an exception. It also seems that SegmentCoreReaders#decRef() has a similar problem where for instance a fieldsReader can throw an exception on close and we never call the core listeners. IMO we need to fix this for 4.7.1", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636718/LUCENE-5553.patch", "patch_content": "none"}
{"library_version": "4.7.1", "change_type": "Build", "change_id": "LUCENE-5511", "change_description": ": \"ant precommit\" / \"ant check-svn-working-copy\" now work again\nwith any working copy format (thanks to svnkit 1.8.4).", "change_title": "Upgrade to SvnKit 1.8.4 for checks", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.7.1,4.8,6.0", "detail_description": "We had a hack since LUCENE-5385 in our build, because svnkit 1.8.0 - 1.8.3 were not able to read svn 1.7 checkouts. Because of this the user had to choose the right svnkit version when executing ant check-svn-working-copy. Since svnkit 1.8.4 we can read all svn working copy formats again, so our checks will work on any checkout without forcefully choosing svnkit version. This patch removes the extra warnings and error messages and update to 1.8.4.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633595/LUCENE-5511.patch", "patch_content": "none"}

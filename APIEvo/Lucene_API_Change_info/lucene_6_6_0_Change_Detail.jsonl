{"library_version": "6.6.0", "change_type": "New Features", "change_id": "LUCENE-7811", "change_description": ": Add a concurrent SortedSet facets implementation.", "change_title": "Add concurrent version of SortedSetDocValuesFacetCounts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "Counting up facets should take advantage of concurrent hardware if the index has multiple segments. At first I tried to add an optional ExecutorService argument to the existing single-threaded SortedSetDocValuesFacetCounts but it was difficult, so I just made a new class. I also internally refactored SortedSetDocValuesFacetCounts to share code between its count and countAll private methods, and switched the FacetsCollector to use DocIdSetBuilder instead of always using a FixedBitSet.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12865717/LUCENE-7811.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7777", "change_description": ": ByteBlockPool.readBytes sometimes throws\nArrayIndexOutOfBoundsException when byte blocks larger than 32 KB\nwere added", "change_title": "ByteBlockPool.readBytes incorrectly throws AIOOBE if length > 32768", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.5.1,6.6,7.0", "detail_description": "I'm using Lucene's OfflineSorter to sort a large data set, and some of the items in the set are > 32 KB in length, which tickled a bug in its readBytes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12862862/LUCENE-7777.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7797", "change_description": ": The static FSDirectory.listAll(Path) method was always\nreturning an empty array.", "change_title": "FSDirectory.listAll will always return empty array", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "which invoke private method will always return empty array.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12864446/LUCENE-7797.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7481", "change_description": ": Fixed missing rewrite methods for SpanPayloadCheckQuery\nand PayloadScoreQuery.", "change_title": "SpanPayloadCheckQuery and PayloadScoreQuery are missing rewrite methods", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "If used with a wildcard query, the result is a failure saying: \"Rewrite query first\" The SpanNearQuery has the rewrite method; however the SpanPayloadCheckQuery just returns the query itself. this works: ``` spanNear([vectrfield:ebyuugz, SpanMultiTermQueryWrapper(vectrfield:e*), SpanMultiTermQueryWrapper(vectrfield:m*), SpanMultiTermQueryWrapper(vectrfield:f*)], 0, true) ``` code to generate the query: ``` private Query getSpanQuery(String[] parts, int howMany, boolean truncate) throws UnsupportedEncodingException { \t\tSpanQuery[] clauses = new SpanQuery[howMany+1]; \t\tclauses[0] = new SpanTermQuery(new Term(\"vectrfield\", parts[0])); // surname \t\tfor (int i = 0; i < howMany; i++) { \t\t\tif (truncate) else } \t\tSpanNearQuery sq = new SpanNearQuery(clauses, 0, true); // match in order \t\treturn sq; \t} ``` and this fails: ``` spanPayCheck(spanNear([vectrfield:ebyuugz, SpanMultiTermQueryWrapper(vectrfield:e*), SpanMultiTermQueryWrapper(vectrfield:m*), SpanMultiTermQueryWrapper(vectrfield:f*)], 1, true), payloadRef: 0;1;2;3 ``` each clause is made of: ``` new SpanMultiTermQueryWrapper<WildcardQuery>(new WildcardQuery(new Term(\"vectrfield\", parts[i+1].substring(0, 1) + \"*\"))); ``` It is a regression; the code was working well in SOLR4.x", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7808", "change_description": ": Fixed PayloadScoreQuery and SpanPayloadCheckQuery\n.equals and .hashCode methods.", "change_title": "PayloadScoreQuery and SpanPayloadCheckQuery have incomplete .equals and .hashCode methods", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "Both PayloadScoreQuery and SpanPayloadCheckQuery have incomplete .equals and .hashCode methods, causing caching issues.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12865414/LUCENE-7808.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7798", "change_description": ": Add .equals and .hashCode to ToParentBlockJoinSortField", "change_title": "add equals/hashCode to ToParentBlockJoinSortField", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "Since SOLR-10521 ToParentBlockJoinSortField is going to be used as query result key, therefore it's worth to implemend proper equality methods.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12864660/LUCENE-7798.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7814", "change_description": ": DateRangePrefixTree (in spatial-extras) had edge-case bugs for\nyears >= 292,000,000.", "change_title": "DateRangePrefixTree trouble finding years >= 292,000,000", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "Using DateRangePrefixTree (in spatial-extras): When indexing years on or after the last million-year period supported by most computer systems, +292,000,000, a range query like [1970 TO *] will fail to find it.  This happens as a result of some off-by-one errors in DateRangePrefixTree.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12866117/LUCENE_7814_DateRangePrefixTree_last_millennia_bug.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5365", "change_description": ",", "change_title": "Typo with QueryNodeOperation.logicalAnd", "detail_type": "Bug", "detail_affect_versions": "4.6,4.7,6.0", "detail_fix_versions": "6.6,7.0", "detail_description": "Intellij IDEA finds a bug/typo in QueryNodeOperation.logicalAnd -> a boolean condition is always false Which means that A and B fails when A is not an ANDQueryNode and B is", "patch_link": "https://issues.apache.org/jira/secure/attachment/12617545/typo.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7818", "change_description": ",", "change_title": "Suspicious condition", "detail_type": "Bug", "detail_affect_versions": "6.5", "detail_fix_versions": "None", "detail_description": "Hi Please, look this code fragment: (q1 instanceof AndQueryNode) is checked twice. Probably it should be: This possible defect found by AppChecker", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7821", "change_description": ": The classic and flexible query parsers, as well as Solr's\n \"lucene\"/standard query parser, should require \" TO \" in range queries,\nand accept \"TO\" as endpoints in range queries.", "change_title": "Classic, flexible, and Solr's standard/\"lucene\" query parsers: range queries should require \" TO \", and accept TO as range endpoints", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,6.7,7.0", "detail_description": "A post on the solr-user mailing list drew my attention to the fact that this is apparently a valid range query under the QueryParser.jj grammer (both for the classic parser and the solr variant – i didn't check flexible)... it's parsed as a valid range query even though there is no {{ TO }} – even though there is nothing in the docs to suggest that the {{ TO }} is intended to be optional. Furthermore, in my experimenting i realized that how the grammer looks for the {{ TO }} keyword seems to be a bit sloppy, making some range queries that should be easy to validate (because they are unambiguous) fail to parse... If the \"sloppy\" parsing behavior is intentional, then the docs should reflect that the {{ TO }} is optional – but it seems like in general we should make these other unambiguous cases parse cleanly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12867684/LUCENE-7821.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7824", "change_description": ": Fix graph query analysis for multi-word synonym rules with common terms (eg. new york, new york city).", "change_title": "Multi-word synonyms rule with common terms at the same position are buggy", "detail_type": "Bug", "detail_affect_versions": "6.5.1,7.0", "detail_fix_versions": "6.6,7.0", "detail_description": "The automaton built from the graph token stream tries to pack common terms in multi word synonyms that appear at the same position. This means that some states inside a multi word synonym can have multiple transitions. As a result the intersection point of the graph are not computed correctly. For example the synonym rule: \"ny, new york city, new york\" is not applied correctly to the query \"ny police\". In this case \"police\" is detected as part of the multi synonyms path and we create the disjunction between:  \"ny police\", \"new york police\", ... I pushed a patch that removes this optim (and creates a single transition from each state) in order to ensure that the intersection points of the graph always showed up at the end of the multi synonym paths. mattweber can you take a look ?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12867627/LUCENE-7824.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7817", "change_description": ": Pass cached query to onQueryCache instead of null.", "change_title": "LRUQueryCache.onQueryCache is always called with null as first parameter", "detail_type": "Bug", "detail_affect_versions": "6.4.1,6.5.1,7.0", "detail_fix_versions": "6.6,7.0", "detail_description": "According to the javadocs, LRUQueryCache.onQueryCache can be used to track usage statistics on cached queries. Unfortunately, due to a bug, the query parameter is always passed as null, making the method practically useless. This PR fixes the problem: https://github.com/apache/lucene-solr/pull/199", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7831", "change_description": ": CodecUtil should not seek to negative offsets.", "change_title": "CodecUtil should not seek to negative offsets", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "In case of truncated files whose length is less than the footer length, CodecUtil might throw confusing exceptions by trying to seek to negative offsets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12868264/LUCENE-7831.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7833", "change_description": ": ToParentBlockJoinQuery computed the min score instead of the max\nscore with ScoreMode.MAX.", "change_title": "Calculating minimum score instead of maximum score in ToParentBlockJoinQuery.BlockJoinScorer.setScoreAndFreq", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "We recently upgraded our Elasticsearch cluster from 5.3 to 5.4 and found a bug when we score nested documents. I spent some time to check the code from Elasticsearch and Lucene and I found a problem in ToParentBlockJoinQuery.BlockJoinScorer.setScoreAndFreq. In the switch statement for the scoreMode in the case \"Max\" the function Math.min is used and I guess it should be Math.max. This would explain our problem in Elasticsearch because we see exactly the problem that instead of the maximum the minimum is returned.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12868781/LUCENE-7833.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7847", "change_description": ": Fixed all-docs-match optimization of range queries on range\nfields.", "change_title": "RangeFieldQuery optimization for \"all docs match\" is broken", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "RangeFieldQuery tries to detect based on the min/max values of the field whether all values match. However this detection is broken and can sometimes consider that all documents match when they actually don't.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869469/LUCENE-7847.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7810", "change_description": ": Fix equals() and hashCode() methods of several join queries.", "change_title": "false positive equality: distinctly diff join queries return equals()==true", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6,6.7,7.0", "detail_description": "While working on SOLR-10583 I was getting some odd test failures that seemed to suggest we were getting false cache hits for Join queries that should have been unique. tracing thorugh the code, the problem seems to be the way TermsQuery implements equals(Object).  This class takes in the fromQuery (used to identify set of documents we \"join from\") and uses it in the equals calculation – but the information about the join field is never passed directly to TermsQuery and the BytesRefs that are passed in can't be compared efficiently (AFAICT), so 2 completely diff calls to JoinUtils.createJoinQuery(...) can result in Query objects that think they are equal() even when they most certainly are not. At a brief glance, it appears that similar bugs exist in TermsIncludingScoreQuery (and possibly GlobalOrdinalsWithScoreQuery, but i didn't look into that class at all)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12865656/LUCENE-7810.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Improvements", "change_id": "LUCENE-7782", "change_description": ": OfflineSorter now passes the total number of items it\nwill write to getWriter", "change_title": "OfflineSorter.getWriter should pass number of items it will write", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "When OfflineSorter needs to write a new file, either a temp file or the final sorted output file, it calls getWriter.  There is a default impl, but experts can override it to create their own writers. It would be helpful to pass the total item count that will be written to this file when calling getWriter because then the (expert) user has a little more information which can be useful when creating the temp or final file. It's low cost for OfflineSorter to track this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12863153/LUCENE-7782.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Improvements", "change_id": "LUCENE-7785", "change_description": ": Move dictionary for Ukrainian analyzer to external dependency.", "change_title": "Move dictionary for Ukrainian analyzer to external dependency", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "Currently the dictionary for Ukrainian analyzer is a blob in the source tree. We should move it out to external dependency, this allows:", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Improvements", "change_id": "LUCENE-7801", "change_description": ": SortedSetDocValuesReaderState now implements\nAccountable so you can see how much RAM it's using", "change_title": "SortedSetDocValuesReaderState should implement Accountable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "This class is used by sorted set facets, and wraps per-dimension MultiDocValues.OrdinalMap which already implement accountable, and it's helpful to know how much heap this structure is taking for computing Lucene's SSDV facets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12864845/LUCENE-7801.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Improvements", "change_id": "LUCENE-7792", "change_description": ": OfflineSorter can now run concurrently if you pass it\nan optional ExecutorService", "change_title": "Add optional concurrency to OfflineSorter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "OfflineSorter is a heavy operation and is really an embarrassingly concurrent problem at heart, and if you have enough hardware concurrency (e.g. fast SSDs, multiple CPU cores) it can be a big speedup. E.g., after reading a partition from the input, one thread can sort and write it, while another thread reads the next partition, etc.  Merging partitions can also be done in the background.  Some things still cannot be concurrent, e.g. the initial read from the input must be a single thread, as well as the final merge and writing to the final output. I think I found a fairly non-invasive way to add optional concurrency to this class, by adding an optional ExecutorService to OfflineSorter's ctor (similar to IndexSearcher) and using futures to represent each partition as we sort, and creating Callable classes for sorting and merging partitions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12864735/LUCENE-7792.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Improvements", "change_id": "LUCENE-7811", "change_description": ": Sorted set facets now use sparse storage when\ncollecting hits, when appropriate.", "change_title": "Add concurrent version of SortedSetDocValuesFacetCounts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "Counting up facets should take advantage of concurrent hardware if the index has multiple segments. At first I tried to add an optional ExecutorService argument to the existing single-threaded SortedSetDocValuesFacetCounts but it was difficult, so I just made a new class. I also internally refactored SortedSetDocValuesFacetCounts to share code between its count and countAll private methods, and switched the FacetsCollector to use DocIdSetBuilder instead of always using a FixedBitSet.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12865717/LUCENE-7811.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Optimizations", "change_id": "LUCENE-7787", "change_description": ": spatial-extras HeatmapFacetCounter will now short-circuit it's\nwork when Bits.MatchNoBits is passed.", "change_title": "Optimize HeatmapFacetCounter to detect Bits.MatchNoBits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "HeatmapFacetCounter should short-circuit when the docSet is instanceof Bits.MatchNoBits. This can save a ton of wasted time in relevant circumstances. (This development was funded by the Harvard Center for Geographic Analysis as part of the HHypermap project)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12863677/LUCENE_7787.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Other", "change_id": "LUCENE-7796", "change_description": ": Make IOUtils.reThrow idiom declare Error return type so\ncallers may use it in a way that compiler knows subsequent code is\nunreachable. reThrow is now deprecated in favor of IOUtils.rethrowAlways\nwith a slightly different semantics (see javadoc).", "change_title": "Make reThrow idiom declare Error return type so callers may use it in a way that compiler knows subsequent code is unreachable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "A spinoff from LUCENE-7792: reThrow can be declared to return an unchecked exception so that callers can choose to use throw reThrow(...) as an idiom to let the compiler know any subsequent code will be unreachable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12865388/LUCENE-7796.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Other", "change_id": "LUCENE-7754", "change_description": ": Inner classes should be static whenever possible.", "change_title": "Findbugs: nested class should be static", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "http://findbugs.sourceforge.net/bugDescriptions.html#SIC_INNER_SHOULD_BE_STATIC", "patch_link": "https://issues.apache.org/jira/secure/attachment/12860537/LUCENE-7754.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Other", "change_id": "LUCENE-7751", "change_description": ": Avoid boxing primitives only to call compareTo.", "change_title": "Findbugs: boxing a primitive to compare", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12860525/LUCENE-7751.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Other", "change_id": "LUCENE-7743", "change_description": ": Never call new String(String).", "change_title": "Findbugs: avoid new String(String)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.6,7.0", "detail_description": "http://findbugs.sourceforge.net/bugDescriptions.html#DM_STRING_CTOR Removing the extra constructor calls will avoid heap allocations while behaving just the same as the original code.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12857535/LUCENE-7743.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Other", "change_id": "LUCENE-7761", "change_description": ": Fixed comment in ReqExclScorer.", "change_title": "Typo in comment in ReqExclScorer", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "6.6,7.0", "detail_description": "There is a typo in the last comment in ReqExclScorer. It should say: \"reqTwoPhaseIterator is MORE costly than exclTwoPhaseIterator, check it last\" The patch fixes this comment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12861232/LUCENE-7761.patch", "patch_content": "none"}
{"library_version": "6.6.0", "change_type": "Other", "change_id": "LUCENE-7815", "change_description": ": Deprecate the PostingsHighlighter. It evolved into the\nUnifiedHighlighter.", "change_title": "Remove PostingsHighlighter in 7.0", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "The UnifiedHighlighter is derived from the PostingsHighlighter, which should be quite obvious to anyone who cares to look at them.  There is no feature in the PH that is not also present in the UH.  The PH is marked as lucene.experimental so we may remove it in 7.0.  The upgrade path is pretty easy given the API similarity.  By removing the PH, the goal is to ease maintenance.  Some issues lately have been applicable to both of these highlighters which is annoying to apply twice.  In one case I forgot to.  And of course there is user confusion by having both. What I propose to do in this issue is move CustomSeparatorBreakIterator and WholeBreakIterator out of the postingshighlight package into the uhighlight package (or perhaps add a common or util should future highlighters need them?).  Then of course remove postingshighlight package.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869394/LUCENE_7815_Remove_PostingsHighlighter.patch", "patch_content": "none"}

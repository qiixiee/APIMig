{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5336", "change_description": ": Add SimpleQueryParser: parser for human-entered queries.", "change_title": "Add a simple QueryParser to parse human-entered queries.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "I would like to add a new simple QueryParser to Lucene that is designed to parse human-entered queries.  This parser will operate on an entire entered query using a specified single field or a set of weighted fields (using term boost). All features/operations in this parser can be enabled or disabled depending on what is necessary for the user.  A default operator may be specified as either 'MUST' representing 'and' or 'SHOULD' representing 'or.'  The features/operations that this parser will include are the following: The key differences between this parser and other existing parsers will be the following:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12613375/LUCENE-5336.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5337", "change_description": ": Add Payload support to FileDictionary (Suggest) and make it more\nconfigurable", "change_title": "Add Payload support to FileDictionary (Suggest) and make it more configurable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "It would be nice to add payload support to FileDictionary, so user can pass in associated payload with suggestion entries.  Currently the FileDictionary has a hard-coded field-delimiter (TAB), it would be nice to let the users configure the field delimiter as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12613215/LUCENE-5337.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5329", "change_description": ": suggest: DocumentDictionary and\nDocumentExpressionDictionary are now lenient for dirty documents\n(missing the term, weight or payload).", "change_title": "Make DocumentDictionary and co more lenient to dirty documents", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Currently DocumentDictionary errors out whenever any document does not have value for any relevant stored fields. It would be nice to make it lenient and instead ignore the invalid documents. Another \"issue\" with the DocumentDictionary is that it only allows string fields as suggestions and binary fields as payloads. When exposing these dictionaries to solr (via https://issues.apache.org/jira/browse/SOLR-5378), it is inconvenient for the user to ensure that a suggestion field is a string field and a payload field is a binary field. It would be nice to have the dictionary \"just work\" whenever a string/binary field is passed to suggestion/payload field. The patch provides one solution to this problem (by accepting string or binary values), though it would be great if there are any other solution to this, without making the DocumentDictionary \"too flexible\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12615210/LUCENE-5329.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5404", "change_description": ": Add .getCount method to all suggesters (Lookup); persist count\nmetadata on .store(); Dictionary returns InputIterator; Dictionary.getWordIterator\nrenamed to .getEntryIterator.", "change_title": "Add support to get number of entries a Suggester Lookup was built with and minor refactorings", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "It would be nice to be able to tell the number of entries a suggester lookup was built with. This would let components using lookups to keep some stats regarding how many entries were used to build a lookup. Additionally, Dictionary could use InputIterator rather than the BytesRefIteratator, as most of the implmentations now use it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626959/LUCENE-5404.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "SOLR-1871", "change_description": ": The RangeMapFloatFunction accepts an arbitrary ValueSource\nas target and default values.", "change_title": "Function Query \"map\" variant that allows \"target\" to be an arbitrary ValueSource", "detail_type": "Improvement", "detail_affect_versions": "1.4", "detail_fix_versions": "4.7,6.0", "detail_description": "Currently, as documented at http://wiki.apache.org/solr/FunctionQuery#map, the \"target\" of a map must be a floating point constant. I propose that you should have at least the option of doing a map where the target is an arbitrary ValueSource. The particular use case that inspired this is that I want to be able to control how missing date fields affected boosting. In particular, I want to be able to use something like this in my function queries: But this might have other uses. I'll attach an initial implementation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12442008/ASF.LICENSE.NOT.GRANTED--SOLR-1871.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5371", "change_description": ": Speed up Lucene range faceting from O(N) per hit to\nO(log(N)) per hit using segment trees; this only really starts to\nmatter in practice if the number of ranges is over 10 or so.", "change_title": "Range faceting should use O(log(N)) search per hit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Today, Lucene's dynamic range faceting uses a simple linear search to find which ranges match, but there are known data structures to do this in log(N) time.  I played with segment trees and wrote up a blog post here: http://blog.mikemccandless.com/2013/12/fast-range-faceting-using-segment-trees.html O(N) cost is actually OK when number of ranges is smallish, which is typical for facet use cases, but then scales badly if there are many ranges.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12619405/LUCENE-5371.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5379", "change_description": ": Add Analyzer for Kurdish.", "change_title": "Kurdish Analyzer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Normalizer+Stemmer+Stopwords for Sorani kurdish (written in the arabic script). The most important piece is the normalization: this varies wildly in practice. The stemmer is a light stemmer, very simple and not aggressive at all. I tested against the pewan test collection, see: baseline is StandardAnalyzer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12620756/LUCENE-5379.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5369", "change_description": ": Added an UpperCaseFilter to make UPPERCASE tokens.", "change_title": "Add an UpperCaseFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "We should offer a standard way to force upper-case tokens.  I understand that lowercase is safer for general search quality because some uppercase characters can represent multiple lowercase ones. However, having upper-case tokens is often nice for faceting (consider normalizing to standard acronyms)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12618696/LUCENE-5369-uppercase-filter.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5345", "change_description": ": Add a new BlendedInfixSuggester, which is like\nAnalyzingInfixSuggester but boosts suggestions that matched tokens\nwith lower positions.", "change_title": "range facets don't work with float/double fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "With LUCENE-5297 we generalized range faceting to accept a ValueSource. But, when I tried to use this to facet by distance (< 1 km, < 2 km, etc.), it's not working ... the problem is that the RangeAccumulator always uses .longVal and assumes this was a double encoded as a long (via DoubleField).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12614909/LUCENE-5345.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-4399", "change_description": ": When sorting by String (SortField.STRING), you can now\nspecify whether missing values should be sorted first (the default),\nusing SortField.setMissingValue(SortField.STRING_FIRST), or last,\nusing SortField.setMissingValue(SortField.STRING_LAST).", "change_title": "Rename AppendingCodec to Appending40Codec", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "In order AppendingCodec to follow Lucene codecs version, I think its name should include a version number (so that, for example, if we get to releave Lucene 4.3 with a new Lucene43Codec, there will also be a new Appending43Codec).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548230/LUCENE-4399.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5099", "change_description": ": QueryNode should have the ability to detach from its node\nparent. Added QueryNode.removeFromParent() that allows nodes to be\ndetached from its parent node.", "change_title": "QueryNode should have the ability to detach from its node parent", "detail_type": "Bug", "detail_affect_versions": "3.6.2,4.3.1", "detail_fix_versions": "4.7", "detail_description": "QueryNodeProcessorImpl should always return the root of the tree after processing, so it needs to make the parent is set to null before returning. Otherwise, the previous parent is leaked and never removed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12623868/LUCENE-5099_adrianocrestani_2014-01-19_4x.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5395", "change_description": "", "change_title": "Upgrade Spatial4j to 0.4", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7", "detail_description": "Spatial4j 0.4 should be released the week of January 13th; a snapshot is published.  A longer version of the delta from 0.4 is in CHANGES.md A couple notable new features are:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12624087/LUCENE-5395_Spatial4j_0_4.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5451", "change_description": "", "change_title": "Update Spatial4j to 0.4.1", "detail_type": "Bug", "detail_affect_versions": "4.7,6.0", "detail_fix_versions": "4.7,6.0", "detail_description": "A user reported a fairly serious issue affecting the latest version of Spatial4j 0.4 https://github.com/spatial4j/spatial4j/issues/72 \"JtsSpatialContextFactory and geo=false with worldBounds fails\" I've already fixed this locally and I'll push out a bug-fix Spatial4j 0.4.1.  Upgrading will be a complete drop-in replacement.  Heck I could do that now but as I work with the user who found the bug I want to see if there's any other problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629776/LUCENE-5451__Upgrade_to_Spatial4j_0_4_1.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "https://github.com/spatial4j/spatial4j/blob/master/CHANGES.md", "change_description": "", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5415", "change_description": ": Add multitermquery (wildcards,prefix,etc) to PostingsHighlighter.", "change_title": "Support wildcard & co in PostingsHighlighter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "PostingsHighlighter uses the offsets encoded in the postings lists for the terms to find query matches. As such, it isn't really suitable for stuff like wildcards for two reasons: 1. an expensive rewrite against the term dictionary (i think other highlighters share this problem) 2. accumulating data from potentially many terms (e.g. reading many postings) However, we could provide an option for some of these queries to work, but in a different way, that avoids these downsides. Instead we can just grab the Automaton representation of the queries, and match it against the content directly (which won't blow up).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12625217/LUCENE-5415.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-3069", "change_description": ": Add two memory resident dictionaries (FST terms dictionary and\nFSTOrd terms dictionary) to improve primary key lookups. The PostingsBaseFormat\nAPI is also changed so that term dictionaries get the ability to block\nencode term metadata, and all dictionary implementations can now plug in any\nPostingsBaseFormat.", "change_title": "Lucene should have an entirely memory resident term dictionary", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.7", "detail_description": "FST based TermDictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. Some environments have enough memory available to keep the entire FST based term dict in memory. We should add a TermDictionary implementation that encodes all needed information for each term into the FST (custom fst.Output) and builds a FST from the entire term not just the delta.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601388/LUCENE-3069.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5353", "change_description": ": ShingleFilter's filler token should be configurable.", "change_title": "ShingleFilter should have a way to specify FILLER_TOKEN", "detail_type": "Improvement", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "Today we have no choice that if pos_inc is > 1 there will be a `_` inserted in between the tokens. We should have the ability to change this character and the char[] that holds it should not be public static since it's mutable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626010/LUCENE-5353.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5320", "change_description": ": Add SearcherTaxonomyManager over search and taxonomy index\ndirectories (i.e. not only NRT).", "change_title": "Create SearcherTaxonomyManager over Directory", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "SearcherTaxonomyManager now only allows working in NRT mode. It could be useful to have an STM which allows reopening a SearcherAndTaxonomy pair over Directories, e.g. for replication. The problem is that if the thread that calls maybeRefresh() is not the one that does the commit(), it could lead to a pair that is not synchronized. Perhaps at first we could have a simple version that works under some assumptions, i.e. that the app does the commit + reopen in the same thread in that order, so that it can be used by such apps + when replicating the indexes, and later we can figure out how to generalize it to work even if commit + reopen are done by separate threads/JVMs. I'll see if SearcherTaxonomyManager can be extended to support it, or a new STM is required.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626096/LUCENE-5320.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5410", "change_description": ": Add fuzzy and near support via '~' operator to SimpleQueryParser.", "change_title": "Add fuzziness support to SimpleQueryParser", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.7,6.0", "detail_description": "It would be nice to add fuzzy query support to the SimpleQueryParser so that: foo~2 generates a FuzzyQuery with an max edit distance of 2 and: \"foo bar\"~2 generates a PhraseQuery with a slop of 2.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626342/LUCENE-5410.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5426", "change_description": ": Make SortedSetDocValuesReaderState abstract to allow\ncustom implementations for Lucene doc values faceting", "change_title": "Make SortedSetDocValuesReaderState customizable", "detail_type": "Improvement", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "We have a reader that have a different data structure (in memory) where the cost of computing ordinals per reader open is too expensive in the realtime setting. We are maintaining in memory data structure that supports all functionality and would like to leverage SortedSetDocValuesAccumulator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626370/sortedsetreaderstate.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5434", "change_description": ": NRT support for file systems that do no have delete on last\nclose or cannot delete while referenced semantics.", "change_title": "NRT support for file systems that do no have delete on last close or cannot delete while referenced semantics.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "See SOLR-5693 and our HDFS support - for something like HDFS to work with NRT, we need an ability for near realtime readers to hold references to their files to prevent deletes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12627205/LUCENE-5434.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5418", "change_description": ": Drilling down or sideways on a Lucene facet range\n(using Range.getFilter()) is now faster for costly filters (uses\nrandom access, not iteration); range facet counts now accept a\nfast-match filter to avoid computing the value for documents that\nare out of bounds, e.g. using a bounding box filter with distance\nrange faceting.", "change_title": "Don't use .advance on costly (e.g. distance range facets) filters", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "If you use a distance filter today (see http://blog.mikemccandless.com/2014/01/geospatial-distance-faceting-using.html ), then drill down on one of those ranges, under the hood Lucene is using .advance on the Filter, which is very costly because we end up computing distance on (possibly many) hits that don't match the query. It's better performance to find the hits matching the Query first, and then check the filter. FilteredQuery can already do this today, when you use its QUERY_FIRST_FILTER_STRATEGY.  This essentially accomplishes the same thing as Solr's \"post filters\" (I think?) but with a far simpler/better/less code approach. E.g., I believe ElasticSearch uses this API when it applies costly filters. Longish term, I think  Query/Filter ought to know itself that it's expensive, and cases where such a Query/Filter is MUST'd onto a BooleanQuery (e.g. ConstantScoreQuery), or the Filter is a clause in BooleanFilter, or it's passed to IndexSearcher.search, we should also be \"smart\" here and not call .advance on such clauses.  But that'd be a biggish change ... so for today the \"workaround\" is the user must carefully construct the FilteredQuery themselves. In the mean time, as another workaround, I want to fix DrillSideways so that when you drill down on such filters it doesn't use .advance; this should give a good speedup for the \"normal path\" API usage with a costly filter. I'm iterating on the lucene server branch (LUCENE-5376) but once it's working I plan to merge this back to trunk / 4.7.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626377/LUCENE-5418.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5440", "change_description": ": Add LongBitSet for managing more than 2.1B bits (otherwise use\nFixedBitSet).", "change_title": "Add LongFixedBitSet and replace usage of OpenBitSet", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Spinoff from here: http://lucene.markmail.org/thread/35gw3amo53dsqsqj. I wrote a LongFixedBitSet which behaves like FixedBitSet, only allows managing more than 2.1B bits. It overcome some issues I've encountered with OpenBitSet, such as the use of set/fastSet as well the implementation of DocIdSet. I'll post a patch shortly and describe it in more detail.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12628953/LUCENE-5440-solr.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5437", "change_description": ": ASCIIFoldingFilter now has an option to preserve the original token\nand emit it on the same position as the folded token only if the actual token was\nfolded.", "change_title": "ASCIIFoldingFilter that emits both unfolded and folded tokens", "detail_type": "Improvement", "detail_affect_versions": "4.7,6.0", "detail_fix_versions": "4.7,6.0", "detail_description": "I've found myself wanting an ASCIIFoldingFilter that emits both the folded tokens and the original, unfolded tokens.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12628224/LUCENE-5437.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5408", "change_description": ": Add spatial SerializedDVStrategy that serializes a binary\nrepresentations of a shape into BinaryDocValues. It supports exact geometry\nrelationship calculations.", "change_title": "SerializedDVStrategy -- match geometries in DocValues", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "I've started work on a new SpatialStrategy implementation I'm tentatively calling SerializedDVStrategy.  It's similar to the JtsGeoStrategy in Spatial-Solr-Sandbox but a little different in the details â€“ certainly faster.  Using Spatial4j 0.4's BinaryCodec, it'll serialize the shape to bytes (for polygons this in internally WKB format) and the strategy will put it in a BinaryDocValuesField.  In practice the shape is likely a polygon but it needn't be.  Then I'll implement a Filter that returns a DocIdSetIterator that evaluates a given document passed via advance(docid)) to see if the query shape matches a shape in DocValues. It's improper usage for it to be used in a situation where it will evaluate every document id via nextDoc().  And in practice the DocValues format chosen should be a disk resident one since each value tends to be kind of big. This spatial strategy in and of itself has no index; it's O(N) where N is the number of documents that get passed thru it.  So it should be placed last in the query/filter tree so that the other queries limit the documents it needs to see.  At a minimum, another query/filter to use in conjunction is another SpatialStrategy like RecursivePrefixTreeStrategy. Eventually once the PrefixTree grid encoding has a little bit more metadata, it will be possible to further combine the grid & this strategy in such a way that many documents won't need to be checked against the serialized geometry.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626124/LUCENE-5408_GeometryStrategy.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "New Features", "change_id": "LUCENE-5457", "change_description": ": Add SloppyMath.earthDiameter(double latitude) that returns an\napproximate value of the diameter of the earth at the given latitude.", "change_title": "Expose SloppyMath earth diameter table", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7", "detail_description": "LUCENE-5271 introduced a table in order to get approximate values of the diameter of the earth given a latitude. This could be useful for other computations so I think it would be nice to have a method that exposes this table.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629791/LUCENE-5457.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5217", "change_description": ",", "change_title": "disable transitive dependencies in maven config", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Our ivy configuration does this: each dependency is specified and so we know what will happen. Unfortunately the maven setup is not configured the same way. Instead the maven setup is configured to download the internet: and it excludes certain things specifically. This is really hard to configure and maintain: we added a 'validate-maven-dependencies' that tries to fail on any extra jars, but all it really does is run a license check after maven \"runs\". It wouldnt find unnecessary dependencies being dragged in if something else in lucene was using them and thus they had a license file. Since maven supports wildcard exclusions: MNG-3832, we can disable this transitive shit completely. We should do this, so its configuration is the exact parallel of ivy.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12611267/LUCENE-5217.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5420", "change_description": ",", "change_title": "'ant get-maven-poms' should support custom version formats", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "LUCENE-5217 changed the way ant get-maven-poms works, so that dependencies are pulled from the Ant build, instead of being hard-coded in the POM templates.  To parse the versions from internal module dependencies, the new internal Ant task GetMavenDependenciesTask uses a regex that expects a dotted/slashed/underscored numeric + optional -SNAPSHOT version format.  As a result, non-conforming versions trigger a build failure - see the lucene-dev mailing list thread \"maven build issues with non-numeric custom version\": <http://mail-archives.apache.org/mod_mbox/lucene-dev/201401.mbox/%3cCAF=Pa5-0sXE9Su1PgF5M+F0T+g3Q=fetWgXDh5Ry1ab3ZRhquQ@mail.gmail.com%3e> This is a regression, since previously, custom version strings worked properly, e.g. ant -Dversion=my-custom-version get-maven-poms.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12625874/LUCENE-5420.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5322", "change_description": ": Clean up / simplify Maven-related Ant targets.", "change_title": "Clean up / simplify Maven-related Ant targets", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Many Maven-related Ant targets are public when they don't need to be, e.g. dist-maven and filter-pom-templates, m2-deploy-lucene-parent-pom, etc. The arrangement of these targets could be simplified if the directories that have public entry points were minimized. generate-maven-artifacts should be runnable from the top level and from lucene/ and solr/.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12613649/LUCENE-5322.lucene-javadoc-url.fix.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5347", "change_description": ": Upgrade forbidden-apis checker to version 1.4.", "change_title": "Update to forbidden-apis 1.4", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "Forbidden-apis 1.4 is out, new features are:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12615220/LUCENE-5347.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-4381", "change_description": ": Upgrade analysis/icu to 52.1.", "change_title": "upgrade ICU", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "ICU will release a new version in about a month. They have a version for testing (http://site.icu-project.org/download/milestone) already out with some interesting features, e.g. dictionary-based CJK segmentation. This issue is just to test it out/integrate the new stuff/etc. We should try out the automation Steve did as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12616699/LUCENE-4381.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5357", "change_description": ": Upgrade StandardTokenizer and UAX29URLEmailTokenizer to\nUnicode 6.3; update UAX29URLEmailTokenizer's recognized top level\ndomains in URLs and Emails from the IANA Root Zone Database.", "change_title": "Upgrade StandardTokenizer & co to latest unicode rules", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "besides any change in data, the rules have also changed (regional indicators, better handling for hebrew, etc)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12617249/LUCENE-5357.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5360", "change_description": ": Add support for developing in Netbeans IDE.", "change_title": "Add support for developing in netbeans IDE", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "It will be nice to have ant target for building netbeans IDE project definition.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12618220/LUCENE-5360-part2.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "SOLR-5590", "change_description": ": Upgrade HttpClient/HttpComponents to 4.3.x.", "change_title": "SolrJ is still on httpcomponents/httpclient version 4.2.x, which has some problems", "detail_type": "Improvement", "detail_affect_versions": "4.5.1", "detail_fix_versions": "4.7,6.0", "detail_description": "SolrJ depends on HttpClient 4.2.x right now, but HttpClient 4.2.x has issues that the ManifoldCF team encountered with handling of form data encoding - issues which are addressed in HttpClient 4.3.x.  We developed a local patch, but Solr will eventually need to go to the new client.  (ManifoldCF would plan to follow shortly thereafter). I tried to get Oleg (PMC chair of HttpComponents) to agree to port the fixed code to the 4.2.x stream but he did not want to do that.  So I believe that that avenue is closed. See CONNECTORS-623 for a detailed description of the problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12621076/SOLR-5590.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5385", "change_description": ": \"ant precommit\" / \"ant check-svn-working-copy\" now work\nfor SVN 1.8 or GIT checkouts. The ANT target prints a warning instead\nof failing. It also instructs the user, how to run on SVN 1.8 working\ncopies.", "change_title": "make precommit work for svn 1.8 or git checkouts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12621638/LUCENE-5385.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5383", "change_description": ": fix changes2html to link pull requests", "change_title": "fix changes2html to link pull requests", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "If someone submits a pull request, i think we should put it in changes.txt in some way similar to the jira issues: e.g. for a JIRA issue we do: changes2html recognizes and expands these to jira issue links. so I think we should be able to do something like: and have it link to the request, too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12621652/LUCENE-5383.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5411", "change_description": ": Upgrade to released JFlex 1.5.0; stop requiring\na locally built JFlex snapshot jar.", "change_title": "Upgrade to released JFlex 1.5.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7", "detail_description": "The JFlex 1.5.0 release will be officially announced shortly.  The jar is already on Maven Central.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12624280/LUCENE-5411.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Build", "change_id": "LUCENE-5465", "change_description": ": Solr Contrib \"map-reduce\" breaks Manifest of all other\nJAR files by adding a broken Main-Class attribute.", "change_title": "Solr Contrib \"map-reduce\" breaks Manifest of all other JAR files by adding a broken Main-Class attribute", "detail_type": "Bug", "detail_affect_versions": "4.7", "detail_fix_versions": "4.7,4.8,6.0", "detail_description": "The addition of the Solr map-reduce contrib created a new \"main.class\" property, which is used by the jarify task. Currently only the map-reduce plugin actually set this property, soall other generated JAR files contain the following line: Main-Class: ${main.class} This happens because the ANT property \"main.class\" is undefined for most modules. Maybe this was added for one of the modules (I assume that the Solr-Morphline JARs use this attribute?). We should add some if/then/else structure to the <jarify/> task that only sets this property, if it is actually defined. Otherwise remove it (I think ANT does this automatically if its empty, means string-empty, have to try out) This leads to an error if the file is double-clicked or started via java -jar: I opened this issue in LUCENE, because jar files from LUCENE and SOLR are affected.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12630390/LUCENE-5465.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5285", "change_description": ": Improved highlighting of multi-valued fields with\nFastVectorHighlighter.", "change_title": "FastVectorHighlighter copies segments scores when splitting segments across multi-valued fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7", "detail_description": "FastVectorHighlighter copies segments scores when splitting segments across multi-valued fields.  This is only a problem when you want to sort the fragments by score. Technically BaseFragmentsBuilder (line 261 in my copy of the source) does the copying. Rather than copying the score I think it'd be more right to pull that copying logic into a protected method that child classes (such as ScoreOrderFragmentsBuilder) can override to do more intelligent things.  Exactly what that means isn't clear to me at the moment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12616173/LUCENE-5285.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5391", "change_description": ": UAX29URLEmailTokenizer should not tokenize no-scheme\ndomain-only URLs that are followed by an alphanumeric character.", "change_title": "UAX29URLEmailTokenizer should not tokenize no-scheme domain-only URLs that are followed by an alphanumeric character", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "The uax29urlemailtokenizer tokenises index2.php as: <URL> index2.ph <ALPHANUM> p While it does not do the same for index.php Screenshot from analyser: http://postimg.org/image/aj6c98n3b/", "patch_link": "https://issues.apache.org/jira/secure/attachment/12622351/LUCENE-5391.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5405", "change_description": ": If an analysis component throws an exception, Lucene\nlogs the field name to the info stream to assist in\ndiagnosis.", "change_title": "Exception strategy for analysis improved", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "SOLR-5623 included some conversation about the dilemmas of exception management and reporting in the analysis chain. I've belatedly become educated about the infostream, and this situation is a job for it. The DocInverterPerField can note exceptions in the analysis chain, log out to the infostream, and then rethrow them as before. No wrapping, no muss, no fuss. There are comments on this JIRA from a more complex prior idea that readers might want to ignore.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626571/LUCENE-5405-4.x.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "SOLR-5661", "change_description": ": PriorityQueue now refuses to allocate itself if the\nincoming maxSize is too large", "change_title": "PriorityQueue has OOM (Requested array size exceeds VM limit) issue", "detail_type": "Bug", "detail_affect_versions": "4.3.1,4.4,4.5,4.5.1,4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "It look like JDK7 change the design for max_array_length logic, it isn't max_jint, and it should be  max_jint - header_size(type). If you deliver the Integer.MaxValue to create the PriorityQueue and have enough memory, you will find it is ok in JVM6 but not work in JVM7. JVM7 will throw OOM error while do array rang checking. It should the compatible issue between JVM6 and JVM7. Maybe need protect in the code logic, throw OOM look like big issues for customer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5228", "change_description": ": IndexWriter.addIndexes(Directory[]) now acquires a\nwrite lock in each Directory, to ensure that no open IndexWriter is\nchanging the incoming indices.  This also means that you cannot pass\nthe same Directory to multiple concurrent addIndexes calls (which is\nanyways unusual).", "change_title": "IndexWriter.addIndexes copies raw files but acquires no locks", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "I see stuff like: \"merge problem with lucene 3 and 4 indices\" (from solr users list), and cannot even think how to respond to these users because so many things can go wrong with IndexWriter.addIndexes(Directory) it currently has in its javadocs: NOTE: the index in each Directory must not be changed (opened by a writer) while this method is running. This method does not acquire a write lock in each input Directory, so it is up to the caller to enforce this. This method should be acquiring locks: its copying RAW FILES. Otherwise we should remove it. If someone doesnt like that, or is mad because its 10ns slower, they can use NoLockFactory.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12624429/LUCENE-5228.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5415", "change_description": ": SpanMultiTermQueryWrapper didn't handle its boost in\nhashcode/equals/tostring/rewrite.", "change_title": "Support wildcard & co in PostingsHighlighter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "PostingsHighlighter uses the offsets encoded in the postings lists for the terms to find query matches. As such, it isn't really suitable for stuff like wildcards for two reasons: 1. an expensive rewrite against the term dictionary (i think other highlighters share this problem) 2. accumulating data from potentially many terms (e.g. reading many postings) However, we could provide an option for some of these queries to work, but in a different way, that avoids these downsides. Instead we can just grab the Automaton representation of the queries, and match it against the content directly (which won't blow up).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12625217/LUCENE-5415.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5409", "change_description": ": ToParentBlockJoinCollector.getTopGroups would fail to\nreturn any groups when the joined query required more than one\nrewrite step", "change_title": "ToParentBlockJoinCollector.getTopGroups returns empty Groups", "detail_type": "Bug", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "A bug is observed to cause unstable results returned by the getTopGroups function of class ToParentBlockJoinCollector. In the scorer generation stage, the ToParentBlockJoinCollector will automatically rewrite all the associated ToParentBlockJoinQuery (and their subqueries), and save them into its in-memory Look-up table, namely joinQueryID (see enroll() method for detail). Unfortunately, in the getTopGroups method, the new ToParentBlockJoinQuery parameter is not rewritten (at least users are not expected to do so). When the new one is searched in the old lookup table (considering the impact of rewrite() on hashCode()), the lookup will largely fail and eventually end up with a topGroup collection consisting of only empty groups (their hitCounts are guaranteed to be zero). An easy fix would be to rewrite the original BlockJoinQuery before invoking getTopGroups method. However, the computational cost of this is not optimal. A better but slightly more complex solution would be to save unrewrited Queries into the lookup table.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12625511/local_history.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5398", "change_description": ": NormValueSource was incorrectly casting the long value\nto byte, before calling Similarity.decodeNormValue.", "change_title": "NormValueSource unable to read long field norm", "detail_type": "Bug", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "Previous Lucene implementation store field norms of all documents in memory, float values are therefore encoded into byte to minimize memory consumption. Recent release no longer have this constraint (see LUCENE-5078, and discussion at http://lucene.markmail.org/message/jtwit3pwu5oiqr2h), users are encouraged to implement their own encodeNormValue() to encode them into/decode from any type including int, byte and long, to fulfil their request for precision. But the legacy NormValueSource still typecast any long encoding into byte, as seen in line 74 in the java file, making any TFIDFSimilarity using more accurate encoding useless. It should be removed for the greater good.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12625848/LUCENE-5398.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5436", "change_description": ": RefrenceManager#accquire can result in infinite loop if\nmanaged resource is abused outside of the RefrenceManager. Decrementing\nthe reference without a corresponding incRef() call can cause an infinite\nloop. ReferenceManager now throws IllegalStateException if currently managed\nresources ref count is 0.", "change_title": "RefrenceManager#accquire can result in infinite loop if manager resource is abused outside of the manager", "detail_type": "Bug", "detail_affect_versions": "4.6.1,4.7,6.0", "detail_fix_versions": "4.7,6.0", "detail_description": "I think I found a bug that can cause the ReferenceManager to stick in an infinite loop if the managed reference is decremented outside of the manager without a corresponding increment. I think this is pretty bad since the debugging of this is a mess and we should rather throw ISE instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12627404/LUCENE-5436.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5443", "change_description": ": Lucene45DocValuesProducer.ramBytesUsed() may throw\nConcurrentModificationException.", "change_title": "DocValuesProducer#ramBytesUsed throws ConcurrentModificationException", "detail_type": "Bug", "detail_affect_versions": "4.6,4.6.1,4.7,6.0", "detail_fix_versions": "4.7,6.0", "detail_description": "this came up in an elasticsearch issue that if you pull #ramBytesUsed() while docvalues are loaded in a seperate thread you see a ConcurrentModificationException here is an example:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12628720/LUCENE-5443.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5444", "change_description": ": MemoryIndex did't respect the analyzers offset gap and\noffsets were corrupted if multiple fields with the same name were\nadded to the memory index.", "change_title": "offsets in MemoryIndex broken when adding field with more than once", "detail_type": "Bug", "detail_affect_versions": "4.6.1", "detail_fix_versions": "4.7,6.0", "detail_description": "When fields with the same name are added more than once to MemoryIndex, the offsets of the previous additions of the field do not seem to be taken into account. As a result, MemoryIndex cannot be used for example with the vector highlighter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629027/LUCENE-5444.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5447", "change_description": ": StandardTokenizer should break at consecutive chars matching\nWord_Break = MidLetter, MidNum and/or MidNumLet", "change_title": "StandardTokenizer should break at consecutive chars matching Word_Break = MidLetter, MidNum and/or MidNumLet", "detail_type": "Bug", "detail_affect_versions": "4.6.1", "detail_fix_versions": "4.7,6.0", "detail_description": "StandardTokenizer should split all of the following sequences into two tokens each, but they are all instead kept intact and output as single tokens: Unfortunately, the word break test data released with Unicode, e.g. for Unicode 6.3 http://www.unicode.org/Public/6.3.0/ucd/auxiliary/WordBreakTest.txt, and incorporated into a versioned Lucene test, e.g. WordBreakTestUnicode_6_3_0, doesn't cover these cases.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629808/LUCENE-5447-take2.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5462", "change_description": ": RamUsageEstimator.sizeOf(Object) is not used anymore to\nestimate memory usage of segments. This used to make\nSegmentReader.ramBytesUsed very CPU-intensive.", "change_title": "Lucene 3.x producers shouldn't use RamUsageEstimator to estimate size", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7", "detail_description": "We have had a few Elasticsearch users who have high CPU usage because of RamUsageEstimator being used to estimate memory usage of Lucene3xFields.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12630072/LUCENE-5462.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-5461", "change_description": ": ControlledRealTimeReopenThread would sometimes wait too\nlong (up to targetMaxStaleSec) when a searcher is waiting for a\nspecific generation, when it should have waited for at most\ntargetMinStaleSec.", "change_title": "ControlledRealTimeReopenThread waitForGeneration might sleep for targetMaxStaleSec instead of targetMinStaleSec", "detail_type": "Bug", "detail_affect_versions": "4.6.1", "detail_fix_versions": "4.8,6.0", "detail_description": "If setting the tagetMinStaleSec to 0, sometimes a call to waitForGeneration will block for targetMaxStaleSec instead of immediately call maybeRefreshBlocking on the manager. In effect to targetMaxStaleSec cannot be set higher than acceptable blocking resolving of a specified generation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12630294/LUCENE-5461.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "API Changes", "change_id": "LUCENE-5339", "change_description": ": The facet module was simplified/reworked to make the\nAPIs more approachable to new users. Note: when migrating to the new\nAPI, you must pass the Document that is returned from FacetConfig.build()\nto IndexWriter.addDocument().", "change_title": "Simplify the facet module APIs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "I'd like to explore simplifications to the facet module's APIs: I think the current APIs are complex, and the addition of a new feature (sparse faceting, LUCENE-5333) threatens to add even more classes (e.g., FacetRequestBuilder).  I think we can do better. So, I've been prototyping some drastic changes; this is very early/exploratory and I'm not sure where it'll wind up but I think the new approach shows promise. The big changes are: Sparse faceting is just another method (getAllDims), on both taxonomy & ssdv facet classes. I haven't created a common base class / interface for all of the search-time facet classes, but I think this may be possible/clean, and perhaps useful for drill sideways. All the new classes are under oal.facet.simple.*. Lots of things that don't work yet: drill sideways, complements, associations, sampling, partitions, etc.  This is just a start ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12619615/LUCENE-5339.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "API Changes", "change_id": "LUCENE-5405", "change_description": ": Make ShingleAnalzyerWrapper.getWrappedAnalyzer() public final", "change_title": "Exception strategy for analysis improved", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "SOLR-5623 included some conversation about the dilemmas of exception management and reporting in the analysis chain. I've belatedly become educated about the infostream, and this situation is a job for it. The DocInverterPerField can note exceptions in the analysis chain, log out to the infostream, and then rethrow them as before. No wrapping, no muss, no fuss. There are comments on this JIRA from a more complex prior idea that readers might want to ignore.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626571/LUCENE-5405-4.x.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "API Changes", "change_id": "LUCENE-5395", "change_description": ": The SpatialArgsParser now only reads WKT, no more \"lat, lon\"\netc. but it's easy to override the parseShape method if you wish.", "change_title": "Upgrade Spatial4j to 0.4", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7", "detail_description": "Spatial4j 0.4 should be released the week of January 13th; a snapshot is published.  A longer version of the delta from 0.4 is in CHANGES.md A couple notable new features are:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12624087/LUCENE-5395_Spatial4j_0_4.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "API Changes", "change_id": "LUCENE-5414", "change_description": ": DocumentExpressionDictionary was renamed to\nDocumentValueSourceDictionary and all dependencies to the lucene-expression\nmodule were removed from lucene-suggest. DocumentValueSourceDictionary now\nonly accepts a ValueSource instead of a convenience ctor for an expression\nstring.", "change_title": "suggest module should not depend on expression module", "detail_type": "Wish", "detail_affect_versions": "4.6,6.0", "detail_fix_versions": "4.7,6.0", "detail_description": "Currently our suggest module depends on the expression module just because the DocumentExpressionDictionary provides some util ctor to pass in an expression directly. That is a lot of dependency for little value IMO and pulls in lots of JARs. DocumentExpressionDictionary should only take a ValueSource instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12625214/LUCENE-5414.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "API Changes", "change_id": "LUCENE-3069", "change_description": ": PostingsWriterBase and PostingsReaderBase are no longer\nresponsible for encoding/decoding a block of terms.  Instead, they\nshould encode/decode each term to/from a long[] and byte[].", "change_title": "Lucene should have an entirely memory resident term dictionary", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.7", "detail_description": "FST based TermDictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. Some environments have enough memory available to keep the entire FST based term dict in memory. We should add a TermDictionary implementation that encodes all needed information for each term into the FST (custom fst.Output) and builds a FST from the entire term not just the delta.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601388/LUCENE-3069.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "API Changes", "change_id": "LUCENE-5425", "change_description": ": FacetsCollector and MatchingDocs use a general DocIdSet,\nallowing for custom implementations to be used when faceting.", "change_title": "Make creation of FixedBitSet in FacetsCollector overridable", "detail_type": "Improvement", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "In FacetsCollector, creation of bits in MatchingDocs are allocated per query. For large indexes where maxDocs are large creating a bitset of maxDoc bits will be expensive and would great a lot of garbage. Attached patch is to allow for this allocation customizable while maintaining current behavior.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12626390/facetscollector.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Optimizations", "change_id": "LUCENE-5372", "change_description": ": Replace StringBuffer by StringBuilder, where possible.", "change_title": "Replace StringBuffer with StringBuilder where possible, add to forbidden-apis", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "This is pretty minor, but I found a few issues with the toString implementations while looking through the facet data structures. The most egregious is the use of string concatenation in the IntArray class. I have fixed that using StringBuilders. I also noticed that other classes were using StringBuffer instead of StringBuilder. According to the javadoc, \"This class is designed for use as a drop-in replacement for StringBuffer in places where the string buffer was being used by a single thread (as is generally the case). Where possible, it is recommended that this class be used in preference to StringBuffer as it will be faster under most implementations.\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12620297/5372-lucene5339.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Optimizations", "change_id": "LUCENE-5271", "change_description": ": A slightly more accurate SloppyMath distance.", "change_title": "A slightly more accurate SloppyMath distance", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "SloppyMath, intriduced in LUCENE-5258, uses earth's avg. (according to WGS84) ellipsoid radius as an approximation for computing the \"spherical\" distance. (The TO_KILOMETERS constant). While this is pretty accurate for long distances (latitude wise) this may introduce some small errors while computing distances close to the equator (as the earth radius there is larger than the avg.) A more accurate approximation would be taking the avg. earth radius at the source and destination points. But computing an ellipsoid radius at any given point is a heavy function, and this distance should be used in a scoring function.. So two optimizations are optional -", "patch_link": "https://issues.apache.org/jira/secure/attachment/12617626/LUCENE-5271.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Optimizations", "change_id": "LUCENE-5399", "change_description": ": Deep paging using IndexSearcher.searchAfter when\nsorting by fields is faster", "change_title": "PagingFieldCollector is very slow with String fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "PagingFieldCollector (sort comparator) is significantly slower with string fields, because of how its \"seen on a previous page\" works: it calls compareDocToValue(int doc, T t) first to check this. (its the only user of this method) This is very slow with String, because no ordinals are used. so each document must lookup ord, then lookup bytes, then compare bytes. I think maybe we should replace this method with an 'after' slot, and just have compareDocToAfter or something. Otherwise we could use a hack-patch like the one i will upload (i did this just to test the performance, although tests do pass).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12623270/LUCENE-5399.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5362", "change_description": ": IndexReader and SegmentCoreReaders now throw\nAlreadyClosedException if the refCount in incremented but\nis less that 1.", "change_title": "IndexReader and friends should check ref count when incrementing", "detail_type": "Improvement", "detail_affect_versions": "4.6", "detail_fix_versions": "4.7,6.0", "detail_description": "IndexReader and SegmentCoreReaders blindly increments it's refcount which could already be counted down to 0 which might allow an IndexReader  to \"rise from the dead\" and use an already closed SCR instance. Even if that is caught we should try best effort to raise ACE asap.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12617472/LUCENE-5362.patch", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Documentation", "change_id": "LUCENE-5384", "change_description": ": Add some tips for making tokenfilters and tokenizers\nto the analysis package overview.", "change_title": "Analysis overview could mention clearAttributes and end", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "It would be helpful to tokenizer implementors for the analysis package overview to mention more things. I'll supply a patch.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Documentation", "change_id": "pull request #12", "change_description": ": Add some tips for making tokenfilters and tokenizers\nto the analysis package overview.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Update wiki-query-syntax link to point towards https://cwiki.apache.org/confluence/display/solr/Query+Syntax+and+Parsing Along the way I've updated the links that existed in some other solrconfig.xml files.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Documentation", "change_id": "LUCENE-5389", "change_description": ": Add more guidance in the analyis documentation\npackage overview.", "change_title": "Even more doc for construction of TokenStream components", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.7,6.0", "detail_description": "There are more useful things to tell would-be authors of tokenizers. Let's tell them.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.7.0", "change_type": "Documentation", "change_id": "pull request #14", "change_description": ": Add more guidance in the analyis documentation\npackage overview.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Update wiki-query-syntax link to point towards : https://cwiki.apache.org/confluence/display/solr/Query+Syntax+and+Parsing Along the way I've updated the Solr query syntax link also within several solrconfig.xml files.", "patch_link": "none", "patch_content": "none"}

{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8662", "change_description": ": TermsEnum.seekExact(BytesRef) to abstract and delegate seekExact(BytesRef)\nin FilterLeafReader.FilterTermsEnum.", "change_title": "Change TermsEnum.seekExact(BytesRef) to abstract + delegate seekExact(BytesRef) in FilterLeafReader.FilterTermsEnum", "detail_type": "Improvement", "detail_affect_versions": "5.5.5,6.6.5,7.6,8.0", "detail_fix_versions": "8.0", "detail_description": "Recently in our production, we found that Solr uses a lot of memory(more than 10g) during recovery or commit for a small index (3.5gb)  The stack trace is:  We reproduced the problem locally with the following code using Lucene code.  I added System.out.println(\"ord: \" + ord); in codecs.blocktree.SegmentTermsEnum.getFrame(int). Please check the attached output of test program.txt.  We found out the root cause: we didn't implement seekExact(BytesRef) method in FilterLeafReader.FilterTerms, so it uses the base class TermsEnum.seekExact(BytesRef) implementation which is very inefficient in this case. The fix is simple, just override seekExact(BytesRef) method in FilterLeafReader.FilterTerms", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8469", "change_description": ": Deprecated StringHelper.compare has been removed.", "change_title": "Inline calls to the deprecated StringHelper.compare", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "In an attempt to limit the number of warnings during compilation I though it'd be nice to clean up our own stuff. This is a start: StringHelper.compare is used throughout the code and is delegated to FutureArrays (where it belongs, as the arguments are byte[], not Strings). This can cause other patches to not apply anymore... so we could apply this to master only. If anybody has a strong feeling about it, please voice it. The patch is trivial.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937669/LUCENE-8469.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8039", "change_description": ": Introduce a \"delta distance\" method set to GeoDistance.  This\nallows distance calculations, especially for paths, to take into account an\n\"excursion\" to include the specified point.", "change_title": "Geo3d: Need a \"distance delta\" distance metric for paths and circles", "detail_type": "Improvement", "detail_affect_versions": "7.1", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "The current \"distance\" method for a path returns a distance computed along the path and then perpendicular to the path.  But, at least in the case of paths, it is often preferable to compute a \"delta\" distance, which would be the minimum straight-line distance assuming a diversion to reach the provided point. A similar \"distance delta\" for a circle would be defined as returning a number exactly the same as is currently returned, with the understanding that the point given would be the destination and not a new waypoint.  Similarly, the distance beyond the end of a path to the provided point would be counted only once, while the distance before the beginning of the path would be counted twice (one leg to the point, and the other leg back from that point to the start point (or nearest path point, if closer). This obviously must be implemented in a backwards-compatible fashion.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8007", "change_description": ": Index statistics Terms.getSumDocFreq(), Terms.getDocCount() are\nnow required to be stored by codecs. Additionally, TermsEnum.totalTermFreq()\nand Terms.getSumTotalTermFreq() are now required: if frequencies are not\nstored they are equal to TermsEnum.docFreq() and Terms.getSumDocFreq(),\nrespectively, because all freq() values equal 1.", "change_title": "Require that codecs always store totalTermFreq, sumDocFreq and sumTotalTermFreq", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Javadocs allow codecs to not store some index statistics. Given discussion that occurred on LUCENE-4100, this was mostly implemented this way to support pre-flex codecs. We should now require that all codecs store these statistics.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12895543/LUCENE-8007.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8038", "change_description": ": Deprecated PayloadScoreQuery constructors have been removed", "change_title": "Decouple payload decoding from Similarity", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "PayloadScoreQuery is the only place that currently uses SimScorer.computePayloadFactor(), and as discussed on LUCENE-8014, this seems like the wrong place for it.  We should instead add a PayloadDecoder abstraction that is passed to PayloadScoreQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896171/LUCENE-8038-master.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8014", "change_description": ": Similarity.computeSlopFactor() and\nSimilarity.computePayloadFactor() have been removed", "change_title": "Remove SimScorer.computeSlopFactor and SimScorer.computePayloadFactor", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "This supersedes LUCENE-8013. We should hardcode computeSlopFactor to 1/(N+1) in SloppyPhraseScorer and move computePayloadFactor to PayloadFunction so that all the payload scoring logic is in a single place.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12897065/LUCENE-8014-tfidfsim.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-7996", "change_description": ": Queries are now required to produce positive scores.", "change_title": "Should we require positive scores?", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Having worked on MAXSCORE recently, things would be simpler if we required that scores are positive. Practically, this would mean So I'd be curious to have opinions whether this would be a sane requirement or whether we need to be able to cope with negative scores eg. because some similarities that we want to support produce negative scores by design.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12900711/LUCENE-7996.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8099", "change_description": ": CustomScoreQuery, BoostedQuery and BoostingQuery have been\nremoved", "change_title": "Deprecate CustomScoreQuery, BoostedQuery and BoostingQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.3", "detail_description": "After LUCENE-7998, these three queries can all be replaced by a FunctionScoreQuery.  Using lucene-expressions makes them much easier to use as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12902659/LUCENE-8099.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8012", "change_description": ": Explanation now takes Number rather than float", "change_title": "Improve Explanation class", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Explanation class is currently nice and simple, and float matches the scoring api, but this does not work well for debugging numerical errors of internal calculations (it usually makes practical sense to use 64-bit double to avoid issues). Also it makes for nasty formatting of integral values such as number of tokens in the collection or even document's length, its just noise to see 10.0 there instead of 10, and scientific notation for e.g. number of documents is just annoying. One idea is to take Number instead of float? Then you could pass in the correct numeric type (int,long,double,float) for internal calculations, parameters, statistics, etc, and output would look nice.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12901937/LUCENE-8012.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8116", "change_description": ": SimScorer now only takes a frequency and a norm as per-document\nscoring factors.", "change_title": "Similarity scores should depend only on freq and norm", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "I would like to enforce that scores only depend on the freq and the norm so that we can index impacts into postings list (LUCENE-4198) and make TermScorer leverage them.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12904583/LUCENE-8116.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8113", "change_description": ": TermContext has been renamed to TermStates, and can now be\nconstructed lazily if term statistics are not required", "change_title": "Allow terms dictionary lookups to be lazy when scores are not needed", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "LUCENE-7311 made it possible to avoid loading TermStates in cached TermQueries.  It would be useful to extend this to other queries that use the terms dictionary.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12904801/LUCENE-8113-rename.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8242", "change_description": ": Deprecated method IndexSearcher#createNormalizedWeight() has\nbeen removed", "change_title": "Rename IndexSearcher.createNormalizedWeight()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "We don't have Weight normalization since LUCENE-7368, so this method name is just plain wrong.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917944/LUCENE-8242.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8267", "change_description": ": Memory codecs removed from the codebase (MemoryPostings,\nMemoryDocValues).", "change_title": "Remove memory codecs from the codebase", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Memory codecs (MemoryPostings*, MemoryDocValues*) are part of random selection of codecs for tests and cause occasional OOMs when a test with huge data is selected. We don't use those memory codecs anywhere outside of tests, it has been suggested to just remove them to avoid maintenance costs and OOMs in tests. [1] [1] https://apache.markmail.org/thread/mj53os2ekyldsoy3", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922411/LUCENE-8267.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8144", "change_description": ": Moved QueryCachingPolicy.ALWAYS_CACHE to the test framework.", "change_title": "Remove QueryCachingPolicy.ALWAYS_CACHE", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "This one is trappy, it looks simple and cool but caching without evidence of reuse is usually a bad idea as it removes the ability to skip over non interesting documents.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924967/LUCENE-8144.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8356", "change_description": ": StandardFilter and StandardFilterFactory have been removed", "change_title": "Remove StandardFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "StandardFilter does literally nothing, and is included all over the place, presumably for historical reasons.  We should just nuke it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927946/LUCENE-8356-solr.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8373", "change_description": ": StandardAnalyzer.ENGLISH_STOP_WORD_SET has been removed", "change_title": "Move ENGLISH_STOP_WORD_SET from StandardAnalyzer to EnglishAnalyzer", "detail_type": "New Feature", "detail_affect_versions": "8.0", "detail_fix_versions": "7.5,8.0", "detail_description": "Follow-up of LUCENE-7444.  English stopwords should be on the EnglishAnalyzer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929724/LUCENE-8373-master.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8388", "change_description": ": Unused PostingsEnum#attributes() method has been removed", "change_title": "Deprecate and remove PostingsEnum#attributes()", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "This method isn't used anywhere in the codebase, and seems to be entirely useless.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12930535/LUCENE-8388-7x.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8405", "change_description": ": TopDocs.maxScore is removed. IndexSearcher and TopFieldCollector\nno longer have an option to compute the maximum score when sorting by field.", "change_title": "Remove TopHits.maxScore", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "I would like to propose removing TopDocs.maxScore. The reasoning is that either you are sorting by score and then its value is easy to access via the score of the best hit. Or you sort by one or more fields and computing it is wasteful: It would be more efficient to collect hits twice: once with scores disabled to get the top hits, and once to get the best score which would run efficiently thanks to impacts and MAXSCORE, especially with a size of 1: The doDocScores option of TopFieldCollector has drawbacks as well but at least doesn't disable early-termination optimizations and doesn't require scores to be computed on every hit. As this would be a significant breaking change, I'm targeting 8.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931903/LUCENE-8405.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8411", "change_description": ": TopFieldCollector no longer takes a fillFields option, it now\nalways fills fields.", "change_title": "Remove fillFields from TopFieldCollector factory methods", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Adding sort values to FieldDoc instances is cheap, so there is no reason to disable it. It's also important eg. to merge top hits from multiple shards with TopDocs#merge.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12932050/LUCENE-8411.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8412", "change_description": ": TopFieldCollector no longer takes a trackDocScores option. Scores\nneed to be set on top hits via TopFieldCollector#populateScores instead.", "change_title": "Remove trackDocScores from TopFieldCollector factory methods", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Computing scores on top hits is fine, but the current way it is implemented - at collection time - requires to read/decode more freqs/norms and compute more scores than necessary. It would be more efficient to compute scores of top hits as a post-collection step by only advancing the scorer to hits that made the top-N list.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12932100/LUCENE-8412.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-6228", "change_description": ": A new Scorable abstract class has been added, containing only those\nmethods from Scorer that should be called from Collectors.  LeafCollector.setScorer()\nnow takes a Scorable rather than a Scorer.", "change_title": "Do not expose full-fledged scorers in LeafCollector.setScorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Currently LeafCollector.setScorer takes a Scorer, which I don't like because several methods should never be called in the context of a Collector (like nextDoc or advance). I think it's even more trappy for methods that might seem to work in some particular cases but will not work in the general case, like getChildren which will not work if you have a specialized BulkScorer or iterating over positions which will not work if you are in a MultiCollector and another leaf collector consumes positions too. So I think we should restrict what can be seen from a collector to avoid such traps.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938158/LUCENE-6228.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8475", "change_description": ": Deprecated constants have been removed from RamUsageEstimator.", "change_title": "Remove deprecated constants in RamUsageEstimator", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "RamUsageEstimator has constants for specific type memory calculation that have been deprecated and should be removed in master (8.0).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937931/LUCENE-8475.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8483", "change_description": ": Scorers may no longer take null as a Weight", "change_title": "Enforce that Scorer cannot have a null Weight", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Follow up to LUCENE-6228.  All Scorers that were passing null as their parent Weights can be replaced by Scorables, so we can now enforce that Scorer must not have a null Weight.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938435/LUCENE-8483.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8352", "change_description": ": TokenStreamComponents is now final, and can take a Consumer<Reader>\nin its constructor", "change_title": "Make TokenStreamComponents final", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "The current design is a little trappy. Any specialised subclasses of TokenStreamComponents (see StandardAnalyzer, ClassicAnalyzer, UAX29URLEmailAnalyzer) are discarded by any subsequent Analyzers that wrap them (see LimitTokenCountAnalyzer, QueryAutoStopWordAnalyzer, ShingleAnalyzerWrapper and other examples in elasticsearch). The current design means each AnalyzerWrapper.wrapComponents() implementation discards any custom TokenStreamComponents and replaces it with one of its own choosing (a vanilla TokenStreamComponents class from examples I've seen). This is a trap I fell into when writing a custom TokenStreamComponents with a custom setReader() and I wondered why it was not being triggered when wrapped by other analyzers. If AnalyzerWrapper is designed to encourage composition it's arguably a mistake to also permit custom TokenStreamComponent subclasses  - the composition process does not preserve the choice of custom classes and any behaviours they might add. For this reason we should not encourage extensions to TokenStreamComponents (or if TSC extensions are required we should somehow mark an Analyzer as \"unwrappable\" to prevent lossy compositions).  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940248/LUCENE-8352.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8498", "change_description": ": LowerCaseTokenizer has been removed, and CharTokenizer no longer\ntakes a normalizer function.", "change_title": "Deprecate/Remove LowerCaseTokenizer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "LowerCaseTokenizer combines tokenization and filtering in a way that prevents us improving the normalization API.  We should deprecate and remove it, as it can be replaced simply with a LetterTokenizer and LowerCaseFilter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940478/LUCENE-8498.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-7875", "change_description": ": Moved MultiFields static methods out of the class.  getLiveDocs is now\nin MultiBits which is now public.  getMergedFieldInfos and getIndexedFields are now in\nFieldInfos.  getTerms is now in MultiTerms.  getTermPositionsEnum and getTermDocsEnum\nwere collapsed and renamed to just getTermPostingsEnum and moved to MultiTerms.", "change_title": "Rename or move most of MultiFields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "MultiFields.java has a bunch of static methods that provide a single LeafReader's view over a bunch of things. This goes to MultiBits (which will become public): These go to FieldInfos: These go to MultiTerms:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12943648/LUCENE-7875.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8513", "change_description": ": MultiFields.getFields is now removed.  Please avoid this class,\nand Fields in general, when possible.", "change_title": "Remove MultiFields.getFields and minimize use of MultiFields instance", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "As part of a longer term objective of removing the Fields class, in this issue I focus on minimizing the use of instances of MultiFields such as MultiFields.getFields and using it's constructor.  This is not about the static utility methods here like getTerms.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940998/LUCENE-8513.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8497", "change_description": ": MultiTermAwareComponent has been removed, and in its place\nTokenFilterFactory and CharFilterFactory now expose type-safe normalize()\nmethods.  This decouples normalization from tokenization entirely.", "change_title": "Rethink multi-term analysis handling", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "The current framework for handling term normalisation works via instanceof checks for MultiTermAwareComponent and casts.  MultiTermAwareComponent itself deals in AbstractAnalysisComponents, and so callers need to cast to the correct component type before use, which is ripe for misuse. We should re-organise all this to be type-safe and usable without casts.  One possibility is to add `normalize` methods to CharFilterFactory and TokenFilterFactory that mirror their existing `create` methods.  The default implementation would return the input unchanged, while filters that should apply at normalization time can delegate to `create`. Related to this, we should deprecate and remove LowerCaseTokenizer, which combines tokenization and normalization in a way that will break this API.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946196/LUCENE-8497.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8597", "change_description": ": IntervalIterator now exposes a gaps() method that reports the\nnumber of gaps between its component sub-intervals.  This can be used in a\nnew filter available via Intervals.maxgaps().", "change_title": "Allow filtering of Intervals by their internal gaps", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "We currently allow filtering of intervals by their total size, via the `Intervals.maxwidth` static method.  This works well enough, but there are several cases where you might want to restrict the gap between two intervals, but don't necessarily know the total width.  For example, if you want to know that an unordered pair `term1 term2` with no width restriction is less than two positions away from another term: there is no current way to construct an interval query that would return this. To enable this, I propose adding a `gaps()` method to IntervalIterator, which returns the number of internal gaps between the iterators constituent sub-iterators.  Terms and phrases, would return 0, and ordered/unordered combinations would return the number of positions separating their immediate children.  Note that this does not include the gaps within any of those children themselves.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12951125/LUCENE-8597.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8609", "change_description": ": Remove IndexWriter#numDocs() and IndexWriter#maxDoc() in favor\nof IndexWriter#getDocStats().", "change_title": "Allow getting consistent docstats from IndexWriter", "detail_type": "Improvement", "detail_affect_versions": "7.7,8.0", "detail_fix_versions": "7.7,8.0", "detail_description": "Today we have #numDocs() and #maxDoc() on IndexWriter. This is enough     to get all stats for the current index but it's subject to concurrency     and might return numbers that are not consistent ie. some cases can     return maxDoc < numDocs which is undesirable. This change adds a getDocStats()     method to index writer to allow fetching consistent numbers for these stats.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "API Changes", "change_id": "LUCENE-8292", "change_description": ": Make TermsEnum fully abstract.", "change_title": "Fix FilterLeafReader.FilterTermsEnum to delegate all seekExact methods", "detail_type": "Bug", "detail_affect_versions": "7.2.1", "detail_fix_versions": "trunk,8.0,8.x,9.0", "detail_description": "FilterLeafReader#FilterTermsEnum wraps another TermsEnum and delegates many methods. It misses some seekExact() methods, thus it is not possible to the delegate to override these methods to have specific behavior (unlike the TermsEnum API which allows that). The fix is straightforward: simply override these seekExact() methods and delegate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12921730/0001-Fix-FilterLeafReader.FilterTermsEnum-to-delegate-see.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8333", "change_description": ": Switch MoreLikeThis.setMaxDocFreqPct to use maxDoc instead of\nnumDocs.", "change_title": "Switch MoreLikeThis.setMaxDocFreqPct to use maxDoc instead of numDocs", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12925130/LUCENE-8333.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-7837", "change_description": ": Indices that were created before the previous major version\nwill now fail to open even if they have been merged with the previous major\nversion.", "change_title": "Use indexCreatedVersionMajor to fail opening too old indices", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Even though in theory we only support reading indices created with version N or N-1, in practice it is possible to run a forceMerge in order to make Lucene accept to open the index since we only record the version that wrote segments and commit points. However as of Lucene 7.0, we also record the major version that was used to initially create the index, meaning we could also fail to open N-2 indices that have only been merged with version N-1. The current state of things where we could read old data without knowing it raises issues with everything that is performed on top of the codec API such as analysis, input validation or norms encoding, especially now that we plan to change the defaults (LUCENE-7730). For instance, we are only starting to reject broken offsets in term vectors in Lucene 7. If we do not enforce the index to be created with either Lucene 7 or 8 once we move to Lucene 8, then it means codecs could still be fed with broken offsets, which is a pity since assuming that offsets go forward makes things easier to encode and also potentially allows for better compression.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12875664/LUCENE-7837.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8020", "change_description": ": Similarities are no longer passed terms that don't exist by\nqueries such as SpanOrQuery, so scoring formulas no longer require\ndivide-by-zero hacks.  IndexSearcher.termStatistics/collectionStatistics return null\ninstead of returning bogus values for a non-existent term or field.", "change_title": "Don't force sim to score bogus terms (e.g. docfreq=0)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Today all sim formulas have to be \"hacked\" to deal with the fact that they may be passed stats such as docFreq=0, totalTermFreq=0. This happens easily with spans and there is even a dedicated test for it. All formulas have hacks such as what you see in https://issues.apache.org/jira/browse/LUCENE-6818: Instead of: they must do tricks such as: There is no good reason for this, it is just sloppiness in the Query/Weight/Scorer api. I think formulas should work unmodified, we shouldn't pass terms that dont exist or bogus statistics. It adds a lot of complexity to the scoring api and makes it difficult to have meaningful/useful explanations, to debug problems, etc. It also makes it really hard to add a new sim.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12894594/LUCENE-8020.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-7996", "change_description": ": FunctionQuery and FunctionScoreQuery now return a score of 0\nwhen the function produces a negative value.", "change_title": "Should we require positive scores?", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Having worked on MAXSCORE recently, things would be simpler if we required that scores are positive. Practically, this would mean So I'd be curious to have opinions whether this would be a sane requirement or whether we need to be able to cope with negative scores eg. because some similarities that we want to support produce negative scores by design.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12900711/LUCENE-7996.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8116", "change_description": ": Similarities now score fields that omit norms as if the norm was\n1. This might change score values on fields that omit norms.", "change_title": "Similarity scores should depend only on freq and norm", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "I would like to enforce that scores only depend on the freq and the norm so that we can index impacts into postings list (LUCENE-4198) and make TermScorer leverage them.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12904583/LUCENE-8116.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8134", "change_description": ": Index options are no longer automatically downgraded.", "change_title": "Disallow changing index options on the fly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Follow-up of LUCENE-8031: changing index options is problematic because the way a field is indexed can influence the way the field length should be computed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12907148/LUCENE-8134.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8031", "change_description": ": Length normalization correctly reflects omission of term frequencies.", "change_title": "DOCS_ONLY fields set incorrect length norms", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Term frequencies are discarded in the DOCS_ONLY case from the postings list but they still count against the length normalization, which looks like it may screw stuff up. I ran some quick experiments on LUCENE-8025, by encoding fieldInvertState.getUniqueTermCount() and it seemed worth fixing (e.g. 20% or 30% improvement potentially). Happy to do testing for real, if we want to fix. But this seems tricky, today you can downgrade to DOCS_ONLY on the fly, and its hard for me to think about that case (i think its generally screwed up besides this, but still).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896189/LUCENE-8031.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-7444", "change_description": ": StandardAnalyzer no longer defaults to removing English stopwords", "change_title": "Remove English stopwords default from StandardAnalyzer in Lucene-Core", "detail_type": "Task", "detail_affect_versions": "6.2", "detail_fix_versions": "8.0", "detail_description": "Yonik said on LUCENE-7318: I think it would make a good default for most Lucene users, and we should graduate it from the analyzers module into core, and make it the default for IndexWriter. This \"StandardAnalyzer\" is specific to English, as it removes English stopwords. That seems to be an odd choice now for a few reasons: Given that removal of english stopwords is the only thing that really makes this analyzer english-centric (and given the negative impact that can have on other languages), it seems like the stopword filter should be removed from StandardAnalyzer. When trying to fix the backwards incompatibility issues in LUCENE-7318, it looks like most unrelated code moved from analysis module to core (and changing package names!!!!  ) was related to word list loading, CharArraySets, and superclasses of StopFilter. If we follow Yonik's suggestion, we can revert all those changes. I agree with hin, an \"universal\" analyzer should not have any language specific stop-words. The other thing is LowercaseFilter, but I'd suggest to simply add a clone of it to Lucene core and leave the analysis-module self-contained.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927628/LUCENE-7444.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8060", "change_description": ": IndexSearcher's search and searchAfter methods now only compute\ntotal hit counts accurately up to 1,000 in order to enable top-hits\noptimizations such as block-max WAND (", "change_title": "Enable top-docs collection optimizations by default", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "We are getting optimizations when hit counts are not required (sorted indexes, MAXSCORE, short-circuiting of phrase queries) but our users won't benefit from them unless we disable exact hit counts by default or we require them to tell us whether hit counts are required. I think making hit counts approximate by default is going to be a bit trappy, so I'm rather leaning towards requiring users to tell us explicitly whether they need total hit counts. I can think of two ways to do that: either by passing a boolean to the IndexSearcher constructor or by adding a boolean to all methods that produce TopDocs instances. I like the latter better but I'm open to discussion or other ideas?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933654/LUCENE-8060.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8135", "change_description": ": IndexSearcher's search and searchAfter methods now only compute\ntotal hit counts accurately up to 1,000 in order to enable top-hits\noptimizations such as block-max WAND (", "change_title": "Implement Block-Max WAND", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "This issue is about building on top of LUCENE-4198 in order to leverage block maximum scores instead of global maximum scores. This is documented in \"Faster Top-k Document Retrieval Using Block-Max Indexes\" (http://engineering.nyu.edu/~suel/papers/bmw.pdf) and called BMW (Block-Max WAND).  Using block max scores adds overhead to scorers, but also provides better upper bounds of the scores and is expected to remain efficient in presence of outliers (LUCENE-8087).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12907311/LUCENE-8135.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8505", "change_description": ": IndexWriter#addIndices will now fail if the target index is sorted but\nthe candidate is not.", "change_title": "IndexWriter#addIndices should not sort indices if they are not already sorted", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Today IndexWriter#addIndices silently re-sort non-sorted indices when they are added in a sorted index. This is not safe because the sort is done entirely in memory and cannot handle big segments efficiently. This leniency was added because prior to 6.5, segments produced by flushes were not sorted, they had to wait for a merge to apply the index sorting. Now that segments are always sorted (LUCENE-7579) we should remove this ability and throw an error if the sort of the current index does not match with the candidate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12939986/LUCENE-8505.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8535", "change_description": ": Highlighter and FVH doesn't support ToParent and ToChildBlockJoinQuery out of the\nbox anymore. In order to highlight on Block-Join Queries a custom WeightedSpanTermExtractor / FieldQuery\nshould be used.", "change_title": "Should we drop support for highlighting block-join queries", "detail_type": "Improvement", "detail_affect_versions": "8.0", "detail_fix_versions": "8.0", "detail_description": "This is a spin-off from LUCENE-6572. We currently depend on the block-join module which is due to the fact that we try to highlight the queries wrapped by the block join queries. The current discussion on LUCENE-6572 mentioned that this doesn't make much sense from an highlighting perspecitve and if we should drop support for it. Lucene 8.0 would be a good time to do so. Thoughts?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8563", "change_description": ": BM25 scores don't include the (k1+1) factor in their numerator\nanymore. This doesn't affect ordering as this is a constant factor which is\nthe same for every document.", "change_title": "Remove k1+1 from the numerator of  BM25Similarity", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Our current implementation of BM25 does As (k1+1) is a constant, it is the same for every term and doesn't modify ordering. It is often omitted and I found out that the \"The Probabilistic Relevance Framework: BM25 and Beyond\" paper by Robertson (BM25's author) and Zaragova even describes adding (k1+1) to the numerator as a variant whose benefit is to be more comparable with Robertson/Sparck-Jones weighting, which we don't care about. A common variant is to add a (k1 + 1) component to the  numerator of the saturation function. This is the same for all  terms, and therefore does not affect the ranking produced.  The reason for including it was to make the final formula  more compatible with the RSJ weight used on its own Should we remove it from BM25Similarity as well? A side-effect that I'm interested in is that integrating other score contributions (eg. via oal.document.FeatureField) would be a bit easier to reason about. For instance a weight of 3 in FeatureField#newSaturationQuery would have a similar impact as a term whose IDF is 3 (and thus docFreq ~= 5%) rather than a term whose IDF is 3/(k1 + 1).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8509", "change_description": ": WordDelimiterGraphFilter will no longer set the offsets of internal\ntokens by default, preventing a number of bugs when the filter is chained with\ntokenfilters that change the length of their tokens", "change_title": "NGramTokenizer, TrimFilter and WordDelimiterGraphFilter in combination can produce backwards offsets", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Discovered by an elasticsearch user and described here: https://github.com/elastic/elasticsearch/issues/33710 The ngram tokenizer produces tokens \"a b\" and \" bb\" (note the space at the beginning of the second token).  The WDGF takes the first token and splits it into two, adjusting the offsets of the second token, so we get \"a\"[0,1] and \"b\"[2,3].  The trim filter removes the leading space from the second token, leaving offsets unchanged, so WDGF sees \"bb\"[1,4]; because the leading space has already been stripped, WDGF sees no need to adjust offsets, and emits the token as-is, resulting in the start offsets of the tokenstream being [0, 2, 1], and the IndexWriter rejecting it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12948735/LUCENE-8509.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8633", "change_description": ": IntervalQuery scores do not use term weighting any more, the score\nis instead calculated as a function of the sloppy frequency of the matching\nintervals.", "change_title": "Remove term weighting from interval scoring", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0,9.0", "detail_description": "IntervalScorer currently uses the same scoring mechanism as SpanScorer, summing the IDF of all possibly matching terms from its parent IntervalsSource and using that in conjunction with a sloppy frequency to produce a similarity-based score.  This doesn't really make sense, however, as it means that terms that don't appear in a document can still contribute to the score, and appears to make scores from interval queries comparable with scores from term or phrase queries when they really aren't. I'd like to explore a different scoring mechanism for intervals, based purely on sloppy frequency and ignoring term weighting.  This should make the scores easier to reason about, as well as making them useful for things like proximity boosting on boolean queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12954816/LUCENE-8633.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8635", "change_description": ": FSTs can now remain off-heap, accessed via\nIndexInput, and the default codec's term dictionary\n(BlockTreeTermsReader) will now leave the FST for the terms index\noff-heap for non-primary-key fields using MMapDirectory, reducing\nheap usage for such fields.", "change_title": "Lazy loading Lucene FST offheap using mmap", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.0,8.x,9.0", "detail_description": "Currently, FST loads all the terms into heap memory during index open. This causes frequent JVM OOM issues if the term size gets big. A better way of doing this will be to lazily load FST using mmap. That ensures only the required terms get loaded into memory. Lucene can expose API for providing list of fields to load terms offheap. I'm planning to take following approach for this: I created a patch (that loads all fields offheap), did some benchmarks using es_rally and results look good.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956792/fst-offheap-rev.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8340", "change_description": ": LongPoint#newDistanceFeatureQuery may be used to boost scores based on\nhow close a value of a long field is from an configurable origin. This is\ntypically useful to boost by recency.", "change_title": "Efficient boosting by recency", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "I would like that we support something like FeatureField.newSaturationQuery but that works with features that are computed dynamically like recency or geo-distance, and is still optimized for top-hits collection. I'm starting with recency because it makes things a bit easier even though I suspect that geo-distance might be a more common need.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12925908/LUCENE-8340.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8482", "change_description": ": LatLonPoint#newDistanceFeatureQuery may be used to boost scores\nbased on the haversine distance of a LatLonPoint field to a provided point. This is\ntypically useful to boost by distance.", "change_title": "Boosting by geo distance", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Similarly to LUCENE-8340 it would be nice to have an easy and efficient way to fold geo distance into scoring formulas in order to boost by proximity.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940356/LUCENE-8482.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8216", "change_description": ": Added a new BM25FQuery in sandbox to blend statistics across several fields\nusing the BM25F formula.", "change_title": "Better cross-field scoring", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "I'd like Lucene to have better support for scoring across multiple fields. Today we have BlendedTermQuery which tries to help there but it probably tries to do too much on some aspects (handling cross-field term queries AND synonyms) and too little on other ones (it tries to merge index-level statistics, but not per-document statistics like tf and norm). Maybe we could implement something like BM25F so that queries across multiple fields would retain the benefits of BM25 like the fact that the impact of the term frequency saturates quickly, which is not the case with BlendedTermQuery if you have occurrences across many fields.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12948852/LUCENE-8216.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8564", "change_description": ": GraphTokenFilter is an abstract class useful for token filters that need\nto read-ahead in the token stream and take into account graph structures.  This\nalso changes FixedShingleFilter to extend GraphTokenFilter", "change_title": "Make it easier to iterate over graphs in tokenstreams", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "We have a number of TokenFilters that read ahead in the token stream (eg synonyms, shingles) and ideally these would understand token graphs as well as linear streams.  FixedShingleFilter already has some mechanisms to deal with graphs; this issue is to extract this logic into a GraphTokenStream class that can then be reused by other token filters", "patch_link": "https://issues.apache.org/jira/secure/attachment/12950384/LUCENE-8564.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8612", "change_description": ": Intervals.extend() treats an interval as if it covered a wider\nspan than it actually does, allowing users to force minimum gaps between\nintervals in a phrase.", "change_title": "Add the ability to enforce gaps between intervals", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "At the moment you can search for intervals with a maximum number of positions between them, but you cannot enforce gaps.  It would be useful to be able to search for `a b [2 spaces] c`.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12952990/LUCENE-8612.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8629", "change_description": ": New interval functions: Intervals.before(), Intervals.after(),\nIntervals.within() and Intervals.overlapping().", "change_title": "Add some more Interval functions", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "There are a few missing functions from the current group of IntervalsSource definitions available:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12953742/LUCENE-8629.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8622", "change_description": ": Adds a minimum-should-match interval function that produces intervals\nspanning a subset of a set of sources.", "change_title": "Add a MinimumShouldMatch interval iterator", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "It would be useful to be able to search for intervals that span some subgroup of a set of iterators, allowing us to build a 'some of ' or 'at least' operator - ie, search for terms that appear near at least 3 of a list of 5 terms.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12953761/LUCENE-8622.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8645", "change_description": ": Intervals.fixField() allows you to report intervals from one field\nas if they came from another.", "change_title": "Add fixed field intervals", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "It can be useful to report intervals from one fields as if they came from another.  For example, fast prefix searches can be implemented by indexing text into two fields, one with the full terms and one with edge-ngrams enabled; to do proximity searches against terms and prefixes, you could wrap a term query against the ngrammed field so that its intervals appear to come from the normal field, and use it an an ordered or unordered interval. This is analogous to the FieldMaskingSpanQuery, but has the advantage that we don't use term statistics for scoring interval queries, so there is no issue with mixing up field weights from different fields.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955248/LUCENE-8645.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8646", "change_description": ": New interval functions: Intervals.prefix() and Intervals.wildcard()", "change_title": "Add multi-term intervals", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "We currently have no support for wildcard-type intervals.  I'd like to explore adding some very basic support for prefix and wildcard interval sources, but we need to ensure that we don't end up with the same performance issues that dog SpanMultiTermQueryWrapper.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955250/LUCENE-8646.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8655", "change_description": ": Add a getter in FunctionScoreQuery class in order to access to the\nunderlying DoubleValuesSource.", "change_title": "No possibility to access to the underlying \"valueSource\" of a FunctionScoreQuery", "detail_type": "Improvement", "detail_affect_versions": "7.6", "detail_fix_versions": "8.0", "detail_description": "After LUCENE-8099, the \"BoostedQuery\" is deprecated by the use of the \"FunctionScoreQuery\". With the BoostedQuery, it was possible to access at its underlying \"valueSource\". But it is not the case with the class \"FunctionScoreQuery\". It has got only a getter for the wrapped query, For development of specific parsers, it would be necessary to access the valueSource of a \"FunctionScoreQuery\". I suggest to add a new getter into the class \"FunctionScoreQuery\" like below:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12957454/LUCENE-8655.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8697", "change_description": ": GraphTokenStreamFiniteStrings correctly handles side paths\ncontaining gaps", "change_title": "GraphTokenStreamFiniteStrings does not correctly handle gaps in the token graph", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.0,9.0", "detail_description": "Currently, side-paths with gaps in can end up being missed entirely when iterating through token streams.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12959127/LUCENE-8697.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "New Features", "change_id": "LUCENE-8702", "change_description": ": Simplify intervals returned from vararg Intervals factory methods", "change_title": "Simplify some IntervalsSources returned by Intervals factories", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "There are a number of Intervals factory methods that take an array of inputs.  We can shortcut cases where the input is just a single IntervalsSource by returning that source directly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12959423/LUCENE-8702.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Improvements", "change_id": "LUCENE-7997", "change_description": ": Add BaseSimilarityTestCase to sanity check similarities.\nSimilarityBase switches to 64-bit doubles internally to help avoid common numeric issues.\nAdd missing range checks for similarity parameters.\nImprove BM25 and ClassicSimilarity's explanations.", "change_title": "More sanity testing of similarities", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "LUCENE-7993 is a potential optimization that we could only apply if the similarity is an increasing functions of freq (all other things like DF and length being equal). This sounds like a very reasonable requirement for a similarity, so we should test it in the base similarity test case and maybe move broken similarities to sandbox?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12893847/LUCENE-7997.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Improvements", "change_id": "LUCENE-8011", "change_description": ": Improved similarity explanations.", "change_title": "Improve similarity explanations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "LUCENE-7997 improves BM25 and Classic explains to better explain: Previously it was pretty cryptic and used confusing terminology like docCount/docFreq without explanation: We should fix other similarities too in the same way, they should be more practical.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Improvements", "change_id": "LUCENE-4198", "change_description": ": Codecs now have the ability to index score impacts.", "change_title": "Allow codecs to index term impacts", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Subtask of LUCENE-4100. Thats an example of something similar to impact indexing (though, his implementation currently stores a max for the entire term, the problem is the same). We can imagine other similar algorithms too: I think the codec API should be able to support these. Currently it really doesnt: Stefan worked around the problem by providing a tool to 'rewrite' your index, he passes the IndexReader and Similarity to it. But it would be better if we fixed the codec API. One problem is that the Postings writer needs to have access to the Similarity. Another problem is that it needs access to the term and collection statistics up front, rather than after the fact. This might have some cost (hopefully minimal), so I'm thinking to experiment in a branch with these changes and see if we can make it work well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12906832/LUCENE-4198.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Improvements", "change_id": "LUCENE-8135", "change_description": ": Boolean queries now implement the block-max WAND algorithm in\norder to speed up selection of top scored documents.", "change_title": "Implement Block-Max WAND", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "This issue is about building on top of LUCENE-4198 in order to leverage block maximum scores instead of global maximum scores. This is documented in \"Faster Top-k Document Retrieval Using Block-Max Indexes\" (http://engineering.nyu.edu/~suel/papers/bmw.pdf) and called BMW (Block-Max WAND).  Using block max scores adds overhead to scorers, but also provides better upper bounds of the scores and is expected to remain efficient in presence of outliers (LUCENE-8087).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12907311/LUCENE-8135.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Improvements", "change_id": "LUCENE-8279", "change_description": ": CheckIndex now cross-checks terms with norms.", "change_title": "Improve CheckIndex on norms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "We should improve CheckIndex to make sure that terms and norms agree on which documents have a value on an indexed field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12921001/LUCENE-8279.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Improvements", "change_id": "LUCENE-8660", "change_description": ": TopDocsCollectors now return an accurate count (instead of a lower bound)\nif the total hit count is equal to the provided threshold.", "change_title": "Include totalHitsThreshold when tracking total hits in TopDocsCollector", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Today the total hits threshold in the top docs collector is not inclusive, this means that total hits are tracked up to totalHitsThreshold-1. After discussing with @jpountz we agreed that this is not intuitive to return a lower bound that is equal to totalHitsThreshold even if the count is accurate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956564/LUCENE-8660.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8040", "change_description": ": Optimize IndexSearcher.collectionStatistics, avoiding MultiFields/MultiTerms", "change_title": "Optimize IndexSearcher.collectionStatistics", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "IndexSearcher.collectionStatistics(field) can do a fair amount of work because with each invocation it will call MultiFields.getTerms(...).  The effects of this are aggravated for queries with many fields since each field will want statistics, and also aggravated when there are many segments.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12897636/LUCENE-8040.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-4100", "change_description": ": Disjunctions now support faster collection of top hits when the\ntotal hit count is not required.", "change_title": "Maxscore - Efficient Scoring", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "8.0", "detail_description": "At Berlin Buzzwords 2012, I will be presenting 'maxscore', an efficient algorithm first published in the IR domain in 1995 by H. Turtle & J. Flood, that I find deserves more attention among Lucene users (and developers). I implemented a proof of concept and did some performance measurements with example queries and lucenebench, the package of Mike McCandless, resulting in very significant speedups. This ticket is to get started the discussion on including the implementation into Lucene's codebase. Because the technique requires awareness about it from the Lucene user/developer, it seems best to become a contrib/module package so that it consciously can be chosen to be used.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12900042/LUCENE-4100.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7993", "change_description": ": Phrase queries are now faster if total hit counts are not\nrequired.", "change_title": "Speed up phrase queries when total hit count is not needed", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Follow-up of LUCENE-4100: When thinking about the API that we needed to introduce to support MAXSCORE, I wondered whether the same API could support other optimizations. The idea is that when running phrase queries, before we start reading positions, we already have access to the term frequency of each term. And the frequency of the phrase is bounded by the minimum term frequency of the involved terms. So if the score for that minimum term frequency is not competitive then it means that the score for the phrase is not competitive either if we can assume that the score increases (or stagnates) when the term freq increases, which sounds like an ok requirement for a sane Similarity?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12901072/LUCENE-7993.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8109", "change_description": ": Boolean queries propagate information about the minimum\ncompetitive score in order to make collection faster if there are disjunctions\nor phrase queries as sub queries, which know how to leverage this information\nto run faster.", "change_title": "Propagate minimum competitive scores in BooleanQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Propagating information about the minimum competitive score means that we will also see speedups for conjunctions of disjunctions, or disjunctions of phrase queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12904008/LUCENE-8109.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8439", "change_description": ": Disjunction max queries can skip blocks to select the top documents\nif the total hit count is not required.", "change_title": "DisjunctionMaxScorer should leverage sub scorers' per-block max scores", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "This issue is similar to https://issues.apache.org/jira/browse/LUCENE-8204 but for the DisjunctionMaxScorer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12934789/LUCENE-8439.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8204", "change_description": ": Boolean queries with a mix of required and optional clauses are\nnow faster if the total hit count is not required.", "change_title": "ReqOptSumScorer should leverage sub scorers' per-block max scores", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Currently it only looks at max scores on the entire segment. Given that per-block max scores usually give lower upper bounds of the score, this should help. This is especially important for LUCENE-8197 to work well since the main query would typically be added as a MUST clauses of a boolean query while the query that scores on features would be a SHOULD clause.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12934795/LUCENE-8204.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8448", "change_description": ": Boolean queries now propagates the mininum score to their sub-scorers.", "change_title": "Slowdown of nested boolean queries after LUCENE-8060", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "Mike's nightly benchmarks revealed that disabling hit counts slowed down nested boolean queries http://people.apache.org/~mikemccand/lucenebench/AndHighOrMedMed.html http://people.apache.org/~mikemccand/lucenebench/AndMedOrHighHigh.html. We are probably not propagating max scores and/or blocks efficiently.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12935581/LUCENE-8448.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8511", "change_description": ": MultiFields.getIndexedFields is now optimized; does not call getMergedFieldInfos", "change_title": "MultiFields.getIndexedFields can be optimized to not use getMergedFieldInfos", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "MultiFields.getIndexedFields calls getMergedFieldInfos.  But getMergedFieldInfos is kinda heavy, doing all sorts of stuff that getIndexedFields doesn't care about.  It can simply loop the leaf readers and collect the results into a Set.  Java 8 streams should make easy work of this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940599/LUCENE-8511.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8507", "change_description": ": TopFieldCollector can now update the minimum competitive score if the primary sort\nis by relevancy and the total hit count is not required.", "change_title": "TopFieldCollector should set minimum competitive score if the primary sort is by relevancy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "When the primary sort in the TopFieldCollector is set to relevancy it is possible to update the minimum competitive score like the TopScoreCollector does.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940424/LUCENE-8507.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8464", "change_description": ": ConstantScoreScorer now implements setMinCompetitveScore in order\nto early terminate the iterator if the minimum score is greater than the constant\nscore.", "change_title": "Implement ConstantScoreScorer#setMinCompetitiveScore", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "We should make it so the iterator returns NO_MORE_DOCS after setMinCompetitiveScore is called with a value that is greater than the constant score.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8607", "change_description": ": MatchAllDocsQuery can shortcut when total hit count is not\nrequired", "change_title": "Allow MatchAllDocsQuery to skip counting hits", "detail_type": "Task", "detail_affect_versions": "8.0", "detail_fix_versions": "8.0", "detail_description": "MatchAllDocsQuery currently uses a private bulk scorer with no specialisations for setMinCompetitiveScore().  We've seen what looks to be something like a halving of the performance of MatchAllDocsQuery in elasticsearch benchmarks running on 8.0 snapshots, and it looks as though this is because it's paying the price of keeping track of competitive scores, but not actually making use of the new infrastructure.  We should modify the bulk scorer to early-terminate if setMinCompetitiveScore() is called with a value greater than the query's boost.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12951527/LUCENE-8607.patch", "patch_content": "none"}
{"library_version": "8.0.0", "change_type": "Optimizations", "change_id": "LUCENE-8585", "change_description": ": Index-time jump-tables for DocValues, for O(1) advance when retrieving doc values.", "change_title": "Create jump-tables for DocValues at index-time", "detail_type": "Improvement", "detail_affect_versions": "8.0", "detail_fix_versions": "8.0", "detail_description": "As noted in LUCENE-7589, lookup of DocValues should use jump-tables to avoid long iterative walks. This is implemented in LUCENE-8374 at search-time (first request for DocValues from a field in a segment), with the benefit of working without changes to existing Lucene 7 indexes and the downside of introducing a startup time penalty and a memory overhead. As discussed in LUCENE-8374, the codec should be updated to create these jump-tables at index time. This eliminates the segment-open time & memory penalties, with the potential downside of increasing index-time for DocValues. The three elements of LUCENE-8374 should be transferable to index-time without much alteration of the core structures: I have no experience with the codec-parts responsible for creating index-structures. I'm quite willing to take a stab at this, although I probably won't do much about it before January 2019. Should anyone else wish to adopt this JIRA-issue or co-work on it, I'll be happy to share.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12951095/LUCENE-8585.patch", "patch_content": "none"}

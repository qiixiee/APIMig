{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-12092", "change_description": ": Remove deprecated UTF8TaxonomyWriterCache. Please use LruTaxonomyWriterCache\ninstead.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10010", "change_description": ": AutomatonQuery, CompiledAutomaton, RunAutomaton, RegExp\nclasses no longer determinize NFAs. Instead it is the responsibility\nof the caller to determinize.", "change_title": "Should we have a NFA Query?", "detail_type": "New Feature", "detail_affect_versions": "9.0", "detail_fix_versions": "None", "detail_description": "Today when a RegexpQuery is created, it will be translated to NFA, determinized to DFA and eventually become an AutomatonQuery, which is very fast. However, not every NFA could be determinized to DFA easily, the example given in LUCENE-9981 showed how easy could a short regexp break the determinize process. Maybe, instead of marking those kind of queries as adversarial cases, we could make a new kind of NFA query, which execute directly on NFA and thus no need to worry about determinize process or determinized DFA size. It should be slower, but also makes those adversarial cases doable. This article has provided a simple but efficient way of searching over NFA, essentially it is a partial determinize process that only determinize the necessary part of DFA. Maybe we could give it a try?", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10368", "change_description": ": IntTaxonomyFacets has been make pkg-private and serves only as an internal\nimplementation detail of taxonomy-faceting.", "change_title": "Reduce visibility of IntTaxonomyFacets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Opening this issue as a follow up from the conversation over here: https://github.com/apache/lucene/pull/578#discussion_r779353869 I'd propose we reduce the visibility to pkg-private and do a little cleanup. This is essentially indicating that IntTaxonomyFacets is an internal implementation detail of taxonomy faceting (shared common logic) as opposed to an intentional extension point for users.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10400", "change_description": ": Remove deprecated dictionary constructors in Kuromoji and Nori", "change_title": "Clean up the constructors' API signature of dictionary classes in kuromoji and nori", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "It was suggested in a few issues/pr comments. Before working on LUCENE-8816 or LUCENE-10393, we'd need to sort the protected constructor APIs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10440", "change_description": ": TaxonomyFacets and FloatTaxonomyFacets have been made pkg-private and only serve\nas internal implementation details of taxonomy-faceting.", "change_title": "Reduce visibility of TaxonomyFacets and FloatTaxonomyFacets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Similar to what we did in LUCENE-10379, let's reduce the public visibility of TaxonomyFacets and FloatTaxonomyFacets to pkg-private since they're really implementation details housing common logic and not really intended as extension points for user faceting.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10431", "change_description": ": MultiTermQuery.setRewriteMethod() has been removed.", "change_title": "AssertionError in BooleanQuery.hashCode()", "detail_type": "Bug", "detail_affect_versions": "8.11.1", "detail_fix_versions": "None", "detail_description": "Hello devs, the constructor of BooleanQuery can under some circumstances trigger a hash code computation before \"clauseSets\" is fully filled. Since BooleanClause is using its query field for the hash code too, it can happen that the \"wrong\" hash code is stored, since adding the clause to the set triggers its hashCode(). If assertions are enabled the check in BooleanQuery, which recomputes the hash code, will notice it and throw an error. exception: I noticed this while trying to upgrade the NetBeans maven indexer modules from lucene 5.x to 8.x https://github.com/apache/netbeans/pull/3558", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10436", "change_description": ": Remove deprecated DocValuesFieldExistsQuery, NormsFieldExistsQuery and\nKnnVectorFieldExistsQuery.", "change_title": "Combine DocValuesFieldExistsQuery, NormsFieldExistsQuery and KnnVectorFieldExistsQuery into a single FieldExistsQuery?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Now that we require consistency across data structures, we could merge DocValuesFieldExistsQuery, NormsFieldExistsQuery and KnnVectorFieldExistsQuery together into a FieldExistsQuery that would require that the field indexes either norms, doc values or vectors?", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10561", "change_description": ": Reduce class/member visibility of all normalizer and stemmer classes.", "change_title": "Reduce class/member visibility of all normalizer and stemmer classes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "This is a spin-off of LUCENE-10312. Constants and methods in those classes are exposed to the outside packages; we should be able to limit the visibility to private or, at least to package private. This change breaks backward compatibility so should be applied to the main branch (10.0) only, and a MIGRATE entry may be needed. Also, they seem unchanged since 2008, we could refactor them to embrace newer Java APIs as we did in https://github.com/apache/lucene/pull/540.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10266", "change_description": ": Move nearest-neighbor search on points to core.", "change_title": "Move nearest-neighbor search on points to core?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "Now that the Points' public API supports running nearest-nearest neighbor search, should we move it to core via helper methods on LatLonPoint and XYPoint?", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "LUCENE-10603", "change_description": ": Remove SortedSetDocValues#NO_MORE_ORDS definition.", "change_title": "Improve iteration of ords for SortedSetDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "After SortedSetDocValues#docValueCount added since Lucene 9.2, should we refactor the implementation of ords iterations using docValueCount instead of NO_MORE_ORDS? Similar how SortedNumericDocValues did From to", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#11813", "change_description": ": Remove Operations.isFinite: the recursive implementation could be problematic\nfor large automatons (WildcardQuery, PrefixQuery, RegExpQuery, etc).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This method is recursive: to avoid eating too much stack we apply a small limit. This means it can't really be used on any largish automata without hitting exception. But the benefit of knowing finite vs infinite in AutomatonTermsEnum is minor: let's not auto-compute this. FuzzyQuery still gets the finite optimization because its finite by definition. PrefixQuery is always infinite. Wildcard/Regex just assume infinite which is safe to do. Remove the auto-computation and the \"trillean\" Boolean parameter. If you dont know that your automaton is finite, pass false to CompiledAutomaton, it is safe. Move this method to AutomatonTestUtil so we can still use it in test asserts. Closes #11809", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#11840", "change_description": ": Query rewrite now takes an IndexSearcher instead of IndexReader to enable concurrent\nrewriting.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Issue: #11838", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#11933", "change_description": ": Remove IOContext from Directory#openChecksumInput.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "ChecksumIndexInput only allows reading files sequentially, so the only IOContext that makes sense is IOContext.READONCE ? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#11814", "change_description": ": Support deletions in IndexRearranger.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "IndexRearranger applies deletes found in the original index, so the rearranged index will not contain any deleted docs. While deleted docs don't show up in search results, they still impact search performance. If we want the rearranged index to be as similar as possible to the original index, we should also include the docs marked for deletion. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12107", "change_description": ": Remove deprecated KnnVectorField, KnnVectorQuery, VectorValues and\nLeafReader#getVectorValues.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Follow-up of #12105 to remove the deprecated classes on main. Removes KnnVectorField, KnnVectorQuery, VectorValues and LeafReader#getVectorValues.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12296", "change_description": ": Make IndexReader and IndexReaderContext classes explicitly sealed.\nThey have already been runtime-checked to only be implemented by the specific classes\nso this is effectively a non-breaking change.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "IndexReaderContext is already effectively sealed since it's constructor does type check throwing Error if this is neither instance of CompositeReaderContext nor LeafReaderContext . This PR simply makes this restriction explicit (codewise) turning attempts to extend this class from a runtime error into a compile time one. I'v also replaced package-provate access modifiers to protected in order to make visibility more in-hand with this classes sealed'ness (effectively changing nothing) and updated deprecated <code> -blocks to @code in Javadocs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12276", "change_description": ": Rename DaciukMihovAutomatonBuilder to StringsToAutomaton.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Spinoff from this dev list thread . This delightful class very efficiently takes a set of terms and produces the already determinized and minimized Automaton matching exactly those terms. I appreciate that the name comes from the authors of the paper that explained this algorithm (thank you!!), but I would like to name it something more approachable to new users so they get a hint about what this powerful class actually does. Any suggestions? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12321", "change_description": ": Reduced visibility of StringsToAutomaton. Please use Automata#makeStringUnion instead.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There's some good suggestions/discussion around renaming this class in #12310 , but I wonder if we should consider making it pkg-private and exposing the build functionality through Automata instead? We already do this with Automata#makeStringUnion , so maybe we could shrink our API footprint and consolidate everything behind Automata ? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12407", "change_description": ": Removed Scorable#docID.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Scorable#docID() exposes the document that is being collected, which makes it impossible to bulk-collect multiple documents at once. Relates #12358", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12580", "change_description": ": Remove deprecated IndexSearcher#getExecutor in favour of executing concurrent tasks using\nthe TaskExecutor that the searcher holds, retrieved via IndexSearcher#getTaskExecutor", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Use getTaskExecutor instead. This is important to enforce tracking of tasks that run in each thread. Relates to #12578", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12599", "change_description": ": Add RandomAccessInput#readBytes method to the RandomAccessInput interface.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We can currently read a RandomAccessInput byte by byte but it does not provide a method to bulk read a chunck of bytes, similary to what DataInput provides with DataInput#readBytes(byte[], int, int). Therefore I would like to propose to add it with a default implementation that reads bye per byte: We can add faster implementation for all know implementations in Lucene code base. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#11023", "change_description": ": Adding -level param to CheckIndex, making the old -fast param the default behaviour.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This issue is a spin-off from discussion in #128 Currently CheckIndex defaults to checking both checksum as well as content inside each segment files for correctness, and requires -fast flag to be explicitly passed in to do checksum only. However, this default setting was there due to lack of checksum feature historically, and is slow for most end-users nowadays as they probably only care about their indices being intact (from random bit flipping for example). This issue is to change the default settings for CheckIndex so that they are more appropriate for end-users. One proposal from @rmuir is the following: Migrated from LUCENE-9984 by Zach Chen ( @zacharymorn ), updated Aug 16 2021 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12873", "change_description": ": Expressions module now uses MethodHandles to define custom functions. Support for\ncustom classloaders was removed.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR rewrites the JavascriptCompiler for the main branch to use more modern JVM features to be easier configurable and does not need a separate classloader per expression (uses hidden classes). Hidden classes are a new feature of Java 15: It allows to define a hidden class with byte code without using a classloader using the access constraints of a given private lookup. This is the same technique used by lambdas and other invokeDynamic features and replaces the old Unsafe API that Lucene never used. The downsides with a separate on-class classloader are described in #8933 . Mainly with many expressions compiled in parallel there's a lock in parent classloader while looking up methods/fields and loading classes. The new Java 15 hidden classes are unloaded by the JVM if the get out of scope, so the workaround with a separate classloader is no longer needed. A second reason why separate classloaders were needed is to allow code access functions form other classloaders. This stopped me doing the whole optimization at first until I rewrote the whole function lookup to be more modern: There is one downside in the hidden class feature that I added a workaround for: Hidden classes never appear in stack traces so my initial version did not print the actual JS function in stack trace. I worked around this by adding \"exception patching\" in the base class. It catches all Throwables throws by the generated code in the subclass and changes the stack trace to include the source code and hidden class name. This allows same user experience before. The current code is not backward compatible and needs an entry in MIGRATE.txt . I will investiagte how to add some helper methods for those people using java.lang.reflect.Method maps. Those can be easily converted to MethodHandle with some wrapper method. Support for separate classloader was removed. This is not a security issue, because the methods available for calling are defined externally and the API user is still responsible to only supply methods which should be accessible by users. The access checking is done by the provider of function map. For safety you can still let run whole of Lucene or the expressions module in a separate module/classloader. @rjernst , @jdconrad : Maybe this is also an idea for Painless. This PR replaces the outdated apache/lucene-solr#1837 . This closes #8933 .", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12243", "change_description": ": Remove TermInSetQuery ctors taking varargs param. SortedSetDocValuesField#newSlowSetQuery,\nSortedDocValuesField#newSlowSetQuery, KeywordField#newSetQuery, KeywordField#newSetQuery now take a collection.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently, the KeywordField class provides a public static method newSetQuery(String field, BytesRef... values) . However, this method only supports varargs parameter for values. I would like to propose adding a new static method that allows for a collections parameter for values, as this would provide greater flexibility. Proposed method signature: public static Query newSetQuery(String field, Collection<BytesRef> values) I am willing to make the changes and submit a pull request. Please let me know if there are any concerns or feedback. Thank you! The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12881", "change_description": ": Performance improvements to MatchHighlighter and MatchRegionRetriever. MatchRegionRetriever can be\nconfigured to not load matches (or content) of certain fields and to force-load other fields so that stored fields\nof a document are accessed once. A configurable limit of field matches placed in the priority queue was added\n(allows handling long fields with lots of hits more gracefully). MatchRegionRetriever utilizes IndexSearcher's\nexecutor to extract hit offsets concurrently.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This patch provides a number of small improvements aimed at improving performance of MatchHighlighter (and MatchRegionRetriever), especially in corner cases like: queries that result in a large number of hits, especially in long fields. This causes hit passage scoring to be time-consuming, only to result in a few \"best\" passages. A configurable maxHitsPerField limit is added to allow capping the number of matches retrieved (and scored) to a reasonable number. queries that require highlighting of hundreds of documents. The major performance bottleneck here is field value loading. MatchRegionRetriever now allows specifying which fields to load unconditionally, as well as filtering fields that contain hits (to skip computing highlights for fields which are used for filtering or are never displayed). Another improvement here is that highlights are now computed in parallel (using index searcher's task executor). There are a few minor API changes so I think it should be targeted for 10.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12855", "change_description": ": Remove deprecated DrillSideways#createDrillDownFacetsCollector extension method.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This extension hook is no longer used on the main branch, so let's remove it. Opened a corresponding PR to mark this deprecated on 9x ( #12854 ) More specifically, the DrillSideways class used to delegate to this method for creating the drill-down collector, but a while back was switched over to delegating to createDrillDownFacetsCollectorManager instead (doing \"manger-based\" collection instead). So users can still override the drill-down collection behavior, but they have to use createDrillDownFacetsCollectorManager , not createDrillDownFacetsCollector . Since nothing actually delegates to createDrillDownFacetsCollector , this is really trappy for users.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#12875", "change_description": ": Ensure token position is always increased in PathHierarchyTokenizer and ReversePathHierarchyTokenizer\nand resulting tokens do not overlap.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Incrementing position attribute for each token in both PathHieararchyTokenizer and ReversePathHieararchyTokenizer . This change makes it possible to use both tokenizers in BaseTokenStreamTestCase.assertAnalyzesTo() method (test cases extended to demonstrate this). This PR solves and surpasses #12750 . @msfroh I included your original commit from your Lucene fork to keep proper commit attribution. Shall we squash all commits?", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13146", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR updates the MR-JAR parts to only have implementations of Java 21. This PR does not remove the sourceSets for Java 21, although this is also our base version: Because compiling against the APIJAR is a hack, we do not want to do this for the main sourceset. So this one still has a separate sourceset with the Java 21 classes of vector and memorysegment. In the current state the Java 21 classes are still put into a MR-JAR part. I don't want to remove this for now: We could merge the Java 21 classes into the main part of the JAR file. The Gradle code could just compare the base version with the MR-JAR sourceset and if the version is identical (minJavaVersion==sourcesetVersion) it could copy the files into the main part of the JAR. I will try this in a separate commit.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13148", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a followup after: #13146 (comment) This places the compilation unit for Java 21 MRJAR classes in the main section of JAR file. As we currentlly only have Java 21 classes, the MR-JAR file is then automatically disabled. It is fully dynamic, so when we add later versions of vector code it will be placed in MRJAR sections again.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13205", "change_description": ": Convert IOContext, MergeInfo, and FlushInfo to record classes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Followup to #13204 : This converts IOContext to a record class. This has several positive effects: The IOContext has some crazy constructors, I left them in, but all have to delegate to the default constructor now.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13219", "change_description": ": The `readOnce`, `load` and `random` flags on `IOContext` have\nbeen replaced with a new `ReadAdvice` enum.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This replaces the load , randomAccess and readOnce flags with a ReadAdvice enum, whose values are aligned with the allowed values to (f|m)advise. Closes #13211", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13242", "change_description": ": Replace `IOContext.READ` with `IOContext.DEFAULT`.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "DEFAULT doesn't mean much today and could be used whenever READ is used. So let's use DEFAULT all the time instead and remove READ .", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13261", "change_description": ": Convert `BooleanClause` class to record class.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Convert BooleanClause class to record class which addresses #13207 partially.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13241", "change_description": ": Remove Accountable interface on KnnVectorsReader.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In Lucene 9.0, we removed the Accountable interface on our index readers and file formats, because heap usage had become so low that this metric had become less relevant, and also much less accurate as there were no longer a couple things that would account for a majority of the RAM usage, but rather many small objects that don't account for much memory on their own and that were too hard to estimate properly. We still kept Accountable on KnnVectorsReader because it used to load lots of things in heap at the time. Since vectors have become much more reasonable since then, I would suggest now removing Accountable on KnnVectorsReader like we did for other file formats in 9.0. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13262", "change_description": ": Removed deprecated constructors from DoubleField, FloatField, IntField, LongField, and LongPoint.\nAdditionally, deprecated methods have been removed from ByteBuffersIndexInput, BooleanQuery and others. Please refer\nto MIGRATE.md for further details.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We have some code that has been deprecated for a long time, see e.g. IntField constructors. Let's remove this deprecated code in the main branch ahead of 10.0. Note: we may have code that is only marked as deprecated in main . This code shouldn't be removed as users will get the deprecation warning for the first time when they upgrade to 10.0. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13337", "change_description": ": Introduce new `IndexInput#prefetch(long)` API to give a hint to\nthe directory about bytes that are about to be read.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This adds IndexInput#prefetch , which is an optional operation that instructs the IndexInput to start fetching bytes from storage in the background. These bytes will be picked up by follow-up calls to the IndexInput#readXXX methods. In the future, this will help Lucene move from a maximum of one I/O operation per search thread to one I/O operation per search thread per IndexInput . Typically, when running a query on two terms, the I/O into the terms dictionary is sequential today. In the future, we would ideally do these I/Os in parallel using this new API. Note that this will require API changes to some classes including TermsEnum . I settled on this API because it's simple and wouldn't require making all Lucene APIs asynchronous to take advantage of extra I/O concurrency, which I worry would make the query evaluation logic too complicated. Currently, only NIOFSDirectory implements this new API. I played with MMapDirectory as well and found an approach that worked better in the benchmark I've been playing with, but I'm not sure it makes sense to implement this API on this directory as it either requires adding an explicit buffer on MMapDirectory , or forcing data to be loaded into the page cache even though the OS may have decided that it's not a good idea due to too few cache hits. This change will require follow-ups to start using this new API when working with terms dictionaries, postings, etc. Relates #13179", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13408", "change_description": ": Moved Weight#bulkScorer() to ScorerSupplier#bulkScorer() to better help parallelize\nI/O for top-level disjunctions. Weight#bulkScorer() still exists for compatibility, but delegates\nto ScorerSupplier#bulkScorer().", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This relates to #13359 : we want to take advantage of the Weight#scorerSupplier call to start scheduling some I/O in the background in parallel across clauses. For this to work properly with top-level disjunctions, we need to move #bulkScorer() from Weight to ScorerSupplier as well, so that the disjunctive BooleanQuery first performs a call to Weight#scorerSupplier() on all inner clauses, and then ScorerSupplier#bulkScorer on all inner clauses. ScorerSupplier#get and ScorerSupplier#bulkScorer only support being called once. This forced me to fix some inefficiencies in bulkScorer() implementations when we would pull scorers and then throw it away when realizing that the strategy we were planning on using was not optimal. This is why e.g. ReqExclBulkScorer now also supports prohibited clauses that produce a two-phase iterator.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13410", "change_description": ": Removed Scorer#getWeight", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I've been working on some refactorings recently, and the fact that Scorer has a getWeight method is very annoying as it requires every simple Scorer implementation, e.g. for testing purposes, to also be able to return its Weight . This method doesn't have strong use-cases, so I suggest that we remove it for 10.0. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13499", "change_description": ": Remove deprecated TopScoreDocCollector + TopFieldCollector methods (#create, #createSharedManager)", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "These methods were deprecated in #240 which is part of Lucene 10.0. Since they are not marked for deprecation in Lucene 9.x, they will not be removed with Lucene 10.0 release (unless we want to deprecate them in 9.x in another backport commit?), but I still think we should remove the internal usage so it will be quicker to remove the methods when the time comes. I'll submit a PR for a 1:1 change to remove the usages. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13632", "change_description": ": CandidateMatcher public matching functions", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In CandidateMatcher , increase visibility of matchQuery() (from protected), finish() and reportError() (from package-private) to public, so that a CandidateMatcher can be used effectively from outside the org.apache.lucene.monitor package. The current access protections on CandidateMatcher make it infeasible for a user of the Lucene Monitor library to use an existing CandidateMatcher if they are outside the org.apache.lucene.monitor package (e.g. as part of the implementation of their own CandidateMatcher ). For example, a user working outside of the org.apache.lucene.monitor package could not build their own version of ParallelMatcher by making use of the matcher from QueryMatch.SIMPLE_MATCHER , because they cannot access matchQuery , reportError or finish on it. This PR takes the simplest solution of increase the visibility of those functions in CandidateMatcher to public . This also requires modifying some existing CandidateMatcher implementations that override protected matchQuery() to instead override public matchQuery() . I've also added some just-compile tests in a subpackage org.apache.lucene.monitor.otherpackage , which call the newly-public functions to verify that they are accessible from outside the org.apache.lucene.monitor package. See previous discussion of this approach and alternatives in #13109 . If this PR gets merged, can you please use my bjacobowitz1@bloomberg.net email address for the squash+merge. Thank you.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13708", "change_description": ": Move Operations.sameLanguage/subsetOf to test-framework.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "These methods run in quadratic time and have been traps in the past: they run in quadratic time. I think originally this was equals() but it is so costly, that we factored out into separate sameLanguage methods. Additionally methods are a bit smelly, they will throw exception if the inputs aren't deterministic, and they'll only assert if there are transitions to dead states... they are really only suitable for tests code. We've been making progress moving some of this code to test framework, this is just the next iteration. Would love to factor out more (dead states checks etc) but that's more difficult.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13733", "change_description": ": Move FacetsCollector#search utility methods to `FacetsCollectorManager`, replace the `Collector`\nargument with a `FacetsCollectorManager` and update the return type to include both `TopDocs` results as well as\nfacets results.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We have a few public static utility search methods in FacetsCollector that accept a Collector as last argument. In practice, these are expected to be called providing a FacetsCollector as last argument. Also, we'd like to remove all the search methods that take a Collector in favour of those that take a CollectorManager (see #12892 ). This commit adds the corresponding functionality to FacetsCollectorManager . The new methods take a FacetsCollectorManager as last argument. The return type has to be adapted to include also the facets results that were before made available through the collector argument. In order for tests to all work I had to add support for keepScores to FacetsCollectorManager which was missing. Closes #13725", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13328", "change_description": ": Convert many basic Lucene classes to record classes, including CollectionStatistics, TermStatistics and LeafMetadata.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Raising this PR since all the tests are passing( ./gradlew test ) but renderJavadoc task is complaining about missing java docs on some record classes(converted in this PR) which I see has the javadocs already eg: TermStats , ReaderSlice . I'm not sure why its flagging those incorrectly or if maybe I'm missing something.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13780", "change_description": ": Remove IndexSearcher#search(List<LeafReaderContext>, Weight, Collector) in favour of the newly\nintroduced IndexSearcher#search(LeafReaderContextPartition[], Weight, Collector).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "With the introduction of intra-segment concurrency, we have introduced a new protected search(LeafReaderContextPartition[], Weight, Collector) method. The previous variant that accepts a list of leaf reader contexts was left deprecated as there is one leftover usages coming from search(Query, Collector) . The hope was that the latter was going to be removed soon as well, but there is actually no need to tie the two removals. It is easier to fold this method into its only caller, in order for it to still bypass the collector manager based methods.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13779", "change_description": ":  First-class random access API for KnnVectorValues\nunifies Byte/FloatVectorValues incorporating RandomAccess* API and introduces\nDocIndexIterator for iterative access in place of direct inheritance from DISI.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "addresses #13778 Key things in this PR: Some of the methods on KnnVectorValues have default impls that throw UnsupportedOperationException enabling subclasses to provide partial implementations and relying on testing to catch missing required methods. I'd like feedback on this. Should we provide implementations we never use, just to make these classes complete? That didn't make sense to me. But the previous alternative of attempting to provide strict adherence to declarative contracts was becoming in my view, overly restrictive and leading to hard-to-maintain code. Some of these readers would only ever be used iteratively. Random access is required for search, but not used when  merging the values themselves, and when we merge we do search, but using a temporary file so that searching is always done over a file-based value. Random access also gets used during merging when the index is sorted, again this is provided by specialized readers, so not every reader needs to implement random access. But the API maintenance is greatly simplified if we allow partial implementation. Anyway that is the idea I am trying out here. Can we live with a little less API purity and gain some simplicity and less boilerplate? Notes for reviewers: There is a lot of code change here, but much of it is repetitive. I recommend starting with KnnVectorValues and checking its DocIndexIterator inner class. The rest of the changes are basically consequences of introducing those abstrations in place of the Random*Values we removed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13845", "change_description": ": Add missing with-discountOverlaps Similarity constructor variants.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "#13757 follow-up", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13820", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The change in #12841 brought this pair of methods to DataInput: If one wishes to write a DataInput delegating to another, existing DataInput, it's no longer possible because the public method is final and the protected method cannot be invoked on the delegate (cross-package access). A corresponding method in DataOutput is not final. I would remove the final modifier from the API for symmetry. Thoughts? No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13825", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fixes #13820", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "API Changes", "change_id": "GITHUB#13830", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Discussed in #13825", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "LUCENE-10010", "change_description": "Introduce NFARunAutomaton to run NFA directly.", "change_title": "Should we have a NFA Query?", "detail_type": "New Feature", "detail_affect_versions": "9.0", "detail_fix_versions": "None", "detail_description": "Today when a RegexpQuery is created, it will be translated to NFA, determinized to DFA and eventually become an AutomatonQuery, which is very fast. However, not every NFA could be determinized to DFA easily, the example given in LUCENE-9981 showed how easy could a short regexp break the determinize process. Maybe, instead of marking those kind of queries as adversarial cases, we could make a new kind of NFA query, which execute directly on NFA and thus no need to worry about determinize process or determinized DFA size. It should be slower, but also makes those adversarial cases doable. This article has provided a simple but efficient way of searching over NFA, essentially it is a partial determinize process that only determinize the necessary part of DFA. Maybe we could give it a try?", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "LUCENE-10626", "change_description": "Hunspell: add tools to aid dictionary editing:\nanalysis introspection, stem expansion and stem/flag suggestion", "change_title": "Hunspell: add tools to aid dictionary editing: analysis introspection, stem expansion and stem/flag suggestion", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The following tools would be nice to have when editing and appending an existing dictionary: 1. See how Hunspell analyzes a given word, with all the involved affix flags: `Hunspell.analyzeSimpleWord` 2. See all forms that the given stem can produce with the given flags: `Hunspell.expandRoot`, `WordFormGenerator.expandRoot` 3. Given a number of word forms, suggest a stem and a set of flags that produce these word forms: `Hunspell.compress`, `WordFormGenerator.compress`.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#12829", "change_description": ": For indices newly created as of 10.0.0 onwards, IndexWriter preserves document blocks indexed via\nIndexWriter#addDocuments or IndexWriter#updateDocuments also when index sorting is configured. Document blocks are\nmaintained alongside their parent documents during sort and merge. IndexWriterConfig now requires a parent field to be\nspecified if index sorting is used together with document blocks.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today index sorting with likely break document blocks added with IndexWriter#addDocuments(...) and friends since the index sorter has no indication of what documents are part of a block. This change proposes a marker field as a requirement for parent documents if the block API is used in conjunction with index sorting. At this point this change requires a NumericDV field only present on the last document of a block or on every document indexed as an individual document iff a parent field is configured on the Sort. This can potentially be extended to a Term which is not as straight forward since validations might be more difficult and today postings are not available when we sort the flushed segment. Relates to #12711", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13233", "change_description": ": Add RomanianNormalizationFilter", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "#12172 raised the issue of two different forms \"in the wild\" for Romanian: We've fixed the stopwords, and fixed snowball stemmer, to complete the job we need a normalizer. This allows the normalization to work correctly for various corner-cases: I've also included a MIGRATE.txt for Romanian as a whole for 10.0, users should reindex Romanian documents.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13449", "change_description": ": Sparse index: optional skip list on top of doc values which is exposed via the\nDocValuesSkipper abstraction. A new flag is added to FieldType.java that configures whether\nto create a \"skip index\" for doc values.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Speaking to Adrien about how a sparse index would look like in lucene, he suggested that the sparse indexing does not need to be a new format bit an additional responsibility if DocValuesFormat . The idea is to add an option to add a skip list on top of doc values and to expose it via the DocValuesSkipper abstraction, which has an API that is similar to what we're doing for impacts today. This provides a very simple index which can be very efficient when the index is sorted and the field belongs to the index sorting. In order to implement it, we added a new flag in FieldType.java that configures whether to create a \"skip index\" for doc values. This flag is only allowed to be set on doc values of type NUMERIC, SORTED_NUMERIC, SORTED and SORTED_SET. Attempting to index other type of doc values with the flag set results on an exception. This flag needs to be persisted on the FieldInfosFormat . This does not require a format change as we have some unused bit flags  in Lucene94FieldInfosFormat that we can use. We have changed the DocValuesFormat to generate the \"skip index\" whenever the flag is set. For this first implementation we went to the most basic implementation which consist in a skip list with just one level. In this level we collect the documents statistics every 4096 documents and we write them into the index. This basic structure already provides interesting numbers. I discussed with Adrien that as a follow up we should introduce more levels to the skip list and optimise the index for low cardinality fields. In order to index a field with a skip list, we added static methods to the doc values field, for example NumericDocValuesField#indexedField(String name, long value) which will generated the right FieldType. In order to query it, you can use the existing  NumericDocValuesField#newSlowRangeQuery(String field, long lowerValue, long upperValue). The generated query will use the skip index if exists by using the DocValuesRangeIterator . Finally, here are some number I got using the geonames data set from lucene util. The first test index the field called modified and adds the field as the primary sort of the index. This basic implementation is already faster that querying using the bkd tree. The IndexSortSortedNumericDocValuesRangeQuery is faster as it contains many optimisations but my expectation is that we can make this index as fast if not faster than this implementation. The second test, we are indexing two fields and sorting the index using them; the countryCode as primary sort and the modified field as secondary sort. Then we execute the range queries on the modified field: In this case the query is slower than the BKD tree but still much faster than the brute approach. The advantage of the new query is that it does not need to build the big bitset that we might need to build with the BKD tree. relates #11432", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13563", "change_description": ": Add levels to doc values skip index.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently the DocValues skipper index collects the stats every 4096 documents that allow implementors to used them to decide if they want to process those documents or they can be skipped. The idea of adding levels is to be able to skip several of those block (called intervals in the code) in one step by collecting the stats of the blocks and adding them to the index. This implementation collects the stats of 8 of those intervals.  For the first level, the first interval gets a new level with the stats from the next 8 intervals, then the 9th interval gets another level with the stats from the following 8 intervals and so on. For the second level, the first interval gets a new level with the stats from the next 8 level1  intervals, which is the same as the stats from the first 64 level 0 intervals and so on. I run some  basic experiments and I must say I did not see much change on the performance, still it feels the right thing to do, therefore I opened this PR. relates #11432", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13597", "change_description": ": Align doc value skipper interval boundaries when an interval contains a constant\nvalue.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Doc values skipper currently creates a new interval every 4096 documents. This can be inefficient in the case that the field has low cardinality and values are sorted. In that case, we might want to align the boundaries for intervals that contains just a single value. Therefore this change proposes to keep adding documents in an interval if the following conditions apply: In this situation we will keep adding documents into the interval until one of the conditions break it. relates #11432", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13604", "change_description": ": Add Kmeans clustering on vectors", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Implement Kmeans clustering algorithm for vectors. Knn algorithms that further reduce memory usage of vectors (such as Product Quantization, RaBitQ etc) require clustering of vectors. This implements KMeans clustering algorithm.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13592", "change_description": ": Take advantage of the doc value skipper when it is primary sort in SortedNumericDocValuesRangeQuery\nand SortedSetDocValuesRangeQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Similar to what we do in IndexSortSortedNumericDocValuesRangeQuery for primary sort, we can do the same in SortedSetDocValuesRangeQuery and SortedNumericDocValuesRangeQuery when it is primary sort and there are not delete documents. We find the first and last document using the skipper and return the dense iterator. relates #11432", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13542", "change_description": ": Add initial support for intra-segment concurrency. IndexSearcher now supports searching across leaf\nreader partitions concurrently. This is useful to max out available resource usage especially with force merged\nindices or big segments. There is still a performance penalty for queries that require segment-level computation\nahead of time, such as points/range queries. This is an implementation limitation that we expect to improve in\nfuture releases, ad that's why intra-segment slicing is not enabled by default, but leveraged in tests when the\nsearcher is created via LuceneTestCase#newSearcher. Users may override IndexSearcher#slices(List) to optionally\ncreate slices that target segment partitions.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR introduces support for optionally creating slices that target leaf reader context partitions, which allow them to be searched concurrently. This is good to maximize resource usage when searching force-merged indices, or indices with rather big segments, by parallelizig search execution across subsets of segments being searched. Note: this PR does not affect default generation of slices. Segments can be partitioned by overriding the IndexSearcher#slices(List<LeafReaderContext>) method to plug in ad-hoc slices creation. Moreover, the existing IndexSearcher#slices static method now creates segment partitions when the additional allowSegmentsPartitions argument is set to true . The overall design of this change is based on the existing search concurrency support that is based on LeafSlice and CollectorManager . A new LeafReaderContextPartition abstraction is introduced, that holds a reference to a LeafReaderContext and the range of doc ids it targets. A LeafSlice noew targets segment partitions, each identified by a LeafReaderContext instance and a range of doc ids. It is possible for a partition to target a whole segment, and for partitions of different segments to be combined into the same leaf slices freely, hence searched by the same thread. It is not possible for multiple partitions of the same segment to be added to the same leaf slice. Segment partitions are searched concurrently leveraging the existing BulkScorer#score(LeafCollector collector, Bits acceptDocs, int min, int max) method, that allows to score a specific subset of documents for a provided LeafCollector , in place of the BulkScorer#score(LeafCollector collector, Bits acceptDocs) that would instead score all documents. The migrate guide has the following new clarifying items around the contract and breaking changes required to support intra-segment concurrency: Note: DrillSideways is the only component that does not support intra-segment concurrency and needs considerable work to do so, due to its requirement that the entire set of docs in a segment gets scored in one go. The default searcher slicing is not affected by this PR, but LuceneTestCase now randomly leverages intra-segment concurrency. An additional newSearcher method is added that takes a Concurrency enum as the last argument in place of the useThreads boolean flag. This is important to disable intra-segment concurrency for DrillSideways related tests that do support inter-segment concurrency but not intra-segment concurrency. While this change introduces support for intra-segment concurrency, it only sets up the foundations of it. There is still a performance penalty for queries that require segment-level computation ahead of time, such as points/range queries. This is an implementation limitation that we expect to improve in future releases, see #13745 . Additionally, we will need to decide what to do about the lack of support for intra-segment concurrency in DrillSideways before we can enable intra-segment slicing by default. See #13753 . I have found the following two issues that revolved around total hits counting, which have now been addressed by adapting TotalHitCountCollectorManager 's behaviour: IndexSearcher#count / TotalHitCountCollector rely on Weight#count(LeafReaderContext)`, which now gets called multiple times against the same leaf and leads to excessive counting of hits. LRUQueryCache caches the return value of Weight#count . When we execute the same query against the same segment multiple times (as part of a single search call), the first time we do the actual counting for the docs that the first partition holds, and subsequent times we should do the same, count hits in each partition of the same segment instead of retrieving the count from the cache. Both these issues are addressed by modifying TotalHitCountCollectorManager to return an enhanced version of TotalHitCountCollector that tracks leaves that it sees, and whether they early terminated or not. The goal is to ensure consistency and correctness across multiple times that a leaf collector for the same leaf is requested. If a partition for a certain leaf early terminates, all of the other partitions of that same segment should also early terminate and not contribute to the total hits count. If a partition for a certain leaf computes hit counts, all of the other partitions of that same segment should also compute hit counts ( as opposed to potentially being early terminated or retrieved from the cache, which only work globally per leaf, and not per leaf partition) I have also adjusted CheckHits to not go outside of the bounds of the doc id range of the current slice. I fixed TestSortRandom s custom ScorerSupplier to not rely on a scorer supplier being pulled once and only once per LeafReaderContext I have found additional issues in the facet module: DrillSideways does not support scoring via the provided range of doc ids, it throws an exception whenever the range is not 0 and Integer.MAX_VALUE . This is not easy to solve. I have disabled intra-segment concurrency in tests when DrillSideways is used. FacetsCollectorManager and RandomSampligFacetsCollector#createManager returned multiple MatchingDocs instances that pointed to the same LeafReaderContext which caused TestStringValueFacetCounts failures. I added an extra round of de-duplication in the reduce method that merges back multiple matching docs from multiple partitions of the same segment into a single per-segment instance. Closes #9721", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "New Features", "change_id": "GITHUB#13741", "change_description": ": Implement Accountable for NFARunAutomaton, fix hashCode implementation of CompiledAutomaton.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As discussed in #13715 this PR fixes:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#13246", "change_description": ": Simplify bytes comparison as long comparison in NumericComparator.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Simplify complex byte array comparisons as long comparison.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "LUCENE-10416", "change_description": ": Update Korean Dictionary to mecab-ko-dic-2.1.1-20180720 for Nori.", "change_title": "Update Korean Dictionary for Nori", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "For Nori - Korean analyzer, there is Korean dictionary named mecab-ko-dic, which is available under an Apache license here: https://bitbucket.org/eunjeon/mecab-ko-dic  The dictionary hasn't been updated in Nori although it has some updates to provide better analysis results. Downloading is available here: https://bitbucket.org/eunjeon/mecab-ko-dic/downloads  There are changes between the currently used version and the latest release version(change log: https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/CHANGES.md)  There's no issue with testing :lucene:analysis:nori:test and building a new binary.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13039852/LUCENE-10416.patch", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "LUCENE-10614", "change_description": ": Properly support getTopChildren in RangeFacetCounts.", "change_title": "Properly support getTopChildren in RangeFacetCounts", "detail_type": "Improvement", "detail_affect_versions": "10.0(main)", "detail_fix_versions": "10.0(main)", "detail_description": "As mentioned in LUCENE-10538, RangeFacetCounts is not implementing getTopChildren. Instead of returning \"top\" ranges, it returns all user-provided ranges in the order the user specified them when instantiating. This is probably more useful functionality, but it would be nice to support getTopChildren as well. LUCENE-10550 is introducing the concept of getAllChildren, so once that lands, we can replace the current implementation of getTopChildren with an actual \"top children\" implementation and direct users to getAllChildren if they want to maintain the current behavior.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "LUCENE-10652", "change_description": ": Add a top-n range faceting example to RangeFacetsExample.", "change_title": "Add a top-n range faceting example to RangeFacetsExample", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "In LUCENE-10614, we modified the behavior of getTopChildren to actually return top-n ranges ordered by count. The original behavior of getTopChildren in RangeFacetsCounts was to return all ranges ordered by constructor-specified range order, and this behavior is now retained in the getAllChildren API (LUCENE-10550). Therefore, it would be helpful to add an example in RangeFacetsExample to demo this change. I replaced the original example of getTopChildren with getAllChildren, and will add an example of the current getTopChildren API soon.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#12447", "change_description": ": Hunspell: speed up the dictionary enumeration on suggestion", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "cache each word's case and the lowercase form group the words by lengths to avoid even visiting entries with unneeded lengths", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#12873", "change_description": ": Expressions module now uses JEP 371 \"Hidden Classes\" with JEP 309\n\"Dynamic Class-File Constants\" to implement Javascript expressions.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR rewrites the JavascriptCompiler for the main branch to use more modern JVM features to be easier configurable and does not need a separate classloader per expression (uses hidden classes). Hidden classes are a new feature of Java 15: It allows to define a hidden class with byte code without using a classloader using the access constraints of a given private lookup. This is the same technique used by lambdas and other invokeDynamic features and replaces the old Unsafe API that Lucene never used. The downsides with a separate on-class classloader are described in #8933 . Mainly with many expressions compiled in parallel there's a lock in parent classloader while looking up methods/fields and loading classes. The new Java 15 hidden classes are unloaded by the JVM if the get out of scope, so the workaround with a separate classloader is no longer needed. A second reason why separate classloaders were needed is to allow code access functions form other classloaders. This stopped me doing the whole optimization at first until I rewrote the whole function lookup to be more modern: There is one downside in the hidden class feature that I added a workaround for: Hidden classes never appear in stack traces so my initial version did not print the actual JS function in stack trace. I worked around this by adding \"exception patching\" in the base class. It catches all Throwables throws by the generated code in the subclass and changes the stack trace to include the source code and hidden class name. This allows same user experience before. The current code is not backward compatible and needs an entry in MIGRATE.txt . I will investiagte how to add some helper methods for those people using java.lang.reflect.Method maps. Those can be easily converted to MethodHandle with some wrapper method. Support for separate classloader was removed. This is not a security issue, because the methods available for calling are defined externally and the API user is still responsible to only supply methods which should be accessible by users. The access checking is done by the provider of function map. For safety you can still let run whole of Lucene or the expressions module in a separate module/classloader. @rjernst , @jdconrad : Maybe this is also an idea for Painless. This PR replaces the outdated apache/lucene-solr#1837 . This closes #8933 .", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#11657", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Apache OpenNLP 2.0.0 has been released. This version contains new implementations of TokenNameFinder and DocumentCategorizer that supports models in the ONNX format. (TokenNameFinder is in NLPNERTaggerOp, DocumentCategorizer is not currently exposed through Lucene.) This task is update the OpenNLP dependency version to 2.0 and to add support for the new interface implementations in the OpenNLP analysis module that was added in #3973 . Migrated from LUCENE-10621 by Jeff Zemerick, 1 vote The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "LUCENE-10621", "change_description": ",", "change_title": "Upgrade to OpenNLP 2.0 and add", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Apache OpenNLP 2.0.0 has been released. This version contains new implementations of TokenNameFinder and DocumentCategorizer that supports models in the ONNX format. (TokenNameFinder is in NLPNERTaggerOp, DocumentCategorizer is not currently exposed through Lucene.) This task is update the OpenNLP dependency version to 2.0 and to add support for the new interface implementations in the OpenNLP analysis module that was added in LUCENE-2899.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#13209", "change_description": ": Upgrade snowball to 26db1ab9.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I'd like to upgrade snowball to make progress on resolving the Romanian situation, after merging this I'd like to merge #12172 which has been hanging out there FOREVER (over a year). I've been waiting on an official snowball release, but I've been waiting a long time, and I think we should just move forward? They haven't released since 2021. This upgrade cuts the size of our \"patch\" that we apply to snowball's sources in half, as the Armenian and Estonian stemmers that were contributed via patches to lucene are now integrated into snowball codebase. Also the Irish stopwords file was incorporated into snowball codebase. One downside: it has a somewhat user-impacting change where the German2 stemmer was folded into the German stemmer properly. You can read about this more on their website: https://snowballstem.org/algorithms/german2/stemmer.html or via the commit: snowballstem/snowball@ b08bdc5 To reduce the impact on users, if German2 is provided, instead of a reflection-exception, we'll map it to German , at least for now... we can remove this \"if\" on our own time? Somehow I suspect Germans will be unhappy with me no matter what I do here :)", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#12172", "change_description": ": Update Romanian stopwords list to include the modern unicode forms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Romanian uses s&t with commas (ș/ț), but for a long time those weren't available, so s&t with cedilla (ş/ţ) were used. Both are still in use, but the comma forms are much more common now. Both should be supported in stopword lists.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Improvements", "change_id": "GITHUB#13707", "change_description": ": Improve Operations.isTotal() to work with non-minimal automata.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Operations.isTotal currently returns false unless the DFA is minimal. This makes the method almost useless and we definitely don't want to encourage minimization just to make such a check. Can we do a better job, e.g. return true for a non-minimal DFA? There's an example test added that fails without the change to demonstrate: This is a draft PR because I still don't like that it uses subsetOf , the code literally makes a minimal \"total DFA\" and compares that the two automata recognize the same language. Because it is total, we only need to call subsetOf once, but I still don't like how heavy it is. Can we do better? See #13706 for more background", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#11857", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#11859", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We discard entries with NOSUGGEST (and some other) flags anyway, so let's bail out of processing them at an earlier stage. This speeds up suggestions for relatively short German words by about 20% for me.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#11893", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#11909", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "add NGramFragmentChecker to quickly check whether insertions/replacements produce strings that are even possible in the language", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#12825", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#12834", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#12372", "change_description": ": Reduce allocation during HNSW construction", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This changes HnswGraphBuilder to re-use the same candidates queues for adding nodes by allocating them in the Builder instance. This saves about 2.5% of build time and takes memory allocations of NQ long[] from 25% of total to 0%.  JFR runs are attached. The difference from the first attempt (which actually made things slower for some graphs) is that it preserves the original code's behavior of using a 1-sized queue for the search in the levels above where the node actually gets added. main.jfr.gz nq2.jfr.gz", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#12552", "change_description": ": Make FSTPostingsFormat load FSTs off-heap.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "FSTs supports to load offheap for a while. As we were trying to use FSTPostingsFormat for some fields we realized heap usage bumped. Upon further investigation we realized the FSTPostingsFormat does not load FSTs offheap. This PR addresses that.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Optimizations", "change_id": "GITHUB#13672", "change_description": ": Leverage doc value skip lists in DocValuesRewriteMethod if indexed.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "DocValuesRewriteMethod (used under-the-hood in MultiTermQuery ) is another spot where we can leverage the new doc value skip lists when present to potentially skip over doc blocks that don't contain one of the query terms.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10599", "change_description": ": LogMergePolicy is more likely to keep merging segments until\nthey reach the maximum merge size.", "change_title": "Improve LogMergePolicy's handling of maxMergeSize", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "LogMergePolicy excludes from merging segments whose size is greater than or equal to maxMergeSize. Since a segment whose size is maxMergeSize-1 is still considered for merging, segments will effectively reach a size somewhere between maxMergeSize and mergeFactor*maxMergeSize before they are not considered for merging anymore. At least this is what I thought. When LogMergePolicy ignores a segment that is too large for merging, it also ignores other segments that are in the same window of mergeFactor segments for merging if they are on the same tier. So actually segments might reach a size that is somewhere between maxMergeSize / mergeFactor^0.75 and maxMergeSize * mergeFactor before they are not considered for merging anymore. Assuming a merge factor of 10 and a max merge size of 1,000 this means that segments will reach their maximum size somewhere between 178 and 10,000. This range is too large and makes maxMergeSize too hard to reason about? Specifically, if you have 10 999-docs segments, then LogDocMergePolicy will happily merge them into a single 9990-docs segment. However if you have one 1,000 segment and 9 180-docs segments, then the 180-docs segments will not get merged with any other segment, even if you keep adding segments to the index. I propose to change this behavior so that when a large segment is encountered, then we wouldn't skip the entire window of mergeFactor segments, but just the segments that are too large.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12220", "change_description": ": Hunspell: disallow hidden title-case entries from compound middle/end.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "if we only have custom-case uART and capitalized UART, we shouldn't accept StandUart as a compound (although we keep hidden \"Uart\" dictionary entries for internal purposes)", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12878", "change_description": ": Fix the declared Exceptions of Expression#evaluate() to match those\nof DoubleValues#doubleValue().", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR fixes the issue found while coding the benchmark of Expressions module in #12873 : The expressions module looks up the variables using the DoubleValues#doubleValue method, which throws IOException . Because the JVM does not do any exceptions checks, only the type system of javac compiler, some code calling Expression#evaluate could suddenly be confronted with a checked IOException , although this is not declared. It is not even possible to catch the Exception because compiler complains that it is not declared. Actually this should also be fixed in 9.x but this changes the method signature of the Expression#evaluate method and the impact is not too high. So it is for Lucene 10 only. The fix is to add the exception to the declaration in the abstract base class (and also the bytecode generator). This also rewrites the sanity check in JavascriptCompiler to check the method signatures.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13498", "change_description": ": Avoid performance regression by constructing lazily the PointTree in NumericComparator,", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We notice a performance regression in Elasticsearch which we could trace back to the changes in #13199 . This change makes constructing NumericComparators more expensive die to the construction of a PointTree eagerly in the constructor. This is particular noticeable when the PointTree is not used later and  your constructing several of them. This. commit proposes to build the PointTree lazily. The original PR proposed it but it was changed due to simplicity. We are only targeting the 9.x line as main has already diverged and doing it in a different way.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "GITHUB#13244", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This switches the default ReadAdvice from NORMAL to RANDOM , which is a better fit for the kind of access pattern that Lucene has. This is expected to reduce page cache trashing and contention on the page table. NORMAL is still available, but never used by any of the file formats.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "GITHUB#13264", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Followup after #13244 : This makes the default ReadAdvice for index files configurable via system property org.apache.lucene.store.defaultReadAdvice . I moved the corresponding constant to the oal.util.Constants class because this makes reading and error reporting easier as @rmuir and I added some framework to safely support security manager and map those variables (we used this for vectorization). It would be good to move all reading of system properties to that class so we do not have the splattered AccessControllers everywhere.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "GITHUB#13293", "change_description": ": ConcurrentMergeScheduler now allows up to 50% of the threads of the host to be used\nfor merging.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Disable ConcurrentMergeScheduler's auto I/O throttling by default. This is motivated by the fact that merges can hardly steal all I/O resources from searches on modern NVMe drives. Merges are still not allowed to use all CPU since they have a budget for the number of threads which is a fraction of the number of threads that the host can run. Closes #13193", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "GITHUB#13277", "change_description": ": IndexWriter treats any java.lang.Error as tragic.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Background: Historically IndexWriter treated OutOfMemoryError special, for defensive reasons. It was expanded to VirtualMachineError, to try to play it safe in similar disastrous circumstances. We should treat any Error as a tragedy, as it isn't an Exception, and it isn't something a \"reasonable\" application should catch. IndexWriter should be reasonable. See #7049 for some of the reasoning. We can't pretend this will detect any possible scenario that might cause harm, e.g. a jvm bug might simply miscompile some code and cause silent corruption. But we should try harder by playing by the rules. Closes #13275", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "GITHUB#12829", "change_description": ": IndexWriter#addDocuments or IndexWriter#updateDocuments now require a parent field name to be\nspecified in IndexWriterConfig is documents blocks are indexed and index time sorting is configured.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today index sorting with likely break document blocks added with IndexWriter#addDocuments(...) and friends since the index sorter has no indication of what documents are part of a block. This change proposes a marker field as a requirement for parent documents if the block API is used in conjunction with index sorting. At this point this change requires a NumericDV field only present on the last document of a block or on every document indexed as an individual document iff a parent field is configured on the Sort. This can potentially be extended to a Term which is not as straight forward since validations might be more difficult and today postings are not available when we sort the flushed segment. Relates to #12711", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "GITHUB#13230", "change_description": ": Remove the Kp and Lovins snowball algorithms which are not supported\nor intended for general use.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently we patch the snowball sources and add two stemmers which are not supported by snowball. They are not tested by their CI build pipeline, etc: Ref: https://github.com/snowballstem/snowball/blob/master/libstemmer/modules.txt#L48-L61 I'd like to remove these from lucene as well, remove snowball.patch , and only use what is supported, to make more progress on normalizing our use of snowball as a library.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "GITHUB#13602", "change_description": ": SearchWithCollectorTask no longer supports the `collector.class` config parameter to load a custom\ncollector implementation. `collector.manager.class` allows users to load a collector manager instead.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit modifies ReadTask to no longer call the deprecated search(Query, Collector). Instead, it creates a collector manager and calls search(Query, CollectorManager). The existing protected createCollector method is removed in favour of createCollectorManager that returns now a CollectorManager in place of a Collector. SearchWithCollectorTask works the same way if \"topScoreDoc\" is provided as config. Loading of a custom collector will no longer work, and needs to be replaced with loading a collector manager by class name instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "GITHUB#13459", "change_description": ": Merges all immutable attributes in FieldInfos.FieldNumbers into one Hashmap saving\nmemory when writing big indices. Fixes an exotic bug when calling clear where not all attributes\nwere cleared.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We currently have in FieldInfos#FieldNumbers the following maps: This maps are updated and read together so it feels wasteful to have separate maps for each one. They can get pretty big too when there is lost of fields. I wonder if we can simplified them into one map. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "LUCENE-10376", "change_description": ": Roll up the loop in VInt/VLong in DataInput.", "change_title": "Roll up the loop in vint/vlong in DataInput", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This issue proposes to roll up the loop in DataInput#readVInt and {{DataInput#readVLong{}}}. Previous talk can be found here: https://github.com/apache/lucene/pull/592. Benchmark: ", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "LUCENE-10253", "change_description": ": The @BadApple annotation has been removed from the test\nframework.", "change_title": "Remove BadApple test annotation", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "Lucene currently doesn't have a single test annotated with BadApple. Let's remove this annotation and try hard to not add any new flaky test? I know Solr is currently using this annotation so I plan to make this change 10.0-only to give Solr time to add this annotation on its end.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "LUCENE-10393", "change_description": ": Unify binary dictionary and dictionary writer in Kuromoji and Nori.", "change_title": "Should we unify the dictionary builder/loader of kuromoji and nori?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "A spin-off from LUCENE-8816. Kuromoji and Nori have many duplicated code in their dictionary builder/loader and we occasionally have to maintain both of them; I'd like to explore the possibility of their unification at some level.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "LUCENE-10475", "change_description": ": Merge dictionary builders in `util` package into `dict` package in Kuromoji and Nori.\nAll classes in `org.apache.lucene.analysis.[ja|ko].util` was moved to `org.apache.lucene.analysis.[ja|ko].dict`.", "change_title": "Reconsider package structure in kuromoji and nori to mininize classes' visibiilty", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "Some internal dictionary classes in `.dict` package are exposed to public in order to share their constants with dictionary writers that are reside in a separate package `.util`. `.util` actually has only dictionary writers/builders, it would make sense to merge all classes `.util` to `.dict` so that we can make the internals package-private. The change was suggested/discussed in https://github.com/apache/lucene/pull/740.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "LUCENE-10493", "change_description": ": Factor out Viterbi algorithm in Kuromoji and Nori to analysis-common.", "change_title": "Can we unify the viterbi search logic in the tokenizers of kuromoji and nori?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main)", "detail_description": "We now have common dictionary interfaces for kuromoji and nori (LUCENE-10393). A natural question would be: is it possible to unify the Japanese/Korean tokenizers? The core methods of the two tokenizers are `parse()` and `backtrace()` to calculate the minimum cost path by Viterbi search. I'd set the goal of this issue to factoring out them into a separate class (in analysis-common) that is shared between JapaneseTokenizer and KoreanTokenizer.  The algorithm to solve the minimum cost path itself is of course language-agnostic, so I think it should be theoretically possible; the most difficult part here might be the N-best path calculation - which is supported only by JapaneseTokenizer and not by KoreanTokenizer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "GITHUB#977", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "See also issue: https://issues.apache.org/jira/browse/LUCENE-9500 This bug was fixed in JDK-16 (see https://bugs.openjdk.org/browse/JDK-8252739 ) so it does not apply to main branch (Java 17) anymore.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "LUCENE-9500", "change_description": ",", "change_title": "Did we hit a DEFLATE bug?", "detail_type": "Bug", "detail_affect_versions": "8.x,9.0,8.7", "detail_fix_versions": "8.x,9.0,8.7", "detail_description": "I've been digging https://ci-builds.apache.org/job/Lucene/job/Lucene-Solr-NightlyTests-master/23/ all day and managed to isolate a simple reproduction that shows the problem. I've been starring at it all day and can't find what we are doing wrong, which makes me wonder whether we're calling DEFLATE the wrong way or whether we hit a DEFLATE bug. I've looked at it so much that I may be missing the most obvious stuff.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "GITHUB#11960", "change_description": ": Hunspell: supported empty dictionaries", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "GITHUB#12239", "change_description": ": Hunspell: reduced suggestion set dependency on the hash table order", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When adding words to a dictionary, suggestions for other words shouldn't change unless they're directly related to the added words. But before, GeneratingSuggester selected 100 best first matches from the hash table, whose order can change significantly after adding any unrelated word. That resulted in unexpected suggestion changes on seemingly unrelated dictionary edits.", "patch_link": "none", "patch_content": "none"}
{"library_version": "10.0.0", "change_type": "Other", "change_id": "GITHUB#9049", "change_description": ": Fixing bug in UnescapedCharSequence#toStringEscaped()", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There are a couple of issues with UnescapedCharSequence: There are no tests for UnescapedCharSequence so these issues have gone unnoticed for quite some time. Migrated from LUCENE-8001 by Shad Storhaug, updated Oct 21 2017 Linked issues: The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}

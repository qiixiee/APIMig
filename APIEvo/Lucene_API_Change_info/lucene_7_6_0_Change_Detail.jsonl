{"library_version": "7.6.0", "change_type": "Build", "change_id": "LUCENE-8504", "change_description": ": Upgrade forbiddenapis to version 2.6.", "change_title": "Update forbiddenapis to version 2.6", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "Forbiddenapis 2.6 was released today. The main change is support for Java 11. It also fixes many bugs with Gradle, but this is not interesting to Lucene. I will just commit the update, the issue is just a placeholder.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Build", "change_id": "LUCENE-8498", "change_description": ": Deprecate LowerCaseTokenizer and CharTokenizer static methods\nthat take normalizer functions", "change_title": "Deprecate/Remove LowerCaseTokenizer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.0", "detail_description": "LowerCaseTokenizer combines tokenization and filtering in a way that prevents us improving the normalization API.  We should deprecate and remove it, as it can be replaced simply with a LetterTokenizer and LowerCaseFilter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940478/LUCENE-8498.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Build", "change_id": "LUCENE-8493", "change_description": ": Stop publishing insecure .sha1 files with releases", "change_title": "Stop publishing .sha1 files with releases", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.5.1,7.6,8.0", "detail_description": "In LUCENE-7935 we added .sha512 checksums to releases and removed .md5 files. According to the Release Distribution Policy (http://www.apache.org/dev/release-distribution#sigs-and-sums): For every artifact distributed to the public through Apache channels, the PMC MUST supply a valid OpenPGP-compatible ASCII-armored detached signature file MUST supply at least one checksum file SHOULD supply a SHA-256 and/or SHA-512 checksum file SHOULD NOT supply a MD5 or SHA-1 checksum file (because these are deprecated) So this Jira will stop publishing .sha1 files, leaving only the .sha512", "patch_link": "https://issues.apache.org/jira/secure/attachment/12939322/LUCENE-8493.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8479", "change_description": ": QueryBuilder#analyzeGraphPhrase now throws TooManyClause exception\nif the number of expanded path reaches the BooleanQuery#maxClause limit.", "change_title": "QueryBuilder#analyzeGraphPhrase should respect max boolean clause", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "Currently there is no limit in the number of span queries that can be mixed by QueryBuilder#analyzeGraphPhrase even if the input graph contains thousands of paths. We should apply the same limit than analyzeGraphBoolean which throws TooManyClauses exception when the number of expanded paths is greater than BooleanQuery#MAX_CLAUSE_COUNT.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12941178/LUCENE-8479.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8522", "change_description": ": throw InvalidShapeException when constructing a polygon and\nall points are coplanar.", "change_title": "Spatial: Polygon touching the negative boundaries of WGS84 fails on Solr", "detail_type": "Bug", "detail_affect_versions": "7.4,7.5,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "When using the WGS84 coordinates system and querying with a polygon touching one of the \"negative\" borders, Solr throws a \"NullPointerException\" error. The query is performed with the \"intersect\" function over a GeoJson polygon specified with the coordinates:  The queried field has been defined as: ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12942787/LUCENE-8522.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8531", "change_description": ": QueryBuilder#analyzeGraphPhrase now creates one phrase query per finite strings\nin the graph if the slop is greater than 0. Span queries cannot be used in this case because\nthey don't handle slop the same way than phrase queries.", "change_title": "QueryBuilder hard-codes inOrder=true for generated sloppy span near queries", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "QueryBuilder.analyzeGraphPhrase() generates SpanNearQuery-s with passed-in phraseSlop, but hard-codes inOrder ctor param as true. Before multi-term synonym support and graph token streams introduced the possibility of generating SpanNearQuery-s, QueryBuilder generated (Multi)PhraseQuery-s, which always interpret slop as allowing reordering edits.  Solr's eDismax query parser generates phrase queries when its pf/pf2/pf3 params are specified, and when multi-term synonyms are used with a graph-aware synonym filter, SpanNearQuery-s are generated that require clauses to be in order; unlike with (Multi)PhraseQuery-s, reordering edits are not allowed, so this is a kind of regression.  See SOLR-12243 for edismax pf/pf2/pf3 context.  (Note that the patch on SOLR-12243 also addresses another problem that blocks eDismax from generating queries at all under the above-described circumstances.) I propose adding a new analyzeGraphPhrase() method that allows configuration of inOrder, which would allow eDismax to specify inOrder=false.  The existing analyzeGraphPhrase() method would remain with its hard-coded inOrder=true, so existing client behavior would remain unchanged.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12944015/LUCENE-8531.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8524", "change_description": ": Add the Hangul Letter Araea (interpunct) as a separator in Nori's tokenizer.\nThis change also removes empty terms and trim surface form in Nori's Korean dictionary.", "change_title": "Nori (Korean) analyzer tokenization issues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "I opened this originally as an Elastic bug, but was asked to re-file it here. (Sorry for the poor formatting. \"pre-formatted\" isn't behaving.) Elastic version {  \"name\" : \"adOS8gy\",  \"cluster_name\" : \"elasticsearch\",  \"cluster_uuid\" : \"GVS7gpVBQDGwtHl3xnJbLw\",  \"version\" : ,  \"tagline\" : \"You Know, for Search\" } Plugins installed: [analysis-icu, analysis-nori] JVM version:  openjdk version \"1.8.0_181\"  OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1~deb9u1-b13)  OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode) OS version:  Linux vagrantes6 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux Description of the problem including expected versus actual behavior: I've uncovered a number of oddities in tokenization in the Nori analyzer. All examples are from Korean Wikipedia or Korean Wiktionary (including non-CJK examples). In rough order of importance: A. Tokens are split on different character POS types (which seem to not quite line up with Unicode character blocks), which leads to weird results for non-CJK tokens: While it is still possible to find these words using Nori, there are many more chances for false positives when the tokens are split up like this. In particular, individual numbers and combining diacritics are indexed separately (e.g., in the Cyrillic example above), which can lead to a performance hit on large corpora like Wiktionary or Wikipedia. Work around: use a character filter to get rid of combining diacritics before Nori processes the text. This doesn't solve the Greek, Hebrew, or English cases, though. Suggested fix: Characters in related Unicode blocks—like \"Greek\" and \"Greek Extended\", or \"Latin\" and \"IPA Extensions\"—should not trigger token splits. Combining diacritics should not trigger token splits. Non-CJK text should be tokenized on spaces and punctuation, not by character type shifts. Apostrophe-like characters should not trigger token splits (though I could see someone disagreeing on this one). B. The character \"arae-a\" (ㆍ, U+318D) is sometimes used instead of a middle dot (·, U+00B7) for lists. When the arae-a is used, everything after the first one ends up in one giant token. 도로ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구 is tokenized as 도로 + ㆍ지반ㆍ수자원ㆍ건설환경ㆍ건축ㆍ화재설비연구. Work around: use a character filter to convert arae-a (U+318D) to a space. Suggested fix: split tokens on all instances of arae-a (U+318D). C. Nori splits tokens on soft hyphens (U+00AD) and zero-width non-joiners (U+200C), splitting tokens that should not be split. Work around: use a character filter to strip soft hyphens and zero-width non-joiners before Nori. Suggested fix: Nori should strip soft hyphens and zero-width non-joiners. D. Analyzing 그레이맨 generates an extra empty token after it. There may be others, but this is the only one I've found. Work around: at a min length token filter with a minimum length of 1. E. Analyzing 튜토리얼 generates a token with an extra space at the end of it. There may be others, but this is the only one I've found. No work around needed, I guess, since this is only the internal representation of the token. I'm not sure if it has any negative effects. Steps to reproduce: 1. Set up Nori analyzer curl -X PUT \"localhost:9200/nori?pretty\" -H 'Content-Type: application/json' -d' {   \"settings\" : {     \"index\": {       \"analysis\": {         \"analyzer\": {           \"text\": }       }     }   } } ' 2. Analyze example tokens: A. POS Types cause token splits curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , , ]     }   } curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , , , , , , , , , ]     }   } } curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , , , ]     }   } } curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } } curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } } curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } } B. arae-a as middle dot curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } } C. soft hyphens and zero-width non-joiners curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } } curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } }  D. 그레이맨 generates empty token curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ , ]     }   } } E. 튜토리얼 has a space added during tokenization curl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d ' ' {   \"detail\" : {     \"custom_analyzer\" : false,     \"analyzer\" : {       \"name\" : \"text\",       \"tokens\" : [ ]     }   } }", "patch_link": "https://issues.apache.org/jira/secure/attachment/12945190/LUCENE-8524.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8550", "change_description": ": Fix filtering of coplanar points when creating linked list on\npolygon tesselator.", "change_title": "Tessellator fails when filtering coplanar points when creating linked list", "detail_type": "Bug", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "Currently when creating the linked list on the tessellator, coplanar points are filtered. The problem is the following: if we have three coplanar points, the code is actually removing the last point, instead it should remove the middle one.  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946231/LUCENE-8550.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8549", "change_description": ": Polygon tessellator throws an error if some parts of the shape\n could not be processed.", "change_title": "Tessellator should throw an error if all points were not processed", "detail_type": "Bug", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "Currently, the tessellation in some situations when it has not successfully process all points in the polygon, it will still return an incomplete/wrong tessellation. For example the following code: will fail as the tessellator return a wrong tessellation containing one triangle.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946063/LUCENE-8549.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8540", "change_description": ": Better handling of min/max values for Geo3d encoding.", "change_title": "Geo3d quantization test failure for MAX/MIN encoding values", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.6,8.0", "detail_description": "Here is a reproducible error:  It seems this test will fail if encoding = Geo3DUtil.MIN_ENCODED_VALUE or encoding = Geo3DUtil.MAX_ENCODED_VALUE. It is related with https://issues.apache.org/jira/browse/LUCENE-7327   ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12945219/LUCENE-8540.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8534", "change_description": ": Fix incorrect computation for triangles intersecting polygon edges in\nshape tessellation.", "change_title": "Another case of Polygon tessellator going into an infinite loop", "detail_type": "Bug", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "Related to LUCENE-8454, another case where tesselator never returns when processing a polygon.  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12945606/LUCENE-8534.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8559", "change_description": ": Fix bug where polygon edges were skipped when checking for intersections.", "change_title": "Tessellator: isIntersectingPolygon method skip polygon edges", "detail_type": "Bug", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "The following condition seems wrong: Any node with the same X or Y is skipped.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946888/LUCENE-8559.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8556", "change_description": ": Use latitude and longitude instead of encoding values to check if triangle is ear\nwhen using morton optimisation.", "change_title": "Tessellator: Polygons can fail when using Morton optimisation", "detail_type": "Bug", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "I experience some errors when processing complex polygons. I realised that if I disable the Morton optimisation, then the errors go away. I studied one of the cases and it seems that when using the optimisation, it is possible to create triangles with points inside of them (see picture attached). There is a point just on the edge of the triangle. When disabling the optimisation, such a triangle is not created. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946652/LUCENE-8556.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8586", "change_description": ": Intervals.or() could get stuck in an infinite loop on certain indexes", "change_title": "Logic error in Intervals.or() can cause endless loop", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "An intervals source of the following type: can cause an infinite loop when queried, due to a bug in the 'no more intervals' logic in DisjunctionIntervalIterator", "patch_link": "https://issues.apache.org/jira/secure/attachment/12950415/LUCENE-8586.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8589", "change_description": ": Fix MultiPhraseQuery to not duplicate terms when building the phrase's weight.", "change_title": "MultiPhraseQuery adds each term twice when computing stats", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.6", "detail_description": "MultiPhraseQuery duplicates each term present in the phrase when computing the weight of the phrase. The bug is only in branch_7x (master works fine).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12950538/LUCENE-8589.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8595", "change_description": ": Fix interleaved DV update and reset. Interleaved update and reset value\nto the same doc in the same updates package looses an update if the reset comes before\nthe update as well as loosing the reset if the update comes frist.", "change_title": "TestMixedDocValuesUpdates.testTryUpdateMultiThreaded fails", "detail_type": "Bug", "detail_affect_versions": "8.0", "detail_fix_versions": "7.6,7.7,8.0", "detail_description": "It does reproduce ... I haven't dug in: ", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Bug fixes", "change_id": "LUCENE-8592", "change_description": ": Fix index sorting corruption due to numeric overflow. The merge of sorted segments\ncan produce an invalid sort if the sort field is an Integer/Long that uses reverse order and contains\nvalues equal to Integer/Long#MIN_VALUE. These values are always sorted first during a merge\n(instead of last because of the reverse order) due to this bug. Indices affected by the bug can be\ndetected by running the CheckIndex command on a distribution that contains the fix (7.6+).", "change_title": "MultiSorter#sort incorrectly sort Integer/Long#MIN_VALUE when the natural sort is reversed", "detail_type": "Bug", "detail_affect_versions": "7.5,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "MultiSorter#getComparableProviders on an integer or long field doesn't handle MIN_VALUE correctly when the natural order is reversed. To handle reverse sort we use the negation of the value but there is no check for overflows so MIN_VALUE for ints and longs are always sorted first (even if the natural order is reversed).  This method is used by index sorting when merging already sorted segments together. This means that a sorted index can be incorrectly sorted if it uses a reverse sort and a missing value set to MIN_VALUE (long or int or values inside the segment that are equals to MIN_VALUE). This a bad bug because it affects the documents order inside segments and only a reindex can restore the correct sort order.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12951021/LUCENE-8592.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "New Features", "change_id": "LUCENE-8496", "change_description": ": Selective indexing - modify BKDReader/BKDWriter to allow users\nto select a fewer number of dimensions to be used for creating the index than\nthe total number of dimensions used for field encoding. i.e., dimensions 0 to N\nmay be used to determine how to split the inner nodes, and dimensions N+1 to D\nare ignored and stored as data dimensions at the leaves.", "change_title": "Explore selective dimension indexing in BKDReader/Writer", "detail_type": "New Feature", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "7.6,8.0", "detail_description": "This issue explores adding a new feature to BKDReader/Writer that enables users to select a fewer number of dimensions to be used for creating the BKD index than the total number of dimensions specified for field encoding. This is useful for encoding dimensional data that is used for interpreting the encoded field data but unnecessary (or not efficient) for creating the index structure. One such example is LatLonShape encoding. The first 4 dimensions may be used to to efficiently search/index the triangle using its precomputed bounding box as a 4D point, and the remaining dimensions can be used to encode the vertices of the tessellated triangle. This causes BKD to act much like an R-Tree for shape data where search is distilled into a 4D point (instead of a more expensive 6D point) and the triangle is encoded using a portion of the remaining (non-indexed) dimensions. Fields that use the full data range for indexing are not impacted and behave as they normally would.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12941711/LatLonShape_SelectiveEncoding.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "New Features", "change_id": "LUCENE-8538", "change_description": ": Add a Simple WKT Shape Parser for creating Lucene Geometries (Polygon, Line,\nRectangle) from WKT format.", "change_title": "Add Simple WKT Shape Parser", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Similar to SimpleGeoJSONPolygonParser for creating Polygon objects from GeoJSON, it would be helpful to have a SimpleWKTParser for creating lucene geometries from WKT. Not only is this useful for simple tests, but also helps for benchmarking from real world data (e.g., PlanetOSM).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12945094/LUCENE-8538.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "New Features", "change_id": "LUCENE-8462", "change_description": ": Adds an Arabic snowball stemmer based on", "change_title": "New Arabic snowball stemmer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "Added a new Arabic snowball stemmer based on https://github.com/snowballstem/snowball/blob/master/algorithms/arabic.sbl As well an Arabic test dataset in `TestSnowballVocabData.zip` from the snowball-data generated from the input file available here https://github.com/snowballstem/snowball-data/tree/master/arabic https://github.com/ibnmalik/golden-corpus-arabic/blob/develop/core/words.txt  It also updates the ant patch-snowball target to be compatible with the java classes generated by the last snowball version (tree: 1964ce688cbeca505263c8f77e16ed923296ce7a). The ant patch-snowball target is retro-compatible with the version of snowball stemmers used in lucene 7.x and ignores already patched classes.  Link to the corresponding Github PR: https://github.com/apache/lucene-solr/pull/449 Edited: updated the corpus link, PR link and description ", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "New Features", "change_id": "https://github.com/snowballstem/snowball/blob/master/algorithms/arabic.sbl", "change_description": ": Adds an Arabic snowball stemmer based on", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "New Features", "change_id": "LUCENE-8554", "change_description": ": Add new LatLonShapeLineQuery that queries indexed LatLonShape fields\nby arbitrary lines.", "change_title": "Add new LatLonShapeLineQuery", "detail_type": "New Feature", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "None", "detail_description": "Its often useful to be able to query a shape index for documents that either INTERSECT or are DISJOINT from a given LINESTRING. Occasionally the linestring of interest may also have a distance component, which creates a buffered query (often used in routing, or shape snapping). This feature first adds a new LatLonShapeLineQuery for querying  LatLonShape fields by arbitrary lines. A distance component can then be added in a future issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946590/LUCENE-8554.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "New Features", "change_id": "LUCENE-8555", "change_description": ": Add dateline crossing support to LatLonShapeBoundingBoxQuery.", "change_title": "Add dateline crossing support to LatLonShapeBoundingBoxQuery", "detail_type": "New Feature", "detail_affect_versions": "7.6,8.0", "detail_fix_versions": "None", "detail_description": "Instead of rewriting into a BooleanQuery, LatLonShapeBoundingBoxQuery should handle dateline crossing support directly in the IntersectVisitor. This feature issue will add support for splitting a LatLonShapeBoundingBoxQuery into an east and west box and comparing the indexed LatLonShape fields against each. INTERSECTS, DISJOINT, and WITHIN will all be handled by the LatLonShapeQuery IntersectVisitor.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12946596/LUCENE-8555.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Improvements", "change_id": "LUCENE-8521", "change_description": ": Change LatLonShape encoding to 7 dimensions instead of 6; where the\nfirst 4 are index dimensions defining the bounding box of the Triangle and the\nremaining 3 data dimensions define the vertices of the triangle.", "change_title": "Change LatLonShape encoding to use selective indexing", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "LUCENE-8496 allows for selecting the first n dimensions to be used for building the index and the remaining dimensions to be used as data dimensions. This feature changes LatLonShape encoding to a 7 dimension encoding instead of 6; where the first 4 are index dimensions defining the bounding box of the LatLonShape.Triangle and the remaining 3 data dimensions defining the vertices of the triangle.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12942311/LUCENE-8521.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Improvements", "change_id": "LUCENE-8557", "change_description": ": LeafReader.getFieldInfos is now documented and tested that it ought to return\nthe same cached instance.  MemoryIndex's impl now pre-creates the FieldInfos instead of\nre-calculating a new instance each time.", "change_title": "LeafReader.getFieldInfos should always return the same instance", "detail_type": "Bug", "detail_affect_versions": "7.5", "detail_fix_versions": "None", "detail_description": "Most implementations of the LeafReader cache an instance of FieldInfos which is returned in the LeafReader.getFieldInfos() method.  There are a few places that currently do not and this can cause performance problems. The most notable example is the lack of caching in Solr's SlowCompositeReaderWrapper which caused unexpected performance slowdowns when trying to use Solr's JSON Facets compared to the legacy facets. This proposed change is mostly relevant to Solr but touches a few Lucene classes.  Specifically: 1. Adds a check to TestUtil.checkReader to verify that LeafReader.getFieldInfos() returns the same instance:  I'm not entirely sure this is wanted or needed but adding it uncovered most of the other LeafReader implementations that were not caching FieldInfos.  I'm happy to remove this part of the patch though.  2. Adds a FieldInfos.EMPTY that can be used in a handful of places  There are several places in the Lucene/Solr tests that were creating empty instances of FieldInfos which were causing the check in #1 to fail.  This fixes those failures and cleans up the code a bit. 3. Fixes a few LeafReader implementations that were not caching FieldInfos Specifically:  4. Minor Solr tweak to avoid calling SolrIndexSearcher.getSlowAtomicReader in FacetFieldProcessorByHashDV.  This change is now optional since SlowCompositeReaderWrapper caches FieldInfos.  As suggested by dsmiley this takes the place of SOLR-12878 since it touches some Lucene code. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12947106/LUCENE-8557.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Improvements", "change_id": "LUCENE-8558", "change_description": ": Replace O(N) lookup with O(1) lookup in PerFieldMergeState#FilterFieldInfos.", "change_title": "Adding NumericDocValuesFields is slowing down the indexing process significantly", "detail_type": "Improvement", "detail_affect_versions": "7.4,7.5", "detail_fix_versions": "7.6,8.0", "detail_description": "The indexing time for my ~2M documents has gone up significantly when I started adding fields of type NumericDocValuesField.  Upon debugging found the bottleneck to be in the PerFieldMergeState#FilterFieldInfos constructor. The contains check in the below code snippet was the culprit. A simple change as below seems to have fixed my issue ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12947143/LUCENE-8558.patch", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Other", "change_id": "LUCENE-8523", "change_description": ": Correct typo in JapaneseNumberFilterFactory javadocs", "change_title": "Fix typo for JapaneseNumberFilterFactory usage", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "Javadocs for JapaneseNumberFilterFactory have a typo -  https://lucene.apache.org/core/7_5_0/analyzers-kuromoji/org/apache/lucene/analysis/ja/JapaneseNumberFilterFactory.html Instead of  <filter class=\"solr.JapaneseNumberFilter\"/> We should have <filter class=\"solr.JapaneseNumberFilterFactory\"/>", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.6.0", "change_type": "Other", "change_id": "LUCENE-8533", "change_description": ": Fix Javadocs of DataInput#readVInt(): Negative numbers are\nsupported, but should be avoided.", "change_title": "DataInput#readVInt() supports negative numbers although not documented", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.6,8.0", "detail_description": "readVInt() has to return positive numbers (and zero), throw some exception in case of negative numbers. While for the sequence of bytes [-1, -1, -1, -1, 15] it returns -1. simplifying readVInt up to last readByte (exclusive): Here i = 268435455 or in binary format is 00001111_11111111_11111111_11111111 Keeping in mind that int is a signed type we have only 3 more bits before overflow happens or in another words (Integer.MAX_VALUE - i) >> 28 == 7 - that's max value could be stored in 5th byte to avoid overflow. Instead of has to be", "patch_link": "https://issues.apache.org/jira/secure/attachment/12944515/LUCENE-8533_fix_readVInt_javadoc.patch", "patch_content": "none"}

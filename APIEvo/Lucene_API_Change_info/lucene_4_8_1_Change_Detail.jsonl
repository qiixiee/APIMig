{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5639", "change_description": ": Fix PositionLengthAttribute implementation in Token class.", "change_title": "Fix implementation of PositionLengthAttribute in Token.java", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.7.3,4.8.1,4.9,6.0", "detail_description": "The Token class misses to correctly implement all clone/copy/equals/... stuff for PositionLengthAttribute.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643144/LUCENE-5639.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5635", "change_description": ": IndexWriter didn't properly handle IOException on TokenStream.reset(),\nwhich could leave the analyzer in an inconsistent state.", "change_title": "Better exception testing for indexwriter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "Currently we only catch exc-handling bugs depending on where a codec does i/o. Instead we should add CrankyCodec and CrankyAnalyzer, which throw random exceptions from any method.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5599", "change_description": ": HttpReplicator did not properly delegate bulk read() to wrapped\nInputStream.", "change_title": "HttpReplicator uses a lot of CPU for large files", "detail_type": "Bug", "detail_affect_versions": "4.7.1", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "The method responseInputStream of HttpClientBase wraps an InputStream in order to close it when it is done reading. However, the wrapper only overwrites the single-byte read() method, every other method is delegated to its parent (java.io.InputStream). Therefore, the more efficient read-methods like read(byte[] b) are all implemented by reading one byte after the other. In my test, it took 20 minutes to copy  an index of 38 GB. With the provided small patch, this was reduced to less than 10 minutes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639761/HttpClientBase.java.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5600", "change_description": ": HttpClientBase did not properly consume a connection if a server\nerror occurred.", "change_title": "HttpReplicator does not properly handle server failures", "detail_type": "Bug", "detail_affect_versions": "4.7.1", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "When ReplicationClient.updateNow() using an HttpReplicator encounters a server error (like Status Code 500), it throws a runtime exception instead of an IOException. Furthermore, it does not close the HttpClient it used, which leads to an Error if a BasicClientConnectionManager is used", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640607/LUCENE-5600.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5628", "change_description": ": Change getFiniteStrings to iterative not recursive\nimplementation, so that building suggesters on a long suggestion\ndoesn't risk overflowing the stack; previously it consumed one Java\nstack frame per character in the expanded suggestion.  If you are building\na suggester this is a nasty trap. (Robert Muir, Simon Willnauer,\nMike McCandless).", "change_title": "SpecialOperations.getFiniteStrings should not recurse", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "Today it consumes one Java stack frame per transition, which when used by AnalyzingSuggester is per character in each token.  This can lead to stack overflows if you have a long suggestion.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643219/LUCENE-5628.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5559", "change_description": ": Add additional argument validation for CapitalizationFilter\nand CodepointCountFilter.", "change_title": "Argument validation for TokenFilters having numeric constructor parameter(s)", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "Some TokenFilters have numeric arguments in their constructors. They should throw IllegalArgumentException for negative or meaningless values. Here is some examples that demonstrates invalid/meaningless arguments :", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638933/LUCENE-5559.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5641", "change_description": ": SimpleRateLimiter would silently rate limit at 8 MB/sec\neven if you asked for higher rates.", "change_title": "SimpleRateLimiter is too simple", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "I was playing with merge throttling and discovered that our SimpleRateLimiter is throttling far more than requested; e.g. I asked for 50 MB/sec merge throttling, but it throttled at more like 8 MB/sec. The problem is we are calling Thread.sleep on too-small (< 1 msec) times; on ordinary (non-real-time) JVMs, anything less than 1 msec is rounded up to 1 msec.  Also, System.nanoTime() is somewhat costly. To fix this, I think we should aggregate the incoming byte count, until it crosses a threshold of enough bytes to warrant pausing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643369/LUCENE-5641.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5644", "change_description": ": IndexWriter clears which threads use which internal\nthread states on flush, so that if an application reduces how many\nthreads it uses for indexing, that results in a reduction of how\nmany segments are flushed on a full-flush (e.g. to obtain a\nnear-real-time reader).", "change_title": "ThreadAffinityDocumentsWriterThreadPool should clear the bindings on flush", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "This class remembers which thread used which DWPT, but it never clears this \"affinity\".  It really should clear it on flush, this way if the number of threads doing indexing has changed we only use as many DWPTs as there are incoming threads.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12644216/LUCENE-5644.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5653", "change_description": ": JoinUtil with ScoreMode.Avg on a multi-valued field\nwith more than 256 values would throw exception.", "change_title": "JoinUtil - ArrayIndexOutOfBoundsException: 256", "detail_type": "Bug", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "TermsWithScoreCollector.MV.Avg refuse to resize array.  It occurs if JoinUtil.createJoinQuery() is called for multivalue join and avg scorring and the nested query found greater than TermsWithScoreCollector.INITIAL_ARRAY_SIZE (256) terms. As a result we have: ava.lang.ArrayIndexOutOfBoundsException: 256\\r\\n\\tat org.apache.lucene.search.join.TermsWithScoreCollector$MV$Avg.collect(TermsWithScoreCollector.java:246)\\r\\n\\tat org.apache.lucene.search.Scorer.score(Scorer.java:65)\\r\\n\\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)\\r\\n\\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)\\r\\n\\tat org.apache.lucene.search.join.JoinUtil.createJoinQuery(JoinUtil.java:80)\\r\\n\\tat org.apache.solr.search.join.ScoreJoinQParserPlugin$SameCoreJoinQuery.rewrite(ScoreJoinQParserPlugin.java:159)\\r\\n\\tat org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:636)\\r\\n\\tat org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:683)\\r\\n\\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:297)\\r\\n\\tat", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643795/LUCENE-5653.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5654", "change_description": ": Fix various close() methods that could suppress\nthrowables such as OutOfMemoryError, instead returning scary messages\nthat look like index corruption.", "change_title": "CompoundFileWriter.close suppresses OOME", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "I'm working on a test case, and it hit OOME while writing the compound file; the test itself is separately buggy, but what was confounding was that CompoundFileWriter continued trying to write to the RAMOutputStream even after it had already hit OOME. RAMOutputStream could be better here (e.g. only increment currentBufferIndex if switchCurrentBuffer succeeds), but also I think we should fix CompoundFileWriter.close to .closeWhileHandlingExc even on non-IOExc. It results in scary looking excs like this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643857/LUCENE-5654.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5656", "change_description": ": Fix rare fd leak in SegmentReader when multiple docvalues\nfields have been updated with IndexWriter.updateXXXDocValue and one\nhits exception.", "change_title": "IndexWriter leaks CFS handles in some exceptional cases", "detail_type": "Bug", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "in trunk: ant test  -Dtestcase=TestIndexWriterOutOfMemory -Dtests.method=testBasics -Dtests.seed=3D485DE153FCA22D -Dtests.nightly=true -Dtests.locale=no_NO -Dtests.timezone=CAT -Dtests.file.encoding=US-ASCII Seems to happen when an exception is thrown here: and the leak is from here:", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5660", "change_description": ": AnalyzingSuggester.build will now throw IllegalArgumentException if\nyou give it a longer suggestion than it can handle", "change_title": "AnalyzingSuggester needs reasonable limits on max suggestion length", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Issues like LUCENE-5628 LUCENE-5659 are hacks around the problem that the AnalyzingSuggester allows automata to \"explode\". I don't think we should try to hack up the automata lib to the point its un-understandable to support this, since its designed for smaller automata such as queries and is appropriate for that. We should instead just set appropriate limits out of box so AnalyzingSuggester doesnt blow up. Remember this is a suggester, it needs to be fast. The fact that the automata lib blows up on stupid unit tests or whatever are doing this, instead of being silently slow, is a good thing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12644101/LUCENE-5660.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5662", "change_description": ": Add missing checks to Field to prevent IndexWriter.abort\nif a stored value is null.", "change_title": "Missing Field null checks can result in aborted segments", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "This is bad because you lose other unrelated documents in IndexWriter's buffer. Simple example:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12644106/LUCENE-5662.patch", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5668", "change_description": ": Fix off-by-one in TieredMergePolicy", "change_title": "Off-by-1 error in TieredMergePolicy", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "When I was comparing performance of different UUIDs, I noticed that TMP was merging too soon and picking non-ideal merges as a result.  The fix is silly: Index: lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java =================================================================== — lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java\t(revision 1593975) +++ lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java\t(working copy) @@ -361,7 +361,7 @@          return spec;        } // OK we are over budget – find best merge!          MergeScore bestScore = null;", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.1", "change_type": "Bug fixes", "change_id": "LUCENE-5671", "change_description": ": Upgrade ICU version to fix an ICU concurrency problem that\ncould cause exceptions when indexing.", "change_title": "Upgrade ICU version", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "This has a bugfix for a concurrency issue, reported on our users list. I think this is bad because it will strike users randomly while indexing/querying. See http://bugs.icu-project.org/trac/ticket/10767 Apparently there is a better fix in the future, but the existing sync is enough to prevent the bug (my test passes 100% of the time with 53.1 whereas it fails 30% of the time with 52.1)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12644853/LUCENE-5671.patch", "patch_content": "none"}

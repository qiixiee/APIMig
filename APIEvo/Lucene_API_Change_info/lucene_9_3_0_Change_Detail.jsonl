{"library_version": "9.3.0", "change_type": "API Changes", "change_id": "LUCENE-10603", "change_description": ": SortedSetDocValues#NO_MORE_ORDS marked @deprecated in favor of iterating with\nSortedSetDocValues#docValueCount().", "change_title": "Improve iteration of ords for SortedSetDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "After SortedSetDocValues#docValueCount added since Lucene 9.2, should we refactor the implementation of ords iterations using docValueCount instead of NO_MORE_ORDS? Similar how SortedNumericDocValues did From to", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "API Changes", "change_id": "GITHUB#978", "change_description": ": Deprecate (remove in Lucene 10) obsolete constants in oal.util.Constants; remove\ncode which is no longer executed after Java 9.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR deprecates (and we will remove them in main) some useless constants in oal.util.Constants : In the rest of the code I removed all \"dead code\". At several places we have code that wasn't executed since Java 9, because we are not allowed to look into runtime classes anymore. I removed that code. This was mainly:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "New Features", "change_id": "LUCENE-10550", "change_description": ": Add getAllChildren functionality to facets", "change_title": "Add getAllChildren functionality to facets", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "Currently Lucene does not support returning range counts sorted by label values, but there are use cases demanding this feature. For example, a user specifies ranges (e.g., [0, 10], [10, 20]) and wants to get range counts without changing the range order. Today we can only call getTopChildren to populate range counts, but it would return ranges sorted by counts (e.g., [10, 20] 100, [0, 10] 50) instead of range values. Lucene has a API, getAllChildrenSortByValue, that returns numeric values with counts sorted by label values, please see LUCENE-7927 for details. Therefore, it would be nice that we can also have a similar API to support range counts. The proposed getAllChildren API is to return value/range counts sorted by label values instead of counts. This proposal was inspired from the discussions with gsmiller when I was working on the LUCENE-10538 PR, and we believe users would benefit from adding this API to Facets. Hope I can get some feedback from the community since this proposal would require changes to the getTopChildren API in RangeFacetCounts. Thanks!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "New Features", "change_id": "LUCENE-10274", "change_description": ": Added facetsets module for high dimensional (hyper-rectangle) faceting", "change_title": "Implement \"hyperrectangle\" faceting", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "I'd be interested in expanding Lucene's faceting capabilities to aggregate a point field against a set of user-provided n-dimensional hyperrectangles. This would be a generalization of LongRangeFacets / DoubleRangeFacets from a single dimension to n-dimensions, and would compliment PointRangeQuery well, providing the ability to facet ahead of \"drilling down\" on such a query. As a motivating use-case, imagine searching against movie documents that contain a 2-dimensional point storing \"awards\" the movie has received. One dimension encodes the year the award was won, while the other encodes the type of award as an ordinal. For example, the film \"Nomadland\" won the \"Academy Awards Best Picture\" award in 2021. Imagine providing a two-dimensional refinement to users allowing them to filter by the combination of award + year in a single action (e.g., using PointRangeQuery) and needing to get facet counts for these combinations ahead of time. Curious if the community thinks this functionality would be useful. Any thoughts?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "New Features", "change_id": "LUCENE-10151", "change_description": "Enable timeout support in IndexSearcher.", "change_title": "Add timeout support to IndexSearcher", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "I'd like to explore adding optional \"timeout\" capabilities to IndexSearcher. This would enable users to (optionally) specify a maximum time budget for search execution. If the search \"times out\", partial results would be available. This idea originated on the dev list (thanks jpountz for the suggestion). Thread for reference: http://mail-archives.apache.org/mod_mbox/lucene-dev/202110.mbox/%3CCAL8PwkZdNGmYJopPjeXYK%3DF7rvLkWon91UEXVxMM4MeeJ3UHxQ%40mail.gmail.com%3E A couple things to watch out for with this change:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Improvements", "change_id": "LUCENE-10078", "change_description": ": Merge on full flush is now enabled by default with a timeout of\n500ms.", "change_title": "Enable merge-on-refresh by default?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "This is a spinoff from the discussion in LUCENE-10073. The newish merge-on-refresh (crazy origin story) feature is a powerful way to reduce searched segment counts, especially helpful for applications using many indexing threads.  Such usage will write many tiny segments on each refresh, which could quickly be merged up during the refresh operation. We would have to implement a default for findFullFlushMerges (LUCENE-10064 is open for this), and then we would need IndexWriterConfig.getMaxFullFlushMergeWaitMillis a non-zero value (this issue).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Improvements", "change_id": "LUCENE-10585", "change_description": ": Facet module code cleanup (copy/paste scrubbing, simplification and some very minor\noptimization tweaks).", "change_title": "Cleanup copy/paste code in facets, particularly in SSDV", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main),9.3", "detail_description": "We've accumulated some copy/paste code in the facets modules, especially in SSDV-related classes. I'm going to take a pass at cleaning this up to help make the code more readable and easier to maintain.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Improvements", "change_id": "LUCENE-10603", "change_description": ": Update SortedSetDocValues iteration to use SortedSetDocValues#docValueCount().", "change_title": "Improve iteration of ords for SortedSetDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "After SortedSetDocValues#docValueCount added since Lucene 9.2, should we refactor the implementation of ords iterations using docValueCount instead of NO_MORE_ORDS? Similar how SortedNumericDocValues did From to", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Improvements", "change_id": "LUCENE-10619", "change_description": ": Optimize the writeBytes in TermsHashPerField.", "change_title": "Optimize the writeBytes in TermsHashPerField", "detail_type": "Improvement", "detail_affect_versions": "9.2", "detail_fix_versions": "9.3", "detail_description": "Because we don't know the length of slice, writeBytes will always write byte one after another instead of writing a block of bytes. May be we could return both offset and length in ByteBlockPool#allocSlice? 1. BYTE_BLOCK_SIZE is 32768, offset is at most 15 bits. 2. slice size is at most 200, so it could fit in 8 bits. So we could put them together into an int -------- offset | length There are only two places where this function is used，the cost of change it is relatively small. When allocSlice could return the offset and length of new Slice, we could change writeBytes like below If it could work, I'd like to raise a pr.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Improvements", "change_id": "GITHUB#983", "change_description": ": AbstractSortedSetDocValueFacetCounts internal code cleanup/refactoring.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "A little refactoring/cleanup of common functionality in AbstractSortedSetDocValueFacetCounts . No functional change.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8519", "change_description": ": MultiDocValues.getNormValues should not call getMergedFieldInfos", "change_title": "MultiDocValues.getNormValues should not call getMergedFieldInfos", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "MultiDocValues.getNormValues should not call getMergedFieldInfos because it's a needless expense.  getNormValues simply wants to know if each LeafReader that has this field has norms as well; that's all.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "GITHUB#961", "change_description": ": BooleanQuery can return quick counts for simple boolean queries.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As suggested by @zhaih on #950 , we could support more cases in BooleanWeight#count . This PR adds support for these cases specifically:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-10618", "change_description": ": Implement BooleanQuery rewrite rules based for minimumShouldMatch.", "change_title": "Implement BooleanQuery rewrite rules based for minimumShouldMatch", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "While looking into a test failure I noticed that we sometimes create weights for boolean queries with no SHOULD clauses and a non-zero minimumNumberShouldMatch. We could rewrite BooleanQuery to MatchNoDocsQuery when the number of SHOULD clauses is less than minimumNumberShouldMatch, and make SHOULD clauses required when the number of SHOULD clauses is equal to minimumNumberShouldMatch. This feels a bit like a degenerate case (why would the use create such a query in the first place?) but this case can also happen to non-degenerate queries if some SHOULD clauses rewrite to a MatchNoDocsQuery and get removed through rewrite.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-10480", "change_description": ": Implement Block-Max-Maxscore scorer for 2 clauses disjunction.", "change_title": "Specialize 2-clauses disjunctions", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "WANDScorer is nice, but it also has lots of overhead to maintain its invariants: one linked list for the current candidates, one priority queue of scorers that are behind, another one for scorers that are ahead. All this could be simplified in the 2-clauses case, which feels worth specializing for as it's very common that end users enter queries that only have two terms?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-10606", "change_description": ": For KnnVectorQuery, optimize case where filter is backed by BitSetIterator", "change_title": "Optimize hit collection of prefilter in KnnVectorQuery for BitSet backed queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "While working on this PR to add prefilter testing support, we saw that hit collection took a long time for BitSetIterator backed scorers (due to iteration over the entire underlying BitSet, and copying it into an internal one) These BitSetIterators can be frequent (as they are used in LRUQueryCache), and bulk collection can be optimized with more knowledge of the underlying iterator", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-10593", "change_description": ": Vector similarity function and NeighborQueue reverse removal.", "change_title": "VectorSimilarityFunction reverse removal", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "org.apache.lucene.index.VectorSimilarityFunction#EUCLIDEAN similarity behaves in an opposite way in comparison to the other similarities: A higher similarity score means higher distance, for this reason, has been marked with \"reversed\" and a function is present to map from the similarity to a score (where higher means closer, like in all other similarities.) Having this counterintuitive behavior with no apparent explanation I could find(please correct me if I am wrong) brings a lot of nasty side effects for the code readability, especially when combined with the NeighbourQueue that has a \"reversed\" itself. In addition, it complicates also the usage of the pattern: Result Queue -> MIN HEAP Candidate Queue -> MAX HEAP In HNSW searchers. The proposal in my Pull Request aims to: 1) the Euclidean similarity just returns the score, in line with the other similarities, with the formula currently used to move from distance to score 2) simplify the code, removing the bound checker that's not necessary anymore 3) refactor here and there to be in line with the simplification 4) refactor of NeighborQueue to clearly state when it's a MIN_HEAP or MAX_HEAP, now debugging is much easier and understanding the HNSW code is much more intuitive", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "GITHUB#984", "change_description": ": Use primitive type data structures in FloatTaxonomyFacets and IntTaxonomyFacets\n#getAllChildren() internal implementation to avoid some garbage creation.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Let's avoid creating some garbage and unnecessary boxing/unboxing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "GITHUB#1010", "change_description": ": Specialize ordinal encoding for common case in SortedSetDocValues.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This follows up the work done in LUCENE-10067 by adding additional specialization for SORTED_SET doc values.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-10657", "change_description": ": CopyBytes now saves one memory copy on ByteBuffersDataOutput.", "change_title": "CopyBytes now saves one memory copy on ByteBuffersDataOutput", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "This is derived from [LUCENE-10627|https://github.com/apache/lucene/pull/987] Code: https://github.com/apache/lucene/pull/1034 The abstract method `copyBytes` in DataOutput have to copy from input to a copyBuffer and then write into ByteBuffersDataOutput.blocks, i think there is unnecessary, we can override it, copy directly from input into output. with override this method, ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "GITHUB#1007", "change_description": ": Optimize IntersectVisitor#visit implementations for certain bulk-add cases.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is very minor, but it caught my eye when looking through PointRangeQuery for other reasons. Unfortunately, I don't know that we have any benchmarks in luceneutil that cover this, but I wouldn't expect anything to show.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Optimizations", "change_id": "LUCENE-10653", "change_description": ": BlockMaxMaxscoreScorer uses heapify instead of individual adds.", "change_title": "Should BlockMaxMaxscoreScorer rebuild its heap in bulk?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "BMMScorer has to frequently rebuild its heap, and does do by clearing and then iteratively calling add. It would be more efficient to heapify in bulk. This is more academic than anything right now though since BMMScorer is only used with two-clause disjunctions, so it's sort of a silly optimization if it's not supporting a greater number of clauses.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Changes in runtime behavior", "change_id": "GITHUB#978", "change_description": ": IndexWriter diagnostics written to index only contain java's runtime version\nand vendor.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR deprecates (and we will remove them in main) some useless constants in oal.util.Constants : In the rest of the code I removed all \"dead code\". At several places we have code that wasn't executed since Java 9, because we are not allowed to look into runtime classes anymore. I removed that code. This was mainly:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10574", "change_description": ": Prevent pathological O(N^2) merging.", "change_title": "Remove O(n^2) from TieredMergePolicy or change defaults to one that doesn't do this", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "Remove floorSegmentBytes parameter, or change lucene's default to a merge policy that doesn't merge in an O(n^2) way. I have the feeling it might have to be the latter, as folks seem really wed to this crazy O(n^2) behavior.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10584", "change_description": ": Properly support #getSpecificValue for hierarchical dims in SSDV faceting.", "change_title": "SSDV facets should support hierarchical paths in #getSpecificValue", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "We added hierarchical pathing capabilities to SSDV faceting recently but it looks like we didn't update #getSpecificValue to work with hierarchical paths.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10582", "change_description": ": Fix merging of overridden CollectionStatistics in CombinedFieldQuery", "change_title": "CombinedFieldQuery fails with distributed field statistics", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "CombinedFieldQuery does not properly combine distributed collection statistics, resulting in an IllegalArgumentException during searches. Originally surfaced in this Elasticsearch issue: https://github.com/elastic/elasticsearch/issues/82817", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10563", "change_description": ": Fix failure to tessellate complex polygon", "change_title": "Unable to Tessellate polygon", "detail_type": "Bug", "detail_affect_versions": "9.1", "detail_fix_versions": "9.3", "detail_description": "Following up to LUCENE-10470, I found some more polygons that cause Tessellator.tessellate to throw \"Unable to Tessellate shape\", which are not covered by the fix to LUCENE-10470. I attached the geojson of 3 failing shapes that I got, and this is the branch I am testing on that demonstrates the tessellation failures.  polygon-1.json polygon-2.json polygon-3.json", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10605", "change_description": ": Fix error in 32bit jvm object alignment gap calculation", "change_title": "fix error in 32bit jvm object alignment gap calculation", "detail_type": "Bug", "detail_affect_versions": "8.11.1", "detail_fix_versions": "9.3", "detail_description": "ArrayUtil.oversize(int minTargetSize, int bytesPerElement) This method is used to calculate the optimal length of an array during expansion.   According to current logic,in order to avoid space waste caused by object alignment gap. In 32-bit JVM,the array length will select the numbers(the current optional columns) in the table below. But the results weren't perfect. For example, if I want to expand byte[2], I will call the method oversize(2,1) to get the size of the next array, which returns 8. But byte [8] is not the best result. Since byte[8] and byte[12] use the same memory space (both are 24 bytes due to alignment gap), So it's best to return 12 here. See the table below.  I used jol-core to calculate object alignment gap Execute the following code: To further verify that the tool's results are correct, I wrote the following code to infer how much space the array of different lengths actually occupies based on when the OOM occursThe conclusion is consistent with jol-core. new byte[5] and new byte[12] use the same amount of memory  In addition - XX: ObjectAlignmentInBytes should also affect the return value of this method. But I don't know whether it is necessary to do this function. If necessary, I will modify it together. Thank you very much! ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "GITHUB#956", "change_description": ": Make sure KnnVectorQuery applies search boost.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Before, the rewritten query DocAndScoreQuery ignored the boost.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10598", "change_description": ": SortedSetDocValues#docValueCount() should be always greater than zero.", "change_title": "SortedSetDocValues#docValueCount() should be always greater than zero", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "This test runs failed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10600", "change_description": ": SortedSetDocValues#docValueCount should be an int, not long", "change_title": "SortedSetDocValues#docValueCount should be an int, not long", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10611", "change_description": ": Fix failure when KnnVectorQuery has very selective filter", "change_title": "KnnVectorQuery throwing Heap Error for Restrictive Filters", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "The HNSW graph search does not consider that visitedLimit may be reached in the upper levels of graph search itself This occurs when the pre-filter is too restrictive (and its count sets the visitedLimit). So instead of switching over to exactSearch, it tries to pop from an empty heap and throws an error  To reproduce this error, we can increase the numDocs here to 20,000 (so that nodes have more neighbors, and visitedLimit is reached faster)  Stacktrace:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10607", "change_description": ": Fix potential integer overflow in maxArcs computions", "change_title": "NRTSuggesterBuilder扩展input时溢出", "detail_type": "Bug", "detail_affect_versions": "9.2", "detail_fix_versions": "9.3", "detail_description": "suggest模块在创建索引时，调用NRTSuggestBuilder的finishTerm来写入suggest索引。 会调用maxNumArcsForDedupByte函数来扩展analyzed,向后扩展3 5 7 .... 255。 当entries长度过长（9000000）时，调用maxNumArcsForDedupByte扩展时  private static int maxNumArcsForDedupByte(int currentNumDedupBytes) { int maxArcs = 1 + (2 * currentNumDedupBytes); if (currentNumDedupBytes > 5) return Math.min(maxArcs, 255); }  另外在扩展时，是否可以选择固定4字节来有序扩展。代替 3 5 7 ... 255的扩展方式 ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "GITHUB#986", "change_description": ": Fix FieldExistsQuery rewrite when all docs have vectors.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Before we were checking the number of vectors in the segment against the total number of documents in IndexReader. This meant FieldExistsQuery would not rewrite to MatchAllDocsQuery when there were multiple segments.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10623", "change_description": ": Error implementation of docValueCount for SortingSortedSetDocValues", "change_title": "Error implementation of docValueCount for SortingSortedSetDocValues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Test failed below：  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Bug Fixes", "change_id": "GITHUB#1028", "change_description": ": Fix error in TieredMergePolicy", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fix error in comparing between bytes of candidates and bytes of max merge. It's wrong to use candidateSize rather than currentCandidateBytes comparing with maxMergeBytes . Minor change to fix it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Other", "change_id": "GITHUB#991", "change_description": ": Update randomizedtesting to 2.8.0, hppc to 0.9.1, morfologik to 2.1.9.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Other", "change_id": "LUCENE-10370", "change_description": ": pass proper classpath/module arguments for forking jvms from within tests.", "change_title": "Fix classpath/module path of tests forking their own Java (TestNRTReplication)", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "TestNRTReplication fails because it assumes classpath can just be copied to a sub-process - this is no longer the case. PR at: https://github.com/apache/lucene/pull/909", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Other", "change_id": "LUCENE-10604", "change_description": ": Improve ability to test and debug triangulation algorithm in Tessellator.", "change_title": "Improve ability to test and debug triangulation algorithm in Tessellator", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.3", "detail_description": "While working on https://issues.apache.org/jira/browse/LUCENE-10563, it was found to be extremely difficult to debug the inner workings of the triangulation algorithm in the Tessellator class. To make things easier, we made use of a monitor interface that tests could use to instrument the algorithm and receive detailed information on the state of the algorithms progress. This issue requests that this interface and a test implementation of it be included in Lucene to facilitate future bug-fixing of this complex algorithm.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Other", "change_id": "GITHUB#922", "change_description": ": Remove unused and confusing FacetField indexing options", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Change the index option for FacetField to just index the DOCS and not the frequencies and offsets (we don't use these values). I still need to think a bit more about the long term implications of this change. Opening this PR as a starting point for now. Existing tests pass. I looked for all instances where the code was traversing through the postings list in the facet module and none of them were relying on the frequency of the term (this is something that confused me at first, do we have no use cases for FacetField that rely on the frequency? apparently not.. If a use case does come up we could change the field type again). I ran the wikimediumall benchmark with a modified localrun.py script that used a new index for the candidate and it did not show any sizeable QPS changes. Size of the facet index also remained the same at 64 MB. I've not thought about this in depth (yet). Existing back-compat tests in branch_9x pass.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.3.0", "change_type": "Build", "change_id": "GITHUB#976", "change_description": ": Exclude Lucene's own JAR files from classpath entries in Eclipse config.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR fixes the Eclipse task to exclude Lucene's own JAR files. I think this problem was introduced when we switched to module system, previously the project dependencies were just directories with class files, now it is lucene's JAR files. This PR is a bit hacky, it applies regex to filename. @dweiss : If you have a better idea how to exclude project dependencies, tell me.", "patch_link": "none", "patch_content": "none"}

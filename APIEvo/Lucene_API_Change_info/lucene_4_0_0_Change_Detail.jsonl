{"library_version": "4.0.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4392", "change_description": ": Class org.apache.lucene.util.SortedVIntList has been removed.", "change_title": "Remove SortedVIntList", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0", "detail_description": "It looks like SortedVIntList only referenced by its test case, maybe we should just remove it?", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4393", "change_description": ": RollingCharBuffer has been moved to the o.a.l.analysis.util\npackage of lucene-analysis-common.", "change_title": "Move RollingCharBuffer to lucene-analysis-common", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0", "detail_description": "It looks like RollingCharBuffer is only used by analyzers. Maybe it would make sense to move it to lucene-analysis-common?", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "New Features", "change_id": "LUCENE-1888", "change_description": ": Added the option to store payloads in the term\nvectors (IndexableFieldType.storeTermVectorPayloads()). Note\nthat you must store term vector positions to store payloads.", "change_title": "Provide Option to Store Payloads on the Term Vector", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Would be nice to have the option to access the payloads in a document-centric way by adding them to the Term Vectors.  Naturally, this makes the Term Vectors bigger, but it may be just what one needs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "New Features", "change_id": "LUCENE-3892", "change_description": ": Add a new BlockPostingsFormat that bulk-encodes docs,\nfreqs and positions in large (size 128) packed-int blocks for faster\nsearch performance.  This was from Han Jiang's 2012 Google Summer of\nCode project", "change_title": "Add a useful intblock postings format (eg, FOR, PFOR, PFORDelta, Simple9/16/64, etc.)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "On the flex branch we explored a number of possible intblock encodings, but for whatever reason never brought them to completion. There are still a number of issues opened with patches in different states. Initial results (based on prototype) were excellent (see http://blog.mikemccandless.com/2010/08/lucene-performance-with-pfordelta-codec.html ). I think this would make a good GSoC project.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12538373/LUCENE-3892-blockFor%26hardcode%28base%29.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "New Features", "change_id": "LUCENE-4323", "change_description": ": Added support for an absolute maximum CFS segment size\n(in MiB) to LogMergePolicy and TieredMergePolicy.", "change_title": "Add max cfs segment size to LogMergePolicy and TieredMergePolicy", "detail_type": "Improvement", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,6.0", "detail_description": "Our application is managing thousands of indexes ranging from a few KB to a few GB in size. To keep the number of files under control and at the same time avoid the overhead of compound file format for large segments, we would like to keep only small segments as CFS. The meaning of \"small\" here is in absolute byte size terms, not as a percentage of the overall index. It is ok and in fact desirable to have the entire index as CFS as long as it is below the threshold. The attached patch adds a new configuration option maxCFSSegmentSize which sets the absolute limit on the compound file segment size, in addition to the existing noCFSRatio, i.e. the lesser of the two will be used. The default is to allow any size (Long.MAX_VALUE) so that the default behavior is exactly as it was before. The patch is for the trunk as of Aug 23, 2012.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12542273/diskFullFix.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "New Features", "change_id": "LUCENE-4339", "change_description": ": Allow deletes against 3.x segments for easier upgrading.\nLucene3x Codec is still otherwise read-only, you should not set it\nas the default Codec on IndexWriter, because it cannot write new segments.", "change_title": "Allow deletions on Lucene3x codecs again", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0", "detail_description": "On dev@lao Hoss reported that a user in Solr was not able to update or delete documents in his 3.x index with Solr 4: On the solr-user list, Dirk Högemann recently mentioned a problem he was seeing when he tried upgrading his existing solr setup from 3.x to 4.0-BETA.  Specifically this exception getting logged... http://find.searchhub.org/document/cdb30099bfea30c6 auto commit error...:java.lang.UnsupportedOperationException: this codec can only be used for reading          at org.apache.lucene.codecs.lucene3x.Lucene3xCodec$1.writeLiveDocs(Lucene3xCodec.java:74)          at org.apache.lucene.index.ReadersAndLiveDocs.writeLiveDocs(ReadersAndLiveDocs.java:278)          at org.apache.lucene.index.IndexWriter$ReaderPool.release(IndexWriter.java:435)          at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:278)          at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2928)          at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2919)          at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2666)          at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2793)          at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2773)          at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:531)          at org.apache.solr.update.CommitTracker.run(CommitTracker.java:214) Dirk was able to work arround this by completely re-indexing, but it seemed strange to me that this would happen. My understanding is that even though an IndexUpgrader tool was now available, it wasn't going to be required for users to use it when upgrading from 3.x to 4.x.  Explicitly upgrading the index format might be a good idea, and might make hte index more performant, but as I understood it, the way things had been implemented with codecs explicitly upgrading the index format wasn't strictly neccessary, and that users should be able to upgrade their lucene apps same way that was supported with other index format upgrades in the past: the old index can be read, and as changes are made new segments will be re-written in the new format.  (Note in particular: at the moment we don't mention IndexUpgrader in MIGRATE.txt at all.) It appears however, based on this stack trace and some other experiements i tried, that any attempts to \"delete\" documents in a segment that is using the Lucene3xCodec will fail. This seems like a really scary time bomb sitaution, because if you upgrade, things will seem to be working – you can even add documents, and depending on the order that you do things, some \"old\" segments may get merged and use the new format, so some deletes of \"old\" documents (in those merged/upgraded) segments may work, but then somewhere down the road, you may try to a delete that affects docs in a still un-merge/upgraded segment, and that delete will fail – 5 minutes later, if another merge has happened, attempting to do the exact same delete may succeed. All of which begs the question: is this a known/intended limitation of the Lucene3xCodec, or an oversight in the Lucene3xCodec? if it's expected, then it seems like we should definitely spell out this limitation in MIGRATE.txt and advocate either full rebuilds, or the use of IndexUpgrader for anyone who's indexes are non-static. On the Solr side of things, i think we should even want to consider automaticly running IndexUpgrader on startup if we detect that the Lucene3xCodec is in use to simplify things – we can't even suggest running \"optimize\" as a quick/easy way to force and index format upgrade because if the 3x index as already optimized then it's a no-op and the index stays in the 3x format. Robert said, that this is a wanted limitation (in fact its explicitely added to the code, without that UOE it \"simply works\"), but I disagree here and lots of other people: In the early days (I mean in the time when it was already read only until we refactored the IndexReader.delete()/Codec stuff), this was working, because the LiveDocs were always handled in a special way. Making it now 100% read-only is in my opinion very bad, as it does not allow to update documents in a 3.x index anymore, so you have no chance, you must run IndexUpgrader. The usual step like opening old Index and adding documents works (because the new documents are added always to new segment), but the much more usual IW.updateDocument() which is commonly used also to add documents fails on old Indexes. This is a no-go, we have to fix this. If we allow the trick with updating LiveDocs on 3.x codec, for the end-user the \"read-only\" stuff in Lucene3x codec would be completely invisible, as he can do everything IndexWriter provides. The other horrible things like changing norms is no longer possible, so deletes are the only thing that affects here. The read-only ness of Lucene3x codec would only be visible to the user when someone tries to explicitly create an index with Lucene3x codec. And I understood the CHANGES/MIGRATE.txt exactly as that. On the list, Robert added a simple patch, reverting the UOE in Lucene3xCodec, so the LiveDocs format is RW again.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543074/LUCENE-4339.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "New Features", "change_id": "SOLR-3441", "change_description": ": ElisionFilterFactory is now MultiTermAware", "change_title": "Make ElisionFilterFactory MultiTermAware", "detail_type": "Improvement", "detail_affect_versions": "3.6", "detail_fix_versions": "4.0,6.0", "detail_description": "The ElisionFilterFactory (which removes l' from l'avion) is not MultiTermAware - which includes release 3.6. I wanted to use a wildcard such as: (l'aub*). Seems simple enough to address. I'll attach a patch.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543217/SOLR-3441.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4391", "change_description": ",", "change_title": "Lucene40Codec methods should be final", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0", "detail_description": "I think all methods but getPostingsFormatForField should be made final so that users can't create a Codec that redefines any of the formats of Lucene40 by subclassing (since the codec name can't be overriden by subclassing, Lucene will fail at loading segments that use such codecs).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12545440/LUCENE-4391.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4440", "change_description": ",", "change_title": "FilterCodec should take a delegate Codec in its ctor", "detail_type": "Improvement", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,4.1,6.0", "detail_description": "FilterCodec has a delegate() method through which an extension can return its delegate Codec. This method is called on every Codec method. Adrien, on LUCENE-4391, failed to pass a Codec in the ctor, since he couldn't called Codec.forName(). Instead, we should just pass e.g. new Lucene40Codec(). I'll post a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12546881/LUCENE-4440.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4299", "change_description": ": Added Terms.hasPositions() and Terms.hasOffsets().\nPreviously you had no real way to know that a term vector field\nhad positions or offsets, since this can be configured on a\nper-field-per-document basis.", "change_title": "No way to find term vectors options at read time", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "The problem is simple: So I propose that instead of returning Terms for Vectors, we return VectorTerms (extends Terms), which just adds hasOffsets() and hasPositions(). e.g. lucene40 already knows this from the bits for the field/doc pair and just returns what it knows.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540479/LUCENE-4299.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4152", "change_description": ": added IndexReader.leaves(), which lets you enumerate\nthe leaf atomic reader contexts for all readers in the tree.", "change_title": "add one-syllable method to IndexReader enumerate subreaders", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Description is exactly as written. getSequentialSubReaders/getTopLevelReaderContext, these method names are way too long/unuseable. They also have tricky semantics (e.g. returning null). In lucene 4, people cannot just use any indexreader and get a merged view. So we need to make this stuff easy on them:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540771/LUCENE-4152.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4304", "change_description": ": removed PayloadProcessorProvider. If you want to change\npayloads (or other things) when merging indexes, its recommended\nto just use a FilterAtomicReader + IndexWriter.addIndexes. See the\nOrdinalMappingAtomicReader and TaxonomyMergeUtils in the facets\nmodule if you want an example of this.", "change_title": "Remove PayloadProcessProvider", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Now that we have pluggable codecs (well, PostingsFormat), an app should use a custom PostingsFormat if it really must change payloads while merging. Alternatively, use a FilteredIndexReader to modify anything during addIndexes (eg the facets use case, modifying payloads). Since this capability can be handled by existing more-generic functions I don't see why we need to keep PPP around in core.  PPP is also fragile because an app generally has no visibility on when a merge commits so it can't know if the payloads it retrieves are pre or post PPP. I think merging shouldn't change postings as a side-effect (by default, anyway, since a custom PF can of course override merge and do whatever it wants).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540831/LUCENE-4304.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4304", "change_description": ": Make CompositeReader.getSequentialSubReaders()\nprotected. To get atomic leaves of any IndexReader use the new method\nleaves() (", "change_title": "Remove PayloadProcessProvider", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Now that we have pluggable codecs (well, PostingsFormat), an app should use a custom PostingsFormat if it really must change payloads while merging. Alternatively, use a FilteredIndexReader to modify anything during addIndexes (eg the facets use case, modifying payloads). Since this capability can be handled by existing more-generic functions I don't see why we need to keep PPP around in core.  PPP is also fragile because an app generally has no visibility on when a merge commits so it can't know if the payloads it retrieves are pre or post PPP. I think merging shouldn't change postings as a side-effect (by default, anyway, since a custom PF can of course override merge and do whatever it wants).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540831/LUCENE-4304.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4152", "change_description": ": Make CompositeReader.getSequentialSubReaders()\nprotected. To get atomic leaves of any IndexReader use the new method\nleaves() (", "change_title": "add one-syllable method to IndexReader enumerate subreaders", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Description is exactly as written. getSequentialSubReaders/getTopLevelReaderContext, these method names are way too long/unuseable. They also have tricky semantics (e.g. returning null). In lucene 4, people cannot just use any indexreader and get a merged view. So we need to make this stuff easy on them:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540771/LUCENE-4152.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4307", "change_description": ": Renamed IndexReader.getTopReaderContext to\nIndexReader.getContext.", "change_title": "rename IR.getTopReaderContext to IR.getContext", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "IR/IS.getTopReaderContext is supposed to read as \"get the ReaderContext, with this reader treated as the top\". But every time I look at it, it reads as \"get context of the top-level reader\". This makes sense for IndexSearcher, but is confusing for IndexReader. I think it should simply be IR.getContext() for IndexReader.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540830/LUCENE-4307.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4316", "change_description": ": Deprecate Fields.getUniqueTermCount and remove it from\nAtomicReader. If you really want the unique term count across all\nfields, just sum up Terms.size() across those fields. This method\nonly exists so that this statistic can be accessed for Lucene 3.x\nsegments, which don't support Terms.size().", "change_title": "Deprecate Fields.getUniqueTermCount, remove AtomicReader.getUniqueTermCount", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Spinoff from LUCENE-4315. This was my comment there:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12541639/LUCENE-4316.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4321", "change_description": ": Change CharFilter to extend Reader directly, as FilterReader\noverdelegates (read(), read(char[], int, int), skip, etc). This made it\nhard to implement CharFilters that were correct. Instead only close() is\ndelegated by default: read(char[], int, int) and correct(int) are abstract\nso that its obvious which methods you should implement.  The protected\ninner Reader is 'input' like CharFilter in the 3.x series, instead of 'in'.", "change_title": "java.io.FilterReader considered harmful", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,6.0", "detail_description": "See Dawid's email: http://find.searchhub.org/document/64b0a28c53faf39 Reader.java is fine, it has lots of methods like read(), read(char[]), read(CharBuffer), skip(), but these all have default implementations delegating to read(char[], int, int). Unfortunately FilterReader delegates too many unnecessary things such as read() and skip() in a broken way. It should have just left these alone. This can cause traps for someone upgrading because they have to override multiple methods, when read(char[], int, int) should be enough, and all Reader methods will then work correctly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12542000/LUCENE-4321.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-3309", "change_description": ": The expert FieldSelector API, used to load only certain\nfields in a stored document, has been replaced with the simpler\nStoredFieldVisitor API.", "change_title": "Add narrow API for loading stored fields, to replace FieldSelector", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "FieldTypebranch", "detail_description": "I think we should \"invert\" the FieldSelector API, with a \"push\" API whereby FieldsReader invokes this API once per field in the document being visited. Implementations of the API can then do arbitrary things like save away the field's size, load the field, clone the IndexInput for later lazy loading, etc. This very thin API would be a mirror image of the very thin index time API we now have (IndexableField) and, importantly, it would have no dependence on our \"user space\" Document/Field/FieldType impl, so apps are free to do something totally custom. After we have this, we should build the \"sugar\" API that rebuilds a Document instance (ie IR.document(int docID)) on top of this new thin API.  This'll also be a good test that the API is sufficient. Relevant discussions from IRC this morning at http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76", "patch_link": "https://issues.apache.org/jira/secure/attachment/12486421/LUCENE-3309.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4343", "change_description": ": Made Tokenizer.setReader final. This is a setter that should\nnot be overriden by subclasses: per-stream initialization should happen\nin reset().", "change_title": "Clear up more Tokenizer.setReader/TokenStream.reset issues", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "spinoff from user-list thread. I think the rename helps, but the javadocs still have problems: they seem to only describe a totally wacky case (CachingTokenFilter) and not the normal case. Ideally setReader would be final I think, but there are a few crazy tokenstreams to fix before I could make that work. Would also need something hackish so MockTokenizer's state machine is still functional. But i worked on fixing up the mess in our various tokenstreams, which is easy for the most part. As part of this I found it was really useful in flushing out test bugs (ones that dont use MockTokenizer, which they really should), if we can do some best-effort exceptions when the consumer is broken and it costs nothing. For example: I think this is worth exploring more... this was really effective at finding broken tests etc. We should see if we can be more thorough/ideally throw better exceptions when consumers are broken and its free.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543114/LUCENE-4343.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4377", "change_description": ": Remove IndexInput.copyBytes(IndexOutput, long).\nUse DataOutput.copyBytes(DataInput, long) instead.", "change_title": "consolidate various copyBytes() methods", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Spinoff of LUCENE-4371: I don't think the default impl (SlicedIndexInput) should overrided BII's copyBytes? Seems ... spooky. There are copyBytes everywhere, mostly not really being used. Particularly DataOutput.copyBytes(DataInput) versus IndexInput.copyBytes(IndexOutput). Bulk merging already uses DataOutput.copyBytes(DataInput), its the most general (as it works on DataInput/Output), and its in dst, src order. I think we should remove IndexInput.copyBytes, its not necessary.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12544737/LUCENE-4377.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4355", "change_description": ": Simplify AtomicReader's sugar methods such as termDocsEnum,\ntermPositionsEnum, docFreq, and totalTermFreq to only take Term as a\nparameter. If you want to do expert things such as pass a different\nBits as liveDocs, then use the flex apis (fields(), terms(), etc) directly.", "change_title": "improve AtomicReader sugar apis", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "I thought about this after looking @ LUCENE-4353: AtomicReader has some sugar APIs that are over top of the flex apis (Fields, Terms, ...). But these might be a little trappy/confusing compared to 3.x.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12544614/LUCENE-4355.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "API Changes", "change_id": "LUCENE-4425", "change_description": ": clarify documentation of StoredFieldVisitor.binaryValue\nand simplify the api to binaryField(FieldInfo, byte[]).", "change_title": "Unclear documentation of StoredFieldVisitor.binaryValue", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.0,6.0", "detail_description": "When reading the binary value of a stored field, a StoredFieldsReader calls StoredFieldVisitor.binaryValue(arr, offset, length). Documentation currently doesn't state whether the byte[] can be reused outside of the scope of StoredFieldVisitor.binaryValue but DocumentStoredFieldVisitor assumes (as of r1389812) that it can. So DocumentStoredFieldVisitor would break with a custom StoredFieldsFormat that would call StoredFieldVisitor.binaryValue with a slice of a reusable buffer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12546638/LUCENE-4425.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4423", "change_description": ": DocumentStoredFieldVisitor.binaryField ignored offset and\nlength.", "change_title": "DocumentStoredFieldVisitor.binaryField ignores offset and length", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0", "detail_description": "This is no problem with SimpleText and Lucene40 since in their cases, offset is always 0 and length the length of the byte[] array, but it might break with custom codecs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12546503/LUCENE-4423.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4297", "change_description": ": BooleanScorer2 would multiply the coord() factor\ntwice for conjunctions: for most users this is no problem, but\nif you had a customized Similarity that returned something other\nthan 1 when overlap == maxOverlap (always the case for conjunctions),\nthen the score would be incorrect.", "change_title": "BooleanScorer2 sometimes multiplies coord() twice into the score", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,3.6.2,6.0", "detail_description": "this is a problem if you have a custom coord impl", "patch_link": "https://issues.apache.org/jira/secure/attachment/12539871/LUCENE-4297.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4298", "change_description": ": MultiFields.getTermDocsEnum(IndexReader, Bits, String, BytesRef)\ndid not work at all, it would infinitely recurse.", "change_title": "Infinite recursion in MultiFields.getTermDocsEnum(IndexReader r, Bits liveDocs, String field, BytesRef term)", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,6.0", "detail_description": "The function calls itself: public static DocsEnum getTermDocsEnum(IndexReader r, Bits liveDocs, String field, BytesRef term) throws IOException The more conservative approach is to pass DocsEnum.FLAG_FREQS as last parameter so we can call the getTermDocsEnum(IndexReader r, Bits liveDocs, String field, BytesRef term, int flags).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540051/LUCENE-4298.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4300", "change_description": ": BooleanQuery's rewrite was not always safe: if you\nhad a custom Similarity where coord(1,1) != 1F, then the rewritten\nquery would be scored differently.", "change_title": "BooleanQuery inconsistently applies coord() if it rewrites itself", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,3.6.2,6.0", "detail_description": "Tripped by the new random sim from LUCENE-4297: The basics are this: I think the rewrite is wrong, we should also rewrite single-query BQs where minNrShouldMatch = 1 and there is a single optional clause.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540465/LUCENE-4300.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4303", "change_description": ": PhoneticFilterFactory and SnowballPorterFilterFactory load their\nencoders / stemmers via the ResourceLoader now instead of Class.forName().\nSolr users should now no longer have to embed these in its war.", "change_title": "Analysis factories should use ResourceLoader, not Class.forName", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0", "detail_description": "This affects SnowballPorterFilterFactory and PhoneticFilterFactory. In Solr I encountered this problem when I specified an encoder and I was forced to put the library in WEB-INF/lib instead of /solr/lib/.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540747/LUCENE-4303_Use_ResourceLoader_not_class_forName.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "SOLR-3737", "change_description": ": StempelPolishStemFilterFactory loaded its stemmer table incorrectly.\nAlso, ensure immutability and use only one instance of this table in RAM (lazy\nloaded) since its quite large.", "change_title": "StempelPolishStemFilterFactory can't find resource '/org/apache/lucene/analysis/pl/stemmer_20000.tbl'", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,6.0", "detail_description": "The Stempel stemmer appears to be broken under Solr in v4.0.0-BETA, very likely related to LUCENE-2510 / LUCENE-4044. When I add the following to the example, I get the below-listed exception on start-up:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12541304/SOLR-3737.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4310", "change_description": ": MappingCharFilter was failing to match input strings\ncontaining non-BMP Unicode characters.", "change_title": "NormalizeCharMap.build creates utf32-keyed automaton and uses it with utf16 keys", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "NormalizeCharMap#build method is inconsistent with later use in MappingCharFilter (note BYTE2 vs. toUTF32 later on).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12541352/LUCENE-4310.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4224", "change_description": ": Add in-order scorer to query time joining and the\nout-of-order scorer throws an UOE.", "change_title": "Simplify MultiValuedCase in TermsIncludingScoreQuery", "detail_type": "Task", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.0", "detail_description": "While looking at LUCENE-4214, i was trying to wrap my head around what this is doing... I think the code specialization in the multivalued scorer doesn't buy us any additional speed? At least according to my benchmarks?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12541745/LUCENE-4224.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4333", "change_description": ": Fixed NPE in TermGroupFacetCollector when faceting on mv fields.", "change_title": "NPE in TermGroupFacetCollector when faceting on mv fields", "detail_type": "Bug", "detail_affect_versions": "4.0-ALPHA,4.0-BETA", "detail_fix_versions": "4.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12542748/LUCENE-4333.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4218", "change_description": ": Document.get(String) and Field.stringValue() again return\nvalues for numeric fields, like Lucene 3.x and consistent with the documentation.", "change_title": "contrary to documentation Document.get(field) on numeric field returns null", "detail_type": "Bug", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.0,6.0", "detail_description": "A call to Numeric num = indexableField.numericValue() comes up with a correct value, whereas Document.get(field) yields null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543716/LUCENE-4218.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-3720", "change_description": ": fix memory-consumption issues with BeiderMorseFilter.", "change_title": "OOM in TestBeiderMorseFilter.testRandom", "detail_type": "Test", "detail_affect_versions": "3.6,4.0-ALPHA", "detail_fix_versions": "4.0,6.0", "detail_description": "This has been OOM'ing a lot... we should see why, its likely a real bug. ant test -Dtestcase=TestBeiderMorseFilter -Dtestmethod=testRandom -Dtests.seed=2e18f456e714be89:310bba5e8404100d:-3bd11277c22f4591 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12545277/LUCENE-3720.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4401", "change_description": ": Fix bug where DisjunctionSumScorer would sometimes call score()\non a subscorer that had already returned NO_MORE_DOCS.", "change_title": "ArrayIndexOutOfBoundsException for surround parser", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,3.6.2,6.0", "detail_description": "I got the following exception when I query solr with \"pcnt(kk w hit) OR (ipad) OR (iphoine))\" and the defType is 'surround'. 18:16:45 SEVERE SolrCore java.lang.ArrayIndexOutOfBoundsException: 2147483647 at org.apache.lucene.search.similarities.TFIDFSimilarity$SloppyTFIDFDocScorer.score(TFIDFSimilarity.java:793) at org.apache.lucene.search.spans.SpanScorer.score(SpanScorer.java:93) at org.apache.lucene.search.DisjunctionSumScorer.afterNext(DisjunctionSumScorer.java:94) at org.apache.lucene.search.DisjunctionSumScorer.nextDoc(DisjunctionSumScorer.java:82) at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:284) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:573) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:275) at org.apache.solr.search.SolrIndexSearcher.getDocListNC(SolrIndexSearcher.java:1390) at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:1265) at org.apache.solr.search.SolrIndexSearcher.search(SolrIndexSearcher.java:390) at org.apache.solr.handler.component.QueryComponent.process(QueryComponent.java:411) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:206) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1656) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:454) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:275) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:484) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:524) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:233) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1065) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:413) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:192) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:999) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:250) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:149) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111) at org.eclipse.jetty.server.Server.handle(Server.java:351) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:454) at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:47) at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:900) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:954) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:66) at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:254) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534) at java.lang.Thread.run(Thread.java:636) 18:16:45 SEVERE SolrDispatchFilter null:java.lang.ArrayIndexOutOfBoundsException: 2147483647 at org.apache.lucene.search.similarities.TFIDFSimilarity$SloppyTFIDFDocScorer.score(TFIDFSimilarity.java:793) at org.apache.lucene.search.spans.SpanScorer.score(SpanScorer.java:93) at org.apache.lucene.search.DisjunctionSumScorer.afterNext(DisjunctionSumScorer.java:94) at org.apache.lucene.search.DisjunctionSumScorer.nextDoc(DisjunctionSumScorer.java:82) at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:284) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:573) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:275) at org.apache.solr.search.SolrIndexSearcher.getDocListNC(SolrIndexSearcher.java:1390) at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:1265) at org.apache.solr.search.SolrIndexSearcher.search(SolrIndexSearcher.java:390) at org.apache.solr.handler.component.QueryComponent.process(QueryComponent.java:411) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:206) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1656) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:454) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:275) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:484) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:524) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:233) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1065) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:413) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:192) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:999) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:250) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:149) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111) at org.eclipse.jetty.server.Server.handle(Server.java:351) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:454) at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:47) at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:900) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:954) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:66) at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:254) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534) at java.lang.Thread.run(Thread.java:636) 18:16:45 SEVERE SolrCore org.apache.solr.common.SolrException: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1,​ http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:300) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1656) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:454) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:275) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:484) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:524) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:233) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1065) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:413) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:192) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:999) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:250) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:149) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111) at org.eclipse.jetty.server.Server.handle(Server.java:351) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:454) at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:47) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:890) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:944) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:634) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:230) at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:66) at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:254) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534) at java.lang.Thread.run(Thread.java:636) Caused by: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1,​ http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:324) at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:167) at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:1) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) ... 1 more Caused by: org.apache.solr.common.SolrException: Server at http://192.168.50.78:8985/solr/ac201209w3_s1 returned non ok status:500,​ message:Server Error at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:373) at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:182) at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:289) ... 10 more 18:16:45 SEVERE SolrDispatchFilter null:org.apache.solr.common.SolrException: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1,​ http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:300) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1656) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:454) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:275) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:484) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:524) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:233) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1065) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:413) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:192) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:999) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:250) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:149) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111) at org.eclipse.jetty.server.Server.handle(Server.java:351) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:454) at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:47) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:890) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:944) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:634) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:230) at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:66) at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:254) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534) at java.lang.Thread.run(Thread.java:636) Caused by: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1,​ http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:324) at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:167) at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:1) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) ... 1 more Caused by: org.apache.solr.common.SolrException: Server at http://192.168.50.78:8985/solr/ac201209w3_s1 returned non ok status:500,​ message:Server Error at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:373) at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:182) at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:289) ... 10 more", "patch_link": "https://issues.apache.org/jira/secure/attachment/12545444/LUCENE-4401.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4411", "change_description": ": when sampling is enabled for a FacetRequest, its depth\nparameter is reset to the default (1), even if set otherwise.", "change_title": "Depth requested in a facetRequest is reset when Sampling is in effect", "detail_type": "Bug", "detail_affect_versions": "3.6.1,4.0,6.0", "detail_fix_versions": "4.0,4.1,3.6.2,6.0", "detail_description": "FacetRequest can be set a Depth parameter, which controls the depth of the result tree to be returned. When Sampling is enabled (and actually used) the Depth parameter gets reset to its default (1).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12546301/LUCENE-4411.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4455", "change_description": ": Fix bug in SegmentInfoPerCommit.sizeInBytes() that was\nreturning 2X the true size, inefficiently.  Also fixed bug in\nCheckIndex that would report no deletions when a segment has\ndeletions, and vice/versa.", "change_title": "CheckIndex shows wrong segment size in 4.0 because SegmentInfoPerCommit.sizeInBytes counts every file 2 times; check for deletions is negated and results in wrong output", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,6.0", "detail_description": "I found this bug in 4.0-RC1 when I compared the checkindex outputs for 4.0 and 3.6.1: There is one \"bug\" in sizeInBytes (which we should NOT fix), is that for 3.x indexes, if they are from 3.0 and have shared doc stores they are overestimated. But that's fine. For this case, the index was a 3.6.1 segment and a 4.0 segment, both showed double size.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547243/LUCENE-4455.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4456", "change_description": ": Fixed double-counting sizeInBytes for a segment\n(affects how merge policies pick merges); fixed CheckIndex's\nincorrect reporting of whether a segment has deletions; fixed case\nwhere on abort Lucene could remove files it didn't create; fixed\nmany cases where IndexWriter could leave leftover files (on\nexception in various places, on reuse of a segment name after crash\nand recovery.", "change_title": "IndexWriter makes unrefed files, and MockDir cannot detect it", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Because MockDir calls crash() before it checks for unreferenced files, deletes are no longer allowed. this means the unreferenced files check is useless!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547470/LUCENE-4456.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Optimizations", "change_id": "LUCENE-4322", "change_description": ": Decrease lucene-core JAR size. The core JAR size had increased a\nlot because of generated code introduced in", "change_title": "Can we make oal.util.packed.BulkOperation* smaller?", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "These source files add up to a lot of sources ... it caused problems when compiling under Maven and InteliJ. I committed a change to make separates files, but in aggregate this is still a lot ... EG maybe we don't need to specialize encode?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543269/LUCENE-4322-2.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Optimizations", "change_id": "LUCENE-4161", "change_description": ": Decrease lucene-core JAR size. The core JAR size had increased a\nlot because of generated code introduced in", "change_title": "Make PackedInts usable by codecs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.0-ALPHA", "detail_description": "Some codecs might be interested in using PackedInts. to read and write fixed-size values efficiently. The problem is that the serialization format is self contained, and always writes the name of the codec, its version, its number of bits per value and its format. For example, if you want to use packed ints to store your postings list, this is a lot of overhead (at least ~60 bytes per term, in case you only use one Writer per term, more otherwise). Users should be able to externalize the storage of metadata to save space. For example, to use PackedInts to store a postings list, one should be able to store the codec name, its version and the number of bits per doc in the header of the terms+postings list instead of having to write it once (or more!) per term.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12535005/LUCENE-4161.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Optimizations", "change_id": "LUCENE-3892", "change_description": ": Decrease lucene-core JAR size. The core JAR size had increased a\nlot because of generated code introduced in", "change_title": "Add a useful intblock postings format (eg, FOR, PFOR, PFORDelta, Simple9/16/64, etc.)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "On the flex branch we explored a number of possible intblock encodings, but for whatever reason never brought them to completion. There are still a number of issues opened with patches in different states. Initial results (based on prototype) were excellent (see http://blog.mikemccandless.com/2010/08/lucene-performance-with-pfordelta-codec.html ). I think this would make a good GSoC project.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12538373/LUCENE-3892-blockFor%26hardcode%28base%29.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Optimizations", "change_id": "LUCENE-4317", "change_description": ": Improve reuse of internal TokenStreams and StringReader\nin oal.document.Field.", "change_title": "Field.java does not reuse its inlined Keyword-TokenStream", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.0,6.0", "detail_description": "Field.java contains a inlined Keyword-TokenStream. Unfortunately this one is recreated all the time, although one reuses the same Field instance. For NumericTokenStream Field.java reuses it, but the Keyword one not. We should apply the same logic and lazy init the TokenStream with a setter for the String value and reset(). This would be looking identical to SetNumeric(xx).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12542027/LUCENE-4317-2.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Optimizations", "change_id": "LUCENE-4327", "change_description": ": Support out-of-order scoring in FilteredQuery for higher\nperformance.", "change_title": "Use BooleanScorer1 for filter-down-low queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12542300/LUCENE-4327.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Optimizations", "change_id": "LUCENE-4364", "change_description": ": Optimize MMapDirectory to not make a mapping per-cfs-slice,\ninstead one map per .cfs file. This reduces the total number of maps.\nAdditionally factor out a (package-private) generic\nByteBufferIndexInput from MMapDirectory.", "change_title": "MMapDirectory makes too many maps for CFS", "detail_type": "Bug", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.0,6.0", "detail_description": "While looking at LUCENE-4123, i thought about this: I don't like how mmap creates a separate mapping for each CFS slice, to me this is way too many mmapings. Instead I think its slicer should map the .CFS file, and then when asked for an offset+length slice of that, it should be using .duplicate()d buffers of that single master mapping. then when you close the .CFS it closes that one mapping. this is probably too scary for 4.0, we should take our time, but I think we should do it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12544210/LUCENE-4364.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Build", "change_id": "LUCENE-4406", "change_description": ",", "change_title": "Print out where tests failed at the end of running the Test Suite", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "It would be nice if, at the end of running ant test, it spit out the names of which tests failed so that one doesn't have to go scrolling up through the output or go run grep on the test-reports as a separate step. For another project, I use: which can likely be modified for Lucene.  I can do it, but wanted to see if others had an opinion.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Build", "change_id": "LUCENE-4407", "change_description": ",", "change_title": "XML-forbidden unicode characters break XML test reports", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "Disallowed by spec. XML unicode characters (in Strings) produce invalid XML reports which then fail on jenkins. I think this would also be the case with regular ant/maven runners but I didn't check. It'd be interesting to see if they cater for this somehow.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Build", "change_id": "LUCENE-4252", "change_description": ": Detect/Fail tests when they leak RAM in static fields", "change_title": "Detect/Fail tests when they leak RAM in static fields", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "We run our junit tests without firing up a JVM each time. But some tests initialize lots of stuff in @BeforeClass and don't properly null it out in an @AfterClass, which can cause a subsequent test in the same JVM to OOM, which is difficult to debug. Inspiration for this was me committing Mike's cool TestPostingsFormat, which forgot to do this: then we were seeing OOMs in several jenkins runs. We should try to detect these leaks in LuceneTestCase with RAMUsageEstimator and fail the test.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12538512/sfi.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Build", "change_id": "LUCENE-4360", "change_description": ": Support running the same test suite multiple times in\nparallel", "change_title": "Support running the same test suite multiple times in parallel", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "The current \"test execution multiplier\" or: generates multiple tests (method executions) under a test class (suite). All these tests, however, are bound to a single class so they must run sequentially and on a single JVM (because of how JUnit works – nesting of rules, class hooks, etc.). Mark pointed out that if somebody has a multi-core CPU then it'd be nice to be able to run a single suite in parallel, possibly in combination with tests.iters (so that a single test method is executed X times on Y parallel JVMs). This is surprisingly easy with the randomized runner because it currently accepts \"duplicate\" suite names and will load-balance them in a normal way. So, if one has Y cores (JVMs) then providing a suite name X times will result in X executions, balanced across Y JVMs. The only problem is how to \"multiply\" suite names. This can be done in a number of ways, starting from a custom resource collection wrapper and ending at a built-in code in the runner itself. I think the custom collection wrapper approach would be interesting, I'll explore this direction.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543733/quickhack.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Build", "change_id": "LUCENE-3985", "change_description": ": Upgrade to randomizedtesting 2.0.0. Added support for\nthread leak detection. Added support for suite timeouts.", "change_title": "Refactor support for thread leaks", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "This will be duplicated in the runner and in LuceneTestCase; try to consolidate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12538824/LUCENE-3985.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Build", "change_id": "LUCENE-4340", "change_description": ": Move all non-default codec, postings format and terms\ndictionary implementations to lucene/codecs.", "change_title": "Move all codecs but Lucene40 to a module", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.0,6.0", "detail_description": "We should move all concrete postings formats and codecs but Lucene40 to a module.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12543873/LUCENE-4340-bloom.patch", "patch_content": "none"}
{"library_version": "4.0.0", "change_type": "Documentation", "change_id": "LUCENE-4302", "change_description": ": Fix facet userguide to have HTML loose doctype like\nall other javadocs.", "change_title": "Javadoc for facet User Guide does not display because of SAXParseException (Eclipse, Maven)", "detail_type": "Bug", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.0,3.6.2,6.0", "detail_description": "I have opened javadoc for Facet API while using Eclipse, which downloaded the javadocs using Maven m2e plugin. When I click on facet User Guide on the overview page I get the following exception in FireFox: http://127.0.0.1:49231/help/nftopic/jar:file:/C:/Users/karl/.m2/repository/org/apache/lucene/lucene-facet/4.0.0-ALPHA/ lucene-facet-4.0.0-ALPHA-javadoc.jar!/org/apache/lucene/facet/doc-files/userguide.html An error occured while processing the requested document: org.xml.sax.SAXParseException; lineNumber: 121; columnNumber: 16; The element type \"br\" must be terminated by the matching end-tag \"</br>\". at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(Unknown Source) at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(Unknown Source) The link, or requested document is: http://127.0.0.1:49231/help/nftopic/jar:file:/C:/Users/karl/.m2/repository/org/apache/lucene/lucene-facet/4.0.0-ALPHA/ lucene-facet-4.0.0-ALPHA-javadoc.jar!/org/apache/lucene/facet/doc-files/userguide.html", "patch_link": "https://issues.apache.org/jira/secure/attachment/12540748/LUCENE-4302.patch", "patch_content": "none"}

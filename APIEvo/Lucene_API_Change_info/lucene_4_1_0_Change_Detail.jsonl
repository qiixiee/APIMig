{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4514", "change_description": ": Scorer's freq() method returns an integer value indicating\nthe number of times the scorer matches the current document. Previously\nthis was only sometimes the case, in some cases it returned a (meaningless)\nfloating point value.  Scorer now extends DocsEnum so it has attributes().", "change_title": "Make Scorer.freq() well defined: number of matches in doc", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Today Scorer.freq() is somewhat useless: because in general it returns the number of matches, but other times it returns a strange floating point value (essentially an intermediate score). I think it should just return the number of matches in the document. This makes it well-defined and useful to consumers (e.g. if they are going to iterate over the positions or whatever in the future).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551340/LUCENE-4514.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4543", "change_description": ": TFIDFSimilarity's index-time computeNorm is now final to\nmatch the fact that its query-time norm usage requires a FIXED_8 encoding.\nOverride lengthNorm and/or encode/decodeNormValue to change the specifics,\nlike Lucene 3.x.", "change_title": "Bring back TFIDFSim.lengthNorm", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "We removed this before because of LUCENE-2828, but the problem there was the delegator (not the lengthNorm method). TFIDFSim requires byte[] norms today. So its computeNorm should be final, calling lengthNorm() that returns a byte. This way there is no possibility for you to do something stupid.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552297/LUCENE-4543.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-3441", "change_description": ": The facet module now supports NRT. As a result, the following\nchanges were made:", "change_title": "Add NRT support to facets", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently LuceneTaxonomyReader does not support NRT - i.e., on changes to LuceneTaxonomyWriter, you cannot have the reader updated, like IndexReader/Writer. In order to do that we need to do the following:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12554354/LUCENE-3441.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4576", "change_description": ": Remove CachingWrapperFilter(Filter, boolean). This recacheDeletes\noption gave less than 1% speedup at the expense of cache churn (filters were\ninvalidated on reopen if even a single delete was posted against the segment).", "change_title": "Remove CachingWrapperFilter recacheDeletes boolean", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "I think this option is bad news, its just a trap that causes caches to be uselessly invalidated. If you really have a totally static index then just expunge your deletes. Let's remove the option and complexity.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555025/LUCENE-4576.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4575", "change_description": ": Replace IndexWriter's commit/prepareCommit versions that take\ncommitData with setCommitData(). That allows committing changes to IndexWriter\neven if the commitData is the only thing that changes.", "change_title": "Allow IndexWriter to commit, even just commitData", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Spinoff from here http://lucene.472066.n3.nabble.com/commit-with-only-commitData-td4022155.html. In some cases, it is valuable to be able to commit changes to the index, even if the changes are just commitData. Such data is sometimes used by applications to register in the index some global application information/state. The proposal is: I will work on a patch a post.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555688/LUCENE-4575.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4565", "change_description": ": TaxonomyReader.getParentArray and .getChildrenArrays consolidated\ninto one getParallelTaxonomyArrays(). You can obtain the 3 arrays that the\nprevious two methods returned by calling parents(), children() or siblings()\non the returned ParallelTaxonomyArrays.", "change_title": "Simplify TaxoReader ParentArray/ChildrenArrays", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "TaxoReader exposes two structures which provide information about a categories parent/childs/siblings: ParentArray and ChildrenArrays. ChildrenArrays are derived (i.e. created) from ParentArray. I propose to consolidate all that into one API ParentInfo, or CategoryTreeInfo (a better name?) which will provide the same information, only from one object. So instead of making these calls: one would make these calls: Not a big change, just consolidate more code into one logical place. All of these arrays will continue to be lazily allocated.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12556266/LUCENE-4565.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4585", "change_description": ": Spatial PrefixTree based Strategies (either TermQuery or\nRecursivePrefix based) MAY want to re-index if used for point data. If a\nre-index is not done, then an indexed point is ~1/2 the smallest grid cell\nlarger and as such is slightly more likely to match a query shape.", "change_title": "Spatial RecursivePrefixTreeFilter has some bugs with indexing non-point shapes", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "RecursivePrefixTreeFilter has some bugs that can occur when searching indexed shapes.  One bug is an unpositioned termsEnum.  It through an exception in testing; I'm not sure what its effects would be in production.  The other couple bugs are hard to describe here but were rare to occur in extensive testing. The effects were probably a slim chance of matching an indexed shape near the query shape. And SpatialPrefixTree does not support an indexed shape that covers the entire globe. These bugs were discovered during development of tests for RPTF LUCENE-4419 which I will submit shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555959/LUCENE-4585_PrefixTree_bugs.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4604", "change_description": ": DefaultOrdinalPolicy removed in favor of OrdinalPolicy.ALL_PARENTS.\nSame for DefaultPathPolicy (now PathPolicy.ALL_CATEGORIES). In addition, you\ncan use OrdinalPolicy.NO_PARENTS to never write any parent category ordinal\nto the fulltree posting payload (but note that you need a special\nFacetsAccumulator - see javadocs).", "change_title": "Implement OrdinalPolicy.NO_PARENTS", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Over at LUCENE-4602, Mike explored the idea of writing just the leaf nodes in the fulltree posting, rather than the full hierarchy. I wrote this simple OrdinalPolicy which achieves that: I think that we should add it as a singleton class to OrdinalPolicy.EXACT_CATEGORIES_ONLY, as wel as make DefaultOrdPolicy as singleton too, under the name FULL_HIERARCHY (feel free to suggest a better name).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560179/LUCENE-4604.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4594", "change_description": ": Spatial PrefixTreeStrategy no longer indexes center points of\nnon-point shapes.  If you want to call makeDistanceValueSource() based on\nshape centers, you need to do this yourself in another spatial field.", "change_title": "Spatial PrefixTreeStrategy shouldn't index center-points with shapes together", "detail_type": "Bug", "detail_affect_versions": "4.0,6.0", "detail_fix_versions": "4.1,6.0", "detail_description": "The Spatial PrefixTreeStrategy will index the center-point of a non-point shape it is given to index, in addition to the shape itself of course.  The rationale was that this point could be picked up by PointPrefixTreeFieldCacheProvider for distance/sorting.  However this approach is buggy since the distinction of grid cells between the center point and the shape itself is lost when the shape gets indexed down to max-levels precision â€“ each grid cell therein appears to be another point that needs to be brought into memory.  It's also possible that the shape is a LineString or some other non-trivial shape in which its center point isn't actually in the shape. Even if you knew this problem would never happen, I think you're better off indexing center points into another spatial field if you want them.  Perhaps arguably this strategy could do that internally?  Wether or not that ends up happening, I just want to remove the problematic behavior now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12556479/LUCENE-4594__PrefixTreeStrategy_should_not_index_center_points.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4615", "change_description": ": Replace IntArrayAllocator and FloatArrayAllocator by ArraysPool.\nFacetArrays no longer takes those allocators; if you need to reuse the arrays,\nyou should use ReusingFacetArrays.", "change_title": "Remove Int/FloatArrayAllocator from facet module?", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Spinoff from LUCENE-4600. It makes me nervous to have allocation tied to our public APIs ... and the ability for Int/FloatArrayAllocator to hold onto N arrays indefinitely makes me even more nervous.  I think we should just trust java/GC to do their job here and free the storage as soon as faceting is done.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560383/LUCENE-4615.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4621", "change_description": ": FacetIndexingParams is now a concrete class (instead of DefaultFIP).\nAlso, the entire IndexingParams chain is now immutable. If you need to override\na setting, you should extend the relevant class.\nAdditionally, FacetSearchParams is now immutable, and requires all FacetRequests\nto specified at initialization time.", "change_title": "FacetIndexing/SearchParams house cleaning", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "FacetIndexingParams lets you configure few things such as OrdinalPolicy, PathPolicy and partitionSize. However, in order to set them you must extend DefaultFacetIndexingParams and override fixedXY(), as the respective getters are final. I'd like to do the following:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560762/LUCENE-4621.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4647", "change_description": ": CategoryDocumentBuilder and EnhancementsDocumentBuilder are replaced\nby FacetFields and AssociationsFacetFields respectively. CategoryEnhancement and\nAssociationEnhancement were removed in favor of a simplified CategoryAssociation\ninterface, with CategoryIntAssociation and CategoryFloatAssociation\nimplementations.\nNOTE: indexes that contain category enhancements/associations are not supported\nby the new code and should be recreated.", "change_title": "Simplify CategoryDocumentBuilder", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "CategoryDocumentBuilder is used to add facet fields to a document. Today the usage is not so straightforward, and I'd like to simplify it. First, to improve usage but also to make cutover to DocValues easier. This clsas does two operations: (1) adds drill-down terms and (2) creates the fulltree payload. Today, since it does it all on terms, there's a hairy TokenStream which does both these operations in one go. For simplicity, I'd like to break this into two steps: Hopefully, I'd like to have FacetsDocumentBuilder (maybe just FacetsBuilder?) which only handles things with no associations, and EnhancementsDocBuilder which extends whatever is needed to add associations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12562851/LUCENE-4647.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4659", "change_description": ": Massive cleanup to CategoryPath API. Additionally, CategoryPath is\nnow immutable, so you don't need to clone() it.", "change_title": "Cleanup CategoryPath", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "CategoryPath is supposed to be a simple object which holds a category path's components, and offers some utility methods that can be used during indexing and search. Currently, it exposes lots of methods which aren't used, unless by tests - I want to get rid of them. Also, the internal implementation manages 3 char[] for holding the path components, while I think it would have been simpler if it maintained a String[]. I'd like to explore that option too (the input is anyway String, so why copy char[]?). Ultimately, I'd like CategoryPath to be immutable. I was able to get rid most of the mutable methods. The ones that remain will probably go away when I move from char[] to String[]. Immuntability is important because in various places in the code we convert a CategoryPath back and forth to String, with TODOs to stop doing that if CP was immutable. Will attach a patch that covers the first step - get rid of unneeded methods and beginning to make it immutable. Perhaps this can be done in multiple commits?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12563474/LUCENE-4659.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4670", "change_description": ": StoredFieldsWriter and TermVectorsWriter have new finish* callbacks\nwhich are called after a doc/field/term has been completely added.", "change_title": "Add TermVectorsWriter.finish{Doc,Field,Term} to make development of new formats easier", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1", "detail_description": "This is especially useful to LUCENE-4599 where actions have to be taken after a doc/field/term has been added.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564076/LUCENE-4670.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4620", "change_description": ": IntEncoder/Decoder were changed to do bulk encoding/decoding. As a\nresult, few other classes such as Aggregator and CategoryListIterator were\nchanged to handle bulk category ordinals.", "change_title": "Explore IntEncoder/Decoder bulk API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Today, IntEncoder/Decoder offer a streaming API, where you can encode(int) and decode(int). Originally, we believed that this layer can be useful for other scenarios, but in practice it's used only for writing/reading the category ordinals from payload/DV. Therefore, Mike and I would like to explore a bulk API, something like encode(IntsRef, BytesRef) and decode(BytesRef, IntsRef). Perhaps the Encoder can still be streaming (as we don't know in advance how many ints will be written), dunno. Will figure this out as we go. One thing to check is whether the bulk API can work w/ e.g. facet associations, which can write arbitrary byte[], and so may decoding to an IntsRef won't make sense. This too we'll figure out as we go. I don't rule out that associations will use a different bulk API. At the end of the day, the requirement is for someone to be able to configure how ordinals are written (i.e. different encoding schemes: VInt, PackedInts etc.) and later read, with as little overhead as possible.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565116/LUCENE-4620.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4683", "change_description": ": CategoryListIterator and Aggregator are now per-segment. As such\ntheir implementations no longer take a top-level IndexReader in the constructor\nbut rather implement a setNextReader.", "change_title": "Change Aggregator and CategoryListIterator to be per-segment", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "As another improvement, these two (mostly CategoryListIterator) should be per-segment. I've got a patch nearly ready, will post tomorrow.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564671/LUCENE-4683.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4226", "change_description": ": New experimental StoredFieldsFormat that compresses chunks of\ndocuments together in order to improve the compression ratio.", "change_title": "Efficient compression of small to medium stored fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "I've been doing some experiments with stored fields lately. It is very common for an index with stored fields enabled to have most of its space used by the .fdt index file. To prevent this .fdt file from growing too much, one option is to compress stored fields. Although compression works rather well for large fields, this is not the case for small fields and the compression ratio can be very close to 100%, even with efficient compression algorithms. In order to improve the compression ratio for small fields, I've written a StoredFieldsFormat that compresses several documents in a single chunk of data. To see how it behaves in terms of document deserialization speed and compression ratio, I've run several tests with different index compression strategies on 100,000 docs from Mike's 1K Wikipedia articles (title and text were indexed and stored): For those who don't know Snappy, it is compression algorithm from Google which has very high compression ratios, but compresses and decompresses data very quickly. (doc = doc-level compression, index = index-level compression) I find it interesting because it allows to trade speed for space (with deflate, the .fdt file shrinks by a factor of 2, much better than with doc-level compression). One other interesting thing is that index/snappy is almost as compact as doc/deflate while it is more than 2x faster at retrieving documents from disk. These tests have been done on a hot OS cache, which is the worst case for compressed fields (one can expect better results for formats that have a high compression ratio since they probably require fewer read/write operations from disk).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547856/LUCENE-4226.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4426", "change_description": ": New ValueSource implementations (in lucene/queries) for\nDocValues fields.", "change_title": "New ValueSource implementations that wrap DocValues", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "We should have ValueSource implementations that wrap DocValues in lucene-queries so that DocValues can be used in function queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547963/LUCENE-4426.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4410", "change_description": ": FilteredQuery now exposes a FilterStrategy that exposes\nhow filters are applied during query execution.", "change_title": "Make FilteredQuery more flexible with regards to how filters are applied", "detail_type": "Improvement", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently FilteredQuery uses either the \"old\" lucene 3 leap frog approach or pushes the filter down together with accepted docs. Yet there might be more strategies required to fit common usecases like geo-filtering where a rather costly function is applied to each document. Using leap frog this might result in a very slow query if the filter is advanced since it might have linear running time to find the next valid document. We should be more flexible with regards to those usecases and make it possible to either tell FQ what to do or plug in a strategy that applied a filter in a different way. The current FQ impl also uses an heuristic to decide if RA or LeapFrog should be used. This is really an implementation detail of the strategy and not of FQ and should be moved out.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12546010/LUCENE-4410.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4404", "change_description": ": New ListOfOutputs (in lucene/misc) for FSTs wraps\nanother Outputs implementation, allowing you to store more than one\noutput for a single input.  UpToTwoPositiveIntsOutputs was moved\nfrom lucene/core to lucene/misc.", "change_title": "Add ListOfOutputs FST Outputs, replacing UpToTwoPositiveIntOutputs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Spinoff from LUCENE-3842.  This just generalizes the UpToTwoPositiveIntOutputs to a list of any arbitrary output, by wrapping any other Outputs impl.  I also made separate methods to write/read a node-final output: since list of values can only occur on a final node output, this impl optimizes and avoids writing an extra byte per label for normal arc labels. This also fixes a bug in Builder that was sometimes failing to join multiple outputs together.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12545786/LUCENE-4404.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-3842", "change_description": ": New AnalyzingSuggester, for doing auto-suggest\nusing an analyzer.  This can create powerful suggesters: if the analyzer\nremove stop words then \"ghost chr...\" could suggest \"The Ghost of\nChristmas Past\"; if SynonymFilter is used to map wifi and wireless\nnetwork to hotspot, then \"wirele...\" could suggest \"wifi router\";\ntoken normalization likes stemmers, accent removel, etc. would allow\nthe suggester to ignore such variations.", "change_title": "Analyzing Suggester", "detail_type": "New Feature", "detail_affect_versions": "3.6,4.0-ALPHA", "detail_fix_versions": "4.1,6.0", "detail_description": "Since we added shortest-path wFSA search in LUCENE-3714, and generified the comparator in LUCENE-3801, I think we should look at implementing suggesters that have more capabilities than just basic prefix matching. In particular I think the most flexible approach is to integrate with Analyzer at both build and query time, such that we build a wFST with: input: analyzed text such as ghost0christmas0past <-- byte 0 here is an optional token separator output: surface form such as \"the ghost of christmas past\" weight: the weight of the suggestion we make an FST with PairOutputs<weight,output>, but only do the shortest path operation on the weight side (like the test in LUCENE-3801), at the same time accumulating the output (surface form), which will be the actual suggestion. This allows a lot of flexibility: According to my benchmarks, suggestions are still very fast with the prototype (e.g. ~ 100,000 QPS), and the FST size does not explode (its short of twice that of a regular wFST, but this is still far smaller than TST or JaSpell, etc).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547053/LUCENE-3842.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4446", "change_description": ": Lucene 4.1 has a new default index format (Lucene41Codec)\nthat incorporates the previously experimental \"Block\" postings format\nfor better search performance.", "change_title": "Switch to BlockPostingsFormat for Lucene 4.1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "This has baked for some time: no crazy fails in hudson or anything. The code (in my opinion) is actually a lot simpler than the current postings format, its faster, the indexes are smaller, and so on. We should probably spend some time just going over the code and adding some more tests and such but I think its time to start looking at cutting over.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-3846", "change_description": ": New FuzzySuggester, like AnalyzingSuggester except it\nalso finds completions allowing for fuzzy edits in the input string.", "change_title": "Fuzzy suggester", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Would be nice to have a suggester that can handle some fuzziness (like spell correction) so that it's able to suggest completions that are \"near\" what you typed. As a first go at this, I implemented 1T (ie up to 1 edit, including a transposition), except the first letter must be correct. But there is a penalty, ie, the \"corrected\" suggestion needs to have a much higher freq than the \"exact match\" suggestion before it can compete. Still tons of nocommits, and somehow we should merge this / make it work with analyzing suggester too (LUCENE-3842).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551210/LUCENE-3846.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4515", "change_description": ": MemoryIndex now supports adding the same field multiple\ntimes.", "change_title": "Make MemoryIndex more memory efficient", "detail_type": "Improvement", "detail_affect_versions": "4.0,4.1,6.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently MemoryIndex uses BytesRef objects to represent terms and holds an int[] per term per field to represent postings. For highlighting this creates a ton of objects for each search that 1. need to be GCed and 2. can't be reused.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551836/LUCENE-4515.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4489", "change_description": ": Added consumeAllTokens option to LimitTokenCountFilter", "change_title": "improve LimitTokenCountFilter and/or it's tests", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "spinning off a discussion about LimitTokenCountFilter  and it's tests from SOLR-3961 (which was about a specific bug in the LimitTokenCountFilterFactory)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12554405/LUCENE-4489.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4566", "change_description": ": Add NRT/SearcherManager.RefreshListener/addListener to\nbe notified whenever a new searcher was opened.", "change_title": "SearcherManager.afterRefresh() issues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "1) ReferenceManager.doMaybeRefresh seems to call afterRefresh even if it didn't refresh/swap, (when newReference == null) 2) It would be nice if users were allowed to override SearcherManager.afterRefresh() to get notified when a new searcher is in action. But SearcherManager and ReaderManager are final, while NRTManager is not. The only way to currently hook into when a new searched is created is using the factory, but if you wish to do some async task then, there are no guarantees that acquire() will return the new searcher, so you have to pass it around and incRef manually. While if allowed to hook into afterRefresh you can just rely on acquire()  & existing infra you have around it to give you the latest one.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555376/LUCENE-4566.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "SOLR-4123", "change_description": ": Add per-script customizability to ICUTokenizerFactory via\nrule files in the ICU RuleBasedBreakIterator format.", "change_title": "ICUTokenizerFactory - per-script RBBI customization", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Initially this started out as an idea for a configuration knob on ICUTokenizer that would allow me to tell it not to tokenize on punctuation.  Through IRC discussion on #lucene, it sorta ballooned.  The committers had a long discussion about it that I don't really understand, so I'll be including it in the comments. I am a Solr user, so I would also need the ability to access the configuration from there, likely either in schema.xml or solrconfig.xml.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555793/SOLR-4123.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4590", "change_description": ": Added WriteEnwikiLineDocTask - a benchmark task for writing\nWikipedia category pages and non-category pages into separate line files.\nextractWikipedia.alg was changed to use this task, so now it creates two\nfiles.", "change_title": "WriteEnwikiLineDoc which writes Wikipedia category pages to a separate file", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "It may be convenient to split Wikipedia's line file into two separate files: category-pages and non-category ones.  It is possible to split the original line file with grep or such. It is more efficient to do it in advance.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12559642/LUCENE-4590.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4290", "change_description": ": Added PostingsHighlighter to the highlighter module. It uses\noffsets from the postings lists to highlight documents.", "change_title": "basic highlighter that uses postings offsets", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "We added IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS so you can efficiently compress character offsets in the postings list, but nothing yet makes use of this. Here is a simple highlighter that uses them: it doesn't have many tests or fancy features, but I think its ok for the sandbox/ (maybe with a couple more tests) Additionally I didnt do any benchmarking.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12539247/LUCENE-4290.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "New Features", "change_id": "LUCENE-4628", "change_description": ": Added CommonTermsQuery that executes high-frequency terms\nin a optional sub-query to prevent slow queries due to \"common\" terms\nlike stopwords.", "change_title": "Add common terms query to gracefully handle very high frequent terms dynamically", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "I had this problem quite a couple of times the last couple of month that searches very often contained super high frequent terms and disjunction queries became way too slow. The main problem was that stopword filtering wasn't really an option since in the domain those high-freq terms where not really stopwords though. So for instance searching for a song title \"this is it\" or for a band \"A\" didn't really fly with stopwords. I thought about that for a while and came up with a query based solution that decides based on a threshold if something is considered a stopword or not and if so it moves the term in two boolean queries one for high-frequent and one for low-frequent such that those high frequent terms are only matched if the low-frequent sub-query produces a match. Yet if all terms are high frequent it makes the entire thing a Conjunction which gave me reasonable results as well as performance.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560841/LUCENE-4628.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4399", "change_description": ": Deprecated AppendingCodec. Lucene's term dictionaries\nno longer seek when writing.", "change_title": "Rename AppendingCodec to Appending40Codec", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "In order AppendingCodec to follow Lucene codecs version, I think its name should include a version number (so that, for example, if we get to releave Lucene 4.3 with a new Lucene43Codec, there will also be a new Appending43Codec).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548230/LUCENE-4399.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4479", "change_description": ": Rename TokenStream.getTokenStream(IndexReader, int, String)\nto TokenStream.getTokenStreamWithOffsets, and return null on failure\nrather than throwing IllegalArgumentException.", "change_title": "TokenSources.getTokenStream() doesn't return correctly for termvectors with positions but no offsets", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "The javadocs for TokenSources.getTokenStream(Terms, boolean) state: \"Low level api. Returns a token stream or null if no offset info available in index. This can be used to feed the highlighter with a pre-parsed token stream\" However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null. This has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets.  All highlighting markup is prepended to the beginning of the field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550089/LUCENE-4479.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4472", "change_description": ": MergePolicy now accepts a MergeTrigger that provides\ninformation about the trigger of the merge ie. merge triggered due\nto a segment merge or a full flush etc.", "change_title": "Add setting that prevents merging on updateDocument", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently we always call maybeMerge if a segment was flushed after updateDocument. Some apps and in particular ElasticSearch uses some hacky workarounds to disable that ie for merge throttling. It should be easier to enable this kind of behavior.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12549685/LUCENE-4472.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4520", "change_description": ": ValueSource.getSortField no longer throws IOExceptions", "change_title": "ValueSource.getSortField shouldn't throw IOException", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "ValueSource.getSortField just returns a new ValueSourceSortField, whose constructor doesn't declare any checked exceptions.  So adding the throws clause to the method declaration means adding pointless try-catch warts to client code.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551555/LUCENE-4520.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4537", "change_description": ": RateLimiter is now separated from FSDirectory and exposed via\nRateLimitingDirectoryWrapper. Any Directory can now be rate-limited.", "change_title": "Move RateLimiter up to Directory and make it IOContext aware", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently the RateLimiter only applies to FSDirectory which is fine in general but always requires casts and other dir. impls (custom ones could benefit from this too.) We are also only able to rate limit merge operations which limits the functionality here a lot. Since we have the context information what the IndexOutput is used for we can use that for rate limiting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552655/LUCENE-4537.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4591", "change_description": ": CompressingStoredFields{Writer,Reader} now accept a segment\nsuffix as a constructor parameter.", "change_title": "Make StoredFieldsFormat more configurable", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.1", "detail_description": "The current StoredFieldsFormat are implemented with the assumption that only one type of StoredfieldsFormat is used by the index. We would like to be able to configure a StoredFieldsFormat per field, similarly to the PostingsFormat. There is a few issues that need to be solved for allowing that: 1) allowing to configure a segment suffix to the StoredFieldsFormat 2) implement SPI interface in StoredFieldsFormat  3) create a PerFieldStoredFieldsFormat We are proposing to start first with 1) by modifying the signature of StoredFieldsFormat#fieldsReader and StoredFieldsFormat#fieldsWriter so that they use SegmentReadState and SegmentWriteState instead of the current set of parameters. Let us know what you think about this idea. If this is of interest, we can contribute with a first path for 1).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560127/LUCENE-4591.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4605", "change_description": ": Added DocsEnum.FLAG_NONE which can be passed instead of 0 as\nthe flag to .docs() and .docsAndPositions().", "change_title": "Add FLAG_NONE to DocsEnum and DocsAndPositionsEnum", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Add a convenience constants FLAGS_NONE to DocsEnum and DocsAndPositionsEnum. Today, if someone e.g. wants to get the docs only, he needs to pass 0 as the flags, but the value of 0 is not documented anywhere. I had to dig in the code the verify that indeed that's the value. I'll attach a patch later.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560217/LUCENE-4605.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4617", "change_description": ": Remove FST.pack() method. Previously to make a packed FST,\nyou had to make a Builder with willPackFST=true (telling it you will later pack it),\ncreate your fst with finish(), and then call pack() to get another FST.\nInstead just pass true for doPackFST to Builder and finish() returns a packed FST.", "change_title": "remove FST.pack() method", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Current method to make a packed FST: 1. Create an FST Builder with willPack=true, telling it you are later going to pack() it. 2. Create your fst with finish() as normal. 3. Take that fst, and call pack() on it to get another FST. This makes no sense. if you pass willPack=true, then I think finish() should just return a packed fst.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560630/LUCENE-4617.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4663", "change_description": ": Deprecate IndexSearcher.document(int, Set). This was not intended\nto be final, nor named document(). Use IndexSearcher.doc(int, Set) instead.", "change_title": "IndexSearcher.document() should not be final", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "IndexSearcher has 3 methods: The last one is confusing for subclasses (e.g. SolrIndexSearcher) that override these methods. for example that one has its own StoredDocument doc(int, Set) method. But now this means a user could always call the wrong method (this final document() method) and get the wrong behavior (versus calling doc()). I think the name is also wrong. it should be doc() like the others.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12563482/LUCENE-4663.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "API Changes", "change_id": "LUCENE-4684", "change_description": ": Made DirectSpellChecker extendable.", "change_title": "Allow DirectSpellChecker  to be extended", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564741/LUCENE-4684.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-1822", "change_description": ": BaseFragListBuilder hard-coded 6 char margin is too naive.", "change_title": "FastVectorHighlighter: SimpleFragListBuilder hard-coded 6 char margin is too naive", "detail_type": "Improvement", "detail_affect_versions": "2.9", "detail_fix_versions": "4.1,6.0", "detail_description": "The new FastVectorHighlighter performs extremely well, however I've found in testing that the window of text chosen per fragment is often very poor, as it is hard coded in SimpleFragListBuilder to always select starting 6 characters to the left of the first phrase match in a fragment.  When selecting long fragments, this often means that there is barely any context before the highlighted word, and lots after; even worse, when highlighting a phrase at the end of a short text the beginning is cut off, even though the entire phrase would fit in the specified fragCharSize.  For example, highlighting \"Punishment\" in \"Crime and Punishment\"  returns \"e and <b>Punishment</b>\" no matter what fragCharSize is specified.  I am going to attach a patch that improves the text window selection by recalculating the starting margin once all phrases in the fragment have been identified - this way if a single word is matched in a fragment, it will appear in the middle of the highlight, instead of 6 characters from the beginning.  This way one can also guarantee that the entirety of short texts are represented in a fragment by specifying a large enough fragCharSize.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548181/LUCENE-1822.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4468", "change_description": ": Fix rareish integer overflows in Lucene41 postings\nformat.", "change_title": "BlockPF.PosVIntBlockFPDelta should be a vlong", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "It seems a stopword for a large collection could easily blow this out of the water. We need a Test2BPositions!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548478/LUCENE-4468.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4486", "change_description": ": Add support for ConstantScoreQuery in Highlighter.", "change_title": "Highlighter doesn't support ConstantScoreQuery", "detail_type": "Bug", "detail_affect_versions": "3.6.1,4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "if you wrap a query into a constant score the highlighter fails to highlight since ConstantScoreQuery is not recognized in WeightedSpanTermExtractor", "patch_link": "https://issues.apache.org/jira/secure/attachment/12549462/LUCENE-4486.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4485", "change_description": ": When CheckIndex terms, terms/docs pairs and tokens,\nthese counts now all exclude deleted documents.", "change_title": "CheckIndex's term stats should not include deleted docs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "I was looking at the CheckIndex output on and index that has deletions, eg: If you compare the test: terms, freq, prox (includes deletions) and the next line (doesn't include deletions), it's confusing because only the 3rd number (tokens) reflects deletions.  I think the first two numbers should also reflect deletions?  This way an app could get a sense of how much \"deadweight\" is in the index due to un-reclaimed deletions...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12549330/LUCENE-4485.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4479", "change_description": ": Highlighter works correctly for fields with term vector\npositions, but no offsets.", "change_title": "TokenSources.getTokenStream() doesn't return correctly for termvectors with positions but no offsets", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "The javadocs for TokenSources.getTokenStream(Terms, boolean) state: \"Low level api. Returns a token stream or null if no offset info available in index. This can be used to feed the highlighter with a pre-parsed token stream\" However, if the Terms instance passed in has positions but no offsets stored, a TokenStream is incorrectly returned, rather than null. This has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets.  All highlighting markup is prepended to the beginning of the field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550089/LUCENE-4479.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "SOLR-3906", "change_description": ": JapaneseReadingFormFilter in romaji mode will return\nromaji even for out-of-vocabulary kana cases (e.g. half-width forms).", "change_title": "Add support for AnalyzingSuggester / coerce it to work for Japanese", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "We should add a factory for this to solr, and try to add a test/example using JapaneseReadingFormFilter, to see if we can at least get some basic auto-suggest working for this language.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550162/SOLR-3906.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4504", "change_description": ": Fix broken sort comparator in ValueSource.getSortField,\nused when sorting by a function query.", "change_title": "Empty results from IndexSearcher.searchAfter() when sorting by FunctionValues", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.3,6.0", "detail_description": "IS.searchAfter() always returns an empty result when using FunctionValues for sorting. The culprit is ValueSourceComparator.compareDocToValue() returning -1 when it should return +1.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550908/LUCENE-4504.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4511", "change_description": ": TermsFilter might return wrong results if a field is not\nindexed or doesn't exist in the index.", "change_title": "TermsFilter might return wrong results if a field is not indexed or not present in the index", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1,6.0", "detail_fix_versions": "4.1,6.0", "detail_description": "TermsFilter returns if a term returns null from AIR#terms(term) while it should just continue. I will upload a test & fix shortly", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551506/LUCENE-4511.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4521", "change_description": ": IndexWriter.tryDeleteDocument could return true\n(successfully deleting the document) but then on IndexWriter\nclose/commit fail to write the new deletions, if no other changes\nhappened in the IndexWriter instance.", "change_title": "tryDeleteDocument returns true (success) but may fail to write changes to the index on commit", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Spinoff from java-user thread subject \"writer.tryDeleteDocument(..) does not delete document\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551571/LUCENE-4521.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4513", "change_description": ": Fixed that deleted nested docs are scored into the\nparent doc when using ToParentBlockJoinQuery.", "change_title": "Deleted nested docs are scored into parent doc.", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1", "detail_description": "If a nested doc is deleted is still scored into the parent doc, which I think isn't right.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551218/LUCENE-4513.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4534", "change_description": ": Fixed WFSTCompletionLookup and Analyzing/FuzzySuggester\nto allow 0 byte values in the lookup keys.", "change_title": "WFST/AnalyzingSuggest don't handle keys containing 0 bytes correctly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "While binary terms w/ 0 bytes are rare, they are \"allowed\" but will cause exceptions with at least WFST/AnalyzingSuggester. I think to fix this we should pass custom Comparator to the offline sorter that decodes each BytesRef key and does the actual comparison we want, instead of using separator and relying on BytesRef.compareTo.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552111/LUCENE-4534.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4532", "change_description": ": DirectoryTaxonomyWriter use a timestamp to denote taxonomy\nindex re-creation, which could cause a bug in case machine clocks were\nnot synced. Instead, it now tracks an 'epoch' version, which is incremented\nwhenever the taxonomy is re-created, or replaced.", "change_title": "TestDirectoryTaxonomyReader.testRefreshReadRecreatedTaxonomy failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "The following failure on Jenkins:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552301/LUCENE-4532.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4544", "change_description": ": Fixed off-by-1 in ConcurrentMergeScheduler that would\nallow 1+maxMergeCount merges threads to be created, instead of just\nmaxMergeCount", "change_title": "possible bug in ConcurrentMergeScheduler.merge(IndexWriter)", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "4.1,6.0", "detail_description": "from dev list: Â¨i suspect that this code is broken. Lines 331 - 343 in org.apache.lucene.index.ConcurrentMergeScheduler.merge(IndexWriter) mergeThreadCount() are currently active merges, they can be at most maxThreadCount, maxMergeCount is number of queued merges defaulted with maxThreadCount+2 and it can never be lower then maxThreadCount, which means that condition in while can never become true. synchronized(this) {         long startStallTime = 0;         while (mergeThreadCount() >= 1+maxMergeCount) {           startStallTime = System.currentTimeMillis();           if (verbose()) try catch (InterruptedException ie) } While confusing, I think the code is actually nearly correct... but I would love to find some simplifications of CMS's logic (it's really hairy). It turns out mergeThreadCount() is allowed to go higher than maxThreadCount; when this happens, Lucene pauses mergeThreadCount()-maxThreadCount of those merge threads, and resumes them once threads finish (see updateMergeThreads).  Ie, CMS will accept up to maxMergeCount merges (and launch threads for them), but will only allow maxThreadCount of those threads to be running at once. So what that while loop is doing is preventing more than maxMergeCount+1 threads from starting, and then pausing the incoming thread to slow down the rate of segment creation (since merging cannot keep up). But ... I think the 1+ is wrong ... it seems like it should just be mergeThreadCount() >= maxMergeCount().", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552504/LUCENE-4544.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4567", "change_description": ": Fixed NullPointerException in analyzing, fuzzy, and\nWFST suggesters when no suggestions were added", "change_title": "WFSTCompletionLookup.lookup() NPE when empty fst", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "the fst Builder.finish() returns null when nothing is accepted, this then results in NPE in lookup(), see patch for extra nullchecks", "patch_link": "https://issues.apache.org/jira/secure/attachment/12554367/LUCENE-4567.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4568", "change_description": ": Fixed integer overflow in\nPagedBytes.PagedBytesData{In,Out}put.getPosition.", "change_title": "Integer overflow in PagedBytes.PagedBytesData{In,Out}put.getPosition", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1", "detail_description": "PagedBytes.PagedBytesData put.getPosition overflow when there are more than Integer.MAX_VALUE bytes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12554480/LUCENE-4568.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4581", "change_description": ": GroupingSearch.setAllGroups(true) was failing to\nactually compute allMatchingGroups", "change_title": "GroupingSearcher.setAllGroups(true) always returns count 0", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Spinoff from java-user thread \"GroupingSearch return 0 when setAllGroups(true)\" ( http://markmail.org/message/cv52f4nr3t5be5xw ).", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4009", "change_description": ": Improve TermsFilter.toString", "change_title": "Implement toString() method in TermsFilter", "detail_type": "Improvement", "detail_affect_versions": "3.5,3.6", "detail_fix_versions": "4.1,6.0", "detail_description": "LUCENE-1049 introduced a enhanced implementation of the toString() method in the BooleanFilter clause. This was an improvement, however I'm still seeing a lot Lucene filter classes not overriding the toString method resulting in a toString returning the classname and the hashcode of the object.  This can be useful sometimes, but it's totally not useful in my case. I want to see the properties set in the filters so I know which Lucene query was created. Now: BooleanFilter(+BooleanFilter(BooleanFilter(+org.apache.lucene.search.TermsFilter@ea81ba60 +org.apache.lucene.search.TermsFilter@26ea3cbc) BooleanFilter(+org.apache.lucene.search.TermsFilter@df621f09 +org.apache.lucene.search.TermsFilter@2f712446))) Wanted behavior: BooleanFilter(+BooleanFilter(BooleanFilter(+inStock:Y +barCode:12345678) BooleanFilter(+isHeavy:N +isDamaged:Y)))", "patch_link": "https://issues.apache.org/jira/secure/attachment/12525038/LUCENE-4009.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4588", "change_description": ": Benchmark's EnwikiContentSource was discarding last wiki\ndocument and had leaking threads in 'forever' mode.", "change_title": "EnwikiContentSource silently swallows the last wiki doc", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Last wiki doc is never returned", "patch_link": "https://issues.apache.org/jira/secure/attachment/12556146/LUCENE-4588.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4585", "change_description": ": Spatial RecursivePrefixTreeFilter had some bugs that only\noccurred when shapes were indexed.  In what appears to be rare circumstances,\ndocuments with shapes near a query shape were erroneously considered a match.\nIn addition, it wasn't possible to index a shape representing the entire\nglobe.", "change_title": "Spatial RecursivePrefixTreeFilter has some bugs with indexing non-point shapes", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "RecursivePrefixTreeFilter has some bugs that can occur when searching indexed shapes.  One bug is an unpositioned termsEnum.  It through an exception in testing; I'm not sure what its effects would be in production.  The other couple bugs are hard to describe here but were rare to occur in extensive testing. The effects were probably a slim chance of matching an indexed shape near the query shape. And SpatialPrefixTree does not support an indexed shape that covers the entire globe. These bugs were discovered during development of tests for RPTF LUCENE-4419 which I will submit shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555959/LUCENE-4585_PrefixTree_bugs.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4595", "change_description": ": EnwikiContentSource had a thread safety problem (NPE) in\n'forever' mode", "change_title": "EnwikiContentSource thread safety problem (NPE) in 'forever' mode", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "If close() is invoked around when an additional input stream reader is recreated for the 'forever' behavior, an uncaught NPE might occur. This bug was probably always there, just exposed now with the EnwikioContentSourceTest added in LUCENE-4588.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12557089/LUCENE-4595.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4587", "change_description": ": fix WordBreakSpellChecker to not throw AIOOBE when presented\nwith 2-char codepoints, and to correctly break/combine terms containing\nnon-latin characters.", "change_title": "WordBreakSpellChecker treats bytes as chars", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Originally opened as SOLR-4115.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12559879/LUCENE-4587.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4596", "change_description": ": fix a concurrency bug in DirectoryTaxonomyWriter.", "change_title": "DirectoryTaxonomyWriter concurrency bug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Mike tripped this error while running some benchmarks: Caused by: java.lang.ArrayIndexOutOfBoundsException: 130         at org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.getParent(DirectoryTaxonomyWriter.java:835)         at org.apache.lucene.facet.index.streaming.CategoryParentsStream.incrementToken(CategoryParentsStream.java:106)         at org.apache.lucene.facet.index.streaming.CountingListTokenizer.incrementToken(CountingListTokenizer.java:63)         at org.apache.lucene.facet.index.streaming.CategoryTokenizer.incrementToken(CategoryTokenizer.java:48)         at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:177)         at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:272)         at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:250)         at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376)         at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1455)         at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1130)         at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1111)         at perf.IndexThreads$IndexThread.run(IndexThreads.java:335) At first we thought this might be related to LUCENE-4565, but he reverted to before that commit and still hit the exception. I modified TestDirTaxoWriter.testConcurrency to index hierarchical categories, thinking that's the cause, but failed to reproduce. Eventually I realized that the test doesn't call getParent(), because it tests DirTaxoWriter concurrency, not concurrent indexing. As soon as I added a call to getParent, I hit this exception too. Adding 'synchronized' to DirTaxoWriter.addCategory seems to avoid that ex. I'll upload a patch with the modifications to the test and dig.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560063/LUCENE-4596.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4594", "change_description": ": Spatial PrefixTreeStrategy would index center-points in addition\nto the shape to index if it was non-point, in the same field.  But sometimes\nthe center-point isn't actually in the shape (consider a LineString), and for\nhighly precise shapes it could cause makeDistanceValueSource's cache to load\nparts of the shape's boundary erroneously too.  So center points aren't\nindexed any more; you should use another spatial field.", "change_title": "Spatial PrefixTreeStrategy shouldn't index center-points with shapes together", "detail_type": "Bug", "detail_affect_versions": "4.0,6.0", "detail_fix_versions": "4.1,6.0", "detail_description": "The Spatial PrefixTreeStrategy will index the center-point of a non-point shape it is given to index, in addition to the shape itself of course.  The rationale was that this point could be picked up by PointPrefixTreeFieldCacheProvider for distance/sorting.  However this approach is buggy since the distinction of grid cells between the center point and the shape itself is lost when the shape gets indexed down to max-levels precision â€“ each grid cell therein appears to be another point that needs to be brought into memory.  It's also possible that the shape is a LineString or some other non-trivial shape in which its center point isn't actually in the shape. Even if you knew this problem would never happen, I think you're better off indexing center points into another spatial field if you want them.  Perhaps arguably this strategy could do that internally?  Wether or not that ends up happening, I just want to remove the problematic behavior now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12556479/LUCENE-4594__PrefixTreeStrategy_should_not_index_center_points.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4629", "change_description": ": IndexWriter misses to delete documents if a document block is\nindexed and the Iterator throws an exception. Documents were only rolled back\nif the actual indexing process failed.", "change_title": "IndexWriter fails to delete documents if Iterator<IndexDocument> throws an exception", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "In DWPT we iterator over a document block and roll back documents if one of the docs fails with a non-aborting exception. Yet, we miss to delete those document if the iterator itself throws an exception. Given the fact that we allow an Iterable on IW we should be prepared for RT exceptions since these documents might be created in a stream fashing rather than already build up. IMO its a valid usecase if you have large documents to not materialize them in memory before indexing or at least we don't require this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560865/LUCENE-4629.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4608", "change_description": ": Handle large number of requested fragments better.", "change_title": "Handle large number of requested fragments better.", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560189/LUCENE-4608.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4633", "change_description": ": DirectoryTaxonomyWriter.replaceTaxonomy did not refresh its\ninternal reader, which could cause an existing category to be added twice.", "change_title": "DirectoryTaxonomyWriter.replaceTaxonomy should refresh the reader", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "While migrating code to Lucene 4.0 I tripped it. If you call replaceTaxonomy() with e.g. a taxonomy index that contains category \"a\", and then you try to add category \"a\" to the new taxonomy, it receives a new ordinal! The reason is that replaceTaxo doesn't refresh the internal IndexReader, but does clear the cache (as it should). This causes the next addCategory to not find category \"a\" in the cache, and not in the reader instance at hand. Simple fix, I'll attach a patch with it and a test exposing the bug.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12561177/LUCENE-4633.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4461", "change_description": ": If you added the same FacetRequest more than once, you would get\ninconsistent results.", "change_title": "Multiple FacetRequest with the same path creates inconsistent results", "detail_type": "Bug", "detail_affect_versions": "3.6", "detail_fix_versions": "4.1,6.0", "detail_description": "Multiple FacetRequest are getting merged into one creating wrong results in this case: FacetSearchParams facetSearchParams = new FacetSearchParams(); \t\tfacetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"author\"), 10)); \t\tfacetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(\"author\"), 10)); Problem can be fixed by defining hashcode and equals in certain way that Lucene recognize we are talking about different requests. Attached test case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560435/LUCENE-4461.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4656", "change_description": ": Fix regression in IndexWriter to work with empty TokenStreams\nthat have no TermToBytesRefAttribute (commonly provided by CharTermAttribute),\ne.g., oal.analysis.miscellaneous.EmptyTokenStream.", "change_title": "Fix IndexWriter working together with EmptyTokenizer and EmptyTokenStream (without CharTermAttribute), fix BaseTokenStreamTestCase", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "TestRandomChains can fail because EmptyTokenizer doesn't have a CharTermAttribute and doesn't compute the end offset (if the offset attribute was added by a filter).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12563126/LUCENE-4656.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4660", "change_description": ": ConcurrentMergeScheduler was taking too long to\nun-pause incoming threads it had paused when too many merges were\nqueued up.", "change_title": "When ConcurrentMergeScheduler stalls incoming threads it has unexpected hysteresis", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Eg if you set maxMergeCount to 2, as soon as a 3rd merge need to kick off, we stall incoming segment-creating threads.  Then we wait ... and we are supposed to resume the threads when the merge count drops back to 2, but instead we are only resuming when merge count gets to 1.  Ie, we stall for too long (= unexpected hysteresis).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12563291/LUCENE-4660.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4662", "change_description": ": Add missing elided articles and prepositions to FrenchAnalyzer's\nDEFAULT_ARTICLES list passed to ElisionFilter.", "change_title": "Elision in FrenchAnalyzer", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.1", "detail_description": "It seems org.apache.lucene.analysis.fr.FrenchAnalyzer.DEFAULT_ARTICLES is missing \"d\" and \"c\", but also \"jusqu\", \"quoiqu\", \"lorsqu\", and \"puisqu\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12563381/LUCENE-4662.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4671", "change_description": ": Fix CharsRef.subSequence method.", "change_title": "CharsRef.subSequence broken", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,3.6.3,6.0", "detail_description": "Looks like CharsRef.subSequence() is currently broken It is implemented as: Since CharsRef constructor is (char[] chars, int offset, int length), Should Be:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12563997/LUCENE-4671.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4465", "change_description": ": Let ConstantScoreQuery's Scorer return its child scorer.", "change_title": "ConstantScoreQuery's scorer does not return child scorers", "detail_type": "Bug", "detail_affect_versions": "4.0-BETA", "detail_fix_versions": "4.1,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548221/constant-score-children.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-4586", "change_description": ": Change default ResultMode of FacetRequest to PER_NODE_IN_TREE.\nThis only affects requests with depth>1. If you execute such requests and\nrely on the facet results being returned flat (i.e. no hierarchy), you should\nset the ResultMode to GLOBAL_FLAT.", "change_title": "Change default ResultMode of FacetRequest to PER_NODE_IN_TREE", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Today the default ResultMode is GLOBAL_FLAT, but it should be PER_NODE_IN_TREE. ResultMode is being used whenever you set the depth of FacetRequest to greater than 1. The difference between the two is: GLOBAL_FLAT is faster to compute than PER_NODE_IN_TREE (it just computes top-K among N total categories), however I think that it's less intuitive, and therefore should not be used as a default. In fact, I think this is kind of an expert usage.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555946/LUCENE-4586.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-2221", "change_description": ": oal.util.BitUtil was modified to use Long.bitCount and\nLong.numberOfTrailingZeros (which are intrinsics since Java 6u18) instead of\npure java bit twiddling routines in order to improve performance on modern\nJVMs/hardware.", "change_title": "Micro-benchmarks for ntz and pop (BitUtils) operations.", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "As suggested by Yonik, I performed a suite of micro-benchmarks to investigate the following: I have tried to run the code on various VMs and architectures, but of course the results may vary depending on the setting. I tested on three different machines: and on various VMs: The times between BitUtil and JDK were mostly identical. However, on 32-bit systems, precached pop2 performed much better. Examples: Note the difference between random and single distributions â€“ most probably due to more cache hits when referring to the lookup table. Other VMs on this 32-bit machine: On 64-bit machines, the results are nearly equal, with pop2 performing slightly worse on SUN's 1.6 compared to JDK and BitUtil  (but this difference is really tiny and not present on all VMs; see IBM J9 and SUN's 1.5 below). The other 64-bit machine (quad-core): I then replaced BitUtil.pop with BitUtil.pop2 in bit-counting methods like xor/and/or. The results are intriguing. On 32-bit systems, there is a measureable gain, like here: On 64-bit systems, when 64-bit values can be manipulated directly in registers, there was nearly no speedup or even a small performance penalty like in here: I'm guessing referencing memory on this fast processors is slower than manipulating registers. On JVMs prior to 1.7, the {{ntz} version from Lucene was much faster in my tests than the one from JDK,  but it also has a greater variance depending on the input bits' distribution (compare the same routine  for random and single below). But then comes the 1.7 HotSport and things change radically, on 32-bit system the JDK's version is much faster for nearly-empty long values: On the 64-bit quad core: And then comes the 1.7, compare JDK's implementation with anything else (especially the time.bench for the single input data. Looks like this is hardware-accelerated. It seems that any change introduced to these routines will hurt somebody in some configuration, so it's really hard for me to make choices. I would definitely opt for the precached pop2 version on 32-bit systems as it seems to  be always faster or equally fast compared to other bit counting options. pop2 looked like this: As for the hardware-accelerated ntz, if one can detect 1.7, then using the JDK's version is speeding up things significantly. But I have not checked how this detection would affect speed if done at run-time (I assume a final static flag wouldn't cause any performance penalty) and it is definitely not worth replacing for folks with older VMs. I will attach raw results as part of the issue if you want to draw your own conclusions. Didn't have access to sparc-machine or to any machine with the newest Intels.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12554324/LUCENE-2221.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4509", "change_description": ": Enable stored fields compression by default in the Lucene 4.1\ndefault codec.", "change_title": "Make CompressingStoredFieldsFormat the new default StoredFieldsFormat impl", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "4.1", "detail_description": "What would you think of making CompressingStoredFieldsFormat the new default StoredFieldsFormat? Stored fields compression has many benefitsÂ : Things to know: I think we could use the same default parameters as in CompressingCodec : Any concerns?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12553226/LUCENE-4509.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4536", "change_description": ": PackedInts on-disk format is now byte-aligned (it used to be\nlong-aligned), saving up to 7 bytes per array of values.", "change_title": "Make PackedInts byte-aligned?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.1", "detail_description": "PackedInts are more and more used to save/restore small arrays, but given that they are long-aligned, up to 63 bits are wasted per array. We should try to make PackedInts storage byte-aligned so that only 7 bits are wasted in the worst case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552376/LUCENE-4536.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4512", "change_description": ": Additional memory savings for CompressingStoredFieldsFormat.", "change_title": "Additional memory savings in CompressingStoredFieldsIndex.MEMORY_CHUNK", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Robert had a great idea to save memory with CompressingStoredFieldsIndex.MEMORY_CHUNK: instead of storing the absolute start pointers we could compute the mean number of bytes per chunk of documents and only store the delta between the actual value and the expected value (avgChunkBytes * chunkNumber). By applying this idea to every n(=1024?) chunks, we would even:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551469/LUCENE-4512.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4443", "change_description": ": Lucene41PostingsFormat no longer writes unnecessary offsets\ninto the skipdata.", "change_title": "BlockPostingsFormat writes unnecessary skipdata", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Seems to me lastStartOffset is unnecessary, when we skip to a document, it implicitly is 0: see BlockPostingsWriter.startDoc. (Unless I'm missing something, all tests pass with \"Block\" if i remove it) Separately we should really think about lastPayloadByteUpto, is this worth it? instead when we actually skip, we could sum the payloadLengthBuffer from 0..curPosBufferUpto as we are going to decode that block anyway?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12546939/LUCENE-4443.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4459", "change_description": ": Improve WeakIdentityMap.keyIterator() to remove GCed keys\nfrom backing map early instead of waiting for reap(). This makes test\nfailures in TestWeakIdentityMap disappear, too.", "change_title": "TestWeakIdentityMap.testConcurrentHashMap fails periodically in jenkins", "detail_type": "Bug", "detail_affect_versions": "3.6.1,4.0-BETA", "detail_fix_versions": "4.1,6.0", "detail_description": "There is either a bug, a test bug, or a jvm bug. I dont care which one it is, but lets fix the intermittent fail or disable the test.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547833/LUCENE-4459.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4473", "change_description": ": Lucene41PostingsFormat encodes offsets more efficiently\nfor low frequency terms (< 128 occurrences).", "change_title": "BlockPF encodes offsets inefficiently", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "when writing a vint block. It should write these like Lucene40 does. Here is geonames (all 19 fields as textfields with offsets): trunk _68_Block_0.pos: 178700442 patch _68_Block_0.pos: 155929641", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548578/LUCENE-4473.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4462", "change_description": ": DocumentsWriter now flushes deletes, segment infos and builds\nCFS files if necessary during segment flush and not during publishing. The latter\nwas a single threaded process while now all IO and CPU heavy computation is done\nconcurrently in DocumentsWriterPerThread.", "change_title": "Publishing flushed segments is single threaded and too costly", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA,4.0-BETA,4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Spinoff from http://lucene.markmail.org/thread/4li6bbomru35qn7w The new TestBagOfPostings failed the build because it timed out after 2 hours ... but in digging I found that it was a starvation issue: the 4 threads were flushing segments much faster than the 1 thread could publish them. I think this is because publishing segments (DocumentsWriter.publishFlushedSegment) is actually rather costly (creates CFS file if necessary, writes .si, etc.). I committed a workaround for now, to prevent starvation (see svn diff -c 1394704 https://svn.apache.org/repos/asf/lucene/dev/trunk), but we really should address the root cause by moving these costly ops into flush() so that publishing is a low cost operation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12548534/LUCENE-4462.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4496", "change_description": ": Optimize Lucene41PostingsFormat when requesting a subset of\nthe postings data (via flags to TermsEnum.docs/docsAndPositions) to use\nForUtil.skipBlock.", "change_title": "Don't decode unnecessary freq blocks in 4.1 codec", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.1,6.0", "detail_description": "TermsEnum.docs() has an expert flag to specify you don't require frequencies. This is currently set by some things that don't need it: we should call ForUtil.skipBlock instead of ForUtil.readBlock in this case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550294/LUCENE-4496.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4497", "change_description": ": Don't write PosVIntCount to the positions file in\nLucene41PostingsFormat, as its always totalTermFreq % BLOCK_SIZE.", "change_title": "Don't write posVIntCount in 4.1 codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Its confusing and unnecessary that we compute this from docFreq for the doc/freq vint count, but write it for the positions case: its totalTermFreq % BLOCK_SIZE.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550304/LUCENE-4497.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4498", "change_description": ": In Lucene41PostingsFormat, when a term appears in only one document,\nInstead of writing a file pointer to a VIntBlock containing the doc id, just\nwrite the doc id.", "change_title": "pulse docfreq=1 DOCS_ONLY for 4.1 codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "We have pulsing codec, but currently this has some downsides: On the other hand the way the 4.1 codec encodes \"primary key\" fields is pretty silly, we write the docStartFP vlong in the term dictionary metadata, which tells us where to seek in the .doc to read our one lonely vint. I think its worth investigating that in the DOCS_ONLY docfreq=1 case, we just write the lone doc delta where we would write docStartFP. We can avoid the hairy reuse problem too, by just supporting this in refillDocs() in BlockDocsEnum instead of specializing. This would remove the additional seek for \"primary key\" fields without really any of the downsides of pulsing today.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550353/LUCENE-4498.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4515", "change_description": ": MemoryIndex now uses Byte/IntBlockPool internally to hold terms and\nposting lists. All index data is represented as consecutive byte/int arrays to\nreduce GC cost and memory overhead.", "change_title": "Make MemoryIndex more memory efficient", "detail_type": "Improvement", "detail_affect_versions": "4.0,4.1,6.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently MemoryIndex uses BytesRef objects to represent terms and holds an int[] per term per field to represent postings. For highlighting this creates a ton of objects for each search that 1. need to be GCed and 2. can't be reused.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12551836/LUCENE-4515.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4538", "change_description": ": DocValues now caches direct sources in a ThreadLocal exposed via SourceCache.\nUsers of this API can now simply obtain an instance via DocValues#getDirectSource per thread.", "change_title": "Cache DocValues DirecSource", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently the user need to make sure that a direct source is not shared between threads and each time someone calls getDirectSource we create a new source which has a reasonable overhead. We can certainly reduce the overhead (maybe different issue) but it should be easier for the user to get a direct source and handle it. More than that is should be consistent with getSource / loadSource.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552267/LUCENE-4538.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4580", "change_description": ": DrillDown.query variants return a ConstantScoreQuery with boost set to 0.0f\nso that documents scores are not affected by running a drill-down query.", "change_title": "Facet DrillDown should return a ConstantScoreQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "DrillDown is a helper class which the user can use to convert a facet value that a user selected into a Query for performing drill-down or narrowing the results. The API has several static methods that create e.g. a Term or Query. Rather than creating a Query, it would make more sense to create a Filter I think. In most cases, the clicked facets should not affect the scoring of documents. Anyway, even if it turns out that it must return a Query (which I doubt), we should at least modify the impl to return a ConstantScoreQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12555911/LUCENE-4580.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4598", "change_description": ": PayloadIterator no longer uses top-level IndexReader to iterate on the\nposting's payload.", "change_title": "Change PayloadIterator to not use top-level reader API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "Currently the facet module uses MultiFields.* to pull the D&PEnum in PayloadIterator, to access the payloads that store the facet ords. It then makes heavy use of .advance and .getPayload to visit all docIDs in the result set. I think we should get some speedup if we go segment by segment instead ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560474/LUCENE-4598.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4661", "change_description": ": Drop default maxThreadCount to 1 and maxMergeCount to 2\nin ConcurrentMergeScheduler, for faster merge performance on\nspinning-magnet drives", "change_title": "Reduce default maxMerge/ThreadCount for ConcurrentMergeScheduler", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "I think our current defaults (maxThreadCount=#cores/2, maxMergeCount=maxThreadCount+2) are too high ... I've frequently found merges falling behind and then slowing each other down when I index on a spinning-magnets drive. As a test, I indexed all of English Wikipedia with term-vectors (= heavy on merging), using 6 threads ... at the defaults (maxThreadCount=3, maxMergeCount=5, for my machine) it took 5288 sec to index & wait for merges & commit.  When I changed to maxThreadCount=1, maxMergeCount=2, indexing time sped up to 2902 seconds (45% faster).  This is on a spinning-magnets disk... basically spinning-magnets disk don't handle the concurrent IO well. Then I tested an OCZ Vertex 3 SSD: at the current defaults it took 1494 seconds and at maxThreadCount=1, maxMergeCount=2 it took 1795 sec (20% slower).  Net/net the SSD can handle merge concurrency just fine. I think we should change the defaults: spinning magnet drives are hurt by the current defaults more than SSDs are helped ... apps that know their IO system is fast can always increase the merge concurrency.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Documentation", "change_id": "LUCENE-4483", "change_description": ": Refer to BytesRef.deepCopyOf in Term's constructor that takes BytesRef.", "change_title": "Make Term constructor javadoc refer to BytesRef.deepCopyOf", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.1,6.0", "detail_description": "The Term constructor from BytesRef javadoc indicates that a clone needs to be made of the BytesRef. But the clone() method of BytesRef is not what is meant, a deep copy needs to be made.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12549037/LUCENE-4483.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Build", "change_id": "LUCENE-4650", "change_description": ": Upgrade randomized testing to version 2.0.8: make the\ntest framework more robust under low memory conditions.", "change_title": "Test framework should be more robust under OOM conditions", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "When tests OOM (or run out of their permgen space) things to wild (hung runner, etc.).", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Build", "change_id": "LUCENE-4603", "change_description": ": Upgrade randomized testing to version 2.0.5: print forked\nJVM PIDs on heartbeat from hung tests", "change_title": "The test framework should report forked JVM PIDs upon heartbeats", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "This would help in getting a stack trace of a hung JVM before the timeout and/or in killing the offending JVM. RR issue: https://github.com/carrotsearch/randomizedtesting/issues/135", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Build", "change_id": "LUCENE-4451", "change_description": ": Memory leak per unique thread caused by\nRandomizedContext.contexts static map. Upgrade randomized testing\nto version 2.0.2", "change_title": "Memory leak per unique thread caused by RandomizedContext.contexts static map", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "In digging on the hard-to-understand OOMEs with TestDirectPostingsFormat ... I found (thank you YourKit) that RandomizedContext (in randomizedtesting JAR) seems to be holding onto all threads created by the test.  The test does create many very short lived threads (testing the thread safety of the postings format), in BasePostingsFormatTestCase.testTerms), and somehow these seem to tie up a lot (~100 MB) of RAM in RandomizedContext.contexts static map. For now I've disabled all thread testing (committed false && inside BPFTC.testTerms), but hopefully we can fix the root cause here, eg when a thread exits can we clear it from that map?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12547250/LUCENE-4451.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Build", "change_id": "LUCENE-4589", "change_description": ": Upgraded benchmark module's Nekohtml dependency to version\n1.9.17, removing the workaround in Lucene's HTML parser for the\nTurkish locale.", "change_title": "Upgrade benchmark modules nekohtml and remove turkish HTML element lowercasing workaround!", "detail_type": "Task", "detail_affect_versions": "4.0", "detail_fix_versions": "4.1,6.0", "detail_description": "LUCENE-4220 added nekohtml as new parser for HTML files in benchamrk module. Unfortunately the nekohtml parser had the well known lowercase dotless-i bug when using the turkish locale. Version 1.9.17 of nekohtml fixes this bug and was released a few days ago (http://nekohtml.sourceforge.net/changes.html). This issue will update it and remove the workaround.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12556166/LUCENE-4589.patch", "patch_content": "none"}
{"library_version": "4.1.0", "change_type": "Build", "change_id": "LUCENE-4601", "change_description": ": Fix ivy availability check to use typefound, so it works\nif called from another build file.", "change_title": "ivy availability check isn't quite right", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.1,6.0", "detail_description": "remove ivy from your .ant/lib but load it up on a build file like so: You have to lie to lucene's build, overriding ivy.available, because for some reason the detection is wrong and will tell you ivy is not available, when it actually is. I tried changing the detector to use available classname=some.ivy.class and this didnt work either... so I don't actually know what the correct fix is.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12560145/LUCENE-4601.patch", "patch_content": "none"}

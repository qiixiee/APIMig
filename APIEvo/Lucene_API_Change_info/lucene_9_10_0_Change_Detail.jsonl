{"library_version": "9.10.0", "change_type": "API Changes", "change_id": "GITHUB#12243", "change_description": ": Mark TermInSetQuery ctors with varargs terms as @Deprecated. SortedSetDocValuesField#newSlowSetQuery,\nSortedDocValuesField#newSlowSetQuery, KeywordField#newSetQuery now take a collection of terms as a param.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently, the KeywordField class provides a public static method newSetQuery(String field, BytesRef... values) . However, this method only supports varargs parameter for values. I would like to propose adding a new static method that allows for a collections parameter for values, as this would provide greater flexibility. Proposed method signature: public static Query newSetQuery(String field, Collection<BytesRef> values) I am willing to make the changes and submit a pull request. Please let me know if there are any concerns or feedback. Thank you! The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "API Changes", "change_id": "GITHUB#11041", "change_description": ": Deprecate IndexSearch#search(Query, Collector) in favor of\nIndexSearcher#search(Query, CollectorManager) for TopFieldCollectorManager\nand TopScoreDocCollectorManager.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "It's a bit trappy that you can create an IndexSearcher with an executor, but that it would always search on the caller thread when calling IndexSearcher#search(Query,Collector) . Let's remove IndexSearcher#search(Query,Collector) , point our users to IndexSearcher#search(Query,CollectorManager) instead, and change factory methods of our main collectors (e.g. TopScoreDocCollector#create ) to return a CollectorManager instead of a Collector ? Migrated from LUCENE-10002 by Adrien Grand ( @jpountz ), updated Apr 06 2022 Pull requests: #240 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "API Changes", "change_id": "GITHUB#12854", "change_description": ": Mark DrillSideways#createDrillDownFacetsCollector as @Deprecated.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This extension hook is only referenced by the also-deprecated DrillSideways#search(DrillDownQuery, Collector) , so let's mark it deprecated as well and remove it on main.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "API Changes", "change_id": "GITHUB#12624", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Split the scratch byte operations inside BytesStore out of FSTWriter and allow the DataOutput to be configurable from FSTCompiler.Builder . Those operations are for building the frontier node. Only the final bytes will be written to the DataOutput. The Builder will still maintain bytesPageBits(int) for backward-compatibility, in which it will use a ByteBuffersDataOutput . Also extract the saving of metadata as a standalone method, as there can be use case where some already saved the FST to a DataOutput when building and only need to save the metadata here. Issue: #12543 The FST produced by this DataOutput-backed FST is not readable directly, and users need to construct a corresponding DataInput and pass it to the FST public constructor. In short the FSTCompiler only compile the FST but not read it. The write-then-read-immediately use case is still supported with ByteBuffersDataOutput.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "API Changes", "change_id": "GITHUB#12831", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Make FSTCompiler.compile() to only return the FSTMetadata. Depending on whether the DataOutput used implements FSTReader or not, the returned FST might be unreadable. Thus having this method only return the FSTMetadata means we will never return an unusable FST. Now to create the FST there will be 2 ways: Note that this PR depends on 2 others to be merged first: FST.fromFSTReader will also handle the case where fstMetadata is null, in which case it would return null. It is essentially a sugar syntactic utility to make the migration easier. So we will change this code: to this: instead of:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "New Features", "change_id": "GITHUB#12679", "change_description": ": Add support for similarity-based vector searches using [Byte|Float]VectorSimilarityQuery. Uses a new\nVectorSimilarityCollector to find all vectors scoring above a `resultSimilarity` while traversing the HNSW graph till\nbetter-scoring nodes are available, or the best candidate is below a score of `traversalSimilarity` in the lowest\nlevel.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Background in #12579 Add support for getting \"all vectors within a radius\" as opposed to getting the \"topK closest vectors\" in the current system I've tried to keep this change minimal and non-invasive by not modifying any APIs and re-using existing HNSW graphs -- changing the graph traversal and result collection criteria to: On a higher level, finding topK results needed HNSW searches to happen in #rewrite because of an interdependence of results between segments - where we want to find the index-level topK from multiple segment-level results. This is kind of against Lucene's concept of segments being independently searchable sub-indexes? Moreover, we needed explicit concurrency ( #12160 ) to perform these in parallel, and these shortcomings would be naturally overcome with the new objective of finding \"all vectors within a radius\" - inherently independent of results from another segment (so we can move searches to a more fitting place?) I could not find much precedent in using HNSW graphs this way (or even the radius-based search for that matter - please add links to existing work if someone is aware) and consequently marked all classes as @lucene.experimental For now I have re-used lots of functionality from AbstractKnnVectorQuery to keep this minimal, but if the use-case is accepted more widely we can look into writing more suitable queries (as mentioned above briefly) Run benchmarks with this new query to see how it compares to the topK based search", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "New Features", "change_id": "GITHUB#12829", "change_description": ": For indices newly created as of 9.10.0 onwards, IndexWriter preserves document blocks indexed via\nIndexWriter#addDocuments or IndexWriter#updateDocuments also when index sorting is configured. Document blocks are\nmaintained alongside their parent documents during sort and merge. IndexWriterConfig accepts a parent field that is used\nto maintain block orders if index sorting is used. Note, this is fully optional in Lucene 9.x while will be mandatory for\nindices that use document blocks together with index sorting as of 10.0.0.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today index sorting with likely break document blocks added with IndexWriter#addDocuments(...) and friends since the index sorter has no indication of what documents are part of a block. This change proposes a marker field as a requirement for parent documents if the block API is used in conjunction with index sorting. At this point this change requires a NumericDV field only present on the last document of a block or on every document indexed as an individual document iff a parent field is configured on the Sort. This can potentially be extended to a Term which is not as straight forward since validations might be more difficult and today postings are not available when we sort the flushed segment. Relates to #12711", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "New Features", "change_id": "GITHUB#12336", "change_description": ": Index additional data per facet label in the taxonomy.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "If a user has data about the ordinals in the main index, it is efficient and convenient to index that data in the taxonomy. This issue is a follow-up to a discussion on the mailing list: https://lists.apache.org/thread/c3y7fx2jkm0r4frds4rzx4sjxc8qspth The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "New Features", "change_id": "GITHUB#12706", "change_description": ": Add support for the final release of Java foreign memory API in Java 22 (and later).\nLucene's MMapDirectory will now mmap Lucene indexes in chunks of 16 GiB (instead of 1 GiB) starting\nfrom Java 19. Indexes closed while queries are running can no longer crash the JVM.\nSupport for vectorized implementations of VectorUtil based on jdk.incubator.vector APIs was added\nfor exactly Java 22. Therefore, applications started with command line parameter\n\"java --add-modules jdk.incubator.vector\" will automatically use the new vectorized implementations\nif running on a supported platform (Java 20/21/22 on x86 CPUs with AVX2 or later or ARM NEON CPUs).\nThis is an opt-in feature and requires explicit Java command line flag! When enabled, Lucene logs\na notice using java.util.logging. Please test thoroughly and report bugs/slowness to Lucene's mailing\nlist.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a very simple PR to add support for Java 22 and later for the MMapDirectory using MemorySegment. The Panama Foreign API is no longer in preview mode it is fully supported backwards-compatible public API. I initially extracted the APIJAR file for Java 22, but actually the code compiled unmodified with that version. This means for the parts of the API we use in MemorySegment does not differ from Java 21 and is byte code compatible. This has several positive effects: The current API is this: https://download.java.net/java/early_access/jdk22/docs/api/java.base/java/lang/foreign/package-summary.html The PR is quite simple: As it is still subject to change, I declare this PR as draft and update it. I will merge it when the JDK 22 goes into rampdown phases (after 2023-12-07). If all looks stable, I will merge at this time, at latest I will merge on the initial release candidate of Java 22 at 2024-02-08, I extended the PR to also cover the vector incubator (see comment below). Combining that into one PR makes testing easier, as we have a separate Jenkins job on this branch.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#12870", "change_description": ": Tighten synchronized loop in DirectoryTaxonomyReader#getOrdinal.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "DirectoryTaxonomyReader.getOrdinal seems to have a wider synchronized loop than needed when checking if the label we've asked about exists in the cache. getBulkOrdinal already has a smaller corresponding synchronized loop .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#12812", "change_description": ": Avoid overflows and false negatives in int slice buffer filled-with-zeros assertion.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In SlicedIntBlockPool , we assert that a buffer we're about to use has previously been filled with zeros. This PR makes it so we exit faster if we find a non-zero value. It also prevents overflow cases that could happen before. Follow-up from #12734 .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#12910", "change_description": ": Refactor around NeighborArray to make it more self-contained.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Not a huge refactor, basically: Such that I think would help further abstraction in the future to have a concurrent version of NeighborArray.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#12999", "change_description": ": Use Automaton for SurroundQuery prefix/pattern matching", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "SurroundQuery still does matching using java.util.Pattern (quite slow). This PR switches over SrndTruncQuery and SrndPrefixQuery to use Automaton-based matching.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#13043", "change_description": ": Support getMaxScore of ConjunctionScorer for non top level scoring clause.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "After introducing topLevelScoringClause, ConjunctionScorer with multiple scorers can be used for non top level scoring clause conjunctions instead of BlockMaxConjunctionScorer even requiredScorers is empty. In such case, ConjunctionScorer returns Infinity as maxScore and it ruins some optimizations like parent WANDScorer. https://github.com/apache/lucene/blob/7d35ae485807147460f63ea58ae495124e972e13/lucene/core/src/java/org/apache/lucene/search/Boolean2ScorerSupplier.java#L218C77-L218C98", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#13055", "change_description": ": Make DEFAULT_STOP_TAGS in KoreanPartOfSpeechStopFilter immutable", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "EnumSet.of() returns a mutable Set that should not be used for static final constants.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Improvements", "change_id": "GITHUB#888", "change_description": ": Use native byte order varhandles to spare CPU's byte swapping.\nTests are running with random byte order to ensure that the order does not affect correctness\nof code. Native order was enabled for LZ4 compression.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "see https://issues.apache.org/jira/browse/LUCENE-10572 #11608", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "LUCENE-10366", "change_description": ": Override readVInt() and readVLong() in ByteBufferDataInput to help Hotspot inline method.", "change_title": "Reduce the number of valid checks for ByteBufferIndexInput#readVInt", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Today, we do not rewrite #readVInt and #readVLong for ByteBufferIndexInput. By default, the logic will call #readByte several times, and we need to check whether ByteBuffer is valid every time. This may not be necessary as we just need a final check.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#12839", "change_description": ": Introduce method to grow arrays up to a given upper limit and use it to reduce overallocation for\nDirectoryTaxonomyReader#getBulkOrdinals.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "ArrayUtils provides methods to grow arrays, overallocating exponentially, with the possibility of requesting a minimum size. Sometimes we have an upper limit to the number of elements that would go into that array. In those situations, it would be nice to have a method that grows up to a limit. For example, DirectoryTaxonomyReader.getBulkOrdinals dynamically grows an array pointing to ordinals missing from the cache ( code ). But we know ahead of time there can't be more missing ordinals than there are ordinals in the taxonomy. We can limit the array growth to avoid overallocating. This pattern might be applicable in multiple places. TermsAutomatonScorer seems to use a similar pattern . The API could look like this: The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#12841", "change_description": ": Move group-varint encoding/decoding logic to DataOutput/DataInput.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "From issue: #12826 The JMH benchmark with this PR on my Mac (Intel chip) : java 17 java 21", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#12997", "change_description": "Avoid reset BlockDocsEnum#freqBuffer when indexHasFreq is false.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As discussion in #12954 , this is the second implementation can be considered. this change yielded a 13% speedup for PKLookup .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#12989", "change_description": ": Split taxonomy facet arrays across reusable chunks of elements to reduce allocations.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "At Amazon product search we use taxonomy facets for the facet filtering we show customers, but this causes high RAM pressure on every refresh as the current implementation allocates a new massive int[] on each refresh, requiring ~2X the transient RAM usage until the old taxonomy reader is fully closed / dereferenced, causing us to over-size our heaps just to handle this short RAM surge. Yet, on each refresh, all that really is happening is a few new ints might be effectively appended to the end of the old int[] -- it is inherently a write once append only data structure.  Reallocating the full massive int[] every time is silly. I think we could switch to a paged int[] structure?  This way the new reader could share nearly all of the old int[] pages, and only make a new last page to hold the few new append ints? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#13033", "change_description": ": PointRangeQuery now exits earlier on segments whose values\ndon't intersect with the query range. When a PointRangeQuery is a required\nclause of a boolean query, this helps save work on other required clauses of\nthe same boolean query.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change makes PointRangeQuery exit early when it knows that it doesn't match a segment. In the case when this query is part of a conjunction, this helps make sure ScorerSupplier#get doesn't get called on other required clauses, which is sometimes an expensive operation (e.g. multi-term queries). This is especially relevant for time-based data combined with LogByteSizePolicy , which gives non-adjacent ranges of timestamps to each segment, which in-turn increases the likelihood that some segments won't match a range query at all.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#13026", "change_description": ": ReqOptSumScorer will now propagate minimum competitive scores\nto the optional clause if the required clause doesn't score. In practice,\nthis will help boolean queries that consist of a mix OF FILTER clauses and\nSHOULD clauses.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "If the required clause doesn't contribute scores, which typically happens if the required clause is a FILTER clause, then the minimum competitive score can be propagated directly to the optional clause.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#13052", "change_description": ": Avoid set.removeAll(list) O(n^2) performance trap in the UpgradeIndexMergePolicy", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When list size is greater than set size, list.contains() will be called for each set element resulting in O(n^2) complexity. When list size is smaller the default removeAll() implementation will iterate the list anyway.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#13036", "change_description": "Optimize counts on two clause term disjunctions.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Calculate count(clause1 OR clause2) as count(clause1) + count(clause2) - count(clause1 AND clause2) as discussed in #12644 . Todo:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#12962", "change_description": ": Speedup concurrent multi-segment HNWS graph search", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "A second implementation of #12794 using Queue instead of MaxScoreAccumulator. Speedup concurrent multi-segment HNWS graph search by exchanging the global top scores  collected so far across segments. These global top scores set the minimum threshold that candidates need to pass to be considered. This allows earlier stopping for segments that don't have good candidates.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Optimizations", "change_id": "GITHUB#13090", "change_description": ": Prevent humongous allocations in ScalarQuantizer when building quantiles.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The initial release of scalar quantization would periodically create a humongous allocation, which can put unwarranted pressure on the GC & on the heap usage as a whole. This commit adjusts this by only allocating a float array of 20*dimensions and averaging the discovered quantiles from there. Why does this work? I benchmarked this over 500k vectors. candidate baseline 100k vectors candidate baseline There does seem to be a slight increase in merge time (these are single threaded numbers) and a slight change in recall. But to me, these seem acceptable given we are no longer allocating a ginormous array.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12866", "change_description": ": Prevent extra similarity computation for single-level HNSW graphs.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "#findBestEntryPoint is used to determine the entry point for the last level of HNSW search It finds the single best-scoring node from all upper levels - but performs an unnecessary computation (along with recording one visited node ) when the graph just has 1 level (so the entry node is just the overall graph's entry node) Also added a test to demonstrate this (fails without the changes in PR) -- where we visit graph.size() + 1 nodes when the topK is high (should be a maximum of graph.size() )", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12558", "change_description": ": Ensure #finish is called on all drill-sideways FacetsCollectors even when no hits are scored.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I recently encountered an Out of Memory bug when IntTaxonomyFacets initialised a dense values array instead of a sparse hash map. This was happening because we encountered a case where sumTotalHits and maxDoc were both 0, and since 0<0 is false, useHashTable() i.e. the method we call to decide if we should use a sparse/dense data structure, returns false and we end up using the dense values array. We arrived at the maxDoc = 0 case when a FacetsCollector that was not null and had totalHits = 0 but an empty List<MatchingDocs> was passed to the method. We end up not iterating the loop and consequently not updating maxDoc value and it stays 0. I think we could explicitly check for maxDoc == 0 in the condition too, but maybe there's a better fix? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12920", "change_description": ": Address bug in TestDrillSideways#testCollectionTerminated that could occasionally cause the test to\nfail with certain random seeds.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This test is really about confirming that #finish is called, so we don't really need to check the hit counts on early termination. The problem with checking the hit counts is that it depends on doc ordering and segment geometry in the index, which is unreliable with the randomized writer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12885", "change_description": ": Fixed the bug that JapaneseReadingFormFilter cannot convert some hiragana to romaji.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I found a bug in using JapaneseReadingFormFilter that some hiragana are not converted to romaji. (For example, \"ぐ\" does not become \"gu\". I noticed this because \"マスキング\" did not get any hits when searching for \"ますきんぐ\".) I believe this is due to the existence of hiragana whose readings are not explicitly defined in the kuromoji dictionary. For hiragana, which is an OOV term, how about treating its conversion to katakana as a reading?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12287", "change_description": ": Fix a bug in ShapeTestUtil.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "boxPolygon and trianglePolygon only uses minX and minY value of give XY Rectangle which results in a polygon with points in single place. With this changes, both methods generate correct polygons.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13031", "change_description": ": ScorerSupplier created by QueryProfilerWeight will propagate topLevelScoringClause to the sub ScorerSupplier.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The topLevelScoringClause is not propagated from QueryProfiler. This causes some optimizations are skipped during profiling.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13059", "change_description": ": Fixed missing IndicNormalization and DecimalDigit filters in TeluguAnalyzer normalization", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "DecimalDigitFilter and IndicNormalizationFilter were mistakenly omitted.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Build", "change_id": "GITHUB#12931", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Spinoff from #12911 where I hit weird/scary exceptions during ./gradlew regenerate with certain JDK version(s) and not others due to a lurking zero width space Unicode character. @uschindler fixed that one case, but perhaps we have others?  And perhaps copy/apply of a patch file is another vector for them to invade us. Can we statically assert we don't accidentally invite these little buggers in? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Build", "change_id": "GITHUB#12936", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Previously this error-prone check was not enabled, requiring code to be simple ascii, because of violations in spatial3d. I used find-replace on the greek letters there and the tests pass. Closes #12931", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Build", "change_id": "GITHUB#12937", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This would have prevented the problem with our gradle file. This is just another extension for validateSourcePatterns task. We have other bad codepoints already in there, so I'd suggest to add those. The uFEFF is there two times (first it detects it at begin of file, so we prevent byte order marks, but later it also disallows it; it is duplicate to have different messages). I also added a strict UTF-8 decoding check when opening the files thanks to @rmuir Code. When a file was accidentally saved in ISO-8859-x or similar bullshit now the source code patterns check reports a violation. The only violation for zero-width codepoints was a copy-pasted comment in jflex, not relevant to code. There were also 2 CSS files with wrong encoding (changed them to UTF-8). This fixes #12931", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#11023", "change_description": ": Removing some dead code in CheckIndex.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This issue is a spin-off from discussion in #128 Currently CheckIndex defaults to checking both checksum as well as content inside each segment files for correctness, and requires -fast flag to be explicitly passed in to do checksum only. However, this default setting was there due to lack of checksum feature historically, and is slow for most end-users nowadays as they probably only care about their indices being intact (from random bit flipping for example). This issue is to change the default settings for CheckIndex so that they are more appropriate for end-users. One proposal from @rmuir is the following: Migrated from LUCENE-9984 by Zach Chen ( @zacharymorn ), updated Aug 16 2021 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#11023", "change_description": ": Removing @lucene.experimental tags in testXXX methods in CheckIndex.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This issue is a spin-off from discussion in #128 Currently CheckIndex defaults to checking both checksum as well as content inside each segment files for correctness, and requires -fast flag to be explicitly passed in to do checksum only. However, this default setting was there due to lack of checksum feature historically, and is slow for most end-users nowadays as they probably only care about their indices being intact (from random bit flipping for example). This issue is to change the default settings for CheckIndex so that they are more appropriate for end-users. One proposal from @rmuir is the following: Migrated from LUCENE-9984 by Zach Chen ( @zacharymorn ), updated Aug 16 2021 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#12934", "change_description": ": Cleaning up old references to Lucene/Solr.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I was looking at a very old issue for a failing unit test with an ant test command. When I tried to run it in gradle (incorrectly), I was greeted with a message that stood out to me: Searching the Lucene codebase ( github search ), we find 18 usages of Lucene/Solr , all in comments/readmes/print messages in tools, most of these can be switched to just Lucene Should we clean this up? If yes, I am happy to do it :) The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#12967", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Mostly changing code like this \"expected: \" + currentFullFlushDelQueue + \"but was: \" to \"expected: \" + currentFullFlushDelQueue + \" but was: \" But also fixing bugs like \"\" + i + numDocs => \"\" + (i + numDocs) . I took the liberty to also do some minor housekeeping in the affected files:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13038", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Using concatenation in the format string like this: Replacing the concatenation with the format placeholders results in a cleaner more compact code:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13040", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13042", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13047", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When arguments do not make use of regexp features replace() is a more efficient option, especially the char -variant.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13048", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13049", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "indexOf(char) runs faster than searching for the same one-character string. It makes sense to use indexOf('x') instead of any indexOf(\"x\") .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13050", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "isEmpty() is a simpler more efficient method without the overhead of extra parameter, additional instanceof check, calling the coder to actually do the comparison, etc. Besides, it aligns nicely with the approach used for collections: list == null || list.isEmpty() .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13051", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "For sets with enum values EnumSet provides a more efficient implementation. Besides, EnumSet.of() results in a shorter, easier to understand code than new HashSet<>(Arrays.asList()) .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13039", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.10.0", "change_type": "Other", "change_id": "GITHUB#13053", "change_description": ": Minor AnyQueryNode code cleanup", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}

{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4810", "change_description": ": EdgeNGramTokenFilter no longer increments position for\nmultiple ngrams derived from the same input token.", "change_title": "Positions are incremented for each ngram in EdgeNGramTokenFilter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Edge ngrams should be like synonyms, with all the ngrams generated from a token having the same position as that original token. The current code increments position. For the text \"molecular biology\", the query \"mol bio\" should match as a phrase in neighboring positions. It does not. You can see this in the Analysis page in the admin UI.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12579779/LUCENE-4810-first-token-position-increment.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4822", "change_description": ": KeywordTokenFilter is now an abstract class. Subclasses\nneed to implement #isKeyword() in order to mark terms as keywords.\nThe existing functionality has been factored out into a new\nSetKeywordTokenFilter class.", "change_title": "Add PatternKeywordTokenFilter to marks keywords based on regular expressions", "detail_type": "Improvement", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "today we need to pass in an explicit set of terms that we want to marks as keywords. It might make sense to allow patterns as well to prevent certain suffixes etc. to be keyworded.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573089/LUCENE-4822.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4642", "change_description": ": Remove Tokenizer's and subclasses' ctors taking\nAttributeSource.", "change_title": "Add create(AttributeFactory) to TokenizerFactory and subclasses with ctors taking AttributeFactory, and remove Tokenizer's and subclasses' ctors taking AttributeSource", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3,6.0", "detail_description": "All tokenizer implementations have a constructor that takes a given AttributeSource as parameter (LUCENE-1826).  These should be removed. TokenizerFactory does not provide an API to create tokenizers with a given AttributeFactory, but quite a few tokenizers have constructors that take an AttributeFactory.  TokenizerFactory should add a create(AttributeFactory) method, as should subclasses for tokenizers with AttributeFactory accepting ctors.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566666/TrieTokenizerFactory.java.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4833", "change_description": ": IndexWriterConfig used to use LogByteSizeMergePolicy when\ncalling setMergePolicy(null) although the default merge policy is\nTieredMergePolicy. IndexWriterConfig setters now throw an exception when\npassed null if null is not a valid value.", "change_title": "Fix default MergePolicy in IndexWriterConfig", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Although the default merge policy is TieredMergePolicy (as documented in IndexWriterConfig constructor), setMergePolicy assumes that the default is LogByteSizeMergePolicy.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573731/LUCENE-4833.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4849", "change_description": ": Made ParallelTaxonomyArrays abstract with a concrete\nimplementation for DirectoryTaxonomyWriter/Reader. Also moved it under\no.a.l.facet.taxonomy.", "change_title": "Make ParallelTaxonomyArrays abstract", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "ParallelTaxonomyArrays, while appearing on TaxonomyReader, actually support only one implementation, that of DirectoryTaxonomyReader. I'd like to make it abstract (perhaps share the children/siblings arrays computation) to allow for other taxonomy reader implementations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574154/LUCENE-4849.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4876", "change_description": ": IndexDeletionPolicy is now an abstract class instead of an\ninterface. IndexDeletionPolicy, MergeScheduler and InfoStream now implement\nCloneable.", "change_title": "IndexWriterConfig.clone should clone the MergeScheduler", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.5,6.0", "detail_description": "ConcurrentMergeScheduler has a List<MergeThread> member to track the running merging threads, so IndexWriterConfig.clone should clone the merge scheduler so that both IndexWriterConfig instances are independant.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594391/LUCENE-4876.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4874", "change_description": ": FilterAtomicReader and related classes (FilterTerms,\nFilterDocsEnum, ...) don't forward anymore to the filtered instance when the\nmethod has a default implementation through other abstract methods.", "change_title": "Don't override non abstract methods that have an impl through other abstract methods in FilterAtomicReader and related classes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Terms.intersect is an optional method. The fact that it is overridden in FilterTerms forces any non-trivial class that extends FilterTerms to override intersect in order this method to have a correct behavior. If FilterTerms did not override this method and used the default impl, we would not have this problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577041/LUCENE-4874.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4642", "change_description": ",", "change_title": "Add create(AttributeFactory) to TokenizerFactory and subclasses with ctors taking AttributeFactory, and remove Tokenizer's and subclasses' ctors taking AttributeSource", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3,6.0", "detail_description": "All tokenizer implementations have a constructor that takes a given AttributeSource as parameter (LUCENE-1826).  These should be removed. TokenizerFactory does not provide an API to create tokenizers with a given AttributeFactory, but quite a few tokenizers have constructors that take an AttributeFactory.  TokenizerFactory should add a create(AttributeFactory) method, as should subclasses for tokenizers with AttributeFactory accepting ctors.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566666/TrieTokenizerFactory.java.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4877", "change_description": ",", "change_title": "Fix analyzer factories to throw exception when arguments are invalid", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Currently if someone typos an argument \"someParamater=xyz\" instead of someParameter=xyz, they get no exception and sometimes incorrect behavior. It would be way better if these factories threw exception on unknown params, e.g. they removed the args they used and checked they were empty at the end.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575267/LUCENE-4877_one_solution_prototype.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "API Changes", "change_id": "LUCENE-4896", "change_description": ": Made PassageFormatter abstract in PostingsHighlighter, made\nmembers of DefaultPassageFormatter protected.", "change_title": "PostingsHighlighter should use a interface of PassageFormatter instead of a class", "detail_type": "Improvement", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "In my project I need a custom PassageFormatter to use with PostingsHighlighter.  I extended PassageFormatter  to override format(...) but if I do that, I don't have access to the private variables.  So instead of changing the scope to protected, it should be more usefull to use a interface for PassageFormatter. like public DefaultPassageFormatter implements PassageFormatter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578466/LUCENE-4896.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "API Changes", "change_id": "LUCENE-4844", "change_description": ": removed TaxonomyReader.getParent(), you should use\nTaxonomyReader.getParallelArrays().parents() instead.", "change_title": "Remove TaxonomyReader.getParent(ord)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "This should have been gone when we introduced .getParallelArrays(). The method simply calls getParallelArrays().parents()[ord], and that's what callers should do. Except one test, no other code in facets calls this method.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574083/LUCENE-4844.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "API Changes", "change_id": "LUCENE-4742", "change_description": ": Renamed spatial 'Node' to 'Cell', along with any method names\nand variables using this terminology.", "change_title": "Rename SpatialPrefixTree's \"Node\" back to \"Cell\"", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3", "detail_description": "SpatialPrefixTree makes \"Node\"s which are basically a rectangular spatial region that is more colloquially referred to as a \"Cell\".  It was named \"Cell\" in the first place and for whatever reason, Ryan and/or Chris renamed it as part of extracting it to a top level class from an inner class.  Most comments and variable names still use the \"cell\" terminology.  I'm working on an algorithm that keeps track of a tree of \"nodes\" and it has gotten confusing which kind of node I'm referring to, as each Node has one cell. In maybe a week or so if there isn't discussion to the contrary, I'm going to commit a rename it back to \"Cell\".  And... while we're on this naming subject, perhaps \"SpatialPrefixTree\" could be named \"SpatialGrid\" ?  FWIW the variables referring to it are always \"grid\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575713/LUCENE-4742_Rename_spatial_Node_back_to_Cell.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4815", "change_description": ": DrillSideways now allows more than one FacetRequest per\ndimension", "change_title": "DrillSideways should allow more than one request for a single dim", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "This is a silly limitation it has today, which makes it hard if you try to build a UI showing a tree-view-like UI on a hierarchical field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12572549/LUCENE-4815.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-3918", "change_description": ": IndexSorter has been ported to 4.3 API and now supports\nsorting documents by a numeric DocValues field, or reverse the order of\nthe documents in the index. Additionally, apps can implement their own\nsort criteria.", "change_title": "Port index sorter to trunk APIs", "detail_type": "Task", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.3,6.0", "detail_description": "LUCENE-2482 added an IndexSorter to 3.x, but we need to port this functionality to 4.0 apis.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12572951/LUCENE-3918.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4817", "change_description": ": Added KeywordRepeatFilter that allows to emit a token twice\nonce as a keyword and once as an ordinary token allow stemmers to emit\na stemmed version along with the un-stemmed version.", "change_title": "Add KeywordRepeaterFilter to emit tokens twice once as keyword and once not as keyword", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3,6.0", "detail_description": "if you want to have a stemmed and an unstemmed version of a token one for recall and one for precision you have to do two fields today in most of the cases. Yet, most of the stemmers respect the keyword attribute so we could add a token filter that emits the same token twice once as keyword and once plain. Folks would most likely need to combine this RemoveDuplicatesTokenFilter but that way we can have stemmed and unstemmed version in the same field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12572763/docs.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4822", "change_description": ": PatternKeywordTokenFilter can mark tokens as keywords based\non regular expressions.", "change_title": "Add PatternKeywordTokenFilter to marks keywords based on regular expressions", "detail_type": "Improvement", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "today we need to pass in an explicit set of terms that we want to marks as keywords. It might make sense to allow patterns as well to prevent certain suffixes etc. to be keyworded.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573089/LUCENE-4822.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4821", "change_description": ": AnalyzingSuggester now uses the ending offset to\ndetermine whether the last token was finished or not, so that a\nquery \"i \" will no longer suggest \"Isla de Muerta\" for example.", "change_title": "AnalyzingSuggester should use end() offset to decide if last token ended or not", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "For example, today if you index \"i love lucy\" and \"isla de muerta\", and then you ask for suggestions for \"i\" and for \"i \" (space after the i) you'll get the same results. But if we use the ending offset, we can determine (I think?) that there were non-token characters after the last token, so that \"i \" would only suggest \"i love lucy\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12572976/LUCENE-4821.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4642", "change_description": ": Add create(AttributeFactory) to TokenizerFactory and\nsubclasses with ctors taking AttributeFactory.", "change_title": "Add create(AttributeFactory) to TokenizerFactory and subclasses with ctors taking AttributeFactory, and remove Tokenizer's and subclasses' ctors taking AttributeSource", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3,6.0", "detail_description": "All tokenizer implementations have a constructor that takes a given AttributeSource as parameter (LUCENE-1826).  These should be removed. TokenizerFactory does not provide an API to create tokenizers with a given AttributeFactory, but quite a few tokenizers have constructors that take an AttributeFactory.  TokenizerFactory should add a create(AttributeFactory) method, as should subclasses for tokenizers with AttributeFactory accepting ctors.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566666/TrieTokenizerFactory.java.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4820", "change_description": ": Add payloads to Analyzing/FuzzySuggester, to record an\narbitrary byte[] per suggestion", "change_title": "Add optional payload to AnalyzingSuggester", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "It's useful to be able to store custom data (eg maybe a primary key or a URL or something) with each suggestion, so that the UI can do things like render an image along with each suggestion, or direct to a specific URL if the user clicks that suggestion, etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573187/LUCENE-4820.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4816", "change_description": ": Add WholeBreakIterator to PostingsHighlighter\nfor treating the entire content as a single Passage.", "change_title": "PassageFormatter in PostingsHighlighter trunk the message returned", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3,6.0", "detail_description": "when I try to highlight the word zero [0]  in the file : org\\apache\\lucene\\search\\postingshighlight\\package.html the 2 last lines weren't return.  There are 4 Passages :  2-65 277-434 434-735 735-968 but the length of the file is 984. in the file : PassageFormatter.format(...) it should return all the original content with the words highlighted. PATCH need to add this at the end of the method // at line : 91 add this if(pos<content.length()){     sb.append(content.substring(pos)); } return sb.toString();", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573469/LUCENE-4816.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4827", "change_description": ": Add additional ctor to PostingsHighlighter PassageScorer\nto provide bm25 k1,b,avgdl parameters.", "change_title": "don't hardcode PostingsHighlighter scoring parameters", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Tuning these parameters can be very useful if you want to tweak how sentences are ranked (e.g. you have a strangeish corpus like wikipedia).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573744/LUCENE-4827.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4607", "change_description": ": Add DocIDSetIterator.cost() and Spans.cost() for optimizing\nscoring.", "change_title": "Add estimateDocCount to DocIdSetIterator", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.3,6.0", "detail_description": "this is essentially a spinnoff from LUCENE-4236 We currently have no way to make any decsision on how costly a DISI is neither when we apply filters nor when we build conjunctions in BQ. Yet we have most of the information already and can easily expose them via a cost API such that BS and FilteredQuery can apply optimizations on per segment basis.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573730/LUCENE-4607.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4795", "change_description": ": Add SortedSetDocValuesFacetFields and\nSortedSetDocValuesAccumulator, to compute topK facet counts from a\nfield's SortedSetDocValues.  This method only supports flat\n(dim/label) facets, is a bit (~25%) slower, has added cost\nper-IndexReader-open to compute its ordinal map, but it requires no\ntaxonomy index and it tie-breaks facet labels in an understandable\n(by Unicode sort order) way.", "change_title": "Add FacetsCollector based on SortedSetDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Recently (LUCENE-4765) we added multi-valued DocValues field (SortedSetDocValuesField), and this can be used for faceting in Solr (SOLR-4490).  I think we should also add support in the facet module? It'd be an option with different tradeoffs.  Eg, it wouldn't require the taxonomy index, since the main index handles label/ord resolving. There are at least two possible approaches: The first approach is much easier so I built a quick prototype using that.  The prototype does the counting, but it does NOT do the top K facets gathering in the end, and it doesn't \"know\" parent/child ord relationships, so there's tons more to do before this is real.  I also was unsure how to properly integrate it since the existing classes seem to expect that you use a taxonomy index to resolve ords. I ran a quick performance test.  base = trunk except I disabled the \"compute top-K\" in FacetsAccumulator to make the comparison fair; comp = using the prototype collector in the patch: I'm impressed that this approach is only ~24% slower in the worst case!  I think this means it's a good option to make available?  Yes it has downsides (NRT reopen more costly, small added RAM usage, slightly slower faceting), but it's also simpler (no taxo index to manage).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570790/pleaseBenchmarkMe.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4843", "change_description": ": Add LimitTokenPositionFilter: don't emit tokens with\npositions that exceed the configured limit.", "change_title": "LimitTokenPositionFilter: don't emit tokens with positions that exceed the configured limit", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Like LimitTokenCountFilter, except it's the token position that's limited rather than the token count.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574036/LUCENE-4843.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4832", "change_description": ": Add ToParentBlockJoinCollector.getTopGroupsWithAllChildDocs, to retrieve\nall children in each group.", "change_title": "Unbounded getTopGroups for ToParentBlockJoinCollector", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "ToParentBlockJoinCollector#getTopGroups method takes several arguments: and one of them is maxDocsPerGroup which specifies upper bound of child documents number returned within each group.  ToParentBlockJoinCollector collects and caches all child documents matched by given ToParentBlockJoinQuery in OneGroup objects during search so it is possible to create GroupDocs with all matched child documents instead of part of them bounded by maxDocsPerGroup. When you specify maxDocsPerGroup new queues(I mean TopScoreDocCollector/TopFieldCollector) will be created for each group with maxDocsPerGroup objects created within each queue which could lead to redundant memory allocation in case of child documents number within group is less than maxDocsPerGroup. I suppose that there are many cases where you need to get all child documents matched by query so it could be nice to have ability to get top groups with all matched child documents without unnecessary memory allocation. Possible solution is to pass negative maxDocsPerGroup in case when you need to get all matched child documents within each group and check maxDocsPerGroup value: if it is negative then we need to create queue with size of matched child documents number; otherwise create queue with size equals to maxDocsPerGroup.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574164/LUCENE-4832.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4846", "change_description": ": PostingsHighlighter subclasses can override where the\nString values come from (it still defaults to pulling from stored\nfields).", "change_title": "PostingsHighlighter should allow [expert] customization on how the field values are loaded", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Today it always loads from stored fields (searcher.doc), but it's sometimes useful to customize this, eg if your app separately already loads stored fields then it can avoid double-loading them.  Or if your app has some other place to pull the values from ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574210/LUCENE-4846.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4853", "change_description": ": Add PostingsHighlighter.highlightFields method that\ntakes int[] docIDs instead of TopDocs.", "change_title": "PostingsHighlighter should let you pass in docids directly [expert]", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Today it just takes TopDocs, which is very convenient, except if you are doing grouping and you have a TopGroups. I think it should let you pass in int[] docIDs ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574570/LUCENE-4853.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4856", "change_description": ": If there are no matches for a given field, return the\nfirst maxPassages sentences", "change_title": "If no Passages are found for a doc, PostingsHighlighter should return first n sentences?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574544/LUCENE-4856.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4859", "change_description": ": IndexReader now exposes Terms statistics: getDocCount,\ngetSumDocFreq, getSumTotalTermFreq.", "change_title": "Expose more stats on IndexReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "IndexReader exposes stats like totalTermFreq(term) and docFreq(term). I'd like to add more stats from Terms, e.g. getDocCount/getSumDocFreq/getSumTotalTermFreq(field) for convenience. The implementation is very easy to do on CompositeReader, so I'll add these impls to BaseCompositeReader. I'll attach a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574557/LUCENE-4859.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4862", "change_description": ": It is now possible to terminate collection of a single\nIndexReader leaf by throwing a CollectionTerminatedException in\nCollector.collect.", "change_title": "Ability to terminate queries on a per-segment basis", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3", "detail_description": "Spin-off of LUCENE-4752. The idea is to add a marker exception that tells IndexSearcher to terminate the collection of the current segment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574582/LUCENE-4862.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4752", "change_description": ": New SortingMergePolicy (in lucene/misc) that sorts documents\nbefore merging segments.", "change_title": "Merge segments to sort them", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3", "detail_description": "It would be awesome if Lucene could write the documents out in a segment based on a configurable order.  This of course applies to merging segments to. The benefit is increased locality on disk of documents that are likely to be accessed together.  This often applies to documents near each other in time, but also spatially.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575615/LUCENE-4752-2.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4860", "change_description": ": Customize scoring and formatting per-field in\nPostingsHighlighter by subclassing and overriding the getFormatter\nand/or getScorer methods.  This also changes Passage.getMatchTerms()\nto return BytesRef[] instead of Term[].", "change_title": "PostingsHighlighter should pass field name to PassageFormatter.format?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "If the app needs to render different fields (eg multi-valued vs single-valued) differently it's tricky now. You can do Passage[0].getMatchTerms()[0].field(), but then that doesn't work if that field hit the empty highlight. I think we should pass the fieldName to format directly?  And then maybe change getMatchTerms() to return BytesRef[] instead (the field name is redundant: they are all the same for all passages passed to format).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574911/LUCENE-4860.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4839", "change_description": ": Added SorterTemplate.timSort, a O(n log n) stable sort algorithm\nthat performs well on partially sorted data.", "change_title": "Sorter API: Use TimSort to sort doc IDs and postings lists", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "TimSort (http://svn.python.org/projects/python/trunk/Objects/listsort.txt, used by python and Java's Arrays.sort(Object[]) in particular) is a sorting algorithm that performs very well on partially-sorted data. Indeed, with TimSort, sorting an array which is in reverse order or a finite concatenation of sorted arrays is a linear operation (instead of O(n ln)). The sorter API could benefit from this algorithm when using Sorter.REVERSE_DOCS or merging several sorted readers for example.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574013/LUCENE-4839.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4644", "change_description": ": Added support for the \"IsWithin\" spatial predicate for\nRecursivePrefixTreeStrategy. It's for matching non-point indexed shapes; if\nyou only have points (1/doc) then \"Intersects\" is equivalent and faster.\nSee the javadocs.", "change_title": "Implement spatial WITHIN query for RecursivePrefixTree", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575331/LUCENE-4644_Spatial_Within_predicate_for_RecursivePrefixTree.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4861", "change_description": ": Make BreakIterator per-field in PostingsHighlighter. This means\nyou can override getBreakIterator(String field) to use different mechanisms\nfor e.g. title vs. body fields.", "change_title": "can we use a single PostingsHighlighter for both whole and snippet highlighting?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Right now, because we pass the BreakIterator to the ctor, you have to make two instances if you sometimes want whole and sometimes want snippets. But I think this is a fairly common use case, eg I want entire title field (with matches highlighted) but I want body field (snippets + highlights).  It would be nice to have this work with a single instance ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12576167/LUCENE-4861.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4645", "change_description": ": Added support for the \"Contains\" spatial predicate for\nRecursivePrefixTreeStrategy.", "change_title": "Implement spatial CONTAINS for RecursivePrefixTree", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12576439/LUCENE-4645__Spatial_Contains.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4898", "change_description": ": DirectoryReader.openIfChanged now allows opening a reader\non an IndexCommit starting from a near-real-time reader (previously\nthis would throw IllegalArgumentException).", "change_title": "Allow DirectoryReader.openIfChanged to an IndexCommit when reader is NRT", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "This throws an IllegalArRgumentException today, but it's easy to fix with a small change to DirectoryReader.  This is useful to do, if you have an NRT reader and want to open a reader against a previous commit point while sharing the SegmentReaders they have in common.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12576642/LUCENE-4898.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4905", "change_description": ": Made the maxPassages parameter per-field in PostingsHighlighter.", "change_title": "make maxPassages per-field in postingshighlighter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "in case you e.g. want smaller summarizes for a bunch of little fields and then a bit bigger one for the body field and so on. We can do this by only changing the 'expert' methods.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577003/LUCENE-4905.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4897", "change_description": ": Added TaxonomyReader.getChildren for traversing a category's\nchildren.", "change_title": "Add a sugar API for traversing categories.", "detail_type": "Improvement", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "Mike McCandless said in lucene-java-user mailing list. \"Maybe we could add some simple sugar APIs? Eg something like Collection<CategoryPath> getChildren(int parentOrd)?  (Or maybe it returns Iterator<CategoryPath>?)\" What about Collection<Integer> getChildren(int parentOrd)? Integer would be more versatile and can easily be converted to CategoryPath with TaxonomyReader.getPath.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12576968/LUCENE-4897.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4902", "change_description": ": Added FilterDirectoryReader to allow easy filtering of a\nDirectoryReader's subreaders.", "change_title": "Add a FilterDirectoryReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "A FilterDirectoryReader would allow you to easily wrap all subreaders of a DirectoryReader with FilterAtomicReaders.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577225/LUCENE-4902.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4858", "change_description": ": Added EarlyTerminatingSortingCollector to be used in conjunction\nwith SortingMergePolicy, which allows to early terminate queries on sorted\nindexes, when the sort order matches the index order.", "change_title": "Early termination with SortingMergePolicy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Spin-off of LUCENE-4752, see https://issues.apache.org/jira/browse/LUCENE-4752?focusedCommentId=13606565&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13606565 and https://issues.apache.org/jira/browse/LUCENE-4752?focusedCommentId=13607282&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13607282 When an index is sorted per-segment, queries that sort according to the index sort order could be early terminated.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577795/LUCENE-4858.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4904", "change_description": ": Added descending sort order to NumericDocValuesSorter.", "change_title": "Sorter API: Make NumericDocValuesSorter able to sort in reverse order", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Today it is only able to sort in ascending order.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577944/LUCENE-4904.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-3786", "change_description": ": Added SearcherTaxonomyManager, to manage access to both\nIndexSearcher and DirectoryTaxonomyReader for near-real-time\nfaceting.", "change_title": "Create SearcherTaxoManager", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "If an application wants to use an IndexSearcher and TaxonomyReader in a SearcherManager-like fashion, it cannot use a separate SearcherManager, and say a TaxonomyReaderManager, because the IndexSearcher and TaxoReader instances need to be in sync. That is, the IS-TR pair must match, or otherwise the category ordinals that are encoded in the search index might not match the ones in the taxonomy index. This can happen if someone reopens the IndexSearcher's IndexReader, but does not refresh the TaxonomyReader, and the category ordinals that exist in the reopened IndexReader are not yet visible to the TaxonomyReader instance. I'd like to create a SearcherTaxoManager (which is a ReferenceManager) which manages an IndexSearcher and TaxonomyReader pair. Then an application will call:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578011/LUCENE-3786.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4915", "change_description": ": DrillSideways now allows drilling down on fields that\nare not faceted.", "change_title": "DrillSideways should accept drill down fields that are not faceted", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Today if you drill down on a field but don't do facet counts on it, DrillSideways throws an exception. The caller can work around this: it must take all such drill-downs and manually move them into the base query as BQ MUST clauses.  But I think DrillSideways should do this. I hit this when trying to add a \"See all...\" link on a facet dim that has way too many labels to normally show: in this case I run a new search, but I facet only on this one dimension, when there could be drill-downs on other dimensions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577461/LUCENE-4915.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4895", "change_description": ": Added support for the \"IsDisjointTo\" spatial predicate for\nRecursivePrefixTreeStrategy.", "change_title": "Implement Spatial \"disjoint\" predicate.", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "The \"IsDisjointTo\" SpatialOperation is not implemented for RecursivePrefixTreeStrategy and some/all others. It has been very low priority because it is simply the inverse of \"Intersects\" which is universally implemented on the SpatialStrategy implementations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577600/LUCENE-4895_Spatial_Disjoint_predicate.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "New Features", "change_id": "LUCENE-4774", "change_description": ": Added FieldComparator that allows sorting parent documents based on\nfields on the child / nested document level.", "change_title": "Add FieldComparator that allows sorting parent docs based on field inside the child docs", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "A field comparator for sorting block join parent docs based on the a field in the associated child docs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578234/LUCENE-4774.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4839", "change_description": ": SorterTemplate.merge can now be overridden in order to replace\nthe default implementation which merges in-place by a faster implementation\nthat could require fewer swaps at the expense of some extra memory.\nArrayUtil and CollectionUtil override it so that their mergeSort and timSort\nmethods are faster but only require up to 1% of extra memory.", "change_title": "Sorter API: Use TimSort to sort doc IDs and postings lists", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "TimSort (http://svn.python.org/projects/python/trunk/Objects/listsort.txt, used by python and Java's Arrays.sort(Object[]) in particular) is a sorting algorithm that performs very well on partially-sorted data. Indeed, with TimSort, sorting an array which is in reverse order or a finite concatenation of sorted arrays is a linear operation (instead of O(n ln)). The sorter API could benefit from this algorithm when using Sorter.REVERSE_DOCS or merging several sorted readers for example.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574013/LUCENE-4839.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4571", "change_description": ": Speed up BooleanQuerys with minNrShouldMatch to use\nskipping.", "change_title": "speedup disjunction with minShouldMatch", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3,6.0", "detail_description": "even minShouldMatch is supplied to DisjunctionSumScorer it enumerates whole disjunction, and verifies minShouldMatch condition on every doc: spo proposes (as well as I get it) to pop nrMatchers-1 scorers from the heap first, and then push them back advancing behind that top doc. For me the question no.1 is there a performance test for minShouldMatch constrained disjunction.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575034/LUCENE-4571.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4863", "change_description": ": StemmerOverrideFilter now uses a FST to represent its overrides\nin memory.", "change_title": "Use FST to hold term in StemmerOverrideFilter", "detail_type": "Improvement", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "follow-up from LUCENE-4857", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575294/LUCENE-4863.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4889", "change_description": ": UnicodeUtil.codePointCount implementation replaced with a\nnon-array-lookup version.", "change_title": "UnicodeUtil.codePointCount microbenchmarks (wtf)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "This is interesting. I posted a link to a state-machine-based UTF8 parser/recognizer: http://bjoern.hoehrmann.de/utf-8/decoder/dfa/ I spent some time thinking if the lookup table could be converted into a stateless computational function, which would avoid a table lookup (which in Java will cause an additional bounds check that will be hard to eliminate I think). This didn't turn out to be easy (it boils down to finding a simple function that would map a set of integers to its concrete permutation; a generalization of minimal perfect hashing). But out of curiosity I though it'd be fun to compare how Lucene's codepoint counting compares to Java's built-in one (Decoder) and a sequence of if's. I've put together a Caliper benchmark that processes 50 million unicode codepoints; one only ASCII, one Unicode. The results are interesting. On my win/I7: Disregard the switch lookup – it's for fun only. But a sequence of if's is significantly faster than the current Lucene's table lookup, especially on ASCII input. And now compare this to Java's built-in decoder... Yes, it's the same benchmark. Wtf? I realize buffers are partially native and probably so is utf8 decoder but by so much?! Again, to put it in context: Wtf? The code is here if you want to experiment. https://github.com/dweiss/utf8dfa I realize the Java version needs to allocate a temporary space buffer but if these numbers hold for different VMs it may actually be worth it...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575694/LUCENE-4889.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4923", "change_description": ": Speed up BooleanQuerys processing of in-order disjunctions.", "change_title": "remove minShouldMatch/speed up DisjunctionSumScorer", "detail_type": "Improvement", "detail_affect_versions": "4.3", "detail_fix_versions": "4.3,6.0", "detail_description": "LUCENE-4571 added a MinShouldMatchScorer: capable of using advance() on subscorers for minShouldMatch > 1. However, we didn't yet cleanup the (now dead) minShouldMatch logic from DisjunctionSumScorer: which is now only used for pure disjunctions. This can also give some reasonable performance improvements for in-order collection.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577965/LUCENE-4923.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4926", "change_description": ": Speed up DisjunctionMatchQuery.", "change_title": "speed up disjunctionmaxscorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Applying the same approach as LUCENE-4923 gives ~ 30% improvement according to luceneutil.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578145/LUCENE-4926.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Optimizations", "change_id": "LUCENE-4930", "change_description": ": Reduce contention in older/buggy JVMs when using\nAttributeSource#addAttribute() because java.lang.ref.ReferenceQueue#poll()\nis implemented using synchronization.", "change_title": "Lucene's use of WeakHashMap at index time prevents full use of cores on some multi-core machines, due to contention", "detail_type": "Improvement", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "Our project is not optimally using full processing power during under indexing load on Lucene 4.2.0.  The reason is the AttributeSource.addAttribute() method, which goes through a WeakHashMap synchronizer, which is apparently single-threaded for a significant amount of time.  Have a look at the following trace: \"pool-1-thread-28\" prio=10 tid=0x00007f47fc104800 nid=0x672b waiting for monitor entry [0x00007f47d19ed000]    java.lang.Thread.State: BLOCKED (on object monitor)         at java.lang.ref.ReferenceQueue.poll(ReferenceQueue.java:98) We’ve had to make significant changes to the way we were indexing in order to not hit this issue as much, such as indexing using TokenStreams which we reuse, when it would have been more convenient to index with just tokens.  (The reason is that Lucene internally creates TokenStream objects when you pass a token array to IndexableField, and doesn’t reuse them, and the addAttribute() causes massive contention as a result.)  However, as you can see from the trace above, we’re still running into contention due to other addAttribute() method calls that are buried deep inside Lucene. I can see two ways forward.  Either not use WeakHashMap or use it in a more efficient way, or make darned sure no addAttribute() calls are done in the main code indexing execution path.  (I think it would be easy to fix DocInverterPerField in that way, FWIW.  I just don’t know what we’ll run into next.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578592/LUCENE-4930.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4868", "change_description": ": SumScoreFacetsAggregator used an incorrect index into\nthe scores array.", "change_title": "SumScoreFacetsAggregator incorrectly indexes into the scores array", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "It indexes into the scores array using 'doc' while it should be using a separate index. I modified the test to fail an fixed it, will commit soon.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4882", "change_description": ": FacetsAccumulator did not allow to count ROOT category (i.e.\ncount dimensions).", "change_title": "FacetsAccumulator.java:185 throws NullPointerException if it's given an empty CategoryPath.", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "When I wanted to count root categories, I used to pass \"new CategoryPath(new String[0])\" to a CountFacetRequest. Since upgrading lucene from 4.1 to 4.2, that threw ArrayIndexOfOutBoundsException, so I passed CategoryPath.EMPTY to a CountFacetRequest instead, and this time I got NullPointerException. It all originates from FacetsAccumulator.java:185 Does someone conspire to prevent others from counting root categories?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575472/LUCENE-4882.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4876", "change_description": ": IndexWriterConfig.clone() now clones its MergeScheduler,\nIndexDeletionPolicy and InfoStream in order to make an IndexWriterConfig and\nits clone fully independent.", "change_title": "IndexWriterConfig.clone should clone the MergeScheduler", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.5,6.0", "detail_description": "ConcurrentMergeScheduler has a List<MergeThread> member to track the running merging threads, so IndexWriterConfig.clone should clone the merge scheduler so that both IndexWriterConfig instances are independant.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594391/LUCENE-4876.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4893", "change_description": ": Facet counts were multiplied as many times as\nFacetsCollector.getFacetResults() is called.", "change_title": "Facet counts in FacetsAccumulator.facetArrays are multiplied as many times as FacetsCollector.getFacetResults is called.", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "In lucene 4.1, only StandardFacetsAccumulator could be instantiated. And as of lucene 4.2, it became possible to instantiate FacetsAccumulator. I invoked FacetsCollector.getFacetResults twice, and I saw doubled facet counts. If I invoke it three times, I see facet counts multiplied three times. It all happens in FacetsAccumulator.accumulate. StandardFacetsAccumulator doesn't have this bug since it frees facetArrays whenever StandardFacetsAccumulator.accumulate is called. Is it a feature or a bug?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12576300/LUCENE-4893.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4888", "change_description": ": Fixed SloppyPhraseScorer, MultiDocs(AndPositions)Enum and\nMultiSpansWrapper which happened to sometimes call DocIdSetIterator.advance\nwith target<=current (in this case the behavior of advance is undefined).", "change_title": "SloppyPhraseScorer calls DocsAndPositionsEnum.advance with target = -1", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "None", "detail_description": "SloppyPhraseScorer calls DocsAndPositionsEnum.advance with target = -1 although the behavior of this method is undefined in such cases.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12576087/LUCENE-4888.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4899", "change_description": ": FastVectorHighlighter failed with StringIndexOutOfBoundsException\nif a single highlight phrase or term was greater than the fragCharSize producing\nnegative string offsets.", "change_title": "FastVectorHighlihgter fails with SIOOB if single phrase or term is > fragCharSize", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1,4.2,3.6.2,4.2.1", "detail_fix_versions": "4.3,6.0", "detail_description": "This has been reported on several occasions like SOLR-4660 /  SOLR-4137 or on the ES mailing list https://groups.google.com/d/msg/elasticsearch/IdyMSPK5gao/nKZq8_NYWmgJ The reason is that the current code expects the fragCharSize > matchLength which is not necessarily true if you use phrases or if you have very long terms like URLs or so. I have a test that reproduces the issue and a fix as far as I can tell (me doesn't have much experience with the highlighter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577044/LUCENE-4899.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4877", "change_description": ": Throw exception for invalid arguments in analysis factories.", "change_title": "Fix analyzer factories to throw exception when arguments are invalid", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Currently if someone typos an argument \"someParamater=xyz\" instead of someParameter=xyz, they get no exception and sometimes incorrect behavior. It would be way better if these factories threw exception on unknown params, e.g. they removed the args they used and checked they were empty at the end.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575267/LUCENE-4877_one_solution_prototype.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4914", "change_description": ": SpatialPrefixTree's Node/Cell.reset() forgot to reset the 'leaf'\nflag.  It affects SpatialRecursivePrefixTreeStrategy on non-point indexed\nshapes, as of Lucene 4.2.", "change_title": "Spatial PrefixTree Node/Cell reset() doesn't reset 'leaf' flag", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "The SpatialPrefixTree Cell (formerly known as \"Node\") was refactored in LUCENE-4794 which made it into Lucene 4.2.  It introduced an explicit internal \"leaf\" flag that was formerly computed by examining shapeRel.  However, reset() doesn't reset this new flag. The bug affects SpatialRecursivePrefixTreeStrategy on non-point indexed shapes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577437/LUCENE-4914__Spatial_Cell_reset%28%29_forgot_the__leaf__flag_.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4913", "change_description": ": FacetResultNode.ordinal was always 0 when all children\nare returned.", "change_title": "FacetResultNode.ordinal is always 0 when all facets are requested", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "Silly bug ... we just fail to set the ordinal in IntFacetResultsHandler.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577432/LUCENE-4913.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4918", "change_description": ": Highlighter closes the given IndexReader if QueryScorer\nis used with an external IndexReader.", "change_title": "Highlighter closes the given IndexReader", "detail_type": "Bug", "detail_affect_versions": "4.2,4.2.1", "detail_fix_versions": "4.3,6.0", "detail_description": "If IndexReader is passed to o.a.l.s.highlight.QueryScorer for scoring, WeightedSpanTermExtractor#getWeightedSpanTermsWithScores closes the parameter reader (IndexReader) instead of closing the member variable 'reader'. To fix, line 519 of WeightedSpanTermExtractor should be changed from IOUtils.close(reader) to IOUtils.close(this.reader).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577776/LUCENE-4918.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4880", "change_description": ": Fix MemoryIndex to consume empty terms from the tokenstream consistent\nwith IndexWriter. Previously it discarded them.", "change_title": "Difference in offset handling between IndexReader created by MemoryIndex and one created by RAMDirectory", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.3,6.0", "detail_description": "MemoryIndex skips tokens that have length == 0 when building the index; the result is that it does not increment the token offset (nor does it store the position offsets if that option is set) for tokens of length == 0.  A regular index (via, say, RAMDirectory) does not appear to do this. When using the ICUFoldingFilter, it is possible to have a term of zero length (the \\u0640 character separated by spaces).  If that occurs in a document, the offsets returned at search time differ between the MemoryIndex and a regular index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12577793/LUCENE-4880.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4885", "change_description": ": FacetsAccumulator did not set the correct value for\nFacetResult.numValidDescendants.", "change_title": "FacetsAccumulator set incorrect value for FacetResult.numValidDescendants", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "This is cheap to compute, since the TopKFRH already must visit all non-zero-count ords under the FacetRequest.categoryPath. This can be useful to a front end, eg to know whether to present a \"More...\" under that dimension or not, whether to use a suggester like LinkedIn's facet UI, etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578063/LUCENE-4885.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4925", "change_description": ": Fixed IndexSearcher.search when the argument list contains a Sort\nand one of the sort fields is the relevance score. Only IndexSearchers created\nwith an ExecutorService are concerned.", "change_title": "IndexSearcher.search is broken when IndexSearcher.executor != null and the sort contains SortField.FIELD_SCORE", "detail_type": "Bug", "detail_affect_versions": "4.2.1", "detail_fix_versions": "4.3", "detail_description": "When executor != null, IndexSearcher performs two passes to compute the top docs. This doesn't work when the sort contains SortField.FIELD_SCORE because the second pass doesn't have access to scores computed in the first pass.  Since search(...) doesn't compute scores when there is a sort, they are all Float.NaN.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578076/LUCENE-4925.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4738", "change_description": ",", "change_title": "Killed JVM when first commit was running will generate a corrupted index", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.3,6.0", "detail_description": "1. Start a NEW IndexWriterBuilder on an empty folder,    add some documents to the index 2. Call commit 3. When the segments_1 file with 0 byte was created, kill the JVM We will end with a corrupted index with an empty segments_1. We only have issue with the first commit crash. Also, if you tried to open an IndexSearcher on a new index. And the first commit on the index was not finished yet. Then you will see exception like: =========================================================================== org.apache.lucene.index.IndexNotFoundException: no segments* file found in org.apache.lucene.store.MMapDirectory@C:\\tmp\\testdir lockFactory=org.apache.lucene.store.NativeFSLockFactory@6ee00df: files: [write.lock, _0.fdt, _0.fdx] \tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:741) \tat org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:52) \tat org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:65) =========================================================================== So when a new index was created, we should first create an empty index. We should not wait for the commit/close call to create the segment file. If we had an empty index there. It won't leave a corrupted index when there were a power issue on the first commit.  And a concurrent IndexSearcher can access to the index(No match is better than exception).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578065/LUCENE-4738.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-2727", "change_description": ",", "change_title": "simulate out of open files in MockDirectoryWrapper", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "currently we do disk-full tests (in writeBytes), but it would be good to have test coverage for the case where the machine runs out of open files.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12458263/LUCENE-2727.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-2812", "change_description": ",", "change_title": "IndexReader.indexExists sometimes returns true when an index isn't present", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "3.1,4.0-ALPHA", "detail_description": "If you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12466140/LUCENE-2812.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4928", "change_description": ": Stored fields and term vectors could become super slow in case\nof tiny documents (a few bytes). This is especially problematic when switching\ncodecs since bulk-merge strategies can't be applied and the same chunk of\ndocuments can end up being decompressed thousands of times. A hard limit on\nthe number of documents per chunk has been added to fix this issue.", "change_title": "Compressed stored fields: make the maximum number of docs in a chunk configurable", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3", "detail_description": "When documents are very small (a few bytes), there can be so many of them in a single chunk that merging can become very slow. Making the maximum number of documents per chunk configurable could help.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578323/LUCENE-4928.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4934", "change_description": ": Fix minor equals/hashcode problems in facet/DrillDownQuery,\nBoostingQuery, MoreLikeThisQuery, FuzzyLikeThisQuery, and block join queries.", "change_title": "AssertingIndexSearcher should do basic QueryUtils/etc checks on every query", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "We can start with QueryUtils.check(query): which does some basic hashcode/equals checks. Ideally we'd strengthen the checks as we fix problems: e.g. add explanations verifications (checkExplanations) and then finally the more intense check() that does more verifications with deleted docs/next/advance.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578756/LUCENE-4934.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4504", "change_description": ": Fix broken sort comparator in ValueSource.getSortField,\nused when sorting by a function query.", "change_title": "Empty results from IndexSearcher.searchAfter() when sorting by FunctionValues", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.3,6.0", "detail_description": "IS.searchAfter() always returns an empty result when using FunctionValues for sorting. The culprit is ValueSourceComparator.compareDocToValue() returning -1 when it should return +1.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12550908/LUCENE-4504.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4937", "change_description": ": Fix incorrect sorting of float/double values (+/-0, NaN).", "change_title": "sort order different in branch_4x than trunk", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3", "detail_description": "I will buy a beer to whoever figures out why +0 sorts before -0 in branch_4x, but works correctly in trunk", "patch_link": "https://issues.apache.org/jira/secure/attachment/12579035/LUCENE-4937.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Documentation", "change_id": "LUCENE-4841", "change_description": ": Added example SimpleSortedSetFacetsExample to show how\nto use the new SortedSetDocValues backed facet implementation.", "change_title": "Add SortedSetDocValuesFacetField example to SimpleFacetsExample", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574078/LUCENE-4841.patch", "patch_content": "none"}
{"library_version": "4.3.0", "change_type": "Build", "change_id": "LUCENE-4879", "change_description": ": Upgrade randomized testing to version 2.0.9:\nFilter stack traces on console output.", "change_title": "Filter stack traces on console output.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,6.0", "detail_description": "We could filter stack traces similar to what ANT's JUnit task does. It'd remove some of the noise and make them shorter. I don't think the lack of stack filtering is particularly annoying and it's always to have an explicit view of what and where happened but since Robert requested this I'll add it. We can always make it a (yet another) test.* option", "patch_link": "none", "patch_content": "none"}

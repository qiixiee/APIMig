{"library_version": "4.9.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5611", "change_description": ": Changing the term vector options for multiple field\ninstances by the same name in one document is not longer accepted;\nIndexWriter will now throw IllegalArgumentException.", "change_title": "Simplify the default indexing chain", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I think Lucene's current indexing chain has too many classes / hierarchy / abstractions, making it look much more complex than it really should be, and discouraging users from experimenting/innovating with their own indexing chains. Also, if it were easier to understand/approach, then new developers would more likely try to improve it ... it really should be simpler. So I'm exploring a pared back indexing chain, and have a starting patch that I think is looking ok: it seems more approachable than the current indexing chain, or at least has fewer strange classes. I also thought this could give some speedup for tiny documents (a more common use of Lucene lately), and it looks like, with the evil optimizations, this is a ~25% speedup for Geonames docs.  Even without those evil optos it's a bit faster. This is very much a work in progress / nocommits, and there are some behavior changes e.g. the new chain requires all fields to have the same TV options (rather than auto-upgrading all fields by the same name that the current chain does)...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641730/LUCENE-5611.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5646", "change_description": ": Remove rare/undertested bulk merge algorithm in\nCompressingStoredFieldsWriter.", "change_title": "stored fields bulk merging doesn't quite work right", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "from doing some profiling of merging: CompressingStoredFieldsWriter has 3 codepaths (as i see it): 1. optimized bulk copy (no deletions in chunk). In this case compressed data is copied over. 2. semi-optimized copy: in this case its optimized for an existing storedfieldswriter, and it decompresses and recompresses doc-at-a-time around any deleted docs in the chunk. 3. ordinary merging In my dataset, i only see #2 happening, never #1. The logic for determining if we can do #1 seems to be: I think the logic for \"chunkLargeEnough\" is out of sync with the MAX_DOCS_PER_CHUNK limit? e.g. instead of: it should be something like: But this only works \"at first\" then falls out of sync in my tests. Once this happens, it never reverts back to #1 algorithm and sticks with #2. So its still not quite right. Maybe jpountz knows off the top of his head...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643485/LUCENE-5646.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5610", "change_description": ": Add Terms.getMin and Terms.getMax to get the lowest and\nhighest terms, and NumericUtils.get{Min/Max}{Int/Long} to get the\nminimum numeric values from the provided Terms.", "change_title": "Add Terms min/max", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Having upper/lower bounds on terms could be useful for various optimizations in the future, e.g. to accelerate sorting (if a segment can't compete, don't even search it), and so on. Its pretty obvious how to get the smallest term, but the maximum term for a field is tricky, but worst case you can do it in ~ log(N) time by binary searching term space.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641246/LUCENE-5610.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5675", "change_description": ": Add IDVersionPostingsFormat, a postings format\noptimized for primary-key (ID) fields that also record a version\n(long) for each ID.", "change_title": "\"ID postings format\"", "detail_type": "New Feature", "detail_affect_versions": "4.9,6.0", "detail_fix_versions": "4.9,6.0", "detail_description": "Today the primary key lookup in lucene is not that great for systems like solr and elasticsearch that have versioning in front of IndexWriter. To some extend BlockTree can \"sometimes\" help avoid seeks by telling you the term does not exist for a segment. But this technique (based on FST prefix) is fragile. The only other choice today is bloom filters, which use up huge amounts of memory. I don't think we are using everything we know: particularly the version semantics. Instead, if the FST for the terms index used an algebra that represents the max version for any subtree, we might be able to answer that there is no term T with version < V in that segment very efficiently. Also ID fields dont need postings lists, they dont need stats like docfreq/totaltermfreq, etc this stuff is all implicit. As far as API, i think for users to provide \"IDs with versions\" to such a PF, a start would to set a payload or whatever on the term field to get it thru indexwriter to the codec. And a \"consumer\" of the codec can just cast the Terms to a subclass that exposes the FST to do this version check efficiently.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646364/LUCENE-5675.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5680", "change_description": ": Add ability to atomically update a set of DocValues\nfields.", "change_title": "Allow updating multiple DocValues fields atomically", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "This has come up on the list (http://markmail.org/message/2wmpvksuwc5t57pg) â€“ it would be good if we can allow updating several doc-values fields, atomically. It will also improve/simplify our tests, where today we index two fields, e.g. the field itself and a control field. In some multi-threaded tests, since we cannot be sure which updates came through first, we limit the test such that each thread updates a different set of fields, otherwise they will collide and it will be hard to verify the index in the end. I was working on a patch and it looks pretty simple to do, will post a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647110/LUCENE-5680.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5717", "change_description": ": Add support for multiterm queries nested inside\nfiltered and constant-score queries to postings highlighter.", "change_title": "Postings highlighter support for multi term queries within filtered and constant score queries", "detail_type": "Improvement", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.9,6.0", "detail_description": "The automata extraction that is done to make multi term queries work with the postings highlighter does support boolean queries but it should also support other compound queries like filtered and constant score.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647611/LUCENE-5717.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5731", "change_description": ",", "change_title": "split direct packed ints from in-ram ones", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Currently there is an oversharing problem in packedints that imposes too many requirements on improving it: None of this flexibility is needed or buys us anything, and it prevents performance improvements (e.g. i just want to add 3 bytes at the end of on-disk streams to reduce the number of bytebuffer calls and thats seriously impossible with the current situation).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648280/LUCENE-5731.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5760", "change_description": ",", "change_title": "Speed up BufferedIndexInput.randomAccessSlice", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Today this uses the default implementation, e.g. for readInt(pos): But this causes the bounds to be checked twice. Just like we did for MMap, we can provide a faster implementation that only checks once: yields ~30% speedup.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650332/LUCENE-5760.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5743", "change_description": ": Add Lucene49NormsFormat, which can compress in some cases\nsuch as very short fields.", "change_title": "new 4.9 norms format", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Norms can eat up a lot of RAM, since by default its 8 bits per field per document. We rely upon users to omit them to not blow up RAM, but its a constant trap. Previously in 4.2, I tried to compress these by default, but it was too slow. My mistakes were: Instead, we can just have a separate norms format that is very careful about what it does, since we understand in general the patterns in the data:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648814/LUCENE-5743.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5748", "change_description": ": Add SORTED_NUMERIC docvalues type, which is efficient\nfor processing numeric fields with multiple values.", "change_title": "SORTED_NUMERIC dv type", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Currently for Strings you have SORTED and SORTED_SET, capable of single and multiple values per document respectively. For multi-numerics, there are only a few choices: Both of these techniques have problems: SORTED_SET isn't bad if you just want to do basic sorting (e.g. min/max) or faceting counts: most of the bloat in the \"terms dict\" is compressed away, and it optimizes the case where the data is actually single-valued, but it falls apart performance-wise if you want to do more complex stuff like solr's analytics component or elasticsearch's aggregations: the ordinals just get in your way and cause additional work, deref'ing each to a byte[] and then decoding that back to a number. Worst of all, any mathematical calculations are off because it discards frequency (deduplicates). using your own custom encoding in BINARY removes the unnecessary ordinal dereferencing, but you trade off bad compression and access: you have no real choice but to do something like vInt within each byte[] for the doc, which means even basic sorting (e.g. max) is slow as its not constant time. There is no chance for the codec to optimize things like dates with GCD compression or optimize the single-valued case because its just an opaque byte[]. So I think it would be good to explore a simple long[] type that solves these problems.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12649994/LUCENE-5748.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "New Features", "change_id": "LUCENE-5754", "change_description": ": Allow \"$\" as part of variable and function names in\nexpressions module.", "change_title": "Allow \"$\" in expression variables", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "The current expressions modules only allows ASCII letters, ASCII digits and the underscore in variable names. It is quite common to javascript developers to use also the dollar sign as identifier part (especially at the beginning of an identifier, see the famous \"jQuery\" $-function). This would allow bindings like \"$score\". The official ECMAScript spec allows the $: http://www.ecma-international.org/publications/files/ECMA-ST-ARCH/ECMA-262,%203rd%20edition,%20December%201999.pdf (see section 7.6) The other stuff there like unicode escapes and all unicode letter crazyness should be excluded for simplicity. Adding the \"$\" is just a one-line change in the grammar and does not conflict with anything else, because \"$\" is not a special character to javascript.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650250/LUCENE-5754.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-5634", "change_description": ": Add reuse argument to IndexableField.tokenStream. This\ncan be used by custom fieldtypes, which don't use the Analyzer, but\nimplement their own TokenStream.", "change_title": "Reuse TokenStream instances in Field", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "If you don't reuse your Doc/Field instances (which is very expert: I suspect few apps do) then there's a lot of garbage created to index each StringField because we make a new StringTokenStream or NumericTokenStream (and their Attributes). We should be able to re-use these instances via a static ThreadLocal...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643080/LUCENE-5634.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-5640", "change_description": ": AttributeSource.AttributeFactory was moved to a\ntop-level class: org.apache.lucene.util.AttributeFactory", "change_title": "Cleanup & deprecate Token class / Improve default AttributeFactory to no longer use reflection", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We should remove code duplication in the Token class: This is too bugy. Most of the methods can be simply removed. This issue will also factor out the basic attributes to a separate implementation class (without the Token extra stuff). Token then just extends this class (in the tokenattributes package) and adds the additional stuff not needed for an Attribute. Token itsself will get deprecated. Also part of the slowdown in the parent issue is caused by ineffective DefaultAttributeFactory, which uses reflection to instantiate new attributes. As we are on Java 7 now, we can use MethodHandle to do this. MethodHandle does access checks only once on creating the factory or when the attribute is seen first. Later invocations are done without any result type conversions and parameter conversions as a statically linked constructor call. This greatly improves performance with Java 8, Java 7 is approx as fast, unless its completely static.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643666/LUCENE-5640-4x.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-4371", "change_description": ": Removed IndexInputSlicer and Directory.createSlicer() and replaced\nwith IndexInput.slice().", "change_title": "consider refactoring slicer to indexinput.slice", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "From LUCENE-4364: In my opinion, we should maybe check, if we can remove the whole Slicer in all Indexinputs? Just make the slice(...) method return the current BufferedIndexInput-based one. This could be another issue, once this is in.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645408/LUCENE-4371.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-5727", "change_description": ",", "change_title": "Remove IndexOutput.seek", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "This has been deprecated / unused for a few years, and several things assume append-only functionality: checksumming in the index format, HDFS integration in solr, etc. I think its time to remove it. We can continue to test 3.x codec with a simple hack in PreFlexRW.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647951/LUCENE-5727.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-5678", "change_description": ",", "change_title": "Investigate to use FileoutputStream instead of RandomAccessFile in FSIndexOutput", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We no longer allow seeking in IndexOutput, so there is no need to use RandomAccessFile. We can change this with a < 1 KiB patch. Further improvements would be to merge this with OutputStreamIndexOutput, so we get many simplifications. There is also no reason anymore to separate DataOutput from IndexOutput. The only additional thing is IndexOutput#getFilePointer(), which is handled by  an internal counter (does not use getFilePointer of the underlying RAF) and checksums.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645465/LUCENE-5678.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5756", "change_description": ": IndexWriter now implements Accountable and IW#ramSizeInBytes()\nhas been deprecated infavor of IW#ramBytesUsed()", "change_title": "Implement Accountable from IndexWriter", "detail_type": "Improvement", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.9,6.0", "detail_description": "We already expose the ram usage inside the FlushPolicy via DWFlushControl and since we now have Accountable I think IW should implement it too to get more info about it's current RAM usage.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650272/LUCENE-5756.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5725", "change_description": ": MoreLikeThis#like now accepts multiple values per field.\nThe pre-existing method has been deprecated in favor of a variable arguments\nfor the like text.", "change_title": "More Like This: necessary improvement to run mlt on multi-field values.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647922/LUCENE-5725.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5711", "change_description": ": MergePolicy accepts an IndexWriter instance\non each method rather than holding state against a single\nIndexWriter instance.", "change_title": "Pass IW to MergePolicy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Related to LUCENE-5708 we keep state in the MP holding on to the IW which prevents sharing the MP across index writers. Aside of this we should really not keep state in the MP it should really only select merges without being bound to the index writer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647132/LUCENE-5711.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5582", "change_description": ": Deprecate IndexOutput.length (just use\nIndexOutput.getFilePointer instead) and IndexOutput.setLength.", "change_title": "Remove IndexOutput.length and .setLength", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Since we removed seeking from IndexOutput, you can just use .getFilePointer() to get the length.  Also, nothing uses .setLength, so I think we should remove it too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639247/LUCENE-5582.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5621", "change_description": ": Deprecate IndexOutput.flush: this is not used by Lucene.", "change_title": "remove IndexOutput.flush()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "This method is extraneous: it just makes the API confusing. Its not actually used anywhere by Lucene, so it shouldn't be mandatory on IndexOutput. Maybe it had some use-case before things were append-only, i dont know, but now its time for it to go.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640970/LUCENE-5621.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5611", "change_description": ": Simplified Lucene's default indexing chain / APIs.\nAttributeSource/TokenStream.getAttribute now returns null if the\nattribute is not present (previously it threw\nIllegalArgumentException).  StoredFieldsWriter.startDocument no\nlonger receives the number of fields that will be added", "change_title": "Simplify the default indexing chain", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I think Lucene's current indexing chain has too many classes / hierarchy / abstractions, making it look much more complex than it really should be, and discouraging users from experimenting/innovating with their own indexing chains. Also, if it were easier to understand/approach, then new developers would more likely try to improve it ... it really should be simpler. So I'm exploring a pared back indexing chain, and have a starting patch that I think is looking ok: it seems more approachable than the current indexing chain, or at least has fewer strange classes. I also thought this could give some speedup for tiny documents (a more common use of Lucene lately), and it looks like, with the evil optimizations, this is a ~25% speedup for Geonames docs.  Even without those evil optos it's a bit faster. This is very much a work in progress / nocommits, and there are some behavior changes e.g. the new chain requires all fields to have the same TV options (rather than auto-upgrading all fields by the same name that the current chain does)...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641730/LUCENE-5611.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5632", "change_description": ": In preparation for coming Lucene versions, the Version\nenum constants were renamed to make them better readable. The constant\nfor Lucene 4.9 is now \"LUCENE_4_9\". Version.parseLeniently() is still\nable to parse the old strings (\"LUCENE_49\"). The old identifiers got\ndeprecated and will be removed in Lucene 5.0.", "change_title": "transition Version constants from LUCENE_MN to LUCENE_M_N", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We should fix this, otherwise the constants will be hard to read (e.g. Version.LUCENE_410, is it 4.1.0 or 4.10 or whatever). I do not want this to be an excuse for an arbitrary 5.0 release that does not have the features expected of a major release", "patch_link": "https://issues.apache.org/jira/secure/attachment/12642658/LUCENE-5632.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5633", "change_description": ": Change NoMergePolicy to a singleton with no distinction between\ncompound and non-compound types.", "change_title": "NoMergePolicy should have one singleton - NoMergePolicy.INSTANCE", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Currently there are two singletons available - MergePolicy.NO_COMPOUND_FILES and MergePolicy.COMPOUND_FILES and it's confusing to distinguish on compound files when the merge policy never merges segments. We should have one singleton - NoMergePolicy.INSTANCE Post to the relevant discussion - http://mail-archives.apache.org/mod_mbox/lucene-java-user/201404.mbox/%3CCAOdYfZXXyVSf9%2BxYaRhr5v2O4Mc6S2v-qWuT112_CJFYhWTPqw%40mail.gmail.com%3E", "patch_link": "https://issues.apache.org/jira/secure/attachment/12642659/LUCENE-5633.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5640", "change_description": ": The Token class was deprecated. Since Lucene 2.9, TokenStreams\nare using Attributes, Token is no longer used.", "change_title": "Cleanup & deprecate Token class / Improve default AttributeFactory to no longer use reflection", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We should remove code duplication in the Token class: This is too bugy. Most of the methods can be simply removed. This issue will also factor out the basic attributes to a separate implementation class (without the Token extra stuff). Token then just extends this class (in the tokenattributes package) and adds the additional stuff not needed for an Attribute. Token itsself will get deprecated. Also part of the slowdown in the parent issue is caused by ineffective DefaultAttributeFactory, which uses reflection to instantiate new attributes. As we are on Java 7 now, we can use MethodHandle to do this. MethodHandle does access checks only once on creating the factory or when the attribute is seen first. Later invocations are done without any result type conversions and parameter conversions as a statically linked constructor call. This greatly improves performance with Java 8, Java 7 is approx as fast, unless its completely static.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643666/LUCENE-5640-4x.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5679", "change_description": ": Consolidated IndexWriter.deleteDocuments(Term) and\nIndexWriter.deleteDocuments(Query) with their varargs counterparts.", "change_title": "Consolidate IndexWriter.deleteDocuments()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Spinoff from here: http://markmail.org/message/7kjlaizqdh7kst4d. We should consolidate the various IW.deleteDocuments().", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645758/LUCENE-5679.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5706", "change_description": ": Removed the option to unset a DocValues field through DocValues\nupdates.", "change_title": "Don't allow unsetting values through DocValues updates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Spinoff from LUCENE-5680: we shouldn't allow unsetting values through DocValues updates, at least not until there's a real usecase for it. This will simplify a lot of the internal code, as well make the numeric update API work w/ primitive long instead of Long.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646717/LUCENE-5706.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5700", "change_description": ": Added oal.util.Accountable that is now implemented by all\nclasses whose memory usage can be estimated.", "change_title": "Add 'accountable' interface for various ramBytesUsed", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Currently this is a disaster. there is ramBytesUsed(), sizeInBytes(), etc etc everywhere, with zero consistency, little javadocs, and no structure. For example, look at LUCENE-5695, where we go back and forth on how to handle \"don't know\". I don't think we should add any more of these methods to any classes in lucene until this has been cleaned up.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647103/LUCENE-5700.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5708", "change_description": ": Remove IndexWriterConfig.clone, so now IndexWriter\nsimply uses the IndexWriterConfig you pass it, and you must create a\nnew IndexWriterConfig for each IndexWriter.", "change_title": "Remove IndexWriterConfig.clone", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We originally added this clone to allow a single IWC to be re-used against more than one IndexWriter, but I think this is a mis-feature: it adds complexity to hairy classes (merge policy/scheduler, DW thread pool, etc.), I think it's buggy today. I think we should just disallow sharing: you must make a new IWC for a new IndexWriter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647551/LUCENE-5708.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5701", "change_description": ": Core closed listeners are now available in the AtomicReader API,\nthey used to sit only in SegmentReader.", "change_title": "Move core closed listeners to AtomicReader", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Core listeners are very helpful when managing per-segment caches (filters, uninverted doc values, etc.) yet this API is only exposed on SegmentReader. If you want to use it today, you need to do instanceof checks, try to unwrap in case of a FilterAtomicReader and finally fall back to a reader closed listener if every other attempt to get the underlying SegmentReader failed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646547/LUCENE-5701.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5678", "change_description": ": IndexOutput no longer allows seeking, so it is no longer required\nto use RandomAccessFile to write Indexes. Lucene now uses standard FileOutputStream\nwrapped with OutputStreamIndexOutput to write index data. BufferedIndexOutput was\nremoved, because buffering and checksumming is provided by FilterOutputStreams,\nprovided by the JDK.", "change_title": "Investigate to use FileoutputStream instead of RandomAccessFile in FSIndexOutput", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We no longer allow seeking in IndexOutput, so there is no need to use RandomAccessFile. We can change this with a < 1 KiB patch. Further improvements would be to merge this with OutputStreamIndexOutput, so we get many simplifications. There is also no reason anymore to separate DataOutput from IndexOutput. The only additional thing is IndexOutput#getFilePointer(), which is handled by  an internal counter (does not use getFilePointer of the underlying RAF) and checksums.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645465/LUCENE-5678.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5703", "change_description": ": BinaryDocValues API changed to work like TermsEnum and not allocate/\ncopy bytes on each access, you are responsible for cloning if you want to keep\ndata around.", "change_title": "Don't allocate/copy bytes all the time in binary DV producers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Our binary doc values producers keep on creating new byte[] arrays and copying bytes when a value is requested, which likely doesn't help performance. This has been done because of the way fieldcache consumers used the API, but we should try to fix it in 5.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648438/LUCENE-5703.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5695", "change_description": ": DocIdSet implements Accountable.", "change_title": "Add DocIdSet.ramBytesUsed", "detail_type": "New Feature", "detail_affect_versions": "4.9,6.0", "detail_fix_versions": "None", "detail_description": "LUCENE-5463 tried to remove calls to RamUsageEstimator.sizeOf(Object) yet it was not always possible to remove the call when there was no other API to compute the memory usage of a particular class. In particular, this is the case for CachingWrapperFilter.sizeInBytes() that needs to be able to get the memory usage of any cacheable DocIdSet instance. We could add DocIdSet.ramBytesUsed in order to remove the need for RamUsageEstimator. This will also help have bounded filter caches and take the size of the cached doc id sets into account when doing evictions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647558/LUCENE-5695.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5757", "change_description": ": Moved RamUsageEstimator's reflection-based processing to RamUsageTester\nin the test-framework module.", "change_title": "Move RAMUsageEstimator \"reflector\" to test-framework", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "This method is \"banned\" already because it can crawl slowly across large object graphs. I think it should be in the test framework. It needs enhancements for testing such as some way to filter what is crawled. Currently lots of code in lucene tries to provide accounting information but its all totally untested. We need to fix the test situation or remove Accountable and such methods completely.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650309/LUCENE-5757.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5761", "change_description": ": Removed DiskDocValuesFormat, it was very inefficient and saved very little\nRAM over the default codec.", "change_title": "Remove DiskDocValuesFormat", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I see users using this, i think they are unaware of the horrible tradeoffs it makes. We don't e.g. have codecs that have the term dictionary entirely on disk or other stupid things in lucene, so we shouldnt be stupid here either.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650482/LUCENE-5761.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "API Changes", "change_id": "LUCENE-5775", "change_description": ": Deprecate JaspellLookup.", "change_title": "JaspellTernarySearchTrie.ramBytesUsed hits StackOverflowError", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I hit this when trying to run LookupBenchmarkTest for LUCENE-5752: I think we should just remove/deprecate this suggester?  The FST based suggesters are far more RAM efficient...", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5603", "change_description": ": hunspell stemmer more efficiently strips prefixes\nand suffixes.", "change_title": "fix hunspell to use FST efficiently", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "previously this was 3 hashes (prefixes, words, suffixes) and it tried to split the words in various ways and do lookups. This was changed to FST, but the algorithm wasn't adjusted to use it properly (e.g. single pass, terminate when it reaches a \"dead end\"). this makes for slower indexing when using this stemmer...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639937/LUCENE-5603.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5599", "change_description": ": HttpReplicator did not properly delegate bulk read() to wrapped\nInputStream.", "change_title": "HttpReplicator uses a lot of CPU for large files", "detail_type": "Bug", "detail_affect_versions": "4.7.1", "detail_fix_versions": "4.8.1,4.9,6.0", "detail_description": "The method responseInputStream of HttpClientBase wraps an InputStream in order to close it when it is done reading. However, the wrapper only overwrites the single-byte read() method, every other method is delegated to its parent (java.io.InputStream). Therefore, the more efficient read-methods like read(byte[] b) are all implemented by reading one byte after the other. In my test, it took 20 minutes to copy  an index of 38 GB. With the provided small patch, this was reduced to less than 10 minutes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639761/HttpClientBase.java.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5591", "change_description": ": pass an IOContext with estimated flush size when applying DV\nupdates.", "change_title": "ReaderAndUpdates should create a proper IOContext when writing DV updates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Today we pass IOContext.DEFAULT. If DV updates are used in conjunction w/ NRTCachingDirectory, it means the latter will attempt to write the entire DV field in its RAMDirectory, which could lead to OOM. Would be good if we can build our own FlushInfo, estimating the number of bytes we're about to write. I didn't see off hand a quick way to guesstimate that - I thought to use the current DV's sizeInBytes as an approximation, but I don't see a way to get it, not a direct way at least. Maybe we can use the size of the in-memory updates to guesstimate that amount? Something like sizeOfInMemUpdates * (maxDoc/numUpdatedDocs)? Is it a too wild guess?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12642689/LUCENE-5591.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5634", "change_description": ": IndexWriter reuses TokenStream instances for String and Numeric\nfields by default.", "change_title": "Reuse TokenStream instances in Field", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "If you don't reuse your Doc/Field instances (which is very expert: I suspect few apps do) then there's a lot of garbage created to index each StringField because we make a new StringTokenStream or NumericTokenStream (and their Attributes). We should be able to re-use these instances via a static ThreadLocal...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643080/LUCENE-5634.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5638", "change_description": ",", "change_title": "Default Attributes are expensive", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Changes like LUCENE-5634 make it clear that the default AttributeFactory stuff has a very high cost: weakmaps/reflection/etc. Additionally I think clearAttributes() is more expensive than it should be: it has to traverse a linked-list, calling clear() per token. Operations like cloning (save/restoreState) have a high cost tll. Maybe we can have a better Default? In other words, rename DEFAULT_ATTRIBUTE_FACTORY to REFLECTION_ATTRIBUTE_FACTORY, and instead have a faster default factory that just has one AttributeImpl with the \"core ones\" that 95% of users are dealing with (TOKEN_ATTRIBUTE_FACTORY?): anything outside of that falls back to reflection.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643271/LUCENE-5638.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5640", "change_description": ",", "change_title": "Cleanup & deprecate Token class / Improve default AttributeFactory to no longer use reflection", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We should remove code duplication in the Token class: This is too bugy. Most of the methods can be simply removed. This issue will also factor out the basic attributes to a separate implementation class (without the Token extra stuff). Token then just extends this class (in the tokenattributes package) and adds the additional stuff not needed for an Attribute. Token itsself will get deprecated. Also part of the slowdown in the parent issue is caused by ineffective DefaultAttributeFactory, which uses reflection to instantiate new attributes. As we are on Java 7 now, we can use MethodHandle to do this. MethodHandle does access checks only once on creating the factory or when the attribute is seen first. Later invocations are done without any result type conversions and parameter conversions as a statically linked constructor call. This greatly improves performance with Java 8, Java 7 is approx as fast, unless its completely static.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12643666/LUCENE-5640-4x.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5609", "change_description": ": Changed the default NumericField precisionStep from 4\nto 8 (for int/float) and 16 (for long/double), for faster indexing\ntime and smaller indices.", "change_title": "Should we revisit the default numeric precision step?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Right now it's 4, for both 8 (long/double) and 4 byte (int/float) numeric fields, but this is a pretty big hit on indexing speed and disk usage, especially for tiny documents, because it creates many (8 or 16) terms for each value. Since we originally set these defaults, a lot has changed... e.g. we now rewrite MTQs per-segment, we have a faster (BlockTree) terms dict, a faster postings format, etc. Index size is important because it limits how much of the index will be hot (fit in the OS's IO cache).  And more apps are using Lucene for tiny docs where the overhead of individual fields is sizable. I used the Geonames corpus to run a simple benchmark (all sources are committed to luceneutil). It has 8.6 M tiny docs, each with 23 fields, with these numeric fields: I tested 4, 8 and 16 precision steps: Index time is with 1 thread (for identical index structure). The query time is time to run 100 random ranges for that field, averaged over 20 iterations.  TermCount is the total number of terms the MTQ rewrote to across all 100 queries / segments, and it gets higher as expected as precStep gets higher, but the search time is not that heavily impacted ... negligible going from 4 to 8, and then some impact from 8 to 16. Maybe we should increase the int/float default precision step to 8 and long/double to 16?  Or both to 16?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640952/LUCENE-5609.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5670", "change_description": ": Add skip/FinalOutput to FST Outputs.", "change_title": "org.apache.lucene.util.fst.FST should skip over outputs it is not interested in", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.9,6.0", "detail_description": "Currently the FST uses the read(DataInput) method from the Outputs class to skip over outputs it actually is not interested in. For most use cases this just creates some additional objects that are immediately destroyed again. When traversing an FST with non-trivial data however this can easily add up to several excess objects that nobody actually ever read.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645578/skipOutput_lucene48.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-4236", "change_description": ": Optimize BooleanQuery's in-order scoring. This speeds up\nsome types of boolean queries.", "change_title": "clean up booleanquery conjunction optimizations a bit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "After LUCENE-3505, I want to do a slight cleanup:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646035/LUCENE-4236.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5694", "change_description": ": Don't score() subscorers in DisjunctionSumScorer or\nDisjunctionMaxScorer unless score() is called.", "change_title": "Fix Disjunction*Scorer not to score unless you ask it to", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "These things scored implicitly on next()/advance() for historical reasons: but now that minShouldMatch is split apart we should remove this, its unnecessary. This allows us to remove the horrendous afterNext and clean things up, too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646284/LUCENE-5694.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5720", "change_description": ": Optimize DirectPackedReader's decompression.", "change_title": "Optimize on disk packed integers part 2", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "These are heavily optimized for the in-RAM case (for example FieldCache uses PackedInts.FAST to make it even faster so), but for the docvalues case they are not: we always essentially use COMPACT, we have only one decoder that must solve all the cases, even the complicated ones, we use BlockPackedWriter for all integers (even if they are ordinals), etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647885/LUCENE-5720.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5722", "change_description": ": Optimize ByteBufferIndexInput#seek() by specializing\nimplementations. This improves random access as used by docvalues codecs\nif used with MMapDirectory.", "change_title": "Speed up MMapDirectory.seek()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "For traditional lucene access which is mostly sequential, occasional advance(), I think this method gets drowned out in noise. But for access like docvalues, its important. Unfortunately seek() is complex today because of mapping multiple buffers. However, the very common case is that only one map is used for a given clone or slice. When there is the possibility to use only a single mapped buffer, we should instead take advantage of ByteBuffer.slice(), which will adjust the internal mmap address and remove the offset calculation. furthermore we don't need the shift/mask or even the negative check, as they are then all handled with the ByteBuffer api: seek is a one-liner (with try/catch of course to convert exceptions). This makes docvalues access 20% faster, I havent tested conjunctions or anyhting like that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647932/LUCENE-5722.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5730", "change_description": ": FSDirectory.open returns MMapDirectory for 64-bit operating\nsystems, not just Linux and Windows.", "change_title": "FSDirectory.open should return mmap for 64-bit OS X", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648068/LUCENE-5730.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5703", "change_description": ": BinaryDocValues producers don't allocate or copy bytes on\neach access anymore.", "change_title": "Don't allocate/copy bytes all the time in binary DV producers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Our binary doc values producers keep on creating new byte[] arrays and copying bytes when a value is requested, which likely doesn't help performance. This has been done because of the way fieldcache consumers used the API, but we should try to fix it in 5.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648438/LUCENE-5703.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5721", "change_description": ": Monotonic compression doesn't use zig-zag encoding anymore.", "change_title": "Monotonic packed could maybe be faster", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This compression is used in lucene for monotonically increasing offsets, e.g. stored fields index, dv BINARY/SORTED_SET offsets, OrdinalMap (used for merging and faceting dv) and so on. Today this stores a +/- deviation from an expected line of y=mx + b, where b is the minValue for the block and m is the average delta from the previous value. Because it can be negative, we have to do some additional work to zigzag-decode. Can we just instead waste a bit for every value explicitly (lower the minValue by the min delta) so that deltas are always positive and we can have a simpler decode? Maybe If we do this, the new guy should assert that values are actually monotic at write-time. The current one supports \"mostly monotic\" but do we really need that flexibility anywhere? If so it could always be kept...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648173/LUCENE-5703.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5750", "change_description": ": Speed up monotonic addressing for BINARY and SORTED_SET\ndocvalues.", "change_title": "Speed up monotonic address access in BINARY/SORTED_SET", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I found this while exploring LUCENE-5748, but it currently applies to both variable length BINARY and SORTED_SET, so I think its worth it to do here first. I think its just a holdover from before MonotonicBlockPackedWriter that to access element N we currently do: Thats because previously we didnt have packed ints that supported > Integer.MAX_VALUE elements. But thats been fixed for a long time. If we just write a 0 first and do this: The access is then much faster. For sorting i see around 20% improvement. We don't lose any compression because we should assume the delta from 0 .. 1 is similar to any other gap N .. N+1", "patch_link": "https://issues.apache.org/jira/secure/attachment/12649623/LUCENE-5750.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5751", "change_description": ": Speed up MemoryDocValues.", "change_title": "Bring MemoryDocValues up to speed", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "This one has fallen behind... It picks TABLE/GCD even when it won't actually save space or help, writes with BlockpackedWriter even when it won't save space, etc. Instead of comparing PackedInts.bitsRequired, factor in acceptableOverheadRatio too to determine \"will save space\". Check if blocking will save space along the same lines (otherwise use regular packed ints). Fix a similar bug in Lucene49 codec along these same lines (comparing PackedInts.bitsRequired instead of what would actually be written)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12649793/LUCENE-5751.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5767", "change_description": ": OrdinalMap optimizations, that mostly help on low cardinalities.", "change_title": "OrdinalMap optimizations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "OrdinalMap does its best to store a mapping from segment to global ordinals with as little memory as possible using MonotonicAppendingLongBuffer. In the low-cardinality case, there are things that could be optimized though:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650597/LUCENE-5767.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Optimizations", "change_id": "LUCENE-5769", "change_description": ": SingletonSortedSetDocValues now supports random access ordinals.", "change_title": "SingletonSortedSet should extend RandomAccessOrds", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "The point of this (exposed via DocValues#singleton) is that you can process single-valued data too, with the SortedSet api. But it doesn't support the RandomAccessOrds API... pretty silly since its either missing or has only one value.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650665/LUCENE-5769.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5738", "change_description": ": Ensure NativeFSLock prevents opening the file channel for the\nlock if the lock is already obtained by the JVM. Trying to obtain an already\nobtained lock in the same JVM can unlock the file might allow other processes\nto lock the file even without explicitly unlocking the FileLock. This behavior\nis operating system dependent.", "change_title": "NativeLock is release if Lock is closed after obtain failed", "detail_type": "Bug", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.9,6.0", "detail_description": "if you obtain the NativeFSLock and try to obtain it again in the same JVM and close if if it fails another process will be able to obtain it. This is pretty trappy though. If you execute the main class twice the problem becomes pretty obvious.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648617/LUCENE-5738.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5673", "change_description": ": MMapDirectory: Work around a \"bug\" in the JDK that throws\na confusing OutOfMemoryError wrapped inside IOException if the FileChannel\nmapping failed because of lack of virtual address space. The IOException is\nrethrown with more useful information about the problem, omitting the\nincorrect OutOfMemoryError.", "change_title": "MMapDirectory shouldn't pass along OOM wrapped as IOException", "detail_type": "Bug", "detail_affect_versions": "4.8", "detail_fix_versions": "4.9,6.0", "detail_description": "The bug here is in java (not MMapDir), but i think we shoudl do something. Users get confused when they configure their JVM to trigger something on OOM, and then see \"OutOfMemoryError: Map Failed\": but their trigger doesnt fire. Thats because in the jdk, when it maps files it catches OutOfMemoryError, asks for a garbage collection, sleeps for 100 milliseconds, then tries to map again. if it fails a second time it wraps the OOM in a generic IOException. I think we should add a try/catch to our filechannel.map", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645225/LUCENE-5673.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5682", "change_description": ": NPE in QueryRescorer when Scorer is null", "change_title": "NPE in QueryRescorer when Scorer is null", "detail_type": "Bug", "detail_affect_versions": "4.8", "detail_fix_versions": "4.9,6.0", "detail_description": "While testing out the QueryRescorer I was getting an NPE on the scorer  when using a TermQuery as the rescore query. Looks like a TermQuery will return a null Scorer if  the term is not present in the index segment. Caused by: java.lang.NullPointerException [junit4]    > \tat org.apache.lucene.search.QueryRescorer.rescore(QueryRescorer.java:89)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645586/LUCENE-5682.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5691", "change_description": ": DocTermOrds lookupTerm(BytesRef) would return incorrect results\nif the underlying TermsEnum supports ord() and the insertion point would\nbe at the end.", "change_title": "DocTermsOrds lookupTerm is wrong in some cases", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "needs the following two conditions: the fix is simple, it just needs to handle SeekStatus.END properly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645846/LUCENE-5691.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5618", "change_description": ",", "change_title": "DocValues updates send wrong fieldinfos to codec producers", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9", "detail_description": "Spinoff from LUCENE-5616. See the example there, docvalues readers get a fieldinfos, but it doesn't contain the correct ones, so they have invalid field numbers at read time. This should really be fixed. Maybe a simple solution is to not write \"batches\" of fields in updates but just have only one field per gen? This removes many-many relationships and would make things easy to understand.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645723/LUCENE-5618.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5636", "change_description": ",", "change_title": "SegmentCommitInfo continues to list unneeded gen'd files", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I thought I handled it in LUCENE-5246, but turns out I didn't handle it fully. I'll upload a patch which improves the test to expose the bug. I know where it is, but I'm not sure how to fix it without breaking index back-compat. Can we do that on experimental features? The problem is that if you update different fields in different gens, the FieldInfos files of older gens remain referenced (still!!). I open a new issue since LUCENE-5246 is already resolved and released, so don't want to mess up our JIRA... The severity of the bug is that unneeded files are still referenced in the index. Everything still works correctly, it's just that .fnm files are still there. But as I wrote, I'm still not sure how to solve it without requiring apps that use dv updates to reindex.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12642875/LUCENE-5636.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5684", "change_description": ": Make best effort to detect invalid usage of Lucene,\nwhen IndexReader is reopened after all files in its index were\nremoved and recreated by the application (the proper way to do\nthis is IndexWriter.deleteAll, or opening an IndexWriter with\nOpenMode.CREATE)", "change_title": "Add best effort detection when index is removed & recreated before openIfChanged", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I dug into the confusing NPE from the java-user email with subject \"[lucene 4.6] NPE when calling IndexReader#openIfChanged\". It happens because the app was opening an IndexReader, then rm -rf the entire index, build a new one, then calling reopen.  This is invalid usage (the app should instead use IW.deleteAll, or open new IW with OpenMode.CREATE), but I'd like to add a minor best effort check...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12645634/LUCENE-5684.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5704", "change_description": ": Fix compilation error with Java 8u20.", "change_title": "Compile error in various DocValues formats with Java 8u20", "detail_type": "Bug", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.9,6.0", "detail_description": "When compiling the codecs with Java 8 update 20 (coming soon), we fail: The constructors initialize a final closeable field inside a try block, which makes the field possibly undefined in the finally block. Previous Java versions somehow did not detect this bug, but it is a real one. Final fields must be initialized before they are used. Earlier compilers did not detect this bug, but I think it is not buggy, because IOUtils can handle null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12646670/LUCENE-5704.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5710", "change_description": ": Include the inner exception as the cause and in the\nexception message when an immense term is hit during indexing", "change_title": "DefaultIndexingChain swallows useful information from MaxBytesLengthExceededException", "detail_type": "Improvement", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.9,6.0", "detail_description": "In DefaultIndexingChain, when a MaxBytesLengthExceededException is caught, the original message is discarded, however, the message contains useful information like the size that exceeded the limit. Lucene should make this information included in the newly thrown IllegalArgumentException.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647134/LUCENE-5710.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5724", "change_description": ": CompoundFileWriter was failing to pass through the\nIOContext in some cases, causing NRTCachingDirectory to cache\ncompound files when it shouldn't, then causing OOMEs.", "change_title": "CompoundFileWriter loses the IOContext sometimes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Nightly build hit OOME with this The test was using NRTCachingDirectory, but the OOME happens because CompoundFileWriter's getOutput fails to pass down the incoming IOContext. IndexWriter has properly set up the IOContext for flush, put a huge file size in there, but by the time NRTCachingDirectory saw it, it was 0 bytes, and then many 100s of MB proceeded to be written into the RAMFile.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647815/LUCENE-5724.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5747", "change_description": ": Project-specific settings for the eclipse development\nenvironment will prevent automatic code reformatting.", "change_title": "Eclipse settings - prevent automatic code reformatting on save", "detail_type": "Bug", "detail_affect_versions": "4.8.1", "detail_fix_versions": "4.9", "detail_description": "If the user has Eclipse globally configured to automatically reformat code or reorganize imports on save, currently these actions will be performed, because the settings created by \"ant eclipse\" do not have any project-specific settings to disable it.  This will add those project-specific settings.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648880/LUCENE-5747.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5768", "change_description": ",", "change_title": "hunspell condition check is buggy", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "I added a hack for the french dictionary, but the hack isn't contained well, so it screws up condition checks with character classes like the following in english:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12650649/LUCENE-5768.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Bug fixes", "change_id": "LUCENE-5777", "change_description": ",", "change_title": "double escaping of dash in hunspell conditions", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "This bug was previously masked by LUCENE-5768. Basically '-' doesnt mean anything special (e.g. what it normally means for a regex), so we usually have to escape it. but some dictionaries like pt_PT already escape it, so now that our parsing works correctly, we are undoing that and creating a bad condition.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651352/LUCENE-5777.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Test Framework", "change_id": "LUCENE-5622", "change_description": ": Fail tests if they print over the given limit of bytes to\nSystem.out or System.err.", "change_title": "Fail tests if they print over the given limit of bytes to System.out or System.err", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "Some tests print so much stuff they are now undebuggable (see LUCENE-5612). From now on, when tests.verbose is false, the number of bytes printed to standard output and error streams will be accounted for and if it exceeds a given limit an assertion will be thrown. The limit is adjustable per-suite using Limit annotation, with the default of 8kb per suite. The check can be suppressed entirely by specifying SuppressSysoutChecks.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12642618/LUCENE-5622.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Test Framework", "change_id": "LUCENE-5619", "change_description": ": Added backwards compatibility tests to ensure we can update existing\nindexes with doc-values updates.", "change_title": "TestBackwardsCompatibility needs updatable docvalues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "We don't test this at all in TestBackCompat. this is scary!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12644614/LUCENE-5619.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Build", "change_id": "LUCENE-5442", "change_description": ": The Ant check-lib-versions target now runs Ivy resolution\ntransitively, then fails the build when it finds a version conflict: when a\ntransitive dependency's version is more recent than the direct dependency's\nversion specified in lucene/ivy-versions.properties.  Exceptions are\nspecifiable in lucene/ivy-ignore-conflicts.properties.", "change_title": "Build system should sanity check transative 3rd party dependencies", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "SOLR-5365 is an example of a bug that croped up because we upgraded a 3rd party dep (tika) w/o realizing that the version we upgraded too depended on a newer version of another 3rd party dep (commons-compress) in a comment in SOLR-5365, Jan suggested that it would be nice if there was an easy way to spot problems like this ... i asked steve about it, thinking maybe this is something the maven build could help with, and he mentioned that there is already an ant task to inspect the ivy transative deps in order to generate the maven deps and it could be used to help detect this sort of problem. opening this issue per steve's request as a reminder to look into this possibility.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12647335/LUCENE-5442.patch", "patch_content": "none"}
{"library_version": "4.9.0", "change_type": "Build", "change_id": "LUCENE-5715", "change_description": ": Upgrade direct dependencies known to be older than transitive\ndependencies: com.sun.jersey.version:1.8->1.9; com.sun.xml.bind:jaxb-impl:2.2.2->2.2.3-1;\ncommons-beanutils:commons-beanutils:1.7.0->1.8.3; commons-digester:commons-digester:2.0->2.1;\ncommons-io:commons-io:2.1->2.3; commons-logging:commons-logging:1.1.1->1.1.3;\nio.netty:netty:3.6.2.Final->3.7.0.Final; javax.activation:activation:1.1->1.1.1;\njavax.mail:mail:1.4.1->1.4.3; log4j:log4j:1.2.16->1.2.17; org.apache.avro:avro:1.7.4->1.7.5;\norg.tukaani:xz:1.2->1.4; org.xerial.snappy:snappy-java:1.0.4.1->1.0.5", "change_title": "Upgrade direct dependencies known to be older than transitive dependencies", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.9,6.0", "detail_description": "LUCENE-5442 added functionality to the check-lib-versions ant task to fail the build if a direct dependency's version conflicts with that of a transitive dependency. ivy-ignore-conflicts.properties contains a list of 19 transitive dependencies with versions that are newer than direct dependencies' versions: https://issues.apache.org/jira/browse/LUCENE-5442?focusedCommentId=14012220&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14012220 We should try to keep that list small.  It's likely that upgrading most of those dependencies will require little effort.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12648249/LUCENE-5715.patch", "patch_content": "none"}

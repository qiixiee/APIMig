{"library_version": "8.1.0", "change_type": "API Changes", "change_id": "LUCENE-3041", "change_description": ": A query introspection API has been added.  Queries should\nimplement a visit() method, taking a QueryVisitor, and either pass the\nvisitor down to any child queries, or call a visitX() or consumeX() method\non it.  All locations in the code that called Weight.extractTerms()\nhave been changed to use this API, and the extractTerms() method has\nbeen deprecated.", "change_title": "Support Query Visting / Walking", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "8.1", "detail_description": "Out of the discussion in LUCENE-2868, it could be useful to add a generic Query Visitor / Walker that could be used for more advanced rewriting, optimizations or anything that requires state to be stored as each Query is visited. We could keep the interface very simple: and then use a reflection based visitor like Earwin suggested, which would allow implementators to provide visit methods for just Querys that they are interested in.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962584/LUCENE-3041-8x.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "API Changes", "change_id": "LUCENE-8735", "change_description": ": Directory.getPendingDeletions is now abstract to ensure\nsubclasses override it. FilterDirectory now delegates the call, ensuring\ncorrect default behaviour for subclasses.", "change_title": "FileAlreadyExistsException after opening old commit", "detail_type": "Bug", "detail_affect_versions": "8.0", "detail_fix_versions": "7.7.1,7.7.2,8.0.1,8.1,9.0", "detail_description": "FilterDirectory.getPendingDeletes() does not delegate calls. This in turn means that IndexFileDeleter does not consider those as relevant files. When opening an IndexWriter for an older commit, excess files are attempted deleted. If an IndexReader exists using one of the newer commits, the excess files may fail to delete (at least on windows or when using the mocking WindowsFS). If then closing and opening the IndexWriter, the information on the pending deletes are gone if a FilterDirectory derivate is used. At the same time, the pending deletes are filtered out of listAll. This leads to a risk of hitting an existing file name, causing a FileAlreadyExistsException. This issue likely only exists on windows. Will create pull request with fix.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "New Features", "change_id": "LUCENE-2562", "change_description": ": The well-known graphical user interface for inspecting Lucene\nindexes \"Luke\" was added as a Lucene module. It can be started from the\nbinary distribution by calling the shell scripts in the module folder\nor from the source checkout by using `ant -f lucene/luke/build.xml run`.\nLuke provides a Swing-based user interface and can be used to open\nLucene or Solr (or Elasticsearch) indexes, inspect documents, check index\ncommits and segments, or test (custom) analyzers. It also has maintenance\nfunctions to check index structures and force merge indexes for archival.\nLuke was originally developed by Andrzej Bialecki, later maintained by\nDmitry Kan and finally rewritten by Tomoko Uchida to use the ASF licensing\ncompatible Swing framework (as shipped with JDKs).", "change_title": "Make Luke a Lucene/Solr Module", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "see \"RE: Luke - in need of maintainer\": http://markmail.org/message/m4gsto7giltvrpuf \"Web-based Luke\": http://markmail.org/message/4xwps7p7ifltme5q I think it would be great if there was a version of Luke that always worked with trunk - and it would also be great if it was easier to match Luke jars with Lucene versions. While I'd like to get GWT Luke into the mix as well, I think the easiest starting point is to straight port Luke to another UI toolkit before abstracting out DTO objects that both GWT Luke and Pivot Luke could share. I've started slowly converting Luke's use of thinlet to Apache Pivot. I haven't/don't have a lot of time for this at the moment, but I've plugged away here and there over the past work or two. There is still a lot to do.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12692907/LUCENE-2562-ivy.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8736", "change_description": ": LatLonShapePolygonQuery returns incorrect WITHIN results\nwith shared boundaries. Point in Polygon now correctly includes boundary\npoints. Box and Polygon relations with triangles have also been improved to\ncorrectly include boundary points.", "change_title": "LatLonShapePolygonQuery returning incorrect WITHIN results with shared boundaries", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.1", "detail_description": "Triangles that are WITHIN a target polygon query that also share a boundary with the polygon are incorrectly reported as CROSSES instead of INSIDE. This leads to incorrect WITHIN query results  as demonstrated in the following test:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12963866/adaptive-decoding.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8712", "change_description": ": Polygon2D does not detect crossings through segment edges.", "change_title": "Polygon2D does not detect crossings in some cases", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "Polygon2D does not detect crossing if the triangle crosses through points of the polygon and none of the points are inside it. For example:  nknize you might want to look at this as I am not sure what to do.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12961397/LUCENE-8712.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8720", "change_description": ": NameIntCacheLRU (in the facets module) had an int\noverflow bug that disabled cleaning of the cache", "change_title": "Integer overflow bug in NameIntCacheLRU.makeRoomLRU()", "detail_type": "Bug", "detail_affect_versions": "7.7.1", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "The NameIntCacheLRU.makeRoomLRU() method has an integer overflow bug because if maxCacheSize >= Integer.MAX_VALUE/2, 2*maxCacheSize will overflow to -(2^30) and the value of n will overflow to a negative integer as well, which will prevent any clearing of the cache whatsoever. Hence, performance will degrade once the cache becomes full because it will be impossible to remove any entries in order to add new entries to the cache. Moreover, comments in NameIntCacheLRU.java and LruTaxonomyWriterCache.java indicate that 2/3 of the cache will be cleared, whereas in fact only 1/3 of the cache is cleared. So as not to change the behavior of the NameIntCacheLRU.makeRoomLRU() method, I have not changed the code to clear 2/3 of the cache but instead I have changed the comments to indicate that 1/3 of the cache is cleared.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962014/LUCENE-NNNN.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8726", "change_description": ": ValueSource.asDoubleValuesSource() could leak a reference to\nIndexSearcher", "change_title": "WrappedDoubleValuesSource can leak IndexSearcher references", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7.2,8.1", "detail_description": "Cause of SOLR-13315 Index-specific DoubleValuesSources can be created by DoubleValuesSource.rewrite(), and the various consumers of these sources are careful not to store these rewritten sources on long-lived objects, such as queries that may be re-used between searchers.  However, the bridge code between ValueSource and DoubleValuesSource does not return a new object from its rewrite method, instead caching the passed-in IndexSearcher, which means references to this searcher may leak.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962458/LUCENE-8726.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8719", "change_description": ": FixedShingleFilter can miss shingles at the end of a token stream if\nthere are multiple paths with different lengths.", "change_title": "FixedShingleFilter can miss paths at the end of a TokenStream", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.1", "detail_description": "If we have a set of paths of differing lengths at the end of a TokenStream, FixedShingleFilter will currently stop when the first path hits the end of the stream, but this may miss extra longer paths.  For example, the stream \"a b:3/c d e f\" with a shingle size of 3 should produce paths \"a b f\", \"a c d\", \"c d e\" and \"d e f\", but at the moment FixedShingleFilter will stop when it finds that the path \"b f\" is too short to produce a shingle.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12961972/LUCENE-8719.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8688", "change_description": ": TieredMergePolicy#findForcedMerges now tries to create the\ncheapest merges that allow the index to go down to `maxSegmentCount` segments\nor less.", "change_title": "Forced merges merge more than necessary", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "A user reported some surprise after the upgrade to Lucene 7.5 due to changes to how forced merges are selected when maxSegmentCount is greater than 1. Before 7.5 forceMerge used to pick up the least amount of merging that would result in an index that has maxSegmentCount segments at most. Now that we share the same logic as regular merges, we are almost sure to pick a maxMergeAtOnceExplicit-segments merge (30 segments) given that merges that have more segments usually score better. This is due to the fact that natural merges assume that merges that run now save work for later, so the more segments get merged, the better. This assumption doesn't hold for forced merges that should run on read-only indices, so there won't be any future merging.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962381/LUCENE-8688.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8477", "change_description": ": Interval disjunctions could miss valid hits if some of the\nclauses of the disjunction are minimized away.  We now rewrite intervals\nif a source contains a disjunction and the internal gaps matter for\nmatching.  This behaviour can be disabled if users are more interested\nin speed rather than accuracy of matching.", "change_title": "Improve handling of inner disjunctions in intervals", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.1", "detail_description": "The current implementation of the disjunction interval produced by Intervals.or is a direct implementation of the OR operator from the Vigna paper.  This produces minimal intervals, meaning that (a) is preferred over (a b), and (b) also over (a b).  This has advantages when it comes to counting intervals for scoring, but also has drawbacks when it comes to matching.  For example, a phrase query for ((a OR (a b)) BLOCK (c)) will not match the document (a b c), because (a) will be preferred over (a b), and (a c) does not match. This ticket is to discuss the best way of dealing with disjunctions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12963158/LUCENE-8477.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8741", "change_description": ": ValueSource.fromDoubleValuesSource() was casting to\nScorer instead of Scorable, leading to ClassCastExceptions", "change_title": "ClassCastException in ValueSource$ScoreAndDoc", "detail_type": "Bug", "detail_affect_versions": "8.0", "detail_fix_versions": "8.1,9.0", "detail_description": "Upgrading to Apache Solr/Lucene 8.0, this popped up in our unit tests: It only happens when, edismax, we boost by geodist: http://mail-archives.apache.org/mod_mbox/lucene-java-user/201903.mbox/browser", "patch_link": "https://issues.apache.org/jira/secure/attachment/12963903/LUCENE-8741.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8754", "change_description": ": Fix ConcurrentModificationException in SegmentInfo if\nattributes are accessed in MergePolicy while the merge is running", "change_title": "SegmentInfo#toString can cause ConcurrentModificationException", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "A recent change increased the likelihood for this issue to show up but it can already happen before since we are using the attributes map in the StoredFieldsFormat for quite some time. I found this issue due to a test failure on our CI: The issue is that we update the attributes map (also we similarly do the same for diagnostics but it's not necessarily causing the issue since the diagnostics map is never modified) during the merge process but access it in the merge policy when looking at running merges and there we call toString on SegmentCommitInfo which happens without any synchronization. This is technically unsafe publication but IW is a mess along those lines and real fixes would require significant changes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8765", "change_description": ": Fixed validation of the number of added points in KD trees.", "change_title": "OneDimensionBKDWriter valueCount validation didn't include leafCount", "detail_type": "Bug", "detail_affect_versions": "7.5,9.0", "detail_fix_versions": "8.1,9.0", "detail_description": "OneDimensionBKDWriter#add checks if valueCount exceeds predefined totalPointCount, but valueCount is only updated for every 1024(DEFAULT_MAX_POINTS_IN_LEAF_NODE) points. We should include leafCount for validation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12966252/0001-Fix-OneDimensionBKDWriter-valueCount-validation-v2.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Bug fixes", "change_id": "LUCENE-8785", "change_description": ": Ensure new threadstates are locked before retrieving the number of active threadstates.\nThis causes assertion errors and potentially broken field attributes in the IndexWriter when\nIndexWriter#deleteAll is called while actively indexing.", "change_title": "TestIndexWriterDelete.testDeleteAllNoDeadlock failure", "detail_type": "Bug", "detail_affect_versions": "7.6", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "I was running Lucene's core tests on an i3.16xlarge EC2 instance (64 cores), and hit this random yet spooky failure: It does not reproduce unfortunately ... but maybe there is some subtle thread safety issue in this code ... this is a hairy part of Lucene", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8673", "change_description": ": Use radix partitioning when merging dimensional points instead\nof sorting all dimensions before hand.", "change_title": "Use radix partitioning when merging dimensional points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "Following the advise of jpountz in LUCENE-8623I have investigated using radix selection when merging segments instead of sorting the data at the beginning. The results are pretty promising when running Lucene geo benchmarks:  edited: table formatting to be a jira table In 2D the index throughput is more or less equal but for higher dimensions the impact is quite big. In all cases the merging process requires much less disk space, I am attaching plots showing the different behaviour and I am opening a pull request.   ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8687", "change_description": ": Optimise radix partitioning for points on heap.", "change_title": "Optimise radix partitioning for points on heap", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "In LUCENE-8673 it was introduced radix partitioning for merging segments. It currently works the same when you have data offline and on-heap. It makes sense when data is on-heap, not to have multiple copies but perform the partitioning always in the same object, similar to what it is done with `MutablePointValues`. This will allow to hold more points in memory as well because we don't have multiple copies of the same data as we recurse.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8699", "change_description": ": Change HeapPointWriter to use a single byte array instead to a list\nof byte arrays. In addition a new interface PointValue is added to abstract out\nthe different formats between offline and on-heap writers.", "change_title": "Use fixed byte array in HeapPointWriter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": " is always created with the same init size and max size. It might make sense to change the implementation to use a byte array instead of a growable structure as it has now. This seems to improve performance: ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8703", "change_description": ": Build point writers in the BKD tree only when they are needed.", "change_title": "Build point writers only when needed on the BKD tree", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "With the introduction of LUCENE-8699, I have realised the BKD tree uses quite a lot of heap even when it is not needed, for example for 1D points. In this issue I propose to create point writers only when needed. In addition I propose to create PointWriters based on the estimated point count given in the constructor.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12959765/LUCENE-8703.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8652", "change_description": ": SynonymQuery can now deboost the document frequency of each term when\nblending the score of the synonym.", "change_title": "Add boosting support in the SynonymQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "The SynonymQuery tries to score multiple terms as if you had indexed them as one term. This is good for \"true\" synonyms where each term should have the same contribution to the final score but this doesn't handle the case where terms have different weights. For scoring purpose it would be nice to be able to assign a boost per term that we could multiply with the term's document frequency in order to take into account the importance of the term within the synonym list.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956368/LUCENE-8652.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8631", "change_description": ": The Korean's user dictionary now picks the longest-matching word and discards\nthe other matches.", "change_title": "How Nori Tokenizer can deal with Longest-Matching", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "I think... Nori tokenizer has one issue. I don’t understand why “Longest-Matching” is NOT working to Nori tokenizer via config mode (config mode: https://www.elastic.co/guide/en/elasticsearch/plugins/6.x/analysis-nori-tokenizer.html).  Here is an example for explaining what is longest-matching. Let assume we have `userdict_ko.txt` including only three Korean single-words such as ‘골드’, ‘브라운’, ‘골드브라운’, and save it to Nori analyzer. After update, we can see that it outputs two tokens such as ‘골드’ and ‘브라운’, when the input is ‘골드브라운’. (In English: ‘골드’ means ‘gold’, ‘브라운’ means ‘brown’, and ‘골드브라운’ means ‘goldbrown’)  With this result, we recognize that “Longest-Matching” is NOT working. If “Longest-Matching” is working, the output must be ‘골드브라운’, which is the longest matching word in the user dictionary.  Curiously enough, when we add user dictionary via custom mode (custom mode: https://github.com/jimczi/nori/blob/master/how-to-custom-dict.asciidoc), we found the result is ‘골드브라운’, where ‘Longest-Matching’ is applied. We think the reason is because learned Mecab engine automatically generates word costs by its own criteria. We hope this mechanism is also applied to config mode.  Would you tell me the way to “Longest-Matching” via config mode (not custom) or give me some hints (e.g. where to modify source codes) to solve this problem?  P.S Recently, I've mailed to jim.ferenczi, who is a developer of Nori, and received his suggestions: - Add a way to set a score to each new rule (this way you could set up a negative cost for the compound word that is less than the sum of the two single words. - Same as above but the cost is computed from the statistics of the training (like the custom dictionary does when you recompile entirely). - Implement longest-match first in the dictionary.  Thanks for your support.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8732", "change_description": ": ConstantScoreQuery can now early terminate the query if the minimum score is\ngreater than the constant score and total hits are not requested.", "change_title": "Allow ConstantScoreQuery to skip counting hits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "We already have a ConstantScoreScorer that knows how to early terminate the collection but the ConstantScoreQuery uses a private scorer that doesn't take advantage of setMinCompetitiveScore. This issue is about reusing the ConstantScoreScorer in the ConstantScoreQuery in order to early terminate queries that don't need to compute the total number of hits.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12963125/LUCENE-8732.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8750", "change_description": ": Implements setMissingValue() on sort fields produced from\nDoubleValuesSource and LongValuesSource", "change_title": "Implement setMissingValue for numeric ValueSource sortFields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1", "detail_description": "We currently have setMissingValue for SortFields based on concrete numeric fields, but not for SortFields derived from LongValuesSource and DoubleValuesSource. This issue is for implementing LongValuesSource.LongValuesSortField.setMissingValue and DoubleValuesSource.DoubleValuesSortField.setMissingValue.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8701", "change_description": ": ToParentBlockJoinQuery now creates a child scorer that disallows skipping over\nnon-competitive documents if the score of a parent depends on the score of multiple\nchildren (avg, max, min). Additionally the score mode `none` that assigns a constant score to\neach parent can early terminate top scores's collection.", "change_title": "Speed up ToParentBlockJoinQuery when total hit count is not needed", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "We spotted a regression on nested queries in the Elastisearch nightly track: https://elasticsearch-benchmarks.elastic.co/index.html#tracks/nested/nightly/30d It seems related to the fact that we propagate the TOP_SCORES score mode to the child query even though we don't compute a max score in the BlockJoinScorer and don't propagate the minimum score either. Since it is not possible to compute a max score for a document that depends on other documents (the children) we should probably force the score mode to COMPLETE to build the child scorer. This should avoid the overhead of loading and reading the impacts. It should also be possible to early terminate queries that use the ScoreMode.None mode since in this case the score of each parent document is the same.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12964000/LUCENE-8701.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8751", "change_description": ": Weight#matches now use the ScorerSupplier to build scorers with a lead cost of 1\n(single document).", "change_title": "Weight#matches should use the scorerSupplier to create scorers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "The default implementation for Weight#matches creates a scorer to check if the document matches. Since this API is per document it would be more efficient to create a ScorerSupplier and then create the scorer with a leadCost of 1.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12964676/LUCENE-8751.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8752", "change_description": ": Japanese new era name 'ä»¤å' (Reiwa) is added to the dictionary used in\nJapaneseTokenizer so that the analyzer handles the era name correctly.\nReiwa is set to replace the Heisei Era on May 1, 2019.", "change_title": "Apply a patch to kuromoji dictionary to properly handle Japanese new era '令和' (REIWA)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "As of May 1st, 2019, Japanese era '元号' (Gengo) will be set to '令和' (Reiwa). See this article for more details: https://www.bbc.com/news/world-asia-47769566 Currently '令和' is splitted up to '令' and '和' by JapaneseTokenizer. It should be tokenized as one word so that Japanese texts including era names are searched as users expect. Because the default Kuromoji dictionary (mecab-ipadic) has not been maintained since 2007, a one-line patch to the source CSV file is needed for this era change. Era name is used in many official or formal documents in Japan, so it would be desirable the search systems properly handle this without adding a user dictionary or using phrase query. FYI, JDK DateTime API will support the new era (in the next updates.) https://blogs.oracle.com/java-platform-group/a-new-japanese-era-for-java The patch is available here: https://github.com/apache/lucene-solr/pull/632 ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12965805/LUCENE-8752.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8671", "change_description": ": Introduced reader attributes allows a per IndexReader configuration\nof codec internals. This enables a per reader configuration if FSTs are on- or off-heap on a\nper field basis", "change_title": "Add setting for moving FST offheap/onheap", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "While LUCENE-8635, adds support for loading FST offheap using mmap, users do not have the  flexibility to specify fields for which FST needs to be offheap. This allows users to tune heap usage as per their workload. Ideal way will be to add an attribute to FieldInfo, where we have put/getAttribute. Then FieldReader can inspect the FieldInfo and pass the appropriate On/OffHeapStore when creating its FST. It can support special keywords like ALL/NONE.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12958117/offheap_generic_settings.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Improvements", "change_id": "LUCENE-8787", "change_description": ": spatial-extras DateRangePrefixTree used to only parse ISO-8601 timestamps with 0 or 3\ndigits of milliseconds precision but now parses other lengths (although > 3 not used).", "change_title": "DateRangeField does not accept ISO 8601 date/time strings with more ore less than 3 decimal places", "detail_type": "Improvement", "detail_affect_versions": "8.0", "detail_fix_versions": "8.1", "detail_description": "Context: Solr running solr:7.7-alpine Docker image Steps to reproduce: [2019-04-21T12:34:56.100Z TO *2019-04-21T12:34:56.1Z*] Expected: Actual:  The timestamp has been created using DateTimeFormatter.ISO_DATE_TIME. I guess Solr should support Strings generated with the java datetime API.  Stacktrace: Solution: The misconception in the current code is that it parses \"milliseconds\" but it's actually the decimal places of seconds. I modified the parser to parse as many decimal places as available. The result is then normalized to milliseconds to fit the Java Calendar implementation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12967520/LUCENE-8773.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8671", "change_description": ": Load FST off-heap also for ID-like fields if reader is not opened\nfrom an IndexWriter.", "change_title": "Add setting for moving FST offheap/onheap", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "While LUCENE-8635, adds support for loading FST offheap using mmap, users do not have the  flexibility to specify fields for which FST needs to be offheap. This allows users to tune heap usage as per their workload. Ideal way will be to add an attribute to FieldInfo, where we have put/getAttribute. Then FieldReader can inspect the FieldInfo and pass the appropriate On/OffHeapStore when creating its FST. It can support special keywords like ALL/NONE.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12958117/offheap_generic_settings.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8730", "change_description": ": WordDelimiterGraphFilter always emits its original token first.  This\nbrings its behaviour into line with the deprecated WordDelimiterFilter, so that\nthe only difference in output between the two is in the position length\nattribute.", "change_title": "Ensure WordDelimiterGraphFilter always emits its original token first", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1", "detail_description": "WordDelimiterFilter and WordDelimiterGraphFilter behave almost identically outside setting position length; the only difference being that WDGF can sometimes emit its original token as the second output token rather than the first.  We should change this to conform to the behaviour of the older filter - this will make it much easier to remove WDF entirely and cut over tests that use it incidentally.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12964459/LUCENE-8730.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-7386", "change_description": ": Disjunctions nested in disjunctions are now flattened. This might\ntrigger changes in the produced scores due to changes to the order in which\nscores of sub clauses are summed up.", "change_title": "Flatten nested disjunctions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.1,9.0", "detail_description": "Now that coords are gone it became easier to flatten nested disjunctions. It might sound weird to write nested disjunctions in the first place, but disjunctions can be created implicitly by other queries such as more-like-this, LatLonPoint.newBoxQuery, non-scoring synonym queries, etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12965309/LUCENE-7386.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Other", "change_id": "LUCENE-8680", "change_description": ": Refactor EdgeTree#relateTriangle method.", "change_title": "Refactor EdgeTree#relateTriangle method", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "This proposal moves all the spatial logic for a component to Polygon2D and Line2D. It improves readability of how each object behaves.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12957605/LUCENE-8680.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Other", "change_id": "LUCENE-8685", "change_description": ": Refactor LatLonShape tests.", "change_title": "Refactor LatLonShape tests", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "The test class TestLatLonShape is becoming pretty big and it has a mixture of test. I would like to put the test that are focus on the encoding in its own test class. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12957912/LUCENE-8685.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Other", "change_id": "LUCENE-8713", "change_description": ": Add Line2D tests.", "change_title": "Add Line2D tests", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "Line2D does not have specific test. This issue will add them. Actually when developing the test, I realised that during the refactoring of this class in LUCENE-8680, a bug was introduced. The patch fixes that as well ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12960862/LUCENE-8713.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Other", "change_id": "LUCENE-8729", "change_description": ": Workaround: Disable accessibility doclints (Java 13+),\nso compilation with recent JDK succeeds.", "change_title": "Java 13: Fix Javadocs (accessibility) issues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "On Policeman Jenkins I isntalled a preview release of JDK 13. The Oracle supplied one does not yet have the issue, but nightly builds of Alexej Shipilev contain a patch that does additional check on Javadocs comments when doclint is enabled, so the next OpenJDK builds of Oracle will likely have the same issue. It fails already in \"javac\". The output is: https://jenkins.thetaphi.de/job/Lucene-Solr-8.x-Linux/275/consoleText Problem is HTML headings (like \"H1\" inside javadocs comments clashing with \"H1\" generated by Javadoc output, or \"H3\" without \"H2\"), in JDK-11 there is already a comment in the Javadocs spec (https://docs.oracle.com/en/java/javase/11/docs/specs/doc-comment-spec.html, \"When writing documentation comments for members, it is best not to use HTML heading tags such as <h1> and <h2>, because the standard doclet creates an entire structured document, and these structural tags might interfere with the formatting of the generated document.\". The error is the following: I think we should fix this and maybe don't use headings at all (as suggested in the Spec), or fix them to be at lease correct. Some hints to issues in latest JDK docs: https://bugs.openjdk.java.net/browse/JDK-8220379 Not sure about doclint in general, I'l ask on maing lists, how this affects 3rd party code!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12963018/LUCENE-8729-workaround.patch", "patch_content": "none"}
{"library_version": "8.1.0", "change_type": "Other", "change_id": "LUCENE-8725", "change_description": ": Make TermsQuery.SeekingTermSetTermsEnum a top level class and public", "change_title": "Make TermsQuery.SeekingTermSetTermsEnum public", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "8.1", "detail_description": "I have come across use-cases where directly accessing TermsQuery can help. If there is no objection I would like to make it public", "patch_link": "https://issues.apache.org/jira/secure/attachment/12965550/LUCENE-8725.patch", "patch_content": "none"}

{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9265", "change_description": ": SimpleFSDirectory is deprecated in favor of NIOFSDirectory.", "change_title": "Deprecate SimpleFSDirectory", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9304", "change_description": ": Removed ability to set DocumentsWriterPerThreadPool on IndexWriterConfig.\nThe DocumentsWriterPerThreadPool is a packaged protected final class which made it impossible\nto customize.", "change_title": "Clean up DWPTPool", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.6", "detail_fix_versions": "9.0,8.6", "detail_description": "DWPTPool currently uses an indirection called ThreadState to hold DWPT instances. This class holds several information that belongs in other places, inherits from ReentrantLock and has a mutable nature. Instead we could pool the DWPT directly and remove other indirections  inside DWPTFlushControl if we move some of the ThreadState properties to DWPT directly. The threadpool also has a problem that is grows it's ThreadStates to the number of concurrently indexing threads but never shrinks it if they are reduced. With pooling DWPT directly this limitation could be removed.  In summary, this component has seen quite some refactoring and requires some cleanups and docs changes in order to stay the test of time.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9339", "change_description": ": MergeScheduler#merge doesn't accept a parameter if a new merge was found anymore.", "change_title": "Only call MergeScheduler when we actually found new merges", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.6", "detail_fix_versions": "9.0,8.6", "detail_description": "IW#maybeMerge calls the MergeScheduler even if it didn't find any merges we should instead only do this if there is in-fact anything there to merge and safe the call into a sync'd method. In addition to this we never respect the newMergesFound parameter. It seems unnecessary and can be removed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9330", "change_description": ": SortFields are now responsible for writing themselves into index headers if they\nare used as index sorts.", "change_title": "Make SortField responsible for index sorting", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Index sorting is currently handled inside Sorter and MultiSorter, with hard-coded implementations dependent on SortField types.  This means that you can't sort by custom SortFields, and also that the logic for handling specific sort types is split between several unrelated classes. SortFields should instead be able to implement their own index sorting methods.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9340", "change_description": ": Deprecate SimpleBindings#add(SortField).", "change_title": "Deprecate and remove the SimpleBindings.add(SortField) method", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "This method is trappy, in that it only works for certain types of SortField and you only find out which at runtime.  We should deprecate it and encourage users to pass an equivalent DoubleValuesSource instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9345", "change_description": ": MergeScheduler is now decoupled from IndexWriter. Instead it accepts a MergeSource\ninterface that offers the basic methods to acquire pending merges, run the merge and do accounting\naround it.", "change_title": "Separate IndexWriter from MergeScheduler", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.6", "detail_description": "MergeScheduler is tightly coupled with IndexWriter which causes IW to expose unnecessary methods. For instance only the scheduler should call IW#getNextMerge() but it's a public method. With some refactorings we can nicely separate the two.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9349", "change_description": ": QueryVisitor.consumeTermsMatching() now takes a\nSupplier<ByteRunAutomaton> to enable queries that build large automata to\nprovide them lazily.  TermsInSetQuery switches to using this method\nto report matching terms.", "change_title": "Avoid parsing all terms in TermInSetQuery.visit()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "TermInSetQuery currently iterates through all its prefix-encoded terms in order to build an array to pass back to its visitor when visit() is called.  This seems like a waste, particularly when the visitor is not actually consuming the terms (for example, when doing a clause-count check before executing a search).  Instead TermInSetQuery should use consumeTermsMatching(), and we should change the signature of this method so that it takes a BytesRunAutomaton supplier to allow for lazy instantiation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-9366", "change_description": ": DocValues.emptySortedNumeric() no longer takes a maxDoc parameter", "change_title": "Unused maxDoc parameter in DocValues.emptySortedNumeric()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "DocValues.emptySortedNumeric() currently takes a maxDoc parameter, which is unused.  We can just remove it, which simplifies a couple of call sites and will make LUCENE-9330 a bit tidier.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "API Changes", "change_id": "LUCENE-7822", "change_description": ": CodecUtil#checkFooter(IndexInput, Throwable) now throws a\nCorruptIndexException if checksums mismatch or if checksums can't be verified.", "change_title": "IllegalArgumentException thrown instead of a CorruptIndexException", "detail_type": "Bug", "detail_affect_versions": "6.5.1", "detail_fix_versions": "8.6", "detail_description": "Similarly to LUCENE-7592 , When an *.si file is corrupted on very specific part an IllegalArgumentException is thrown instead of a CorruptIndexException. StackTrace (Lucene 6.5.1): Simple fix would be to add IllegalArgumentException to the catch list at  org/apache/lucene/index/SegmentInfos.java:289 Other variations for the stacktraces:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12868208/LUCENE-7822.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "New Features", "change_id": "LUCENE-7889", "change_description": ": Grouping by range based on values from DoubleValuesSource and LongValuesSource", "change_title": "Allow grouping on DoubleValuesSource ranges", "detail_type": "New Feature", "detail_affect_versions": "7.0", "detail_fix_versions": "8.6", "detail_description": "LUCENE-7701 made it easier to define new ways of grouping results.  This issue adds functionality to group the values of a DoubleValuesSource into a set of ranges.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12874894/LUCENE-7889.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "New Features", "change_id": "LUCENE-8962", "change_description": ": Add IndexWriter merge-on-commit feature to selectively merge small segments on commit,\nsubject to a configurable timeout, to improve search performance by reducing the number of small\nsegments for searching", "change_title": "Can we merge small segments during refresh, for faster searching?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6,8.7", "detail_description": "Two improvements were added: 8.6 has merge-on-commit (by Froh et. all), 8.7 has merge-on-refresh (by Simon).  See MergePolicy.findFullFlushMerges The original description follows: With near-real-time search we ask IndexWriter to write all in-memory segments to disk and open an IndexReader to search them, and this is typically a quick operation. However, when you use many threads for concurrent indexing, IndexWriter will accumulate write many small segments during refresh and this then adds search-time cost as searching must visit all of these tiny segments. The merge policy would normally quickly coalesce these small segments if given a little time ... so, could we somehow improve {{IndexWriter'}}s refresh to optionally kick off merge policy to merge segments below some threshold before opening the near-real-time reader?  It'd be a bit tricky because while we are waiting for merges, indexing may continue, and new segments may be flushed, but those new segments shouldn't be included in the point-in-time segments returned by refresh ... One could almost do this on top of Lucene today, with a custom merge policy, and some hackity logic to have the merge policy target small segments just written by refresh, but it's tricky to then open a near-real-time reader, excluding newly flushed but including newly merged segments since the refresh originally finished ... I'm not yet sure how best to solve this, so I wanted to open an issue for discussion!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12996016/failed-tests.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9276", "change_description": ": Use same code-path for updateDocuments and updateDocument in IndexWriter and\nDocumentsWriter.", "change_title": "Consolidate DW(PT)#updateDocument and #updateDocuments", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.5", "detail_fix_versions": "None", "detail_description": "While I was working on another IW related issue I made some changes to DW#updateDocument but forgot DW#updateDocuments which is annoying since the code is 99% identical. The same applies to DWPT#updateDocument[s]. IMO this is the wrong place to optimize in order to safe one or two object creations. Maybe we can remove this code duplication.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9279", "change_description": ": Update dictionary version for Ukrainian analyzer to 4.9.1", "change_title": "Update dictionary version for Ukrainian analyzer to 4.9.0", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Update morfologik dictionary version to 4.9.0 for Ukrainian analyzer. There's about 80k of new lemmas", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-8050", "change_description": ": PerFieldDocValuesFormat should not get the DocValuesFormat on a field that has no doc values.", "change_title": "PerFieldDocValuesFormat's merge should not grab field DVF if DocValuesType.NONE", "detail_type": "Improvement", "detail_affect_versions": "6.3", "detail_fix_versions": "8.6", "detail_description": "Since LUCENE-7456 (Lucene 6.3), PerFieldDocValuesFormat delegates the merge to the actual field DVF's merge.  Great, but unfortunately it will call getDocValuesFormatForField on all fields (in FieldInfos) even those that have no DocValues (DocValuesType.NONE).  It won't ultimately actually write anything to those DVFs but there may be some overhead and furthermore it's now more awkward to write a subclass of PFDVF that deliberately throws an exception from getDocValuesFormatForField for some fields. AFAICT this appears to be a non-issue for PerFieldPostingsFormat's merge because it's use of MultiFields filters out IndexOptions.NONE", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898032/LUCENE_8050.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9304", "change_description": ": Removed ThreadState abstraction from DocumentsWriter which allows pooling of DWPT directly and\nimproves the approachability of the IndexWriter code.", "change_title": "Clean up DWPTPool", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.6", "detail_fix_versions": "9.0,8.6", "detail_description": "DWPTPool currently uses an indirection called ThreadState to hold DWPT instances. This class holds several information that belongs in other places, inherits from ReentrantLock and has a mutable nature. Instead we could pool the DWPT directly and remove other indirections  inside DWPTFlushControl if we move some of the ThreadState properties to DWPT directly. The threadpool also has a problem that is grows it's ThreadStates to the number of concurrently indexing threads but never shrinks it if they are reduced. With pooling DWPT directly this limitation could be removed.  In summary, this component has seen quite some refactoring and requires some cleanups and docs changes in order to stay the test of time.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9324", "change_description": ": Add an ID to SegmentCommitInfo in order to compare commits for equality and make\nsnapshots incremental on generational files.", "change_title": "Give IDs to SegmentCommitInfo", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "We already have IDs in SegmentInfo, which are useful to uniquely identify segments. Having IDs on SegmentCommitInfo would be useful too in order to compare commits for equality and make snapshots incremental on generational files too.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9342", "change_description": ": TotalHits' relation will be EQUAL_TO when the number of hits is lower than TopDocsColector's numHits", "change_title": "Collector's totalHitsThreshold should not be lower than numHits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "While looking at SOLR-13289 I noticed this situation. If I create a collector with numHits greater than totalHitsThreshold, and the number of hits in the query is somewhere between those two numbers, the collector’s totalHitRelation will be TotalHits.Relation.GREATER_THAN_OR_EQUAL_TO, however the count will be accurate in this case. While this doesn't violate the current contract, the totalHitRelation could be changed to TotalHits.Relation.EQUAL_TO", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9353", "change_description": ": Metadata of the terms dictionary moved to its own file, with the\n`.tmd` extension. This allows checksums of metadata to be verified when\nopening indices and helps save seeks when opening an index.", "change_title": "Move metadata of the terms dictionary to its own file", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Currently opening a terms index requires jumping to the end of the terms index and terms dictionaries to decode some metadata such as sumTtf or file pointers where information for a given field is located. It'd be nicer to have it in a separate file, which would also have the benefit of letting us verify checksums for this part of the content.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9359", "change_description": ": SegmentInfos#readCommit now always returns a\nCorruptIndexException if the content of the file is invalid.", "change_title": "SegmentInfos.readCommit should verify checksums in case of error", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "SegmentInfos.readCommit only calls checkFooter if reading the commit succeeded. We should also call it in case of errors in order to be able to distinguish hardware errors from Lucene bugs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9393", "change_description": ": Make FunctionScoreQuery use ScoreMode.COMPLETE for creating the inner query weight when\nScoreMode.TOP_DOCS is requested.", "change_title": "FunctionScoreQuery shouldn’t use TOP_DOCS for creating inner weight", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "FunctionScoreQuery.createWeight creates the weight of the inner query using the scoreMode from it’s input parameter, however, FunctionScoreQuery can’t really use WAND algorithm, and the Scorer used will ignore calls to set competitive scores.  FunctionScoreQuery should just turn TOP_DOCS to COMPLETE before creating the inner query's weight.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9392", "change_description": ": Make FacetsConfig.DELIM_CHAR publicly accessible (Ankur Goel))", "change_title": "Change the visibility of o.a.l.f.FacetsConfig.DELIM_CHAR to public", "detail_type": "Improvement", "detail_affect_versions": "8.5.2", "detail_fix_versions": "8.6", "detail_description": "FacetsConfig.DELIM_CHAR is marked as private.  An application that wants to use this delimiter (in a unit test for example) is forced to re-declare it in the application code. This can break the application if tetshe value of DELIM_CHAR is changed in FacetsConfig", "patch_link": "https://issues.apache.org/jira/secure/attachment/13005167/LUCENE-9392.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9397", "change_description": ": UniformSplit supports encodable fields metadata.", "change_title": "UniformSplit supports encodable fields metadata", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "UniformSplit already supports custom encoding for term blocks. This is an extension to also support encodable fields metadata.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9396", "change_description": ": Improved truncation detection for points.", "change_title": "Improve truncation detection for points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "With the refactoring of LUCENE-9148, it becomes possible to improve corruption detection by serializing the length of the index and data files in the meta file instead of relying on CodecUtil#retrieveChecksum.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Improvements", "change_id": "LUCENE-9402", "change_description": ": Let MultiCollector handle minCompetitiveScore", "change_title": "Let MultiCollector Scorer handle minCompetitiveScore calls", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "See SOLR-14554. MultiCollector creates a scorer that explicitly prevents setting the minCompetitiveScore: Solr uses MultiCollector when scores are requested (to collect the max score), which means it wouldn't be able to use WAND algorithm.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-9254", "change_description": ": UniformSplit keeps FST off-heap.", "change_title": "FST off-heap support for UniformSplit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "UniformSplit should support off-heap FST the same way BlockTree postings format does, with same AUTO mode, and with same ability to force on/off heap with reader attributes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-8103", "change_description": ": DoubleValuesSource and QueryValueSource now use a TwoPhaseIterator if one is provided by the Query.", "change_title": "QueryValueSource should use TwoPhaseIterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "QueryValueSource (in \"queries\" module) is a ValueSource representation of a Query; the score is the value.  It ought to try to use a TwoPhaseIterator from the query if it can be offered. This will prevent possibly expensive advancing beyond documents that we aren't interested in.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12996434/LUCENE-8103.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-9287", "change_description": ": UsageTrackingQueryCachingPolicy no longer caches DocValuesFieldExistsQuery.", "change_title": "Never cache DocValuesFieldExistsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Currently DocValuesFieldExistsQuery is cached if there are no DV updates on the field in question. This query is pretty efficient and some experiments show that caching the result might be much more expensive than the benefits we get from caching the result. For example, indexing 10 million documents with long values and then executing this query: I wonder if we should never cache this query as it seems the overhead of caching the results is not worthy. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-9286", "change_description": ": FST.Arc.BitTable reads directly FST bytes. Arc is lightweight again and FSTEnum traversal faster.", "change_title": "FST arc.copyOf clones BitTables and this can lead to excessive memory use", "detail_type": "Bug", "detail_affect_versions": "8.5", "detail_fix_versions": "8.6", "detail_description": "I see a dramatic increase in the amount of memory required for construction of (arguably large) automata. It currently OOMs with 8GB of memory consumed for bit tables. I am pretty sure this didn't require so much memory before (the automaton is ~50MB after construction). Something bad happened in between. Thoughts, broustant, sokolov?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-7788", "change_description": ": fail precommit on unparameterised log messages and examine for wasted work/objects", "change_title": "fail precommit on unparameterised log messages and examine for wasted work/objects", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "SOLR-10415 would be removing existing unparameterised log.trace messages use and once that is in place then this ticket's one-line change would be for 'ant precommit' to reject any future unparameterised log.trace message use.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12864027/LUCENE-7788.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-9273", "change_description": ": Speed up geometry queries by specialising Component2D spatial operations. Instead of using a generic\nrelate method for all relations, we use specialize methods for each one. In addition, the type of triangle is\ncomputed at deserialization time, therefore we can be more selective when decoding points of a triangle.", "change_title": "Speed up geometry queries by specialising Component2D spatial operations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "This is a follow-up from an observation of jpountz where it notice that regardless of the spatial operation we are executing (e.g Intersects), we are always calling the method component2D#relateTriangle which it would be less expensive if we have an specialise method for intersects. The other frustrating thing is that regardless of the type of triangle we are dealing with, we are decoding all points of the triangle. In addicting most of the implementation of component2D#relateTriangle contain code that check the type of triangle to then call specialise methods. In this issue it is proposed to replace the method component2D#relateTriangle by the following methods: component2D#intersectsTriangle component2D#intersectsLine component2D#containsTriangle component2D#containsLine For consistency we add as well the methods: component2D#withinPoint component2D#withinLine Finally, the resolution of the triangle type his added to the decoding of the triangle.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-9087", "change_description": ": Build always trees with full leaves and lower the default value for maxPointsPerLeafNode to 512.", "change_title": "Should the BKD tree use a fixed maxPointsInLeafNode?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Currently the BKD tree uses a fixed maxPointsInLeafNode provided in the constructor. For the current default codec the value is set to 1024. This is a good compromise between memory usage and performance of the BKD tree. Lowering this value can increase search performance but it has a penalty in memory usage. Now that the BKD tree can be load off-heap, this can be less of a concern. Note that lowering too much that value can hurt performance as well as the tree becomes too deep and benefits are gone. For data types that use the tree as an effective R-tree (ranges and shapes datatypes) the benefits are larger as it can minimise the overlap between leaf nodes. Finally, creating too many leaf nodes can be dangerous at write time as memory usage depends on the number of leaf nodes created. The writer creates a long array of length = numberOfLeafNodes. What I am wondering here is if we can improve this situation in order to create the most efficient tree? My current ideas are: Any thoughts?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Optimizations", "change_id": "LUCENE-9148", "change_description": ": Points now write their index in a separate file.", "change_title": "Move the BKD index to its own file.", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Lucene60PointsWriter stores both inner nodes and leaf nodes in the same file, interleaved. For instance if you have two fields, you would have <leaf_nodes_A, inner_nodes_A, leaf_nodes_B, inner_nodes_B>. It's not ideal since leaves and inner nodes have quite different access patterns. Should we split this into two files? In the case when the BKD index is off-heap, this would also help force it into RAM with MMapDirectory#setPreload. Note that Lucene60PointsFormat already has a file that it calls \"index\" but it's really only about mapping fields to file pointers in the other file and not what I'm discussing here. But we could possibly store the BKD indices in this existing file if we want to avoid creating a new one.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9259", "change_description": ": Fix wrong NGramFilterFactory argument name for preserveOriginal option", "change_title": "NGramFilter use wrong argument name for preserve option", "detail_type": "Bug", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "9.0,8.6", "detail_description": "LUCENE-7960 added the possibility to preserve the original term when using NGram filters. The documentation says to enable it with 'preserveOriginal' and it works for EdgeNGram filter. But NGram filter requires the initial planned option 'keepShortTerms' to enable this feature. This inconsistency is confusing. I'll provide a patch with a possible fix.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12996003/LUCENE-9259-master.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8849", "change_description": ": DocValuesRewriteMethod.visit wasn't visiting its embedded query", "change_title": "DocValuesRewriteMethod.visit should visit the MTQ", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "The DocValuesRewriteMethod implements the QueryVisitor API (visit method) in a way that surprises me.  It does not visit the wrapped MTQ query.  Shouldn't it?  Here is what I think it should do, similar to other query wrappers: CC romseygeek", "patch_link": "https://issues.apache.org/jira/secure/attachment/12995497/LUCENE-8849.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9258", "change_description": ": DocTermsIndexDocValues assumed it was operating on a SortedDocValues (single valued) field when\nit could be multi-valued used with a SortedSetSelector", "change_title": "DocTermsIndexDocValues should not assume it's operating on a SortedDocValues field", "detail_type": "Bug", "detail_affect_versions": "7.7.2,8.4", "detail_fix_versions": "8.6", "detail_description": "When requesting a new ValueSourceScorer (with getRangeScorer) from DocTermsIndexDocValues , the latter instantiates a new iterator on SortedDocValues regardless of the fact that the underlying field can actually be of a different type (e.g. a SortedSetDocValues processed through a SortedSetSelector).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12995374/LUCENE-9258.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9164", "change_description": ": Ensure IW processes all internal events before it closes itself on a rollback.", "change_title": "Should not consider ACE a tragedy if IW is closed", "detail_type": "Bug", "detail_affect_versions": "9.0,8.5,8.4.2", "detail_fix_versions": "9.0,8.6", "detail_description": "If IndexWriter is closed or being closed, AlreadyClosedException is expected. We should not consider it a tragic event in this case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12991924/LUCENE-9164.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8908", "change_description": ": Return default value from objectVal when doc doesn't match the query in QueryValueSource", "change_title": "Specified default value not returned for query() when doc doesn't match", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "The 2 arg version of the \"query()\" was designed so that the second argument would specify the value used for any document that does not match the query pecified by the first argument – but the \"exists\" property of the resulting ValueSource only takes into consideration wether or not the document matches the query – and ignores the use of the second argument. The work around is to ignore the 2 arg form of the query() function, and instead wrap he query function in def(). for example:  def(query($something), $defaultval) instead of query($something, $defaultval)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12996883/LUCENE-8908.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9133", "change_description": ": Fix for potential NPE in TermFilteredPresearcher for empty fields", "change_title": "TermFilteredPresearcher NPE issue with empty fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Encountered a situation where a document contained a field value which analyzed away to nothing. The result was a NPE in TermFilteredPresearher#buildQuery. We filter out purely empty fields before creating the Document for  Monitor#match but it's hard to know up front when a non-empty field will produce zero tokens after analysis. Will follow up with a PR containing a simple unit test that triggers the NPE along with a proposed fix.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9309", "change_description": ": Wait for #addIndexes merges when aborting merges.", "change_title": "IW#addIndices(CodecReader) might delete files concurrently to IW#rollback", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "During work on LUCENE-9304 mikemccand ran into a failure: While this unfortunately doesn't reproduce it's likely a bug that exists for quite some time but never showed up until LUCENE-9147 which uses a temporary output. That's fine but with IW#addIndices(CodecReader...) not registering the merge it does in the IW we never wait for the merge to finish while rollback and if that merge finishes concurrently it will also remove these .tmp files. There are many ways to fix this and I can work on a patch, but hey do we really need to be able to add indices while we index and do that on an open and live IW or can it be a tool on top of it?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9337", "change_description": ": Ensure CMS updates it's thread accounting datastructures consistently.\nCMS today releases it's lock after finishing a merge before it re-acquires it to update\nthe thread accounting datastructures. This causes threading issues where concurrently\nfinishing threads fail to pick up pending merges causing potential thread starvation on\nforceMerge calls.", "change_title": "CMS might miss to pickup pending merges when maxMergeCount changes while merges are running", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "We found a test hanging on an IW#forceMerge on elastics CI on an innocent looking test: after spending quite some time trying to reproduce without any luck I tried to review all involved code again to understand possible threading issues. What I found is that if maxMergeCount gets changed on CMS while there are merges running and the forceMerge gets kicked off at the same time the running merges return we might miss to pick up the final pending merges which causes the forceMerge to hang. I was able to build a test-case that is very likely to fail on every run without the fix. While I think this is not a critical bug from how likely it is to happen in practice, if it happens it's basically a deadlock unless the IW sees any other change that kicks off a merge. Lemme walk through the issue. Lets say we have 1 pending merge and 2 merge threads running on CMS. The forceMerge is already waiting for merges to finish. Once the first merge thread finishes we try to check if we need to stall it here but since it's a merge thread we return here and don't pick up another merge here.  Now the second running merge thread checks the condition here  while the first one is finishing up. But before it can actually update the internal datastructures here it releases the CMS lock and the calculation in the stall method on how many threads are running is off causing the second thread also to step out of the maybeStall method not picking up the pending merge.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9314", "change_description": ": Single-document monitor runs were using the less efficient MultiDocumentBatch\nimplementation.", "change_title": "Lucene monitor module uses ByteBuffersDirectory rather than MemoryIndex when matching a single document", "detail_type": "Improvement", "detail_affect_versions": "8.2,8.3,8.4,8.5", "detail_fix_versions": "8.6", "detail_description": "Lucene monitor function, match single document, wraps a single document into a array of documents. Hence, it always calls the function, match many documents,  which builds a MultiDocumentBatch rather than a SingletonDocumentBatch. The former uses ByteBuffersDirectory while later uses MemoryIndex. As per documentation, MemoryIndex is a high-performance single-document main memory Apache Lucene fulltext search index. Hence, Lucene monitor should use it when matching a single document. The patch routes match single document to a SingletonDocumentBatch.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13001225/LUCENE-9314.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9362", "change_description": ": Fix equality check in ExpressionValueSource#rewrite. This fixes rewriting of inner value sources.", "change_title": "ExpressionValueSource has buggy rewrite method", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "ExpressionValuesSource does not actually rewrite itself due to small mistake in check of inner rewrites. should be changed to", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9405", "change_description": ": IndexWriter incorrectly calls closeMergeReaders twice when the merged segment is 100% deleted.", "change_title": "IndexWriter incorrectly calls closeMergeReaders twice when the merged segment is 100% deleted", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "This is the first spinoff from a controversial PR to add a new index-time feature to Lucene to merge small segments during commit.  This can substantially reduce the number of small index segments to search. See specifically this discussion there. IndexWriter seems to be missing a success = true inside mergeMiddle in the case where all segments being merged have 100% deletions and the segments will simply be dropped. In this case, in master today, I think we are incorrectly calling closeMergedReaders twice, first with suppressExceptions = false and second time with true. There is a dedicated test case here showing the issue, but that test case relies on changes in the controversial feature (added MergePolicy.findFullFlushMerges). I think it should be possible to make another test case show the bug without that controversial feature, and I am unsure why our existing randomized tests have not uncovered this yet ...", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9400", "change_description": ": Tessellator might build illegal polygons when several holes share the shame vertex.", "change_title": "Tessellator might fail when several holes share the same vertex", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "We found a new case where tessellation might fail on a valid polygon. The issue shows when a polygon has several holes sharing the same vertex. In this case the merging logic for holes might fail creating an invalid polygon. For example a polygon like this one:  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9417", "change_description": ": Tessellator might build illegal polygons when several holes share are connected to the same\nvertex.", "change_title": "Tessellator might fail when several holes share are connected to the same vertex", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Related to LUCENE-9400, tessellator might fail when several polygons are connected to the same polygon vertex, and this vertex is connected from more than one segment. For example the following polygon:    Holes get connected the following way:    Two holes are connected to the same point, and this point is shared across different segments. We need to make sure we connect in such a way that we are not crossing each other.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9418", "change_description": ": Fix ordered intervals over interleaved terms", "change_title": "Ordered intervals can give inaccurate hits on interleaved terms", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Given the text 'A B A C', an ordered interval over 'A B C' will return the inaccurate interval [2, 3], due to the way minimization is handled after matches are found.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9257", "change_description": ": Always keep FST off-heap. FSTLoadMode, Reader attributes and openedFromWriter removed.", "change_title": "FSTLoadMode should not be BlockTree specific as it is used more generally in index package", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "FSTLoadMode and its associate attribute key (static String) are currently defined in BlockTreeTermsReader, but they are actually used outside of BlockTree in the general \"index\" package. CheckIndex and ReadersAndUpdates are using these enum and attribute key to drive the FST load mode through the SegmentReader which is not specific to a postings format. They have an unnecessary dependency to BlockTreeTermsReader. We could move FSTLoadMode out of BlockTreeTermsReader, to make it a public enum of the \"index\" package. That way CheckIndex and ReadersAndUpdates do not import anymore BlockTreeTermsReader. This would also allow other postings formats to use the same enum (e.g. LUCENE-9254)", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9272", "change_description": ": Checksums of the terms index are now verified when\nLeafReader#checkIntegrity is called rather than when opening the index.", "change_title": "No longer verify checksum of the tip file when opening the terms dict", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "We currently verify the checksum of the tip file when opening the terms dictionary. This made sense when we would load the FST on-heap, but now that we load it off-heap, maybe we should verify its integrity in checkIntegrity instead, like we do for other files that we don't have to read eagerly?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9270", "change_description": ": Update Javadoc about normalizeEntry in the Kuromoji DictionaryBuilder.", "change_title": "Update Javadoc about normalizeEntry in Kuromoji DictionaryBuilder", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The normalizeEntry option is missing from the Javadoc of Kuromoji DictionaryBuilder.  Without this explanation, users don't know what it means until they see the code. Also, if user follows the usage of Javadoc, it will not be built. So the following contents need to be applied: 1) Change usage  before:    java -cp [lucene classpath] org.apache.lucene.analysis.ja.util.DictionaryBuilder \\      ${inputDir} ${outputDir} ${encoding}  after:    java -cp [lucene classpath] org.apache.lucene.analysis.ja.util.DictionaryBuilder \\      ${inputDir} ${outputDir} ${encoding} ${normalizeEntry} 2) Add description about normalizeEntry ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9275", "change_description": ": Make TestLatLonMultiPolygonShapeQueries more resilient for CONTAINS queries.", "change_title": "TestLatLonMultiPolygonShapeQueries failure", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "This test can fail for big circle queries when it goes over the pole. reproduce with:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9244", "change_description": ": Adjust TestLucene60PointsFormat#testEstimatePointCount2Dims so it does not fail when a point\nis shared by multiple leaves.", "change_title": "TestLucene60PointsFormat#testEstimatePointCount2Dims failure", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "Reproduce with: The test assumes that a point can only be shared at most by two leaves. But in the 2D case, in an extreme case, a point can actually be shared by 4 leaves. In the failure, the point is shared by 3 leaves.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9271", "change_description": ": ByteBufferIndexInput was refactored to work on top of the\nByteBuffer API.", "change_title": "Make BufferedIndexInput work on a ByteBuffer", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "Currently BufferedIndexInput works on a byte[] but its main implementation, in NIOFSDirectory, has to implement a hack to maintain a ByteBuffer view of it that it can use in calls to the FileChannel API. Maybe we should instead make BufferedIndexInput work directly on a ByteBuffer? This would also help reuse the existing ByteBuffer#get(|Short|Int|long) methods instead of duplicating them from DataInput.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9191", "change_description": ": Make LineFileDocs's random seeking more efficient, making tests using LineFileDocs faster", "change_title": "Fix linefiledocs compression or replace in tests", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "LineFileDocs(random) is very slow, even to open. It does a very slow \"random skip\" through a gzip compressed file. For the analyzers tests, in LUCENE-9186 I simply removed its usage, since TestUtil.randomAnalysisString is superior, and fast. But we should address other tests using it, since LineFileDocs(random) is slow! I think it is also the case that every lucene test has probably tested every LineFileDocs line many times now, whereas randomAnalysisString will invent new ones. Alternatively, we could \"fix\" LineFileDocs(random), e.g. special compression options (in blocks)... deflate supports such stuff. But it would make it even hairier than it is now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13002011/LUCENE-9191.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9338", "change_description": ": Refactors SimpleBindings to improve type safety and cycle detection", "change_title": "Clean up type safety in SimpleBindings", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "SimpleBindings holds its bindings as a Map<String, Object>, and then casts things when it builds its value sources.  We can instead store a map of Supplier<DoubleValuesSource> and avoid casts entirely.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9358", "change_description": ": Change the way the multi-dimensional BKD tree builder generates the intermediate tree representation to be\nequal to the one dimensional case to avoid unnecessary tree and leaves rotation.", "change_title": "BKDTree: remove unnecessary tree rotation for the one dimensional case", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "This is a spin-off of LUCENE-9807. The reason we need to rotate the one dimensional tree is that the expected representation when we pack the index is different to the tree generated by the one dimensional logic. It would be easy to harmonise how we generate this tree representation to be the same in the one dimensional case and the multi-dimensional case and therefore change the index-packing logic to work on that representation.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9288", "change_description": ": poll_mirrors.py release script can handle HTTPS mirrors.", "change_title": "poll_mirrors.py release script doesn't handle HTTPS", "detail_type": "Bug", "detail_affect_versions": "9.0,8.5", "detail_fix_versions": "8.6", "detail_description": "During the 8.5.0 release, the poll_mirrors.py script incorrectly reported that the release artifacts were not on various mirrors or on maven central, because it is configured to hit these endpoints using the `http` schema, where most of them now only accept `https`.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13000117/poll-mirrors.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9232", "change_description": ": Fix or suppress 13 resource leak precommit warnings in lucene/replicator", "change_title": "disable gradle daemon by default", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "We should disable the gradle daemon by default: the user can opt-in by changing the properties file. If you forget to do this, you end out with leaked JVMs everywhere. It won't just leak one daemon, it will leak multiple ones. I ran ps on my laptop, surprised at 13:00 to find 2 leaked gradle jvms when I hadn't used the thing since 07:00.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12993804/LUCENE-9232.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Other", "change_id": "LUCENE-9398", "change_description": ": Always keep BKD index off-heap. BKD reader does not implement Accountable any more.", "change_title": "Read BKD tree always off-heap", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "We currently only read the BKD tree index off-heap in case of MMAP. This was consistent to the way we were reading FST. In LUCENE-9257 we change it so now it is always read off-heap so we can probably do the same for the BKD index as we always read the tree sequentially. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Build", "change_id": "LUCENE-9376", "change_description": ": Fix or suppress 20 resource leak precommit warnings in lucene/search", "change_title": "Fix or suppress 20 resource leak precommit warnings in lucene/search", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "There are 20 resource leak precommit warnings in org/apache/lucene/search:", "patch_link": "https://issues.apache.org/jira/secure/attachment/13003409/LUCENE-9376.patch", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Build", "change_id": "LUCENE-9380", "change_description": ": Fix auxiliary class warnings in Lucene", "change_title": "Fix auxiliary class warnings in Lucene", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.6.0", "change_type": "Build", "change_id": "LUCENE-9389", "change_description": ": Enhance gradle logging calls validation: eliminate getMessage()", "change_title": "Enhance logging messages in Lucene's Luke module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "SOLR-14280 fixed a logging problem in SolrConfig by removing a few getMessage() calls. We could enhance this solution by modifying gradle's logging calls validation and forbid getMessage() calls during logging. We should check the existing code and eliminate such calls. It is possible to suppress the warning using //logok. erickerickson gerlowskija", "patch_link": "https://issues.apache.org/jira/secure/attachment/13005553/LUCENE-9389.patch", "patch_content": "none"}

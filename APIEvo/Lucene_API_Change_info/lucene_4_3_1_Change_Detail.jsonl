{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "SOLR-4813", "change_description": ": Fix SynonymFilterFactory to allow init parameters for\ntokenizer factory used when parsing synonyms file.", "change_title": "Unavoidable IllegalArgumentException occurs when SynonymFilterFactory's setting has tokenizer factory's parameter.", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.3.1,4.4,6.0", "detail_description": "When I write SynonymFilterFactory' setting in schema.xml as follows, ... IllegalArgumentException (\"Unknown parameters\") occurs. However TokenizerFactory's params should be set to loadTokenizerFactory method in [FST|Slow]SynonymFilterFactory. (ref. SOLR-2909) I think, the problem was caused by LUCENE-4877 (\"Fix analyzer factories to throw exception when arguments are invalid\") and SOLR-3402 (\"Parse Version outside of Analysis Factories\").", "patch_link": "https://issues.apache.org/jira/secure/attachment/12583179/SOLR-4813__4x.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4935", "change_description": ": CustomScoreQuery wrongly applied its query boost twice\n(boost^2).", "change_title": "CustomScoreQuery has broken boosting", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "CustomScoreQuery wrongly applies boost^2 instead of boost. It wrongly incorporates its boost into the normalization factor passed down to subquery (like booleanquery does) and also multiplies it directly in its scorer. The only reason the test passes today is because it compares raw score magnitudes when querynorm is on, which normalizes this away. Changing the test to use newSearcher() demonstrates the brokenness.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578833/LUCENE-4935.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4948", "change_description": ": Fixed ArrayIndexOutOfBoundsException in PostingsHighlighter\nif you had a 64-bit JVM without compressed OOPS: IBM J9, or Oracle with\nlarge heap/explicitly disabled.", "change_title": "Stink bug in PostingsHighlighter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "This test fail reproduces on IBM J9: I think it's because J9 grows arrays in a different progression than other JVMs ... we should fix PostingsHighlighter to forcefully grow the arrays to the same length instead of this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12579865/LUCENE-4948.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4953", "change_description": ": Fixed ParallelCompositeReader to inform ReaderClosedListeners of\nits synthetic subreaders. FieldCaches keyed on the atomic childs will be purged\nearlier and FC insanity prevented.  In addition, ParallelCompositeReader's\ntoString() was changed to better reflect the reader structure.", "change_title": "readerClosedListener is not invoked for ParallelCompositeReader's leaves", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "There was a test failure last night: It reproduces, and happens because ParallelCompositeReader isn't invoking the reader listeners on its .leaves() when everything is closed.  I made a separate test case to show the issue ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580829/LUCENE-4953.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4968", "change_description": ": Fixed ToParentBlockJoinQuery/Collector: correctly handle parent\nhits that had no child matches, don't throw IllegalArgumentEx when\nthe child query has no hits, more aggressively catch cases where childQuery\nincorrectly matches parent documents", "change_title": "Several ToParentBlockJoinQuery/Collector issues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "I hit several issues with ToParentBlockJoinQuery/Collector:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581062/LUCENE-4968.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4970", "change_description": ": Fix boost value of rewritten NGramPhraseQuery.", "change_title": "NGramPhraseQuery is not boosted.", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.3.1", "detail_description": "If I apply setBoost() method to NGramPhraseQuery, Score will not change. I think, setBoost() is forgatten after optimized in rewrite() method.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581349/LUCENE-4970.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4974", "change_description": ": CommitIndexTask was broken if no params were set.", "change_title": "CommitIndexTask is broken if no params are set", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "If you put a CommitIndex in a benchmark algorithm with no params, you get NPE from IW.setCommitData, because you are not allowed to pass null. It's a trivial fix - CommitIndexTask should call setCommitData only if commitData is not null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581481/LUCENE-4974.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4986", "change_description": ": Fixed case where a newly opened near-real-time reader\nfails to reflect a delete from IndexWriter.tryDeleteDocument", "change_title": "NRT reader doesn't see changes after successful IW.tryDeleteDocument", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "Reported by Reg on the java-user list, subject \"TrackingIndexWriter.tryDeleteDocument(IndexReader, int) vs deleteDocuments(Query)\": When IW.tryDeleteDocument succeeds, it marks the document as deleted in the pending BitVector in ReadersAndLiveDocs, but then when the NRT reader checks if it's still current by calling IW.nrtIsCurrent, we fail to catch changes to the BitVector, resulting in the NRT reader thinking it's current and not reopening.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582319/LUCENE-4986.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4991", "change_description": ": Fix handling of synonyms in classic QueryParser.getFieldQuery for\nterms not separated by whitespace. PositionIncrementAttribute was ignored, so with\ndefault AND synonyms wrongly became mandatory clauses, and with OR, the\ncoordination factor was wrong.", "change_title": "QueryParser doesnt handle synonyms correctly for chinese", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "As reported multiple times on the user list: http://find.searchhub.org/document/eaf0e88a6a0d4d1f http://find.searchhub.org/document/abf28043c52b6efc http://find.searchhub.org/document/1313794632c90826 The logic here is not forming the right query structures and ignoring positionIncrementAttribute from the tokenStream. This also screws up scoring and queries for decompounding too (because they go thru this exact situation if they add the original compound as a synonym).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582484/LUCENE-4991.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4994", "change_description": ": Fix PatternKeywordMarkerFilter to have public constructor.", "change_title": "PatternKeywordMarkerFilter is final and has protected ctor and cannot be instantiated by non-Lucene code", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.3.1,6.0", "detail_description": "I tried to write a test for LUCENE-4993 but recognized that a copy'n'paste error made the ctor of this filter protected. The sister SetKeywordMarkerFilter has public ctor.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4993", "change_description": ": Fix BeiderMorseFilter to preserve custom attributes when\ninserting tokens with position increment 0.", "change_title": "BeiderMorseFilter inserts tokens with positionIncrement=0, but ignores all custom attributes except OffsetAttribute", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.3.1,6.0", "detail_description": "BeiderMorseFilter inserts sometimes additional phonetic tokens for the same source token. Currently it calls clearAttributes before doing this and sets the new token's term, positionIncrement=0 and the original offset. This leads to problems if the TokenStream contains other attributes inserted before (like KeywordAttribute, FlagsAttribute,...). Those are all reverted to defaults for the inserted tokens. The TokenFilter should remove the special case done for preserving offsets and instead to captureState() and restoreState().", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582540/LUCENE-4993.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Bug Fixes", "change_id": "LUCENE-5002", "change_description": ": IndexWriter#deleteAll() caused a deadlock in DWPT / DWSC if a\nDwPT was flushing concurrently while deleteAll() aborted all DWPT. The IW\nshould never wait on DWPT via the flush control while holding on to the IW\nLock.", "change_title": "Deadlock in DocumentsWriterFlushControl", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,4.3.1,6.0", "detail_description": "Hi all, We have an obvious deadlock between a \"MaybeRefreshIndexJob\" thread calling ReferenceManager.maybeRefresh(ReferenceManager.java:204) and a \"RebuildIndexJob\" thread calling IndexWriter.deleteAll(IndexWriter.java:2065). Lucene wants to flush in the \"MaybeRefreshIndexJob\" thread trying to intrinsically lock the IndexWriter instance at DocumentsWriterPerThread.java:563 before notifyAll()ing the flush. Simultaneously the \"RebuildIndexJob\" thread who already intrinsically locked the IndexWriter instance at IndexWriter#deleteAll wait()s at DocumentsWriterFlushControl.java:245 for the flush forever causing a deadlock.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12583645/LUCENE-5002.patch", "patch_content": "none"}
{"library_version": "4.3.1", "change_type": "Optimizations", "change_id": "LUCENE-4938", "change_description": ": Don't use an unnecessarily large priority queue in IndexSearcher\nmethods that take top-N.", "change_title": "IndexSearcher.search() with sort doesnt do min(maxdoc, n)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3.1,6.0", "detail_description": "It does this without a sort though. This caused TestFunctionQuerySort.testSearchAfterWhenSortingByFunctionValues to OOM (why only sometimes?)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12579550/LUCENE-4938.patch", "patch_content": "none"}

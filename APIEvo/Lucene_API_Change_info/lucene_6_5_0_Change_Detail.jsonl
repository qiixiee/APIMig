{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7740", "change_description": ": Refactor Range Fields to remove Field suffix (e.g., DoubleRange),\nmove InetAddressRange and InetAddressPoint from sandbox to misc module, and\nrefactor all other range fields from sandbox to core.", "change_title": "Refactor Range Fields and graduate from sandbox", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This task refactors Range fields as follows: 1. Remove the Field suffix to make them more consistent with their Point counterpart.  2. Graduate InetAddressRange from sandbox to misc 3. Graduate all other *Range classes (e.g. DoubleRange) from sandbox to core", "patch_link": "https://issues.apache.org/jira/secure/attachment/12857530/LUCENE-7740.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7624", "change_description": ": TermsQuery has been renamed as TermInSetQuery and moved to core.", "change_title": "Consider moving TermsQuery to core", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "TermsQuery current sits in the queries module, but it's used in both spatial-extras and in facets, and currently is the only reason that the facets module has a dependency on queries.  I think it's a generally useful query, and would fit in perfectly well in core. This would also allow us to explore rewriting BooleanQuery to TermsQuery to avoid the max-clauses limit", "patch_link": "https://issues.apache.org/jira/secure/attachment/12846330/LUCENE-7624.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7637", "change_description": ": TermInSetQuery requires that all terms come from the same field.", "change_title": "TermInSetQuery should require that all terms come from the same field", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Spin-off from LUCENE-7624. Requiring that all terms are in the same field would make things simpler and more consistent with other queries. It might also make it easier to improve this query in the future since other similar queries like AutomatonQuery also work on the per-field basis. The only downside is that querying terms across multiple fields would be less efficient, but this does not seem to be a common use-case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12847631/LUCENE-7637.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7644", "change_description": ": FieldComparatorSource.newComparator() and\nSortField.getComparator() no longer throw IOException", "change_title": "FieldComparatorSource.newComparator() doesn't need to throw IOException", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5", "detail_description": "All IO operations on FieldComparator are done in .getLeafComparator(), so there's no need to declare IOException on their constructors. This bubbles back up and lets us remove throws clauses from a number of constructors, TopDocs.merge(), and so on.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12848099/LUCENE-7644.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7643", "change_description": ": Replaced doc-values queries in lucene/sandbox with factory\nmethods on the *DocValuesField classes.", "change_title": "Move IndexOrDocValuesQuery to queries (or core?)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "I was just doing some benchmarking to check that IndexOrDocValues actually makes things faster when it is supposed to: In the above results, the number after the query type indicates the percentage of docs in the index that it matches. With the baseline, range queries are simple point range queries, while the patch is an IndexOrDocValuesQuery that wraps both a point range query and a doc values query that matches the same documents. As expected, AndTerm35Range10 performs the same in both cases since the range is supposed to lead the iteration, so the IndexOrDocValuesQuery is rewritten to the wrapped point range query. However with AndTerm02Range25 the range cost is higher than the term cost so the range is only used for verifying matches and the IndexOrDocValuesQuery rewrites to the wrapped doc values query, yielding a speedup since we do not have to evaluate the range against the whole index. I think the -2/-3% difference we are seeing for everything else than AndTerm02Range25 is noisy since term queries execute exactly the same way in both cases, yet they have this slight slowdown too. I would like to make it easier to use by moving IndexOrDocValuesQuery and DocValuesRangeQuery to a different module than sandbox, and giving the doc values range query an API that is closer to point ranges by making the bounds required (null disallowed) and removing the includeLower and includeUpper parameters. I wanted to move to queries initially but maybe core is better, that way we could link from the point API to IndexOrDocValuesQuery as a way to make queries on fields that have both points and doc values more efficient.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12848502/LUCENE-7643.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7659", "change_description": ": Added a IndexWriter#getFieldNames() method (experimental) to return\nall field names as visible from the IndexWriter. This would be useful for\nIndexWriter#updateDocValues() calls, to prevent calling with non-existent\ndocValues fields", "change_title": "IndexWriter should expose field names", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "While working on SOLR-5944, I needed a way to know whether applying an update to a DV is possible (i.e. the DV exists or not), while deciding upon whether or not to apply the update as an in-place update or a regular full document update. This information is present at the IndexWriter in a FieldInfos instance, and can be exposed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849394/LUCENE-7659.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-6959", "change_description": ": Removed ToParentBlockJoinCollector in favour of\nParentChildrenBlockJoinQuery, that can return the matching children documents per\nparent document. This query should be executed for each matching parent document\nafter the main query has been executed.", "change_title": "Remove ToParentBlockJoinCollector", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "This collector uses the getWeight() and getChildren() methods from the passed in Scorer, which are not always available (eg. disjunctions expose fake scorers) hence the need for a dedicated IndexSearcher (ToParentBlockJoinIndexSearcher). Given that this is the only collector in this case, I would like to remove it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849819/LUCENE_6959.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7628", "change_description": ": Scorer.getChildren() now only returns Scorers that are\npositioned on the current document, and can throw an IOException.\nAssertingScorer checks that getChildren() is not called on an unpositioned\nScorer.", "change_title": "Add a getMatchingChildren() method to DisjunctionScorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5", "detail_description": "This one is a bit convoluted, so bear with me... The luwak highlighter works by rewriting queries into their Span-equivalents, and then running them with a special Collector.  At each matching doc, the highlighter gathers all the Spans objects positioned on the current doc and collects their positions using the SpanCollection API. Some queries can't be translated into Spans.  For those queries that generate Scorers with ChildScorers, like BooleanQuery, we can call .getChildren() on the Scorer and see if any of them are SpanScorers, and for those that aren't we can call .getChildren() again and recurse down.  For each child scorer, we check that it's positioned on the current document, so non-matching subscorers can be skipped. This all works correctly except in the case of a DisjunctionScorer where one of the children is a two-phase iterator that has matched its approximation, but not its refinement query.  A SpanScorer in this situation will be correctly positioned on the current document, but its Spans will be in an undefined state, meaning the highlighter will either collect incorrect hits, or it will throw an Exception and prevent hits being collected from other subspans. We've tried various ways around this (including forking SpanNearQuery and adding a bunch of slow position checks to it that are used only by the highlighting code), but it turns out that the simplest fix is to add a new method to DisjunctionScorer that only returns the currently matching child Scorers.  It's a bit of a hack, and it won't be used anywhere else, but it's a fairly small and contained hack.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12847632/LUCENE-7628.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7702", "change_description": ": Removed GraphQuery in favour of simple boolean query.", "change_title": "Remove GraphQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "With LUCENE-7638 and LUCENE-7699 the GraphQuery wrapper is no longer needed and we can use standard queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12853794/LUCENE-7702.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7707", "change_description": ": TopDocs.merge now takes a boolean option telling it\nwhen to use the incoming shard index versus when to assign the shard\nindex itself, allowing users to merge shard responses incrementally\ninstead of once all shard responses are present.", "change_title": "Only assign ScoreDoc#shardIndex if it was already assigned to non default (-1) value", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "When you use TopDocs.merge today it always overrides the ScoreDoc#shardIndex value. The assumption that is made here is that all shard results are merges at once which is not necessarily the case. If for instance incremental merge phases are applied the shard index doesn't correspond to the index in the outer TopDocs array. To make this a backwards compatible but yet non-controversial change we could change the internals of TopDocs#merge to only assign this value unless it's not been assigned before to a non-default (-1) value to allow multiple or sparse top docs merging.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12854562/LUCENE-7707.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-6819", "change_description": ": Index-time boosts are deprecated. As a replacement, index-time\nscoring factors should be indexed into a doc value field and combined at\nquery time using eg. FunctionScoreQuery.", "change_title": "Deprecate index-time boosts?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Follow-up of this comment: https://issues.apache.org/jira/browse/LUCENE-6818?focusedCommentId=14934801&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14934801 Index-time boosts are a very expert feature whose behaviour is tight to the Similarity impl. Additionally users have often be confused by the poor precision due to the fact that we encode values on a single byte. But now we have doc values that allow you to encode any values the way you want with as much precision as you need so maybe we should deprecate index-time boosts and recommend to encode index-time scoring factors into doc values fields instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12855185/LUCENE-6819-wip.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "API Changes", "change_id": "LUCENE-7700", "change_description": ": A cleanup of merge throughput control logic. Refactored all the\ncode previously scattered throughout the IndexWriter and\nConcurrentMergeScheduler into a more accessible set of public methods (see\nMergePolicy.OneMergeProgress, MergeScheduler.wrapForMerge and\nOneMerge.mergeInit).", "change_title": "Move throughput control and merge aborting out of IndexWriter's core?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Here is a bit of a background: Looking at the code it seems to me that everything with respect to I/O control could be nicely pulled out into classes that explicitly control the merging process, that is only MergePolicy and MergeScheduler. By default, one could even run without any additional I/O accounting overhead (which is currently in there, even if one doesn't use the CMS's throughput control). Such refactoring would also give a chance to nicely move things where they belong – job aborting into OneMerge (currently in RateLimiter), rate limiter lifecycle bound to OneMerge (MergeScheduler could then use per-merge or global accounting, as it pleases). Just a thought and some initial refactorings for discussion.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12857025/LUCENE-7700.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7738", "change_description": ": Add new InetAddressRange for indexing and querying InetAddress\nranges.", "change_title": "Add new InetAddressRangeField", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This issue adda a new InetAddressRangeField for indexing and querying InetAddress ranges.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12857451/LUCENE-7738.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7449", "change_description": ": Add CROSSES relation support to RangeFieldQuery.", "change_title": "Add CROSSES query support to RangeField", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "RangeField currently supports INTERSECTS, WITHIN, and CONTAINS query behavior. This feature adds support for an explicit CROSSES query. Unlike INTERSECT and OVERLAP queries the CROSSES query finds any indexed ranges whose interior (within range) intersect the interior AND exterior (outside range) of the query range.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12852860/LUCENE-7449.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7623", "change_description": ": Add FunctionScoreQuery and FunctionMatchQuery", "change_title": "Add FunctionScoreQuery and FunctionMatchQuery", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.5", "detail_description": "We should update the various function scoring queries to use the new DoubleValues API", "patch_link": "https://issues.apache.org/jira/secure/attachment/12847603/LUCENE-7623.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7619", "change_description": ": Add WordDelimiterGraphFilter, just like\nWordDelimiterFilter except it produces correct token graphs so that\nproximity queries at search time will produce correct results", "change_title": "Add WordDelimiterGraphFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Currently, WordDelimiterFilter doesn't try to set the posLen attribute and so it creates graphs like this:  but with this patch (still a work in progress) it creates this graph instead:  This means (today) positional queries when using WDF at search time are buggy, but since we fixed LUCENE-7603, with this change here you should be able to use positional queries with WDGF. I'm also trying to produce holes properly (removes logic from the current WDF that swallows a hole when whole token is just delimiters). Surprisingly, it's actually quite easy to tweak WDF to create a graph (unlike e.g. SynonymGraphFilter) because it's already creating the necessary new positions, and its output graph never has side paths, except for single tokens that skip nodes because they have posLen > 1.  I.e. the only fix to make, I think, is to set posLen properly.  And it really helps that it does its own \"new token buffering + sorting\" already.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12846853/LUCENE-7619.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7656", "change_description": ": Added the LatLonDocValuesField.new(Box/Distance)Query() factory\nmethods that are the equivalent of factory methods on LatLonPoint but operate\non doc values. These new methods should be wrapped in an IndexOrDocValuesQuery\nfor best performance.", "change_title": "Implement geo box and distance queries using doc values.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Having geo box and distance queries available as both point and doc-values-based queries means we could use them with IndexOrDocValuesQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849481/LUCENE-7656.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7673", "change_description": ": Added MultiValued[Int/Long/Float/Double]FieldSource that given a\nSortedNumericSelector.Type can give a ValueSource view of a\nSortedNumericDocValues field.", "change_title": "Add MultiValued[Int/Long/Float/Double]FieldSource for SortedNumericDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Right now [Int/Long/Float/Double]FieldSource can give a ValueSource view of a NumericDocValues field. This Jira is to add MultiValued[Int/Long/Float/Double]FieldSource that given a SortedNumericSelector.Type can give a ValueSource view of a SortedNumericDocValues field I considered instead of adding new classes an optional selector parameter to the existing [Int/Long/Float/Double]FieldSource, but I think adding different classes makes a cleaner API and it’s clear that for MultiValued* case, the selector is a mandatory parameter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12850535/LUCENE-7673.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7465", "change_description": ": Add SimplePatternTokenizer and\nSimplePatternSplitTokenizer, using Lucene's regexp/automaton\nimplementation for analysis/tokenization", "change_title": "Add a PatternTokenizer that uses Lucene's RegExp implementation", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "I think there are some nice benefits to a version of PatternTokenizer that uses Lucene's RegExp impl instead of the JDK's: I named it SimplePatternTokenizer, and it still needs a factory and improved tests, but I think it's otherwise close. It currently does not take a group parameter because Lucene's RegExps don't yet implement sub group capture.  I think we could add that at some point, but it's a bit tricky. This doesn't even have group=-1 support (like String.split) ... I think if we did that we should maybe name it differently (SimplePatternSplitTokenizer?).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12830234/LUCENE-7465.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7688", "change_description": ": Add OneMergeWrappingMergePolicy class.", "change_title": "add a OneMergeWrappingMergePolicy class", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "This ticket splits out the lucene part of the changes proposed in SOLR-10046 for a conversation on whether or not the OneMergeWrappingMergePolicy class would best be located in Lucene or in Solr. (As an aside, the proposed use of java.util.function.UnaryOperator causes ant documentation-lint to fail, I have created LUCENE-7689 separately for that.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12852067/LUCENE-7688.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7686", "change_description": ": The near-real-time document suggester can now\nefficiently filter out duplicate suggestions", "change_title": "NRT suggester should have option to filter out duplicates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Some of the other suggesters have this ability, and it's quite simple to add it to the NRT suggester as long as the thing we are filtering on is the suggest key itself, not e.g. another stored field from the document.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12853247/LUCENE-7686.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "New Features", "change_id": "LUCENE-7712", "change_description": ": SimpleQueryParser now supports default fuzziness\nsyntax, mapping foo~ to a FuzzyQuery with edit distance 2.", "change_title": "SimpleQueryString should support auto fuziness", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Apparently the simpleQueryString query does not support auto fuziness as the query string does. So foo:bar~1 works for both simple query string and query string queries. But foo:bar~ works for query string query but not for simple query string query.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12856392/LUCENE-7712.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7630", "change_description": ": Fix (Edge)NGramTokenFilter to no longer drop payloads\nand preserve all attributes.", "change_title": "EdgeNGramTokenFilter drops payloads", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "6.5,7.0", "detail_description": "Using an EdgeNGramTokenFilter after a DelimitedPayloadTokenFilter discards the payloads, where as most other filters copy the payload to the new tokens. I added a test for this issue and a possible fix at https://github.com/xabbu42/lucene-solr/tree/edgepayloads Greetings Nathan Gass", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7679", "change_description": ": MemoryIndex was ignoring omitNorms settings on passed-in\nIndexableFields.", "change_title": "MemoryIndex.addField() ignores some FieldType settings", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.5", "detail_description": "Spotted by a luwak user: https://github.com/flaxsearch/luwak/issues/135.  MemoryIndex never omits norms, which means that it can produce incorrect scores.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12851361/LUCENE-7679.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7692", "change_description": ": PatternReplaceCharFilterFactory now implements MultiTermAware.", "change_title": "PatternReplaceCharFilterFactory should implement MultiTermAware", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "The multi-term aware marker API is useful to know which analysis components to apply when analyzing prefix or wildcard queries. I think PatternReplaceCharFilterFactory qualifies? For the record, we have MappingCharFilterFactory that does a similar job (except that it takes an explicit map of replacements  rather than regular expressions) and implements MultiTermAware.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7685", "change_description": ": ToParentBlockJoinQuery and ToChildBlockJoinQuery now use the\nrewritten child query in their equals and hashCode implementations.", "change_title": "Remove equals/rewrite hacks from block join queries", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "These queries try to ensure that rewritten queries are equal to the original query by keeping around the original query that was used to instantiate the join query. However this does not buy anything, and could even prevent two queries that rewrite to the same form to be considered equals.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12852039/LUCENE-7685.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7698", "change_description": ": CommonGramsQueryFilter was producing a disconnected\ntoken graph, messing up phrase queries when it was used during query\nparsing", "change_title": "CommonGramsQueryFilter in the query analyzer chain breaks phrase queries", "detail_type": "Bug", "detail_affect_versions": "6.4,6.4.1", "detail_fix_versions": "6.4.2,7.0", "detail_description": "(Please pardon me if the project or component are wrong!) CommonGramsQueryFilter breaks phrase queries. The behavior also seems to change with addition or removal of adjacent terms. Steps to reproduce: 1.) Download and extract Solr (in my test case version 6.4.1) somewhere. 2.) Modify server/solr/configsets/sample_techproducts_configs/conf/managed-schema and modify text_general fieldType by adding CommonGrams(Query)Filter before stopWordFilter: <fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\">       <analyzer type=\"index\">         <tokenizer class=\"solr.StandardTokenizerFactory\"/>         <filter class=\"solr.CommonGramsFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />         <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />         <!-- in this example, we will only use synonyms at query time         <filter class=\"solr.SynonymFilterFactory\" synonyms=\"index_synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/>         -->         <filter class=\"solr.LowerCaseFilterFactory\"/>       </analyzer>       <analyzer type=\"query\">         <tokenizer class=\"solr.StandardTokenizerFactory\"/>         <filter class=\"solr.CommonGramsQueryFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\"/>         <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" />         <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>         <filter class=\"solr.LowerCaseFilterFactory\"/>       </analyzer>     </fieldType> 3.) Add \"with\" to server/solr/configsets/sample_techproducts_configs/conf/stopwords.txt and make sure the file has correct line endings (extracted from Solr zip it seems to contain DOS/Windows lien endings which may break things). 4.) Run the techproducts example with \"bin/solr -e techproducts\" 5.) Browse to <http://localhost:8983/solr/techproducts/select?q=%22iPod%20with%20Video%22&debugQuery=true> 6.) Observe that parsedquery in the debug output is empty 7.) Browse to <http://localhost:8983/solr/techproducts/select?q=%22Apple%2060%20GB%20iPod%20with%20Video%20Playback%20Black%22&debugQuery=true> 8.) Observe that parsedquery contains ipod_with as expected but not with_video.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12853280/LUCENE-7698.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7708", "change_description": ": ShingleFilter without unigram was producing a disconnected\ntoken graph, messing up queries when it was used during query\nparsing", "change_title": "Track PositionLengthAttribute abuse", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Some token filters uses the position length attribute of the token stream to encode the number of terms they put in a single token.  This breaks the query parsing because it creates disconnected graph.  I've tracked down the abusive case to 2 candidates: I don't think these filters should set the position length at all so the best would be to remove the attribute from these token filters but this could break BWC. Though this is a serious bug since shingles and cjk bigram now produce invalid queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12854546/LUCENE-7708.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7055", "change_description": ": Added Weight#scorerSupplier, which allows to estimate the cost\nof a Scorer before actually building it, in order to optimize how the query\nshould be run, eg. using points or doc values depending on costs of other\nparts of the query.", "change_title": "Better execution path for costly queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "In Lucene 5.0, we improved the execution path for queries that run costly operations on a per-document basis, like phrase queries or doc values queries. But we have another class of costly queries, that return fine iterators, but these iterators are very expensive to build. This is typically the case for queries that leverage DocIdSetBuilder, like TermsQuery, multi-term queries or the new point queries. Intersecting such queries with a selective query is very inefficient since these queries build a doc id set of matching documents for the entire index. Is there something we could do to improve the execution path for these queries? One idea that comes to mind is that most of these queries could also run on doc values, so maybe we could come up with something that would help decide how to run a query based on other parts of the query? (Just thinking out loud, other ideas are very welcome)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845560/LUCENE-7055.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7643", "change_description": ": IndexOrDocValuesQuery allows to execute range queries using\neither points or doc values depending on which one is more efficient.", "change_title": "Move IndexOrDocValuesQuery to queries (or core?)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "I was just doing some benchmarking to check that IndexOrDocValues actually makes things faster when it is supposed to: In the above results, the number after the query type indicates the percentage of docs in the index that it matches. With the baseline, range queries are simple point range queries, while the patch is an IndexOrDocValuesQuery that wraps both a point range query and a doc values query that matches the same documents. As expected, AndTerm35Range10 performs the same in both cases since the range is supposed to lead the iteration, so the IndexOrDocValuesQuery is rewritten to the wrapped point range query. However with AndTerm02Range25 the range cost is higher than the term cost so the range is only used for verifying matches and the IndexOrDocValuesQuery rewrites to the wrapped doc values query, yielding a speedup since we do not have to evaluate the range against the whole index. I think the -2/-3% difference we are seeing for everything else than AndTerm02Range25 is noisy since term queries execute exactly the same way in both cases, yet they have this slight slowdown too. I would like to make it easier to use by moving IndexOrDocValuesQuery and DocValuesRangeQuery to a different module than sandbox, and giving the doc values range query an API that is closer to point ranges by making the bounds required (null disallowed) and removing the includeLower and includeUpper parameters. I wanted to move to queries initially but maybe core is better, that way we could link from the point API to IndexOrDocValuesQuery as a way to make queries on fields that have both points and doc values more efficient.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12848502/LUCENE-7643.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7662", "change_description": ": If index files are missing, throw CorruptIndexException instead\nof the less descriptive FileNotFound or NoSuchFileException", "change_title": "Index with missing files should throw CorruptIndexException", "detail_type": "Bug", "detail_affect_versions": "6.4", "detail_fix_versions": "6.5,7.0", "detail_description": "Similar to what we did in LUCENE-7592 for EOF, we should catch missing files and rethrow those as CorruptIndexException. If a particular codec can handle missing files, it should be proactive check for those optional files and not throw anything, so I think we can safely do this at SegmentReader or SegmentCoreReaders level. Stack trace copied from SOLR-10006:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12851790/LUCENE-7662.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7680", "change_description": ": UsageTrackingQueryCachingPolicy never caches term filters anymore\nsince they are plenty fast. This also has the side-effect of leaving more\nspace in the history for costly filters.", "change_title": "Never cache term filters", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Currently we just require term filters to be used a lot in order to cache them. Maybe instead we should look into never caching them. This should not hurt performance since term filters are plenty fast, and would make other filters more likely to be cached since we would not \"pollute\" the history with filters that are not worth caching.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12851645/LUCENE-7680.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7677", "change_description": ": UsageTrackingQueryCachingPolicy now caches compound queries a bit\nearlier than regular queries in order to improve cache efficiency.", "change_title": "Cache compound filters earlier than regular queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Say you keep reusing a boolean filter that looks like \"A OR B\" and never use the A and B queries out of that boolean query. Currently, after this filter has been used 5 times, we would cache both A, B and \"A OR B\", which means that cache entries for A and B would only be built for the purpose of building a cache entry for \"A OR B\", which is wasteful. By caching compound queries a bit earlier, we could make it less likely to happen since:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12851206/LUCENE-7677.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7710", "change_description": ": BlockPackedReader throws CorruptIndexException and includes\nIndexInput description instead of plain IOException", "change_title": "BlockPackedReader to throw better exception", "detail_type": "Improvement", "detail_affect_versions": "4.10.3", "detail_fix_versions": "6.5,7.0", "detail_description": "BlockPackedReader doesn't tell us which file we failed reading. Here's a stack trace from a 4.10.3 install, but it applies to trunk as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12854502/LUCENE-7710.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7695", "change_description": ": ComplexPhraseQueryParser to support query time synonyms", "change_title": "Unknown query type SynonymQuery in ComplexPhraseQueryParser", "detail_type": "Bug", "detail_affect_versions": "6.4", "detail_fix_versions": "6.5,7.0", "detail_description": "We sometimes receive this exception using ComplexPhraseQueryParser via Solr 6.4.0. Some terms do fine, others don't. This query: returns results just fine. The next one: Gives results as well! But this one: Returns the following exception:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12855982/LUCENE-7695.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Improvements", "change_id": "LUCENE-7747", "change_description": ": QueryBuilder now iterates lazily over the possible paths when building a graph query", "change_title": "QueryBuilder should build side-paths query (graph query) lazily", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "In LUCENE-7638 we generate a query for each multi-token path in the graph and combine them at the end in a boolean query.   This can lead to OOM when the number of path is big, instead we should build the disjunction of these paths lazily in order to throw \"too many clauses\" early if the number of paths is bigger than max boolean clauses. For instance a shingle filter with shingles of different size produces a graph with multiple side paths at each position. If the input query has a lot of tokens, the number of paths (query) created is exponential. For this use case it is maybe preferable to disallow graph query analysis completely but when allowed we should also be protected against combinatorial explosion.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12859081/LUCENE-7747.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7641", "change_description": ": Optimized point range queries to compute documents that do not\nmatch the range on single-valued fields when more than half the documents in\nthe index would match.", "change_title": "Speed up point ranges that match most documents", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "If a point range matches most documents and  every document has exactly one value, then we could make things faster by computing the set of documents that do NOT match the range instead. It was not possible until recently since figuring out whether a range query matches most documents was not possible, but we can now use the new PointValues.estimatePointcount API to do that: we could just check whether the cost of the inverse visitor is lower than the cost of the regular range visitor.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12848036/LUCENE-7641.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7656", "change_description": ": Speed up for LatLonPointDistanceQuery by computing distances even\nless often.", "change_title": "Implement geo box and distance queries using doc values.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Having geo box and distance queries available as both point and doc-values-based queries means we could use them with IndexOrDocValuesQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849481/LUCENE-7656.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7661", "change_description": ": Speed up for LatLonPointInPolygonQuery by pre-computing the\nrelation of the polygon with a grid.", "change_title": "Speed up LatLonPointInPolygonQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "We could apply the same idea as LUCENE-7656 to LatLonPointInPolygonQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849668/LUCENE-7661.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7660", "change_description": ": Speed up LatLonPointDistanceQuery by improving the detection of\nwhether BKD cells are entirely within the distance close to the dateline.", "change_title": "LatLonPointDistanceQuery could skip distance computations more often", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Currently the logic that check whether all points of a box are within the circle does not take longitude wrapping into account. As a consequence, if you run a distance query whose circle crosses the dateline, we might be performing LOTs of distance computations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849505/LUCENE-7660.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7654", "change_description": ": ToParentBlockJoinQuery now implements two-phase iteration and\ncomputes scores lazily in order to be faster when used in conjunctions.", "change_title": "To-parent block joins should implement two-phase iteration", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "To-parent block joins are a good fit for two-phase iteration. Currently the iterator implementation visits all child matches before returning a parent match. Instead, an approximation would only need to find one child match in order to return a potential parent match. This would make to-parent block joins more efficient when used in conjunctions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849113/LUCENE-7654.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7667", "change_description": ": BKDReader now calls `IntersectVisitor.grow()` on larger\nincrements.", "change_title": "BKDReader could call grow on larger increments", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Currently, we only call grow() on leaf nodes. We could make it grow with larger increments by calling grow() on the number of leaf cells under the current node when the relation is CELL_INSIDE_QUERY (logic that we already have for point count estimations).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849939/LUCENE-7667.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7638", "change_description": ": Query parsers now analyze the token graph for articulation\npoints (or cut vertices) in order to create more efficient queries for\nmulti-token synonyms.", "change_title": "Optimize graph query produced by QueryBuilder", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5", "detail_description": "The QueryBuilder creates a graph query when the underlying TokenStream contains token with PositionLengthAttribute greater than 1. These TokenStreams are in fact graphs (lattice to be more precise) where synonyms can span on multiple terms.  Currently the graph query is built by visiting all the path of the graph TokenStream. For instance if you have a synonym like \"ny, new york\" and you search for \"new york city\", the query builder would produce two pathes: \"new york city\", \"ny city\" This can quickly explode when the number of multi terms synonyms increase.  The query \"ny ny\" for instance would produce 4 pathes and so on. For boolean queries with should or must clauses it should be more efficient to build a boolean query that merges all the intersections in the graph. So instead of \"new york city\", \"ny city\" we could produce: \"+((+new +york) ny) +city\" The attached patch is a proposal to do that instead of the all path solution. The patch transforms multi terms synonyms in graph query for each intersection in the graph. This is not done in this patch but we could also create a specialized query that gives equivalent scores to multi terms synonyms like the SynonymQuery does for single term synonyms. For phrase query this patch does not change the current behavior but we could also use the new method to create optimized graph SpanQuery. mattweber I think this patch could optimize a lot of cases where multiple muli-terms synonyms are present in a single request. Could you take a look ?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12850430/LUCENE-7638.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7699", "change_description": ": Query parsers now use span queries to produce more efficient\nphrase queries for multi-token synonyms.", "change_title": "Apply graph articulation points optimization to phrase graph queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5", "detail_description": "Follow-up to LUCENE-7638 that applies the same articulation point logic to graph phrases using span queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12853790/LUCENE-7699.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7742", "change_description": ": Fix places where we were unboxing and then re-boxing\naccording to FindBugs", "change_title": "FindBugs: unboxing followed by re-boxing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "http://findbugs.sourceforge.net/bugDescriptions.html#BX_UNBOXING_IMMEDIATELY_REBOXED", "patch_link": "https://issues.apache.org/jira/secure/attachment/12858785/LUCENE-7742.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Optimizations", "change_id": "LUCENE-7739", "change_description": ": Fix places where we unnecessarily boxed while parsing\na numeric value according to FindBugs", "change_title": "FindBugs: boxing/unboxing to parse a primitive", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "As suggested in LUCENE-3973, we could add findbugs as a precommit hook if we get the number of issues reported down to zero.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12858784/LUCENE-7739.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "LUCENE-7653", "change_description": ": Update randomizedtesting to version 2.5.0.", "change_title": "Update randomizedtesting to version 2.5.0", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Version 2.5.0, among other minor improvements, adds a set of checks for non-empty work directory of forked JVMs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "LUCENE-7665", "change_description": ": Remove grouping dependency from the join module.", "change_title": "Remove grouping dependency from the join module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Follow up from LUCENE-6959.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849889/LUCENE_7665.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "SOLR-10023", "change_description": ": Add non-recursive 'test-nocompile' target: Only runs unit tests.\nJars are not downloaded; compilation is not updated; and Clover is not enabled.", "change_title": "Improve single unit test run time with ant.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "It seems to take 2 minutes and 45 seconds to run a single test with the latest build design and the test itself is only 4 seconds. I've noticed this for a long time, and it seems because ant is running through a billion targets first. I haven't checked yet, so maybe it's a Solr specific issue? I'll check with Lucene and move this issue if necessary. There is hopefully something we can do to improve this though. At least we should try and get some sharp minds to take first / second look. If I did not use an IDE so much to run tests, this would drive me nuts.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849561/SOLR-10023-add-test-only-target.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "LUCENE-7694", "change_description": ": Update forbiddenapis to version 2.3.", "change_title": "Update forbiddenapis to 2.3", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Forbiddenapis 2.3 was released an hour ago. This is just a maintenance update, the full release notes are here: https://github.com/policeman-tools/forbidden-apis/wiki/Changes#version-23-released-2017-02-13", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "LUCENE-7693", "change_description": ": Replace \"org.apache.\" logic in GetMavenDependenciesTask.", "change_title": "revisit \"org.apache.\" logic in GetMavenDependenciesTask.java", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Objective: Motivation: Approach:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12852562/LUCENE-7693.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "LUCENE-7726", "change_description": ": Fix HTML entity bugs in Javadocs to be able to build with\nJava 9.", "change_title": "Fix Javadocs HTML entity bugs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "As of jdk9-ea-b158, ant documentation seems to build the core javadocs just fine, but fails on the  lucene/memory/ javadocs... looking at the generated html files turns up this... The source java file has this... ...which does in fact seem to be invalid HTML ... aren't & always suppose to be encoded as & ... even in URLs? I'm suprised the java8 javadocs/linter don't warn about this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12855481/LUCENE-7726.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Build", "change_id": "LUCENE-7727", "change_description": ": Replace end-of-life Markdown parser \"Pegdown\" by \"Flexmark\"\nfor compatibility with Java 9.", "change_title": "Replace EOL'ed pegdown by flexmark-java for Java 9 compatibility", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "The documentation tasks use a library called \"pegdown\" to convert Markdown to HTML. Unfortunately, the developer of pegdown EOLed it and points the users to a faster replacement: flexmark-java (https://github.com/vsch/flexmark-java). This would not be important for us, if pegdown would work with Java 9, but it is also affected by the usual \"setAccessible into private Java APIs\" issue (see my talk at FOSDEM: https://fosdem.org/2017/schedule/event/jigsaw_challenges/). The migration should not be too hard, its just a bit of Groovy Code rewriting and dependency changes. This is the pegdown problem:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12855619/LUCENE-7727.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Other", "change_id": "LUCENE-7666", "change_description": ": Fix typos in lucene-join package info javadoc.", "change_title": "Typos in lucene-join package info javadoc", "detail_type": "Improvement", "detail_affect_versions": "6.4", "detail_fix_versions": "6.5,7.0", "detail_description": "Steps: -view the \"Query-time joins\" section of the lucene-join package javadoc https://lucene.apache.org/core/6_4_0/join/index.html?org/apache/lucene/search/join/package-summary.html Expected behaviour: The example Java code is valid. Actual behaviour: There are some typos.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849904/LUCENE-7666.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Other", "change_id": "LUCENE-7658", "change_description": ": queryparser/xml CoreParser now implements SpanQueryBuilder interface.", "change_title": "queryparser/xml CoreParser to implement SpanQueryBuilder interface", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "straightforward change:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849268/LUCENE-7658.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Other", "change_id": "LUCENE-7664", "change_description": ": GeoPointField and its queries are deprecated in favor\nof LatLonPoint, which offers faster indexing and searching\nperformance, smaller index, and less search-time heap usage.", "change_title": "Deprecate GeoPointField & queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "The new dimensional points implementations for geo distance, polygon, shape filtering are substantially faster and creates a smaller index than the postings based GeoPointField.  They also offer nearest neighbor search, which GeoPointField doesn't. I think we should deprecate GeoPointField and focus on the points implementations. We have still other legacy postings based geo implementations but I think we should keep them for now since they have functionality that points doesn't yet have: the ability to index a shape and search for shapes overlapping the indexed shapes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12850036/LUCENE-7664.patch", "patch_content": "none"}
{"library_version": "6.5.0", "change_type": "Other", "change_id": "LUCENE-7715", "change_description": ": NearSpansUnordered simplifications.", "change_title": "Simplify NearSpansUnordered", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "6.5,7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12854955/LUCENE-7715.patch", "patch_content": "none"}

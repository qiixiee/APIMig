{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6485", "change_description": ": Add CustomSeparatorBreakIterator to postings\nhighlighter which splits on any character. For example, it\ncan be used with getMultiValueSeparator render whole field\nvalues.", "change_title": "Add a custom separator break iterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Lucene currently includes a WholeBreakIterator used to highlight entire fields using the postings highlighter, without breaking their content into sentences. I would like to contribute a CustomSeparatorBreakIterator that breaks when a custom char separator is found in the text. This can be used for instance when wanting to highlight entire fields, value per value. One can subclass PostingsHighlighter and have getMultiValueSeparator return a control character, like U+0000 , then use the custom break iterator to break on U+0000 so that one snippet per value will be generated.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12733233/LUCENE-6485.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6459", "change_description": ": Add common suggest API that mirrors Lucene's\nQuery/IndexSearcher APIs for Document based suggester.\nAdds PrefixCompletionQuery, RegexCompletionQuery,\nFuzzyCompletionQuery and ContextQuery.", "change_title": "[suggest] Query Interface for suggest API", "detail_type": "New Feature", "detail_affect_versions": "5.1", "detail_fix_versions": "5.3,6.0", "detail_description": "This patch factors out common indexing/search API used by the recently introduced NRTSuggester.  The motivation is to provide a query interface for FST-based fields (SuggestField and ContextSuggestField)  to enable suggestion scoring and more powerful automaton queries. Previously, only prefix ‘queries’ with index-time weights were supported but we can also support: CompletionQuery is used to query SuggestField and ContextSuggestField. A CompletionQuery produces a CompletionWeight,  which allows CompletionQuery implementations to pass in an automaton that will be intersected with a FST and allows boosting and  meta data extraction from the intersected partial paths. A CompletionWeight produces a CompletionScorer. A CompletionScorer  executes a Top N search against the FST with the provided automaton, scoring and filtering all matched paths. Return documents with values that match the prefix of an analyzed term text  Documents are sorted according to their suggest field weight. Return documents with values that match the prefix of a regular expression Documents are sorted according to their suggest field weight. Return documents with values that has prefixes within a specified edit distance of an analyzed term text. Documents are ‘boosted’ by the number of matching prefix letters of the suggestion with respect to the original term text. suggestion_weight * boost where suggestion_weight and boost are all integers.  boost = # of prefix characters matched Return documents that match a CompletionQuery filtered and/or boosted by provided context(s). NOTE: ContextQuery should be used with ContextSuggestField to query suggestions boosted and/or filtered by contexts. Running ContextQuery against a SuggestField will error out. suggestion_weight  * context_boost where suggestion_weight and context_boost are all integers When used with FuzzyCompletionQuery, suggestion_weight * (context_boost + fuzzy_boost) To use ContextQuery, use ContextSuggestField instead of SuggestField. Any CompletionQuery can be used with  ContextSuggestField, the default behaviour is to return suggestions from all contexts. Context for every completion hit  can be accessed through SuggestScoreDoc#context.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735705/LUCENE-6459.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6487", "change_description": ": Spatial Geo3D API now has a WGS84 ellipsoid world model option.", "change_title": "Add WGS84 capability to geo3d support", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "WGS84 compatibility has been requested for geo3d.  This involves working with an ellipsoid rather than a unit sphere.  The general formula for an ellipsoid is: x^2/a^2 + y^2/b^2 + z^2/c^2 = 1", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735303/LUCENE-6487.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6477", "change_description": ": Add experimental BKD geospatial tree doc values format\nand queries, for fast \"bbox/polygon contains lat/lon points\"", "change_title": "Add BKD tree for spatial shape query intersecting indexed points", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "I'd like to explore using dedicated spatial trees for faster shape intersection filters than postings-based implementations. I implemented the tree data structure from https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf The idea is simple: it builds a full binary tree, partitioning 2D space, alternately on lat and then lon, into smaller and smaller rectangles until a leaf has <= N (default 1024) points. It cannot index shapes (just points), and can then do fast shape intersection queries.  Multi-valued fields are supported. I only implemented the \"point is contained in this bounding box\" query for now, but I think polygon shape querying should be easy to implement using the same approach from LUCENE-6450. For indexing, you add BKDPointField (takes lat, lon) to your doc, and must set up your Codec use BKDTreeDocValuesFormat for that field. This DV format wraps Lucene50DVFormat, but then builds the disk-based BKD tree structure on the side.  BKDPointInBBoxQuery then requires this DVFormat, and casts it to gain access to the tree. I quantize each incoming double lat/lon to 32 bits precision (so 64 bits per point) = ~9 milli-meter lon precision at the equator, I think.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12736804/LUCENE-6477.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6526", "change_description": ": Asserting(Query|Weight|Scorer) now ensure scores are not computed\nif they are not needed.", "change_title": "Make AssertingWeight check that scores are not computed when needsScores is false", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Today nothing prevents you from calling score() if you don't need scores. But we could make AssertingWeight check it in order to make sure that we do not waste resources computing something we don't need.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12737705/LUCENE-6526.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6481", "change_description": ": Add GeoPointField, GeoPointInBBoxQuery,\nGeoPointInPolygonQuery for simple \"indexed lat/lon point in\nbbox/shape\" searching.", "change_title": "Improve GeoPointField type to only visit high precision boundary terms", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Current GeoPointField LUCENE-6450  computes a set of ranges along the space-filling curve that represent a provided bounding box.  This determines which terms to visit in the terms dictionary and which to skip. This is suboptimal for large bounding boxes as we may end up visiting all terms (which could be quite large). This incremental improvement is to improve GeoPointField to only visit high precision terms in boundary ranges and use the postings list for ranges that are completely within the target bounding box. A separate improvement is to switch over to auto-prefix and build an Automaton representing the bounding box.  That can be tracked in a separate issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12732671/LUCENE-6481_WIP.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-5954", "change_description": ": The segments_N commit point now stores the Lucene\nversion that wrote the commit as well as the lucene version that\nwrote the oldest segment in the index, for faster checking of \"too\nold\" indices", "change_title": "Store lucene version in segment_N", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "It would be nice to have the version of lucene that wrote segments_N, so that we can use this to determine which major version an index was written with (for upgrading across major versions).  I think this could be squeezed in just after the segments_N header.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12737963/LUCENE-5954.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6519", "change_description": ": BKDPointInPolygonQuery is much faster by avoiding\nthe per-hit polygon check when a leaf cell is fully contained by the\npolygon.", "change_title": "BKD polygon queries should avoid per-hit filtering when cell is fully enclosed", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "In LUCENE-6481, nknize added methods to test for the relationship between an axis-aligned rect vs the query polygon, e.g. is the rect fully contained by the polygon, overlaps its boundaries, or fully outside the polygon. I think we should also use those methods to speed up BKDPointInPolygonQuery, to decide on recursively visiting the tree, how to handle the leaf blocks under internal nodes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738773/LUCENE-6519.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6549", "change_description": ": Add preload option to MMapDirectory.", "change_title": "add MMapDirectory.preload", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Patch to expose MappedByteBuffer.load() as a simple boolean option. This can be used as an alternative to copy-into-RAMDirectory. Users who want sophisticated logic about this can use FileSwitchDirectory or other methods.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739040/LUCENE-6549.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6504", "change_description": ": Add Lucene53Codec, with norms implemented directly\nvia the Directory's RandomAccessInput api.", "change_title": "implement norms with random access API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "We added this api in LUCENE-5729 but we never explored implementing norms with it. These are generally the largest consumer of heap memory and often a real hassle for users.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735593/LUCENE-6504.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6539", "change_description": ": Add new DocValuesNumbersQuery, to match any document\ncontaining one of the specified long values.  This change also\nmoves the existing DocValuesTermsQuery and DocValuesRangeQuery\nto Lucene's sandbox module, since in general these queries are\nquite slow and are only fast in specific cases.", "change_title": "Add DocValuesNumbersQuery, like DocValuesTermsQuery but works only with long values", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "This query accepts any document where any of the provided set of longs was indexed into the specified field as a numeric DV field (NumericDocValuesField or SortedNumericDocValuesField).  You can use it instead of DocValuesTermsQuery when you have field values that can be represented as longs. Like DocValuesTermsQuery, this is slowish in general, since it doesn't use an inverted data structure, but in certain cases (many terms/numbers and fewish matching hits) it should be faster than using TermsQuery because it's done as a \"post filter\" when other (faster) query clauses are MUST'd with it. In such cases it should also be faster than DocValuesTermsQuery since it skips having to resolve terms -> ords.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738866/LUCENE-6539.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6577", "change_description": ": Give earlier and better error message for invalid CRC.", "change_title": "Give earlier and better error message for invalid CRC", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "From digging into LUCENE-6576: We use a 32-bit CRC but store it in 64-bits, but today 32 bits are unused. We should check this on both read and write and if bits are invalid (because something is wrong in the JVM, or because on-disk stuff just got corrupted in that way), deliver a good exception.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739914/LUCENE-6577.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6544", "change_description": ": Geo3D: (1) Regularize path & polygon construction, (2) add\nPlanetModel.surfaceDistance() (ellipsoidal calculation), (3) cache lat & lon\nin GeoPoint, (4) add thread-safety where missing -- Geo3dShape.", "change_title": "Geo3d cleanup: Regularize path and polygon construction, plus consider adding ellipsoid surface distance method", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Geo3d's way of constructing polygons and paths differs in that in one case you construct points and the other you feed lat/lon values directly to the builder.  Probably both should be supported for both kinds of entity. Also it may be useful to have an accurate point-point ellipsoidal distance function.  This is expensive and would be an addition to the arc distance we currently compute.  It would probably be called \"surface distance\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740382/LUCENE-6544.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6606", "change_description": ": SegmentInfo.toString now confesses how the documents\nwere sorted, when SortingMergePolicy was used", "change_title": "add SortingMergePolicy's sorter (if any) to SegmentInfo.toString", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "github pull request with proposed change to follow, it also extends TestSortingMergePolicy to randomly choose regular or reverse sorting order.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6524", "change_description": ": IndexWriter can now be initialized from an already open\nnear-real-time or non-NRT reader.", "change_title": "Create an IndexWriter from an already opened NRT or non-NRT reader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "I'd like to add a new ctor to IndexWriter, letting you start from an already opened NRT or non-NRT DirectoryReader.  I think this is a long missing API in Lucene today, and we've talked in the past about different ways to fix it e.g. factoring out a shared reader pool between writer and reader. One use-case, which I hit in LUCENE-5376: if you have a read-only index, so you've opened a non-NRT DirectoryReader to search it, and then you want to \"upgrade\" to a read/write index, we don't handle that very gracefully now because you are forced to open 2X the SegmentReaders. But with this API, IW populates its reader pool with the incoming SegmentReaders so they are shared on any subsequent NRT reopens / segment merging / deletes applying, etc. Another (more expert) use case is allowing rollback to an NRT-point. Today, you can only rollback to a commit point (segments_N).  But an NRT reader also reflects a valid \"point in time\" view of the index (it just doesn't have a segments_N file, and its ref'd files are not fsync'd), so with this change you can close your old writer, open a new one from this NRT point, and revert all changes that had been done after the NRT reader was opened from the old writer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740227/LUCENE-6524.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6578", "change_description": ": Geo3D can now compute the distance from a point to a shape, both\ninner distance and to an outside edge. Multiple distance algorithms are\navailable.", "change_title": "Geo3d: arcDistanceToShape() method may be useful", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "I've got an application that seems like it may need the ability to compute a new kind of arc distance, from a GeoPoint to the nearest edge/point of a GeoShape.  Adding this method to the interface, and corresponding implementations, would increase the utility of the package for ranking purposes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742868/LUCENE-6578-dws.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6632", "change_description": ": Geo3D: Compute circle planes more accurately.", "change_title": "Geo3d: More accurate way of computing circle planes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "The Geo3d code that computes circle planes in GeoPath and GeoCircle is less accurate the smaller the circle.  There's a better way of computing this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742512/LUCENE-6632.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6653", "change_description": ": Added general purpose BytesTermAttribute to basic token\nattributes package that can be used for TokenStreams that solely produce\nbinary terms.", "change_title": "Cleanup TermToBytesRefAttribute", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "While working on LUCENE-6652, I figured out that there were so many test with wrongly implemented TermsToBytesRefAttribute. In addition, the whole concept back from Lucene 4.0 was no longer correct: Instead we should remove the fillBytesRef() method from the interface and let getBytesRef() populate and return the BytesRef. It does not matter if the attribute reuses the BytesRef or returns a new one. It just get consumed like a standard CharTermAttribute. You get a BytesRef and can use it until you call incrementToken(). As the TermsToBytesRefAttribute is marked experimental, I see no reason why we should not change the semantics to be more easy to understand and behave like all other attributes. I will add a note to the backwards incompatible changes in Lucene 5.3.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743175/LUCENE-6653.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6365", "change_description": ": Add Operations.topoSort, to run topological sort of the\nstates in an Automaton", "change_title": "Optimized iteration of finite strings", "detail_type": "Improvement", "detail_affect_versions": "5.0", "detail_fix_versions": "5.3,6.0", "detail_description": "Replaced Operations.getFiniteStrings() by an optimized FiniteStringIterator. Benefits: Avoid huge hash set of finite strings. Avoid massive object/array creation during processing. \"Downside\": Iteration order changed, so when iterating with a limit, the result may differ slightly. Old: emit current node, if accept / recurse. New: recurse / emit current node, if accept. The old method Operations.getFiniteStrings() still exists, because it eases the tests. It is now implemented by use of the new FiniteStringIterator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743577/LUCENE-6365.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6365", "change_description": ": Replace Operations.getFiniteStrings with a\nmore scalable iterator API (FiniteStringsIterator)", "change_title": "Optimized iteration of finite strings", "detail_type": "Improvement", "detail_affect_versions": "5.0", "detail_fix_versions": "5.3,6.0", "detail_description": "Replaced Operations.getFiniteStrings() by an optimized FiniteStringIterator. Benefits: Avoid huge hash set of finite strings. Avoid massive object/array creation during processing. \"Downside\": Iteration order changed, so when iterating with a limit, the result may differ slightly. Old: emit current node, if accept / recurse. New: recurse / emit current node, if accept. The old method Operations.getFiniteStrings() still exists, because it eases the tests. It is now implemented by use of the new FiniteStringIterator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743577/LUCENE-6365.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6589", "change_description": ": Add a new org.apache.lucene.search.join.CheckJoinIndex class\nthat can be used to validate that an index has an appropriate structure to\nrun join queries.", "change_title": "Less leniency in lucene/join", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "The lucene/join module expects a certain index structure but nothing validates it. Then at search time it either needs to validate the index structure in a slow way or be lenient and cope with what it is given.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742503/LUCENE-6589.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6659", "change_description": ": Remove IndexWriter's unnecessary hard limit on max concurrency", "change_title": "Remove IndexWriterConfig.get/setMaxThreadStates", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Ever since LUCENE-5644, IndexWriter will aggressively reuse its internal thread states across threads, whenever one is free. I think this means we can safely remove the sneaky maxThreadStates limit (default 8) that we have today: IW will only ever allocate as many thread states as there are actual concurrent threads running through it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743738/LUCENE-6659.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6547", "change_description": ": Add GeoPointDistanceQuery, matching all points within\nthe specified distance from the center point.  Fix\nGeoPointInBBoxQuery to handle dateline crossing.", "change_title": "Add dateline crossing support to GeoPointInBBox and GeoPointDistance Queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "The current GeoPointInBBoxQuery only supports bounding boxes that are within the standard -180:180 longitudinal bounds. While its perfectly fine to require users to split dateline crossing bounding boxes in two, GeoPointDistanceQuery should support distance queries that cross the dateline.  Since morton encoding doesn't support unwinding this issue will add dateline crossing to GeoPointInBBoxQuery and GeoPointDistanceQuery classes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12745752/LUCENE-6547.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6694", "change_description": ": Add LithuanianAnalyzer and LithuanianStemmer.", "change_title": "Snowball stemmer for Lithuanian language", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Snowball stemmer for Lithuanian language.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12746969/LUCENE-6694.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6695", "change_description": ": Added a new BlendedTermQuery to blend statistics across several\nterms.", "change_title": "BlendedTermQuery", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "It is sometimes desirable to ignore differences between index statistics of several terms so that they produce the same scores, for instance if you resolve synonyms at search time or if you want to search across several fields. Elasticsearch has been using this approach for its multi_match query for some time now. We already blend statistics in TopTermsBlendedFreqScoringRewrite (used by FuzzyQuery) but it could be helpful to have a dedicated query to choose manually which terms to blend stats from.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12747010/LUCENE-6695.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6706", "change_description": ": Added a new PayloadScoreQuery that generalises the behaviour of\nPayloadTermQuery and PayloadNearQuery to all Span queries.", "change_title": "Support Payload scoring for all SpanQueries", "detail_type": "New Feature", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3", "detail_description": "I need a way to have payloads influence the score of SpanOrQuery's.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748012/LUCENE-6706.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6697", "change_description": ": Add experimental range tree doc values format and\nqueries, based on a 1D version of the spatial BKD tree, for a faster\nand smaller alternative to postings-based numeric and binary term\nfiltering.  Range trees can also handle values larger than 64 bits.", "change_title": "Use 1D KD tree for alternative to postings based numeric range filters", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Today Lucene uses postings to index a numeric value at multiple precision levels for fast range searching.  It's somewhat costly: each numeric value is indexed with multiple terms (4 terms by default) ... I think a dedicated 1D BKD tree should be more compact and perform better. It should also easily generalize beyond 64 bits to arbitrary byte[], e.g. for LUCENE-5596, but I haven't explored that here. A 1D BKD tree just sorts all values, and then indexes adjacent leaf blocks of size 512-1024 (by default) values per block, and their docIDs, into a fully balanced binary tree.  Building the range filter is then just a recursive walk through this tree. It's the same structure we use for 2D lat/lon BKD tree, just with 1D instead.  I implemented it as a DocValuesFormat that also writes the numeric tree on the side.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748026/LUCENE-6697.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6647", "change_description": ": Add GeoHash string utility APIs", "change_title": "Add GeoHash String Utilities to core GeoUtils", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "GeoPointField uses morton encoding to efficiently pack lat/lon values into a single long. GeoHashing effectively does the same thing but uses base 32 encoding to represent this long value as a \"human readable\" string.  Many user applications already use the string representation of the hash. This issue simply adds the base32 string representation of the already computed morton code.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748240/LUCENE-6647.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6710", "change_description": ": GeoPointField now uses full 64 bits (up from 62) to encode\nlat/lon", "change_title": "GeoPointField should use full 64 bit encoding", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Current GeoPointField uses 31 bits for each lat and long, respectively.  This causes a precision error for the maximum bounds for 2D mapping applications (e.g., instead of maximum of 180, 90 the max value handled is 179.999999, 89.999999). This issue improves precision for the full 2D map boundaries by using the full 32 bit range for lat/lon values, resulting in a morton hash using the full 64 bit range.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748300/LUCENE-6710.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6580", "change_description": ": SpanNearQuery now allows defined-width gaps in its subqueries", "change_title": "Allow defined-width gaps in SpanNearQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "SpanNearQuery is not quite an exact Spans replacement for PhraseQuery at the moment, because while you can ask for an overall slop in an ordered match, you can't specify exactly where the gaps should appear.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748680/LUCENE-6580.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "New Features", "change_id": "LUCENE-6712", "change_description": ": Use doc values to post-filter GeoPointField hits that\nfall in boundary cells, resulting in smaller index, faster searches\nand less heap used for each query", "change_title": "GeoPointField should cut over to DocValues for boundary filtering", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Currently GeoPointField queries only use the Terms Dictionary for ranges that fall within and on the boundary of the query shape.  For boundary ranges the full precision terms are iterated, for within ranges the postings list is used. Instead of iterating full precision terms for boundary ranges, this enhancement cuts over to DocValues for post-filtering boundary terms. This allows us to increase precisionStep for GeoPointField thereby reducing the number of terms and the size of the index. This enhancement should also provide a boost in query performance since visiting more docs and fewer terms should be more efficient than visiting fewer docs and more terms.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748727/LUCENE-6712.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6508", "change_description": ": Simplify Lock api, there is now just\nDirectory.obtainLock() which returns a Lock that can be\nreleased (or fails with exception). Add lock verification\nto IndexWriter. Improve exception messages when locking fails.", "change_title": "Simplify Directory/lock api", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "See LUCENE-6507 for some background. In general it would be great if you can just acquire an immutable lock (or you get a failure) and then you close that to release it. Today the API might be too much for what is needed by IW.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12736000/LUCENE-6508-deadcode1.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6529", "change_description": ": Removed an optimization in UninvertingReader that was causing\nincorrect results for Numeric fields using precisionStep", "change_title": "NumericFields + SlowCompositeReaderWrapper + UninvertedReader + -Dtests.codec=random can results in incorrect SortedSetDocValues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Digging into SOLR-7631 and SOLR-7605 I became fairly confident that the only explanation of the behavior i was seeing was some sort of bug in either the randomized codec/postings-format or the UninvertedReader, that was only evident when two were combined and used on a multivalued Numeric Field using precision steps.  But since i couldn't find any -Dtests.codec or -Dtests.postings.format options that would cause the bug 100% regardless of seed, I switched tactices and focused on reproducing the problem using UninvertedReader directly and checking the SortedSetDocValues.getValueCount(). I now have a test that fails frequently (and consistently for any seed i find), but only with -Dtests.codec=random – override it with -Dtests.codec=default and everything works fine (based on the exhaustive testing I did in the linked issues, i suspect every named codec works fine - but i didn't re-do that testing here) The failures only seem to happen when checking the SortedSetDocValues.getValueCount() of a SlowCompositeReaderWrapper around the UninvertedReader – which suggests the root bug may actually be in SlowCompositeReaderWrapper? (but still has some dependency on the random codec)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738727/LUCENE-6529.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6551", "change_description": ": Add missing ConcurrentMergeScheduler.getAutoIOThrottle\ngetter", "change_title": "Add CMS.getAutoIOThrottle getter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Currently you can enable and disable CMS's auto IO throttle but you can't get it!  Silly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739080/LUCENE-6551.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6552", "change_description": ": Add MergePolicy.OneMerge.getMergeInfo and rename\nsetInfo to setMergeInfo", "change_title": "Add OneMerge.getMergeInfo", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "So you can get the SegmentCommitInfo for the newly merged segment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739086/LUCENE-6552.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6525", "change_description": ": Deprecate IndexWriterConfig's writeLockTimeout.", "change_title": "Deprecate IndexWriterConfig's write lock timeout", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Followup from LUCENE-6508 We should ultimately remove this parameter, it is just sugar over a \"sleeping lock factory\" today that sleeps and retries until timeout, like the old code. But really if you want a lock that blocks until its obtained, you can simply specify the sleeping lock factory yourself (and have more control over what it does!), or maybe an NIO implementation based on the blocking FileChannel.lock() or something else. So this stuff should be out of indexwriter and not baked into our APIs. I would like to: 1) deprecate this, mentioning to use the sleeping factory instead 2) change default of deprecated timeout to 0, so you only sleep if you ask. I am not really sure if matchVersion can be used, because today the default itself is also settable with a static setter <-- OVERENGINEERED", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739428/LUCENE-6525.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6466", "change_description": ": Moved SpanQuery.getSpans() and .extractTerms() to SpanWeight", "change_title": "Move SpanQuery.getSpans() to SpanWeight", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "SpanQuery.getSpans() should only be called on rewritten queries, so it seems to make more sense to have this being called from SpanWeight", "patch_link": "https://issues.apache.org/jira/secure/attachment/12736170/LUCENE-6466-2.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6371", "change_description": ",", "change_title": "Improve Spans payload collection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Spin off from LUCENE-6308, see the comments there from around 23 March 2015.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739606/LUCENE-6371-5x.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6490", "change_description": ",", "change_title": "TestPayloadNearQuery fails with NPE", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "ant test  -Dtestcase=TestPayloadNearQuery -Dtests.method=test -Dtests.seed=24743B1132665845 -Dtests.slow=true -Dtests.locale=es_NI -Dtests.timezone=Israel -Dtests.asserts=true -Dtests.file.encoding=US-ASCII", "patch_link": "https://issues.apache.org/jira/secure/attachment/12733923/LUCENE-6490.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6583", "change_description": ": FilteredQuery is deprecated and will be removed in 6.0. It should\nbe replaced with a BooleanQuery which handle the query as a MUST clause and\nthe filter as a FILTER clause.", "change_title": "Remove FilteredQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Now that BooleanQuery can handle filters, FilteredQuery should be removed in trunk and deprecated in 5.x.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740369/LUCENE-6583.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6553", "change_description": ": The postings, spans and scorer APIs no longer take an acceptDocs\nparameter. Live docs are now always checked on top of these APIs.", "change_title": "Simplify how we handle deleted docs in read APIs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Today, all scorers and postings formats need to be able to handle deleted documents. I suspect that the reason is that we want to be able to make sure to not perform costly operations on documents that are deleted. For instance if you run a phrase query, reading positions on a document which is deleted is useless. I suspect this is also a source of inefficiencies since in some cases we apply deleted documents several times: for instance conjunctions apply deleted docs to every sub scorer. However, with the new two-phase iteration API, we have a way to make sure that we never run expensive operations on deleted documents: we could first iterate over the approximation, then check that the document is not deleted, and finally confirm the match. Since approximations are cheap, applying deleted docs after them would not be an issue. I would like to explore removing the \"Bits acceptDocs\" parameter from TermsEnum.postings, Weight.scorer, SpanWeight.getSpans and Weight.BulkScorer, and add it to BulkScorer.score. This way, bulk scorers would be the only API which would need to know how to apply deleted docs, which I think would be more manageable since we only have 3 or 4 impls. And DefaultBulkScorer would be implemented the way described above: first advance the approximation, then check deleted docs, then confirm the match, then collect. Of course that's only in the case the scorer supports approximations, if it does not, it means it is cheap so we can directly iterate the scorer and check deleted docs on top.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12741334/LUCENE-6553.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6634", "change_description": ": PKIndexSplitter now takes a Query instead of a Filter to decide\nhow to split an index.", "change_title": "PKIndexSplitter should take queries, not filters", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "We could make PKIndexSplitter work on the Query API instead of Filter. This way lucene/misc would not depend on oal.search.Filter anymore.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742529/LUCENE-6634.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6643", "change_description": ": GroupingSearch from lucene/grouping was changed to take a Query\nobject to define groups instead of a Filter.", "change_title": "Remove dependency of lucene/grouping on oal.search.Filter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Given that Filter is on the way out, we should use the Query API instead of Filter in the grouping module.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742825/LUCENE-6643.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6554", "change_description": ": ToParentBlockJoinFieldComparator was removed because of a bug\nwith missing values that could not be fixed. ToParentBlockJoinSortField now\nworks with string or numeric doc values selectors. Sorting on anything else\nthan a string or numeric field would require to implement a custom selector.", "change_title": "ToBlockJoinFieldComparator wrapping is illegal", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "The following test case triggers an AssertionError: The reason is that when a parent document does not have children, ToParentBlockJoinComparator simply omits to forward calls to copy to the wrapped comparator. So the wrapped comparator ends up with allocated slots that have 0 as an ordinal (the default value in an array) and a null value, which is illegal since 0 is a legal ordinal which can't map to null. This can't be fixed without adding new methods to the already crazy comparator API, so I think there is nothing we can do but remove this comparator. It would be possible to achieve the same functionnality by implementing something similar to SortedNumericSelector, except that it would have to select across several docs instead of values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742526/LUCENE-6554.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6648", "change_description": ": All lucene/facet APIs now take Query objects where they used to\ntake Filter objects.", "change_title": "Remove dependency of lucene/facet on oal.search.Filter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Given that Filter is going away, we should stop using it in the facet module.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743017/LUCENE-6648.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6640", "change_description": ": Suggesters now take a BitsProducer object instead of a Filter\nobject to reduce the scope of doc IDs that may be returned, emphasizing the\nfact that these objects need to support random-access.", "change_title": "Remove dependency of lucene/suggest on oal.search.Filter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "oal.search.Filter is on the way out, yet the suggest module is still using it in order to filter suggestions and it can't be replaced with queries given that true (out-of-order) random access is required.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743094/LUCENE-6640.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6646", "change_description": ": Make EarlyTerminatingCollector take a Sort object directly\ninstead of a SortingMergePolicy.", "change_title": "make the EarlyTerminatingSortingCollector constructor SortingMergePolicy-free", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "motivation and summary of proposed changes to follow via github pull request", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6649", "change_description": ": BitDocIdSetFilter and BitDocIdSetCachingWrapperFilter are now\ndeprecated in favour of BitSetProducer and QueryBitSetProducer, which do not\nextend oal.search.Filter.", "change_title": "Remove dependency of lucene/join on oal.search.Filter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Similarly to other modules, lucene/join should not use Filter anymore.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743112/LUCENE-6649.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6607", "change_description": ": Factor out geo3d into its own spatial3d module.", "change_title": "Move geo3d to Lucene's sandbox module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Geo3d is a powerful low-level geo API, recording places on the earth's surface in the index in three dimensions (as 3 separate numbers) and offering fast shape intersection/distance testing at search time. daddywri originally contributed this in LUCENE-6196, and we put it in spatial module, but I think a more natural place for it, for now anyway, is Lucene's sandbox module: it's very new, its APIs/abstractions are very much in flux (and the higher standards for abstractions in the spatial module cause disagreements: LUCENE-6578), daddywri and others could iterate faster on changes in sandbox, etc. This would also un-block issues like LUCENE-6480, allowing GeoPointField and BKD trees to also use geo3d.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12744893/LUCENE.6607.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6531", "change_description": ": PhraseQuery is now immutable and can be built using the\nPhraseQuery.Builder class.", "change_title": "Make PhraseQuery immutable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Mutable queries are an issue for automatic filter caching since modifying a query after it has been put into the cache will corrupt the cache. We should make all queries immutable (up to the boost) to avoid this issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738629/LUCENE-6531.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6570", "change_description": ": BooleanQuery is now immutable and can be built using the\nBooleanQuery.Builder class.", "change_title": "Make BooleanQuery immutable", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "In the same spirit as LUCENE-6531 for the PhraseQuery, we should make BooleanQuery immutable. The plan is the following: I would also like to add some static utility methods for common use-cases of this query, for instance: Hopefully this will help keep tests not too verbose, and the latter will also help with the FilteredQuery derecation/removal.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740140/LUCENE-6570.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6702", "change_description": ": NRTSuggester: Add a method to inject context values at index time\nin ContextSuggestField. Simplify ContextQuery logic for extracting contexts and\nadd dedicated method to consider all context values at query time.", "change_title": "[suggest] Make Context Query and Field extensible", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "5.3,6.0", "detail_description": "ContextSuggestField indexes context information along with  suggestions, which can be used to filter and/or boost suggestions using  ContextQuery. It would be useful to make ContextSuggestField extensible such that subclasses  can inject context values at index-time, without having to specify the  contexts in its ctor. ContextQuery can be made extensible by allowing sub-classes to override how context automaton is created from provided query contexts.  Currently, ContextQuery uses a context value of \"*\" to consider all context values,  It makes sense to have a dedicated addAllContexts() instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12747898/LUCENE-6702.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "API Changes", "change_id": "LUCENE-6719", "change_description": ": NumericUtils getMinInt, getMaxInt, getMinLong, getMaxLong now\nreturn null if there are no terms for the specified field, previously these\nmethods returned primitive values and raised an undocumented NullPointerException\nif there were no terms for the field.", "change_title": "NumericUtils.getMinLong and NumericUtils.getMaxLong have undefined behavior when no docs have value - throw NPE", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Tracked down a possible cause of SOLR-7866 to situations where a (numeric) field doesn't have any values in an index and you try to get the min/max. javadocs for NumericUtils.getMinLong and NumericUtils.getMaxLong don't actually say what that method will do in this case, throw NPE when it happens", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748967/LUCENE-6719.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6500", "change_description": ": ParallelCompositeReader did not always call\nclosed listeners. This was fixed by", "change_title": "ParallelCompositeReader does not always call closed listeners", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "CompositeParallelReader misses to call closed listeners when the reader which is provided at construction time does not wrap leaf readers directly, such as a multi reader over directory readers.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735278/LUCENE-6500.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6501", "change_description": ": ParallelCompositeReader did not always call\nclosed listeners. This was fixed by", "change_title": "Flatten subreader structure in ParallelCompositeReader", "detail_type": "Improvement", "detail_affect_versions": "5.2", "detail_fix_versions": "5.3,6.0", "detail_description": "The current implementation of ParallelCompositeReader reassembles the whole subreader structure of the wrapped reader with ParallelLeafReader and ParallelCompositeReader. This leads to bugs like described in LUCENE-6500. This reaches back to the time when this reader was reimplemented for the first time shortly before release of 4.0. Shortly afterwards, we completely changed our search infrastructure to just call leaves() and working with them. The method getSequentialSubReaders was made protected, just to be implemented by subclasses (like this one). But no external code can ever call it. Also the search API just rely on the baseId in relation to the top-level reader (to correctly present document ids). The structure is completely unimportant. This issue will therefore simplify ParallelCompositeReader to just fetch all LeafReaders and build a flat structure of ParallelLeafReaders from it. This also has the nice side-effect, that only the parallel leaf readers must be equally sized, not their structure. This issue will solve LUCENE-6500 as a side effect. I just opened a new issue for discussion and to have this listed as \"feature\" and not bug. In general, we could also hide the ParallelLeafReader class and make it an implementation detail. ParallelCompositeReader would be the only entry point -> because people could pass any IndexReader structure in, a single AtomicReader would just produce a CompositeReader with one leaf. We could then also rename it back to ParallelReader (like it was in pre Lucene4).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735383/LUCENE-6501.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6520", "change_description": ": Geo3D GeoPath.done() would throw an NPE if adjacent path\nsegments were co-linear.", "change_title": "Geo3D GeoPath: co-linear end-points result in NPE", "detail_type": "Bug", "detail_affect_versions": "5.2", "detail_fix_versions": "5.3", "detail_description": "FAILED:  org.apache.lucene.spatial.spatial4j.Geo3dRptTest.testOperations daddywri says: This is happening because the endpoints that define two path segments are co-linear.  There's a check for that too, but clearly it's not firing properly in this case for some reason.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12737447/LUCENE-6520.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-5805", "change_description": ": QueryNodeImpl.removeFromParent was doing nothing in a\ncostly manner", "change_title": "QueryNodeImpl.removeFromParent does a lot of work without any effect", "detail_type": "Bug", "detail_affect_versions": "4.7.2,4.9", "detail_fix_versions": "5.3,6.0", "detail_description": "The method removeFromParent of QueryNodeImpl, calls getChildren on the parent and removes any occurrence of \"this\" from the result. However, since a few releases, getChildren returns a copy of the children list, so the code has no effect (except creating a copy of the children list which will then be thrown away).  Even worse, since setChildren calls removeFromParent on any previous child, setChildren now has a complexity of O(n^2) and creates a lot of throw-away copies of the children list (for nodes with a lot of children)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12737395/LUCENE-5805.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6533", "change_description": ": SlowCompositeReaderWrapper no longer caches its live docs\ninstance since this can prevent future improvements like a\ndisk-backed live docs", "change_title": "SlowCompositeReaderWrapper is caching an AssertingBits instance", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "I hit this curious failure in the new TestGeoPointQuery:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738772/LUCENE-6533.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6558", "change_description": ": Highlighters now work with CustomScoreQuery", "change_title": "Highlighter not working for CustomScoreQuery", "detail_type": "Bug", "detail_affect_versions": "5.2", "detail_fix_versions": "5.3,6.0", "detail_description": "Highlighter and FastVectorHighlighter not working for CustomScoreQuery", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739195/LUCENE-6558.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6560", "change_description": ": BKDPointInBBoxQuery now handles \"dateline crossing\"\ncorrectly", "change_title": "Handle \"crosses dateline\" cases in BKDPointInBBoxQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Just like LUCENE-6547 but for BKD bbox queries ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739263/LUCENE-6560.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6564", "change_description": ": Change PrintStreamInfoStream to use thread safe Java 8\nISO-8601 date formatting (in Lucene 5.x use Java 7 FileTime#toString\nas workaround); fix output of tests to use same format.", "change_title": "Fix thread safety in PrintStreamInfoStream, unify logging format with tests", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Noticed while debugging some IW output in an unit test that milliseconds were not output in the date, changed this to reuse the date format used by PrintStreamInfoStream.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739665/LUCENE-6564-5x.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6593", "change_description": ": Fixed ToChildBlockJoinQuery's scorer to not refuse to advance\nto a document that belongs to the parent space.", "change_title": "ToChildBlockJoinScorer should not fail when advanced on a parent document", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "ToChildBlockJoinScorer fails if you advance it on a parent document. While this was fine if you wrapped it in a FilteredQuery, it is not if you wrap it in a BooleanQuery because of its approximation support: it can be advanced to a document that has been returned by the approximation of another clause but not confirmed yet. So ToChildBlockJoinScorer should accept any valid doc ID as a target.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740704/LUCENE-6593.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6591", "change_description": ": Never write a negative vLong", "change_title": "Never write negative vLongs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Today we only assert that the incoming argument to writeVLong is non-negative; I think this is quite important and should be a real check?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740676/LUCENE-6591.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6588", "change_description": ": Fix how ToChildBlockJoinQuery deals with acceptDocs.", "change_title": "ToChildBlockJoinQuery does not calculate parent score if the first child is not in acceptDocs", "detail_type": "Bug", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3", "detail_description": "There is a bug in ToChildBlockJoinQuery that causes the score calculation to be skipped if the first child of a new parent doc is not in acceptDocs. I will attach test showing the failure and a patch to fix it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740634/0003-implements-ToChildBlockJoinQuery.explain.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6597", "change_description": ": Geo3D's GeoCircle now supports a world-globe diameter.", "change_title": "Geo3d circle creation that covers whole globe throws an IllegalArgumentException", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "The following GeoCircle construction: ... fails as follows: The reason is that the plane normal vector cannot be computed in that case.  A special case is warranted for circles that cover the whole globe.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12741443/LUCENE-6597.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6608", "change_description": ": Fix potential resource leak in BigramDictionary.", "change_title": "Potential resource leak in BigramDictionary.java", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "None", "detail_description": "The input and output object streams are being closed in the try block. These resources will not be closed if an exception occurs in the try block We can use the finally block to explicitly close these resources or use the new try-with-resources construct where they are implicitly closed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6614", "change_description": ": Improve partition detection in IOUtils#spins() so it\nworks with NVMe drives.", "change_title": "IOUtils.spins doesn't work for NVMe drives", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "NVMe is the faster (than AHCI) protocol for newer SSDs that plug into the PCIE bus. I just built a new beast box with one of these drives, and the partition is named /dev/nvme0n1p1 while the device is /dev/nvme0n1 by Linux - this also appears in /sys/block with rotational=0. I think steve_rowe also has an NVME drive ... thetaphi (who got the box working for me: thank you!!!) has ideas on how to fix it!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742130/LUCENE-6614.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6586", "change_description": ": Fix typo in GermanStemmer, causing possible wrong value\nfor substCount.", "change_title": "There is a typo in GermanStemmer that can lead to wrong stemming", "detail_type": "Bug", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3,6.0", "detail_description": "There is a small typo in GermanStemmer that leads to a wrong calclulation of the substCount in line 203: should be I created a Pull Request for this some time ago, but it was apprently overlooked: https://github.com/apache/lucene-solr/pull/141", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6658", "change_description": ": Fix IndexUpgrader to also upgrade indexes without any\nsegments.", "change_title": "IndexUpgrader doesn't upgrade an index if it has zero segments", "detail_type": "Bug", "detail_affect_versions": "4.10.4,5.2.1", "detail_fix_versions": "4.10.5,5.3,6.0", "detail_description": "IndexUpgrader uses merges to do its job. Therefore, if you use it to upgrade an index with no segments, it will do nothing - it won't even update the version numbers in the segments file, meaning that later versions of Lucene will fail to open the index, despite the fact that you \"upgraded\" it. The suggested workaround when this was raised on the mailing list in January seems to be to use filesystem magic to look at the files, figure out whether there are any segments, and write a new empty index if there are none. This sounds easy, but there are probably traps. For instance, there might be files in the directory which don't really belong to the index. Earlier versions of Lucene used to have a FilenameFilter which was usable to distinguish one from the other, but that seems to have disappeared, making it less obvious how to do this. This issue is presumed to exist in 3.x as well, I just haven't encountered it yet because the only empty indices I have hit have been later versions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743728/LUCENE-6658.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6677", "change_description": ": QueryParserBase fails to enforce maxDeterminizedStates when\ncreating a WildcardQuery", "change_title": "QueryParserBase ignores maxDeterminizedStates when creating a wildcard query", "detail_type": "Bug", "detail_affect_versions": "4.10.3,6.0", "detail_fix_versions": "5.3,6.0", "detail_description": "I think that QueryParserBase should construct the WildcardQuery with the provided maxDeterminizedStates and not the default one.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12745280/LUCENE-6677.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6680", "change_description": ": Preserve two suggestions that have same key and weight but\ndifferent payloads", "change_title": "BlendedInfixSuggester dedup bug", "detail_type": "Bug", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3,6.0", "detail_description": "I expect the following test to pass, but it's failing in the latest Lucene 5.2.1: This test is failing because the BlendedInfixSuggester internally uses a TreeSet for storing the results and the corresponding Comparator only uses text+weight meaning that results with different payloads are collapsed into one. mikemccand, The idea here is that if two ingested documents have the same title and weight, but different payloads, then they are two different things and folding them into a single document would mean loosing the payload information.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12745464/LUCENE-6680.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6681", "change_description": ": SortingMergePolicy must override MergePolicy.size(...).", "change_title": "SortingMergePolicy to override MergePolicy.size(...)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "github pull request to follow", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6682", "change_description": ": StandardTokenizer performance bug: scanner buffer is\nunnecessarily copied when maxTokenLength doesn't change.  Also stop silently\nmaxing out buffer size (and effectively also max token length) at 1M chars,\nbut instead throw an exception from setMaxTokenLength() when the given\nlength is greater than 1M chars.", "change_title": "StandardTokenizer performance bug: buffer is unnecessarily copied when maxTokenLength doesn't change", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "From Piotr Idzikowski on java-user mailing list http://markmail.org/message/af26kr7fermt2tfh: I am developing own analyzer based on StandardAnalyzer. I realized that tokenizer.setMaxTokenLength is called many times. Does it make sense if length stays the same? I see it finally calls this one( in StandardTokenizerImpl ): So it just copies old array content into the new one.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6696", "change_description": ": Fix FilterDirectoryReader.close() to never close the\nunderlying reader several times.", "change_title": "FilterDirectoryReader.doClose() should call in.close() not in.doClose()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "FilterDirectoryReader.doClose() calls in.doClose(). This is wrong because if you call close() on both the underlying reader and a wrapper around it, then doClose() will have been called several times, which will break ref counting. Instead, FilterDirectoryReader.doClose() should call in.close() so that it is a no-op if the underlying reader is already closed, or so that calling close() on the underlying reader afterwards will be a no-op.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12747023/LUCENE-6696.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6334", "change_description": ": FastVectorHighlighter failed to highlight phrases across\nmore than one value in a multi-valued field.", "change_title": "Fast Vector Highlighter does not properly span neighboring term offsets", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "If you are using term vectors for fast vector highlighting along with a multivalue field while matching a phrase that crosses two elements, then it will not properly highlight even though it properly finds the correct values to highlight. A good example of this is when matching source code, where you might have lines like: Matching the phrase \"four five\" will return However, it does not properly highlight \"four\" (on the first line) and \"five\" (on the second line) and it is returning too many lines, but not all of them. The problem lies in the BaseFragmentsBuilder at line 269 because it is not checking for cross-coverage. Here is a possible solution:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12747605/LUCENE-6334.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6704", "change_description": ": GeoPointDistanceQuery was visiting too many term ranges,\nconsuming too much heap for a large radius", "change_title": "GeoPointInBBox/Distance queries can throw OOME", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "After investigating LUCENE-6685 the performance issues and improvements related to GeoPointInBBox/Distance queries could be categorized into two separate issues: 1. OOME caused by an unnecessary number of ranges computed for Point Distance Queries (bug in the GeoPointTermEnum base class where the bounding box was used for intersections instead of the point radius) 2. API improvements providing configurable detail parameters. This issue addresses 1.  LUCENE-6685 will further investigate the need for 2 (after working 1 and correcting for unnecessary range detail, it looks like 2 may not be needed)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748242/LUCENE-6704.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "SOLR-5882", "change_description": ": fix ScoreMode.Min at ToParentBlockJoinQuery", "change_title": "Introduce score local parameter for {!parent} query parser", "detail_type": "New Feature", "detail_affect_versions": "4.8", "detail_fix_versions": "5.3", "detail_description": "I propose to have ability to configure scoring mode by optional local parameter Syntax for parent queries is: Child query enables scoring by default.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748372/SOLR-5882.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6718", "change_description": ": JoinUtil.createJoinQuery failed to rewrite queries before\ncreating a Weight.", "change_title": "GlobalOrdinalsQuery calls createWeight without rewriting first", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748692/LUCENE-6718.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6713", "change_description": ": TooComplexToDeterminizeException claims to be serializable\nbut wasn't", "change_title": "TooComplexToDeterminizeException claims to be serializable but actually isn't?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "This custom exception class, added in LUCENE-6046, claims to be Serializable since in inherits from Throwable yet if you try to serialize it you'll hit runtime exceptions because its members don't implement Serializable. We intentionally pushed Java serialization requirements out of Lucene a while back (LUCENE-2908), but maybe for custom exception classes which unfortunately necessarily claim to implement Serializable we need to do something? We could just mark the members transient here, but that would mean when you unserialize you get null members on the other end, e.g. you would no longer know which RegExp was problematic ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748675/LUCENE-6713.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6723", "change_description": ": Fix date parsing problems in Java 9 with date formats using\nEnglish weekday/month names.", "change_title": "Date field problems using ExtractingRequestHandler and java 9 (b71)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Tracking bug to note that the (Tika based) ExtractingRequestHandler will not work properly with jdk9 starting with build71. This first manifested itself with failures like this from the tests... Workarround noted by Uwe... The test passes on JDK 9 b71 with: -Dargs=\"-Djava.locale.providers=JRE,SPI\" This reenabled the old Locale data. I will add this to the build parameters of policeman Jenkins to stop this from failing. To me it looks like the locale data somehow is not able to correctly parse weekdays and/or timezones. I will check this out tomorrow and report a bug to the OpenJDK people. There is something fishy with CLDR locale data. There are already some bugs open, so work is not yet finished (e.g. sometimes it uses wrong timezone shortcuts,...)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748821/SOLR-7770.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Bug fixes", "change_id": "LUCENE-6618", "change_description": ": Properly set MMapDirectory.UNMAP_SUPPORTED when it is now allowed\nby security policy.", "change_title": "MmapDirectory checkUnmapSupported is buggy", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "If you ban this by policy, it should return false. Instead today all writes will fail by default.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742159/LUCENE-6618.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6501", "change_description": ": The subreader structure in ParallelCompositeReader\nwas flattened, because the current implementation had too many\nhidden bugs regarding refounting and close listeners.\nIf you create a new ParallelCompositeReader, it will just take\nall leaves of the passed readers and form a flat structure of\nParallelLeafReaders instead of trying to assemble the original\nstructure of composite and leaf readers.", "change_title": "Flatten subreader structure in ParallelCompositeReader", "detail_type": "Improvement", "detail_affect_versions": "5.2", "detail_fix_versions": "5.3,6.0", "detail_description": "The current implementation of ParallelCompositeReader reassembles the whole subreader structure of the wrapped reader with ParallelLeafReader and ParallelCompositeReader. This leads to bugs like described in LUCENE-6500. This reaches back to the time when this reader was reimplemented for the first time shortly before release of 4.0. Shortly afterwards, we completely changed our search infrastructure to just call leaves() and working with them. The method getSequentialSubReaders was made protected, just to be implemented by subclasses (like this one). But no external code can ever call it. Also the search API just rely on the baseId in relation to the top-level reader (to correctly present document ids). The structure is completely unimportant. This issue will therefore simplify ParallelCompositeReader to just fetch all LeafReaders and build a flat structure of ParallelLeafReaders from it. This also has the nice side-effect, that only the parallel leaf readers must be equally sized, not their structure. This issue will solve LUCENE-6500 as a side effect. I just opened a new issue for discussion and to have this listed as \"feature\" and not bug. In general, we could also hide the ParallelLeafReader class and make it an implementation detail. ParallelCompositeReader would be the only entry point -> because people could pass any IndexReader structure in, a single AtomicReader would just produce a CompositeReader with one leaf. We could then also rename it back to ParallelReader (like it was in pre Lucene4).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735383/LUCENE-6501.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6538", "change_description": ": Also include java.vm.version and java.runtime.version\nin per-segment diagnostics", "change_title": "Improve per-segment diagnostics for IBM J9 JVM", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Spinoff from http://lucene.markmail.org/thread/dq4wioomu4o346ej where I noticed that the per-segment diagnostics (seen from CheckIndex) only report 1.7.0 as the JVM version, without any update level. Talking to rcmuir it looks like we just need to add java.vm.version and java.runtime.version sysprops into the diagnostics.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738618/LUCENE-6538.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6537", "change_description": ": NearSpansOrdered no longer tries to minimize its\nSpan matches.  This means that the matching algorithm is entirely\nlazy.  All spans returned by the previous implementation are still\nreported, but matching documents may now also return additional\nspans that were previously discarded in preference to shorter\noverlapping ones.", "change_title": "Make NearSpansOrdered use lazy iteration", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12738653/LUCENE-6537.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6569", "change_description": ": Optimize MultiFunction.anyExists and allExists to eliminate\nexcessive array creation in common 2 argument usage", "change_title": "MultiFunction.anyExists - creating FunctionValues[] objects for every document", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "In the class org.apache.lucene.queries.function.valuesource.MultiFunction there is the following method signature (line 52) public static boolean allExists(int doc, FunctionValues... values) this method is called from the class org.apache.lucene.queries.function.valuesource.DualFloatFunction (line 68) public boolean exists(int doc) Because MultiFunction.allExists uses Java varargs syntax (\"...\") a new FunctionValues[] object will be created every time this call takes place. The problem is that the call takes place in a document level function, which means that it will create new objects in the heap for every document in the query results. for example if you use the following boost function (where ds and dc1 are both TrieDateField) bf=min(ms(ds,dc1),604800000) You will get extra objects created for each document in the result set, which has a big impact on performance and memory usage if you are searching a large result set.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739721/SOLR-7618.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-2880", "change_description": ": Span queries now score more consistently with regular queries.", "change_title": "SpanQuery scoring inconsistencies", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Spinoff of LUCENE-2879. You can see a full description there, but the gist is that SpanQuery sums up freqs with \"sloppyFreq\". However this slop is simply spans.end() - spans.start() For a SpanTermQuery for example, this means its scoring 0.5 for TF versus TermQuery's 1.0. As you can imagine, I think in practical situations this would make it difficult for SpanQuery users to really use SpanQueries for effective ranking, especially in combination with non-Spanqueries (maybe via DisjunctionMaxQuery, etc) The problem is more general than this simple example: for example SpanNearQuery should be consistent with PhraseQuery's slop.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740052/LUCENE-2880.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6601", "change_description": ": FilteredQuery now always rewrites to a BooleanQuery which handles\nthe query as a MUST clause and the filter as a FILTER clause.\nLEAP_FROG_QUERY_FIRST_STRATEGY and LEAP_FROG_FILTER_FIRST_STRATEGY do not\nguarantee anymore which iterator will be advanced first, it will depend on the\nrespective costs of the iterators. QUERY_FIRST_FILTER_STRATEGY and\nRANDOM_ACCESS_FILTER_STRATEGY still consume the filter using its random-access\nAPI, however the returned bits may be called on different documents compared\nto before.", "change_title": "Change FilteredQuery.FilterStrategy to use the two-phase iteration API", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "We could change FilterStrategy so that instead of being a factory of scorers, it would just rewrite filters in such a way that they can decide which of the iterator or random-access API should be used.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12741607/LUCENE-6601.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6542", "change_description": ": FSDirectory's ctor now works with security policies or file systems\nthat restrict write access.", "change_title": "FSDirectory throws AccessControlException unless you grant write access to the index", "detail_type": "Bug", "detail_affect_versions": "5.1", "detail_fix_versions": "5.3,6.0", "detail_description": "Hit this during my attempted upgrade to Lucene 5.1.0. (Yeah, I know 5.2.0 is out, and we'll be using that in production anyway, but the merge takes time.) Various tests of ours test Directory stuff against methods which the security policy won't allow tests to write to. Changes in FSDirectory mean that it now demands write access to the directory. 4.10.4 permitted read-only access.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742973/LUCENE-6542.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6651", "change_description": ": The default implementation of AttributeImpl#reflectWith(AttributeReflector)\nnow uses AccessControler#doPrivileged() to do the reflection. Please consider\nimplementing this method in all your custom attributes, because the method will be\nmade abstract in Lucene 6.", "change_title": "Remove private field reflection (setAccessible) in AttributeImpl#reflectWith", "detail_type": "Improvement", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3,6.0", "detail_description": "In AttributeImpl we currently have a \"default\" implementation of reflectWith (which is used by toString() and other methods) that uses reflection to list all private fields of the implementation class and reports them to the AttributeReflector (used by Solr and Elasticsearch to show analysis output). Unfortunately this default implementation needs to access private fields of a subclass, which does not work without doing Field#setAccessible(true). And this is done without AccessController#doPrivileged()! There are 2 solutions to solve this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743343/LUCENE-6651-5x.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6639", "change_description": ": LRUQueryCache and CachingWrapperQuery now consider a query as\n\"used\" when the first Scorer is pulled instead of when a Scorer is pulled on\nthe first segment on an index.", "change_title": "LRUQueryCache.CachingWrapperWeight not calling policy.onUse() if the first scorer is skipped", "detail_type": "Bug", "detail_affect_versions": "5.3", "detail_fix_versions": "5.3", "detail_description": "The method org.apache.lucene.search.LRUQueryCache.CachingWrapperWeight.scorer(LeafReaderContext) starts with which can result in a missed call for queries that return a null scorer for the first segment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742602/LUCENE-6639.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6579", "change_description": ": IndexWriter now sacrifices (closes) itself to protect the index\nwhen an unexpected, tragic exception strikes while merging.", "change_title": "Unexpected merge exceptions should be tragic to IndexWriter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Today our behavior is weird: we will fail the merge (which is running in a background thread if you are using the default CMS), pause for 1.0 seconds, and then the next chance we get, kick off the merge again. I think this is a poor default, e.g. on disk full we will just keep \"trying\" and filling up disk again, wasting IO/CPU. I think IW should declare this a tragedy instead?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12744541/LUCENE-6579.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6691", "change_description": ": SortingMergePolicy.isSorted now considers FilterLeafReader instances.\nEarlyTerminatingSortingCollector.terminatedEarly accessor added.\nTestEarlyTerminatingSortingCollector.testTerminatedEarly test added.", "change_title": "tweak SortingMergePolicy.getSortDescription", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Building tests for SOLR-5730 identified that early termination can be omitted for sorted segments because the LeafReader concerned is not a SegmentReader - github pull request to illustrate to follow: LUCENE-6065 \"remove \"foreign readers\" from merge, fix LeafReader instead.\" also concerns SortingMergePolicy.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6609", "change_description": ": Add getSortField impls to many subclasses of FieldCacheSource which return\nthe most direct SortField implementation.  In many trivial sort by ValueSource usages, this\nwill result in less RAM, and more precise sorting of extreme values due to no longer\nconverting to double.", "change_title": "FieldCacheSource (or it's subclasses) should override getSortField", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "ValueSource defines the following method... ...where ValueSourceSortField builds up a ValueSourceComparator containing a double[] based on the FunctionValues of the original ValueSource. meanwhile, the abstract FieldCacheSource exists as a base implementation for classes like IntFieldSource and DoubleFieldSource which wrap a ValueSource around DocValues for the specified field. But neither FieldCacheSource nor any of it's subclasses override the getSortField(boolean) method – so attempting to sort on something like an IntFieldSource winds up using a bunch of ram to build that double[] to give users a less accurate sort (because of casting) then if they just sorted directly on the field. is there any good reason why FieldCacheSource subclases like IntFieldSource shouldn't all override getSortField with something like... ?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748549/LUCENE-6609.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6548", "change_description": ": Some optimizations for BlockTree's intersect with very\nfinite automata", "change_title": "Optimize BlockTree's Terms.intersect a bit for \"very finite\" automata", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "I've been digging into why BlockTree's Terms.intersect impl is slower for a \"very finite\" union-of-terms test over random IDs (LUCENE-3893) and I found a few performance improvements that at least for that one use case gets a ~11% speedup.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739504/LUCENE-6548.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6585", "change_description": ": Flatten conjunctions and conjunction approximations into\nparent conjunctions. For example a sloppy phrase query of \"foo bar\"~5\nwith a filter of \"baz\" will internally leapfrog foo,bar,baz as one\nconjunction.", "change_title": "Make ConjunctionDISI flatten sub ConjunctionDISI instances", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Today ConjunctionDISI wraps some sub (two-phase) iterators. I would like to improve it by flattening sub iterators when they implement ConjunctionDISI. In practice, this would make \"+A +(+B +C)\" be executed more like \"+A +B +C\" (only in terms of matching, scoring would not change). My motivation for this is that if we don't flatten and are unlucky, we can sometimes hit some worst cases. For instance consider the 3 following postings lists (sorted by increasing cost): A: 1, 1001, 2001, 3001, ... C: 0, 2, 4, 6, 8, 10, 12, 14, ... B: 1, 3, 5, 7, 9, 11, 13, 15, ... If we run \"+A +B +C\", then everything works fine, we use A as a lead, and advance B 1000 by 1000 to find the next match (if any). However if we run \"+A +(+B +C)\", then we would iterate B and C 2 by 2 over the entire doc ID space when trying to find the first match which occurs on or after A:1. This is an extreme example which is unlikely to happen in practice, but flattening would also help a bit on some more common cases. For instance imagine that A, B and C have respective costs of 100, 10 and 1000. If you search for \"+A +(+B +C)\", then we will use the most costly iterator (C) to confirm matches of B (the least costly iterator, used as a lead) while it would have been more efficient to confirm matches of B with A first, since A is less costly than C.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12741761/LUCENE-6585.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6325", "change_description": ": Reduce RAM usage of FieldInfos, and speed up lookup by\nnumber, by using an array instead of TreeMap except in very sparse\ncases", "change_title": "improve perf and memory of FieldInfos.fieldInfo(int)", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "FieldInfos.fieldInfo(int) looks up a field by number and returns its FieldInfo. This method is called per-field-per-doc in things like stored fields and vectors readers. Unfortunately, today this method is always backed by a TreeMap. In most cases a simple array is better, its faster and uses less memory. These changes made significant difference in stored fields checkindex time with my test index (had only 10 fields). Maybe it helps merge as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742134/LUCENE-6325.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6617", "change_description": ": Reduce heap usage for small FSTs", "change_title": "Reduce FST's ram usage", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Spinoff from LUCENE-6199, pulling out just the FST RAM reduction changes. The FST data structure tries to be a RAM efficient representation of a sorted map, but there are a few things I think we can do to trim it even more:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742153/LUCENE-6617.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6616", "change_description": ": IndexWriter now lists the files in the index directory\nonly once on init, and IndexFileDeleter no longer suppresses\nFileNotFoundException and NoSuchFileException.  This also improves\nIndexFileDeleter to delete segments_N files last, so that in the\npresence of a virus checker, the index is never left in a state\nwhere an expired segments_N references non-existing files", "change_title": "IndexWriter should list files once on init, and IFD should not suppress FNFE", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Some nice ideas rcmuir had for cleaning up IW/IFD on init ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743602/LUCENE-6616.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6645", "change_description": ": Optimized the way we merge postings lists in multi-term queries\nand TermsQuery. This should especially help when there are lots of small\npostings lists.", "change_title": "BKD tree queries should use BitDocIdSet.Builder", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "When I was iterating on BKD tree originally I remember trying to use this builder (which makes a sparse bit set at first and then upgrades to dense if enough bits get set) and being disappointed with its performance. I wound up just making a FixedBitSet every time, but this is obviously wasteful for small queries. It could be the perf was poor because I was always .or'ing in DISIs that had 512 - 1024 hits each time (the size of each leaf cell in the BKD tree)?  I also had to make my own DISI wrapper around each leaf cell... maybe that was the source of the slowness, not sure. I also sort of wondered whether the SmallDocSet in spatial module (backed by a SentinelIntSet) might be faster ... though it'd need to be sorted in the and after building before returning to Lucene.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12745437/LUCENE-6645-spatial.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6668", "change_description": ": Optimized storage for sorted set and sorted numeric doc values\nin the case that there are few unique sets of values.", "change_title": "Optimize SortedSet/SortedNumeric storage for the few unique sets use-case", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Robert suggested this idea: if there are few unique sets of values, we could build a lookup table and then map each doc to an ord in this table, just like we already do for table compression for numerics. I think this is especially compelling given that SortedSet/SortedNumeric are our two only doc values types that use O(maxDoc) memory because of the offsets map. When this new strategy is used, memory usage could be bounded to a constant.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12744500/LUCENE-6668.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6690", "change_description": ": Sped up MultiTermsEnum.next() on high-cardinality fields.", "change_title": "Speed up MultiTermsEnum.next()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "OrdinalMap is very useful when computing top terms on a multi-index segment. However I've seen it being occasionally slow to build, which was either making facets (when the ordinals map is computed lazily) or reopen (when computed eagerly) slow. So out of curiosity, I tried to profile ordinal map building on a simple index: 10M random strings of length between 0 and 20 stored as a SORTED doc values field. The index has 19 segments. The bottleneck was MultiTermsEnum.next() (by far) due to lots of BytesRef comparisons (UTF8SortedAsUnicodeComparator). MultiTermsEnum stores sub enums in two different places: A non-exhausted enum is in exactly one of these data-structures. When moving to the next term, MultiTermsEnum advances all enums in top, then adds them to queue and finally, pops all enum that are on the same term back into top. We could save reorderings of the priority queue by not removing entries from the priority queue and then calling updateTop to advance enums which are on the current term. This is already what we do for disjunctions of doc IDs in DISIPriorityQueue. On the index described above and current trunk, building an OrdinalMap has to call UTF8SortedAsUnicodeComparator.compare 80114820 times and runs in 1.9 s. With the change, it calls UTF8SortedAsUnicodeComparator.compare 36900694 times, BytesRef.equals 16297638 times and runs in 1.4s (~26% faster).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12746127/LUCENE-6690.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Optimizations", "change_id": "LUCENE-6621", "change_description": ": Removed two unused variables in analysis/stempel/src/java/org/\negothor/stemmer/Compile.java", "change_title": "two unused variables in analysis/stempel/src/java/org/egothor/stemmer/Compile.java", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "5.3,6.0", "detail_description": "In the file Compile.java, the variables stems and words are unused. Although words gets incremented further in the file, it does not get referenced or used elsewhere. stems is neither incremented nor used elsewhere in the project. Are these variables redundant?", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Build", "change_id": "LUCENE-6518", "change_description": ": Don't report false thread leaks from IBM J9\nClassCache Reaper in test framework.", "change_title": "\"classcache reaper\" needs exemption from thread leaks", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "This happens on IBM JDK (some system thread or what have you). its 100% reproducible if you just cd lucene/analysis/smartcn and run 'ant test': we time out waiting for the thread to die and fail like this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12737445/LUCENE-6518.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Build", "change_id": "LUCENE-6567", "change_description": ": Simplify payload checking in SpanPayloadCheckQuery", "change_title": "No need for out-of-order payload checks in SpanPayloadCheckQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Since LUCENE-6537, all composite Spans implementations collect their payloads in-order, so we don't need special logic for that case anymore.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739611/LUCENE-6567.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Build", "change_id": "LUCENE-6568", "change_description": ": Make rat invocation depend on ivy configuration being set up", "change_title": "rat-sources-typedef doesn't respect ivy configuration", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "rat-sources-typedef invoked as a part of precommit doesn't respect settings in lucene/ivy-settings.xml", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Build", "change_id": "LUCENE-6683", "change_description": ": ivy-fail goal directs people to non-existent page", "change_title": "ivy-fail goal directs people to non-existant page", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The ivy-fail goal suggests going to http://wiki.apache.org/lucene-java/HowToContribute#antivy which doesn't have a section on ant/ivy anymore. The last revision to have this was #41, in 2013. http://wiki.apache.org/lucene-java/HowToContribute?action=recall&rev=41", "patch_link": "https://issues.apache.org/jira/secure/attachment/12745838/LUCENE-6683.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Build", "change_id": "LUCENE-6693", "change_description": ": Updated Groovy to 2.4.4, Pegdown to 1.5, Svnkit to 1.8.10.\nAlso fixed some PermGen errors while running full build caused by\nthese updates: Tasks are now installed from root's build.xml.", "change_title": "Permgen errors in 5.x on Jenkins builds with JDK 1.7", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Since I updated Groovy and other tools, 5.x builds fail with permgen errors in Jenkins. During the build, Groovy (which is large) is loaded three times and this sums up. See Revision: 1692103 I reverted the Groovy update in 5.x for now. The fix is to make the top-level build.xml also load common-build.xml and resolve groovy before the build starts.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12746816/LUCENE-6693.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Build", "change_id": "LUCENE-6741", "change_description": ": Fix jflex files to regenerate the java files correctly.", "change_title": "Ant regenerate fails to be no-op on 5.x", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,5.4,6.0", "detail_description": "When running \"ant regenerate\" in analysis/common, it creates java files that do not even compile.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Test Framework", "change_id": "LUCENE-6637", "change_description": ": Fix FSTTester to not violate file permissions\non -Dtests.verbose=true.", "change_title": "FSTTester writes to CWD if -Dtests.verbose failing with AccessControlException(FilePermission)", "detail_type": "Bug", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3,6.0", "detail_description": "This was found by Mesbah (M.) Alam who is testing our stuff with IBM J9. He ran tests with -Dtests.verbose=true, hitting this bug in FSTTester. I would like to remove the whole toDot() stuff there - or pass it to System.out instead. Another alternative would be to write it to FST's working dir (dir.createOutput; but unfortunately this is no OutputStream or Writer...) Any opinions?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742612/LUCENE-6637.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Test Framework", "change_id": "LUCENE-6542", "change_description": ": LuceneTestCase now has runWithRestrictedPermissions() to run\nan action with reduced permissions. This can be used to simulate special\nenvironments (e.g., read-only dirs). If tests are running without a security\nmanager, an assume cancels test execution automatically.", "change_title": "FSDirectory throws AccessControlException unless you grant write access to the index", "detail_type": "Bug", "detail_affect_versions": "5.1", "detail_fix_versions": "5.3,6.0", "detail_description": "Hit this during my attempted upgrade to Lucene 5.1.0. (Yeah, I know 5.2.0 is out, and we'll be using that in production anyway, but the merge takes time.) Various tests of ours test Directory stuff against methods which the security policy won't allow tests to write to. Changes in FSDirectory mean that it now demands write access to the directory. 4.10.4 permitted read-only access.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742973/LUCENE-6542.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Test Framework", "change_id": "LUCENE-6652", "change_description": ": Removed lots of useless Byte(s)TermAttributes all over test\ninfrastructure.", "change_title": "Remove tons of BytesRefAttribute/BytesRefAttributeImpl duplicates in tests", "detail_type": "Test", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3,6.0", "detail_description": "While implementing LUCENE-6651, I found a lot of duplicates of the same class (in different variants) which is used by tests to generate binary terms. As we now have support for binary terms in Field class itsself, we should remove all of those attributes accross tests.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Test Framework", "change_id": "LUCENE-6563", "change_description": ": Improve MockFileSystemTestCase.testURI to check if a path\ncan be encoded according to local filesystem requirements. Otherwise\nstop test execution.", "change_title": "MockFileSystemTestCase.testURI should be improved to handle cases where OS/JVM cannot create non-ASCII filenames", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "ant test -Dtestcase=TestVerboseFS -Dtests.method=testURI -Dtests.file.encoding=UTF-8 fails (for example) with 'Oracle Corporation 1.8.0_45 (64-bit)' when the default sun.jnu.encoding system property is (for example) ANSI_X3.4-1968 [details to follow]", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743931/LUCENE-6563.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-6553", "change_description": ": The iterator returned by the LeafReader.postings method now\nalways includes deleted docs, so you have to check for deleted documents on\ntop of the iterator.", "change_title": "Simplify how we handle deleted docs in read APIs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "Today, all scorers and postings formats need to be able to handle deleted documents. I suspect that the reason is that we want to be able to make sure to not perform costly operations on documents that are deleted. For instance if you run a phrase query, reading positions on a document which is deleted is useless. I suspect this is also a source of inefficiencies since in some cases we apply deleted documents several times: for instance conjunctions apply deleted docs to every sub scorer. However, with the new two-phase iteration API, we have a way to make sure that we never run expensive operations on deleted documents: we could first iterate over the approximation, then check that the document is not deleted, and finally confirm the match. Since approximations are cheap, applying deleted docs after them would not be an issue. I would like to explore removing the \"Bits acceptDocs\" parameter from TermsEnum.postings, Weight.scorer, SpanWeight.getSpans and Weight.BulkScorer, and add it to BulkScorer.score. This way, bulk scorers would be the only API which would need to know how to apply deleted docs, which I think would be more manageable since we only have 3 or 4 impls. And DefaultBulkScorer would be implemented the way described above: first advance the approximation, then check deleted docs, then confirm the match, then collect. Of course that's only in the case the scorer supports approximations, if it does not, it means it is cheap so we can directly iterate the scorer and check deleted docs on top.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12741334/LUCENE-6553.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-6633", "change_description": ": DuplicateFilter has been deprecated and will be removed in 6.0.\nDiversifiedTopDocsCollector can be used instead with a maximum number of hits\nper key equal to 1.", "change_title": "Remove DuplicateFilter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.3", "detail_description": "DuplicateFilter is a filter which aims at only exposing a single document per unique field value. It has some flaws, for instance it only works if you have a single leaf and we now have a better way to handle this use-case with DiversifiedTopDocsCollector. So I suggest that we just remove it and recommend users to use DiversifiedTopDocsCollector instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12742508/LUCENE-6633.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-6653", "change_description": ": The workflow for consuming the TermToBytesRefAttribute was changed:\ngetBytesRef() now does all work and is called on each token, fillBytesRef()\nwas removed. The implementation is free to reuse the internal BytesRef\nor return a new one on each call.", "change_title": "Cleanup TermToBytesRefAttribute", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "While working on LUCENE-6652, I figured out that there were so many test with wrongly implemented TermsToBytesRefAttribute. In addition, the whole concept back from Lucene 4.0 was no longer correct: Instead we should remove the fillBytesRef() method from the interface and let getBytesRef() populate and return the BytesRef. It does not matter if the attribute reuses the BytesRef or returns a new one. It just get consumed like a standard CharTermAttribute. You get a BytesRef and can use it until you call incrementToken(). As the TermsToBytesRefAttribute is marked experimental, I see no reason why we should not change the semantics to be more easy to understand and behave like all other attributes. I will add a note to the backwards incompatible changes in Lucene 5.3.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743175/LUCENE-6653.patch", "patch_content": "none"}
{"library_version": "5.3.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-6682", "change_description": ": StandardTokenizer.setMaxTokenLength() now throws an exception if\na length greater than 1M chars is given.  Previously the effective max token\nlength (the scanner's buffer) was capped at 1M chars, but getMaxTokenLength()\nincorrectly returned the previously requested length, even when it exceeded 1M.", "change_title": "StandardTokenizer performance bug: buffer is unnecessarily copied when maxTokenLength doesn't change", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "From Piotr Idzikowski on java-user mailing list http://markmail.org/message/af26kr7fermt2tfh: I am developing own analyzer based on StandardAnalyzer. I realized that tokenizer.setMaxTokenLength is called many times. Does it make sense if length stays the same? I see it finally calls this one( in StandardTokenizerImpl ): So it just copies old array content into the new one.", "patch_link": "none", "patch_content": "none"}

{"library_version": "6.3.0", "change_type": "New Features", "change_id": "LUCENE-7438", "change_description": ": New \"UnifiedHighlighter\" derivative of the PostingsHighlighter that\ncan consume offsets from postings, term vectors, or analysis.  It can highlight phrases\nas accurately as the standard Highlighter. Light term vectors can be used with offsets\nin postings for fast wildcard (MultiTermQuery) highlighting.", "change_title": "UnifiedHighlighter", "detail_type": "Improvement", "detail_affect_versions": "6.2", "detail_fix_versions": "6.3", "detail_description": "The UnifiedHighlighter is an evolution of the PostingsHighlighter that is able to highlight using offsets in either postings, term vectors, or from analysis (a TokenStream). Luceneâ€™s existing highlighters are mostly demarcated along offset source lines, whereas here it is unified â€“ hence this proposed name. In this highlighter, the offset source strategy is separated from the core highlighting functionalty. The UnifiedHighlighter further improves on the PostingsHighlighterâ€™s design by supporting accurate phrase highlighting using an approach similar to the standard highlighterâ€™s WeightedSpanTermExtractor. The next major improvement is a hybrid offset source strategythat utilizes postings and â€œlightâ€ term vectors (i.e. just the terms) for highlighting multi-term queries (wildcards) without resorting to analysis. Phrase highlighting and wildcard highlighting can both be disabled if youâ€™d rather highlight a little faster albeit not as accurately reflecting the query. Weâ€™ve benchmarked an earlier version of this highlighter comparing it to the other highlighters and the results were exciting! Itâ€™s tempting to share those results but itâ€™s definitely due for another benchmark, so weâ€™ll work on that. Performance was the main motivator for creating the UnifiedHighlighter, as the standard Highlighter (the only one meeting Bloomberg Lawâ€™s accuracy requirements) wasnâ€™t fast enough, even with term vectors along with several improvements we contributed back, and even after we forked it to highlight in multiple threads.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12831730/LUCENE_7438_UH_benchmark.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "New Features", "change_id": "LUCENE-7490", "change_description": ": SimpleQueryParser now parses '*' to MatchAllDocsQuery", "change_title": "SimpleQueryParser should parse \"*\" as MatchAllDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "6.2.1", "detail_fix_versions": "6.3,7.0", "detail_description": "It would be beneficial for SimpleQueryString to parse as a MatchAllDocsQuery, rather than a \"field:*\" query. Related discussion on the Elasticsearch project about this: https://github.com/elastic/elasticsearch/issues/10632", "patch_link": "https://issues.apache.org/jira/secure/attachment/12832765/0001-Parse-as-MatchAllDocsQuery-in-SimpleQueryParser.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7507", "change_description": ": Upgrade morfologik-stemming to version 2.1.1 (fixes security\nmanager issue with Polish dictionary lookup).", "change_title": "Update morfologik-stemming to version 2.1.1", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7472", "change_description": ": MultiFieldQueryParser.getFieldQuery() drops queries that are\nneither BooleanQuery nor TermQuery.", "change_title": "MultiFieldQueryParser.getFieldQuery() drops queries that are neither BooleanQuery nor TermQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2.2,6.3,7.0", "detail_description": "From http://mail-archives.apache.org/mod_mbox/lucene-java-user/201609.mbox/%3c944985a6ac27425681bd27abe9d90602@ska-wn-e132.ptvag.ptv.de%3e, Oliver Kaleske reports: Hi, in updating Lucene from 6.1.0 to 6.2.0 I came across the following: We have a subclass of MultiFieldQueryParser (MFQP) for creating a custom type of Query, which calls getFieldQuery() on its base class (MFQP). For each of its search fields, this method has a Query created by calling getFieldQuery() on QueryParserBase. Ultimately, we wind up in QueryBuilder's createFieldQuery() method, which depending on the number of tokens (etc.) decides what type of Query to return: a TermQuery, BooleanQuery, PhraseQuery, or MultiPhraseQuery. Back in MFQP.getFieldQuery(), a variable maxTerms is determined depending on the type of Query returned: for a TermQuery or a BooleanQuery, its value will in general be nonzero, clauses are created, and a non-null Query is returned. However, other Query subclasses result in maxTerms=0, an empty list of clauses, and finally null is returned. To me, this seems like a bug, but I might as well be missing something. The comment \"// happens for stopwords\" on the return null statement, however, seems to suggest that Query types other than TermQuery and BooleanQuery were not considered properly here. I should point out that our custom MFQP subclass so far does some rather unsophisticated tokenization before calling getFieldQuery() on each token, so characters like '*' may still slip through. So perhaps with proper tokenization, it is guaranteed that only TermQuery and BooleanQuery can come out of the chain of getFieldQuery() calls, and not handling (Multi)PhraseQuery in MFQP.getFieldQuery() can never cause trouble? The code in MFQP.getFieldQuery dates back to LUCENE-2605: Add classic QueryParser option setSplitOnWhitespace() to control whether to split on whitespace prior to text analysis.  Default behavior remains unchanged: split-on-whitespace=true. (06 Jul 2016), when it was substantially expanded. Best regards, Oliver", "patch_link": "https://issues.apache.org/jira/secure/attachment/12831157/LUCENE-7472.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7456", "change_description": ": PerFieldPostings/DocValues was failing to delegate the\nmerge method", "change_title": "PerField(DocValues|Postings)Format do not call the per-field merge methods", "detail_type": "Bug", "detail_affect_versions": "6.2.1", "detail_fix_versions": "6.3,7.0", "detail_description": "While porting some old codec code from Lucene 4.3.1, I couldn't get the per-field formats to call upon the per-field merge methods; the default merge method was always being called. I think this is a side-effect of LUCENE-5894. Attached is a patch with a test that reproduces the error and an associated fix that pass the unit tests.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12830512/LUCENE-7456-v2.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7468", "change_description": ": ASCIIFoldingFilter should not emit duplicated tokens when\npreserve original is on.", "change_title": "ASCIIFoldingFilter should not emit duplicated tokens when preserve original is on", "detail_type": "Bug", "detail_affect_versions": "trunk,4.7", "detail_fix_versions": "6.3,7.0", "detail_description": "The ASCIIFoldingFilter seems to make the bold assumption that any tokens that contain a char outside the ASCII range will be folded. The problem is that when preserve original is true we capture and restore the state even if the token remains unmodified. This causes term frequencies to double for such words and probably extra space used when positions/offsets are stored in the postings.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12830699/LUCENE-7468.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7484", "change_description": ": FastVectorHighlighter failed to highlight SynonymQuery", "change_title": "FastVectorHighlighter fails to highlight SynonymQuery", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "6.3,7.0", "detail_description": "SynonymQuery are ignored by the FastVectorHighlighter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12832446/LUCENE-7484.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7476", "change_description": ": JapaneseNumberFilter should not invoke incrementToken\non its input after it's exhausted", "change_title": "Fix transient failure in JapaneseNumberFilter run from TestFactories", "detail_type": "Bug", "detail_affect_versions": "6.2.1", "detail_fix_versions": "6.3,7.0", "detail_description": "Repeatedly running TestFactories show this test to fail ~10% of the time. I believe the fix is trivial and related to loosing the state of the underlying input stream when testing some analyzer life cycle flows.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12832489/LUCENE-7476.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7486", "change_description": ": DisjunctionMaxQuery does not work correctly with queries that\nreturn negative scores.", "change_title": "DisjunctionMaxScorer Initializes scoreMax to Zero Preventing From Using Negative Scores", "detail_type": "Bug", "detail_affect_versions": "5.5.2", "detail_fix_versions": "6.3,7.0", "detail_description": "We are using a log of probability for scoring, which gives us negative scores. DisjunctionMaxScorer initializes scoreMax in the score(...) function to zero preventing us from using negative scores.  Is there a reason it couldn't be initialized to something like this: float scoreMax = Float.MAX_VALUE * -1;", "patch_link": "https://issues.apache.org/jira/secure/attachment/12832561/LUCENE-7486.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7491", "change_description": ": Suddenly turning on dimensional points for some fields\nthat already exist in an index but didn't previously index\ndimensional points could cause unexpected merge exceptions", "change_title": "Unexpected merge exception when merging sparse points fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.3,7.0", "detail_description": "Spinoff from this user thread: http://markmail.org/thread/vwdvjgupyz6heep5 If you have a segment that has points, but a given field (\"id\") didn't index points, and a later segment where field \"id\" does index points, when try to merge those segments we hit this (incorrect!) exception:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12832854/LUCENE-7491.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6914", "change_description": ": Fixed DecimalDigitFilter in case of supplementary code points.", "change_title": "DecimalDigitFilter skips characters in some cases (supplemental?)", "detail_type": "Bug", "detail_affect_versions": "5.4", "detail_fix_versions": "5.5.4,6.3,7.0", "detail_description": "Found this while writing up the solr ref guide for DecimalDigitFilter. With input like \"ðŸ™ðŸ¡ðŸ ðŸœ\" (\"Double Struck\" 1984) the filter produces \"1ðŸ¡8ðŸœ\" (1, double struck 9, 8, double struck 4)  add some non-decimal characters in between the digits (ie: \"ðŸ™xðŸ¡xðŸ xðŸœ\") and you get the expected output (\"1x9x8x4\").  This doesn't affect all decimal characters though, as evident by the existing test cases. Perhaps this is an off by one bug in the \"if the original was supplementary, shrink the string\" code path?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12774917/LUCENE-6914.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7493", "change_description": ": FacetCollector.search threw an unexpected exception if\nyou asked for zero hits but wanted facets", "change_title": "Support of TotalHitCountCollector for FacetCollector.search api if numdocs passed as zero.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.3,7.0", "detail_description": "Hi, I want to do drill down search using FacetCollection below is the code FacetsCollector facetCollector = new FacetsCollector(); TopDocs topDocs = FacetsCollector.search(st.searcher, filterQuery, limit, facetCollector); I just want facet information so I pass limit as zero but I get error \"numHits must be > 0; please use TotalHitCountCollector if you just need the total hit count\". For FacetCollector there is no way to initialize 'TotalHitCountCollector'. Internally it always create either 'TopFieldCollector' or 'TopScoreDocCollector' which does not allow limit as 0. So if limit should be zero then there should be a way that 'TotalHitCountCollector' should be initialized. Better way would be to provide an api which takes query and collector as inputs just like 'drillSideways.search(filterQuery, totalHitCountCollector)'.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12833890/LUCENE-7493-Pass.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7505", "change_description": ": AnalyzingInfixSuggester returned invalid results when\nallTermsRequired is false and context filters are specified", "change_title": "AnalyzingInfixSuggester returns wrong results with allTermsRequired=false plus contexts", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.3,7.0", "detail_description": "I hit this bug while dog-food eating (attempting to upgrade http://jirasearch.mikemccandless.com from 4.6.x to 6.x): If you ask AnalyzingInfixSuggester for suggestions, but 1) you do not require all terms to match, and 2) you provide a context for filtering the results, then you'll get back results that did not match any of the terms from the user's query but did match solely the context filter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12834132/LUCENE-7505.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7429", "change_description": ": AnalyzerWrapper can now modify the normalization chain too and\nDelegatingAnalyzerWrapper does the right thing automatically.", "change_title": "DelegatingAnalyzerWrapper should delegate normalization too", "detail_type": "Bug", "detail_affect_versions": "6.2", "detail_fix_versions": "6.3,7.0", "detail_description": "This is something that I overlooked in LUCENE-7355: (Delegating)AnalyzerWrapper uses the default implementation of initReaderForNormalization and normalize, meaning that by default the normalization is a no-op. It should delegate to the wrapped analyzer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12826370/LUCENE-7429.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7135", "change_description": ": Lucene's check for 32 or 64 bit JVM now works around security\nmanager blocking access to some properties", "change_title": "Constants check for JRE bitness causes SecurityException under WebStart", "detail_type": "Bug", "detail_affect_versions": "5.5", "detail_fix_versions": "5.5.4,6.3,7.0", "detail_description": "I have an app that I deploy via WebStart that uses Lucene 5.2.1 (we are locked to 5.2.1 because that's what LanguageTool uses). When running under the WebStart security manager, there are two locations where exceptions are thrown and prevent pretty much all Lucene classes from initializing. This is true even when we sign everything and specify <security><all-permissions/></security>. The latter is still present in the latest version. My patch illustrates one solution that appears to be working for us. (This patch, together with a backport of the fix to LUCENE-6923, seems to fix the issue for our purposes. However if you really wanted to make my day you could put out a maintenance release of 5.2 with both fixes included.)", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Improvements", "change_id": "LUCENE-7439", "change_description": ": FuzzyQuery now matches all terms within the specified\nedit distance, even if they are short terms", "change_title": "Should FuzzyQuery match short terms too?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.3,7.0", "detail_description": "Today, if you ask FuzzyQuery to match abcd with edit distance 2, it will fail to match the term ab even though it's 2 edits away. Its javadocs explain this: On the one hand, I can see that this behavior is sort of justified in that 50% of the characters are different and so this is a very \"weak\" match, but on the other hand, it's quite unexpected since edit distance is such an exact measure so the terms should have matched. It seems like the behavior is caused by internal implementation details about how the relative (floating point) score is computed.  I think we should fix it, so that edit distance 2 does in fact match all terms with edit distance <= 2.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12828200/LUCENE-7439.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Improvements", "change_id": "LUCENE-7496", "change_description": ": Better toString for SweetSpotSimilarity", "change_title": "Better toString for SweetSpotSimilarity", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.3,7.0", "detail_description": "Spinoff from SOLR-8370 where we display Similarity class in use in the Admin UI. SweetSpotSimilarity does not provide a toString method, so it will incorreclty print ClassicSimilarity.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12833367/LUCENE-7496.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Improvements", "change_id": "LUCENE-7520", "change_description": ": Highlighter's WeightedSpanTermExtractor shouldn't attempt to expand a MultiTermQuery\nwhen its field doesn't match the field the extraction is scoped to.", "change_title": "WeightedSpanTermExtractor should not rewrite MultiTermQuery all the time", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.3", "detail_description": "Currently WeightedSpanTermExtractor will rewrite MultiTermQuery regardless of the field being requested for highlighting. In some case like SOLR-2216, It can be costly and cause TooManyClauses exception for no reason.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12834817/LUCENE-7520.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Optimizations", "change_id": "LUCENE-7501", "change_description": ": BKDReader should not store the split dimension explicitly in the\n1D case.", "change_title": "Do not encode the split dimension in the index in the 1D case", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.3,7.0", "detail_description": "When there is a single dimension, the split dimension is always 0, so we do not need to encode it in the index of the BKD tree. This would be 33% memory saving for half floats, 20% for ints/floats, 11% for longs/doubles and 6% for ip addresses.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12836525/LUCENE-7501.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Other", "change_id": "LUCENE-7513", "change_description": ": Upgrade randomizedtesting to 2.4.0.", "change_title": "Update to randomizedtesting 2.4.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Update to randomizedtesting 2.4.0. Should help us diagnose issues with hanging JVMs (SOLR-9618). There's also an addition of \"biased\" (evil) random number generation routines. Perhaps they'll be interesting to some of you. https://github.com/randomizedtesting/randomizedtesting/releases/tag/release%2F2.4.0", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Other", "change_id": "LUCENE-7452", "change_description": ": Block join query exception suggests how to find a doc, which\n violates orthogonality requirement.", "change_title": "improve exception message: child query must only match non-parent docs, but parent docID=180314...", "detail_type": "Improvement", "detail_affect_versions": "6.2", "detail_fix_versions": "6.3,7.0", "detail_description": "when parent filter intersects with child query the exception exposes internal details: docnum and scorer class. I propose an exception message to suggest to execute a query intersecting them both. There is an opinion to add this  suggestion in addition to existing details.  My main concern against is, when index is constantly updated even SOLR-9582 allows to search for docnum it would be like catching the wind, also think about cloud case. But, user advised with executing query intersection can catch problem documents even if they occurs sporadically.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12829655/LUCENE-7452.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Other", "change_id": "LUCENE-7438", "change_description": ": Renovate the Benchmark module's support for benchmarking highlighting. All\nhighlighters are supported via SearchTravRetHighlight.", "change_title": "UnifiedHighlighter", "detail_type": "Improvement", "detail_affect_versions": "6.2", "detail_fix_versions": "6.3", "detail_description": "The UnifiedHighlighter is an evolution of the PostingsHighlighter that is able to highlight using offsets in either postings, term vectors, or from analysis (a TokenStream). Luceneâ€™s existing highlighters are mostly demarcated along offset source lines, whereas here it is unified â€“ hence this proposed name. In this highlighter, the offset source strategy is separated from the core highlighting functionalty. The UnifiedHighlighter further improves on the PostingsHighlighterâ€™s design by supporting accurate phrase highlighting using an approach similar to the standard highlighterâ€™s WeightedSpanTermExtractor. The next major improvement is a hybrid offset source strategythat utilizes postings and â€œlightâ€ term vectors (i.e. just the terms) for highlighting multi-term queries (wildcards) without resorting to analysis. Phrase highlighting and wildcard highlighting can both be disabled if youâ€™d rather highlight a little faster albeit not as accurately reflecting the query. Weâ€™ve benchmarked an earlier version of this highlighter comparing it to the other highlighters and the results were exciting! Itâ€™s tempting to share those results but itâ€™s definitely due for another benchmark, so weâ€™ll work on that. Performance was the main motivator for creating the UnifiedHighlighter, as the standard Highlighter (the only one meeting Bloomberg Lawâ€™s accuracy requirements) wasnâ€™t fast enough, even with term vectors along with several improvements we contributed back, and even after we forked it to highlight in multiple threads.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12831730/LUCENE_7438_UH_benchmark.patch", "patch_content": "none"}
{"library_version": "6.3.0", "change_type": "Build", "change_id": "LUCENE-7292", "change_description": ": Fix build to use \"--release 8\" instead of \"-release 8\" on\nJava 9 (this changed with recent EA build b135).", "change_title": "Change build system to use \"--release 8\" instead of \"-source/-target\" when invoking javac", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "6.1,6.3,7.0", "detail_description": "Currently we pass -source 1.8 -target 1.8 to javac and javadoc when compiling our source code. We all know that this brings problems, because cross-compiling does not really work. We create class files that are able to run on Java 8, but when it is compiled with java 9, it is not sure that some code may use Java 9 APIs that are not available in Java 8. Javac prints a warning about this (it complains about the bootclasspath not pointing to JDK 8 when used with source/target 1.8). Java 8 is the last version of Java that has this trap. From Java 9 on, instead of passing source and target, the recommended way is to pass a single -release 8 parameter to javac (see http://openjdk.java.net/jeps/247). This solves the bootsclasspath problem, because it has all the previous java versions as \"signatures\" (like forbiddenapis), including deprecated APIs,... everything included. You can find this in the $JAVA_HOME/lib/ct.sym file (which is a ZIP file, so you can open it with a ZIP tool of your choice). In Java 9+, this file also contains all old APIs from Java 6+. When invoking the compiler with -release 8, there is no risk of accidentally using API from newer versions. The migration here is quite simple: As we require Java 8 already, there is (theoretically) no need to pass source and target anymore. It is enough to just pass -release 8 if we detect Java 9 as compiling JVM. Nevertheless I plan to do the following: By this we could theoretically remove the check from smoketester about the compiling JDK (the MANIFEST check), because although compiled with Java 9, the class files were actually compiled against the old Java API from ct.sym file. I will also align the warnings to reenable -Xlint:options.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12805292/LUCENE-7292.patch", "patch_content": "none"}

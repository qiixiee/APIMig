{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6747", "change_description": ": FingerprintFilter is a TokenFilter that outputs a single\ntoken which is a concatenation of the sorted and de-duplicated set of\ninput tokens. Useful for normalizing short text in clustering/linking\ntasks.", "change_title": "FingerprintFilter - a TokenFilter for clustering/linking purposes", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "A TokenFilter that emits a single token which is a sorted, de-duplicated set of the input tokens. This approach to normalizing text is used in tools like OpenRefine[1] and elsewhere [2] to help in clustering or linking texts. The implementation proposed here has a an upper limit on the size of the combined token which is output. [1] https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth [2] https://rajmak.wordpress.com/2013/04/27/clustering-text-map-reduce-in-python/", "patch_link": "https://issues.apache.org/jira/secure/attachment/12752231/fingerprintv4.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6875", "change_description": ": New Serbian Filter.", "change_title": "New Serbian Filter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "This is a new Serbian filter that works with regular Latin text (the current filter works with \"bald\" Latin). I described in detail what does it do and why is it necessary at the wiki.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770793/LUCENE-6875.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6720", "change_description": ": New FunctionRangeQuery wrapper around ValueSourceScorer\n(returned from ValueSource/FunctionValues.getRangeScorer()).", "change_title": "new FunctionRangeQuery, plus ValueSourceScorer improvements", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "This issue provides a new FunctionRangeQuery, which is basically a wrapper around ValueSourceScorer (retrieved from FunctionValues.getRangeScorer).  It replaces ValueSourceFilter in the spatial module.  Solr has a class by the same name which is similar but isn't suitable to being ported. Also, it includes refactorings to the ValueSourceScorer, to include performance enhancements by making it work with the TwoPhaseIterator API. note: I posted this to LUCENE-4251 initially but then felt it's really its own issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748897/LUCENE-6720__FunctionRangeQuery.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6724", "change_description": ": Add utility APIs to GeoHashUtils to compute neighbor\ngeohash cells", "change_title": "Add support for computing GeoHash neighbors to GeoHashUtils", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "This simple feature adds the ability to compute the geohash neighbor(s) at a given level, from a provided geohash. Such a utility is beneficial for simple grid faceting/aggregations and geohash based bounding box queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748893/LUCENE-6724.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6737", "change_description": ": Add DecimalDigitFilter which folds unicode digits to basic latin.", "change_title": "Add DecimalDigitFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "TokenFilter that folds all unicode digits (http://unicode.org/cldr/utility/list-unicodeset.jsp?a=[:General_Category=Decimal_Number:]) to 0-9. Historically a lot of the impacted analyzers couldn't even tokenize numbers at all, but now they use standardtokenizer for numbers/alphanum tokens. But its usually the case you will find e.g. a mix of both ascii digits and \"native\" digits, and today that makes searching difficult. Note this only impacts decimal digits, hence the name DecimalDigitFilter. So no processing of chinese numerals or anything crazy like that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12750411/LUCENE-6737.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6699", "change_description": ": Add integration of BKD tree and geo3d APIs to give\nfast, very accurate query to find all indexed points within an\nearth-surface shape", "change_title": "Integrate lat/lon BKD and spatial3d", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "I'm opening this for discussion, because I'm not yet sure how to do this integration, because of my ignorance about spatial in general and spatial3d in particular Our BKD tree impl is very fast at doing lat/lon shape intersection (bbox, polygon, soon distance: LUCENE-6698) against previously indexed points. I think to integrate with spatial3d, we would first need to record lat/lon/z into doc values.  Somewhere I saw discussion about how we could stuff all 3 into a single long value with acceptable precision loss?  Or, we could use BinaryDocValues?  We need all 3 dims available to do the fast per-hit query time filtering. But, second: what do we index into the BKD tree?  Can we \"just\" index earth surface lat/lon, and then at query time is spatial3d able to give me an enclosing \"surface lat/lon\" bbox for a 3d shape?  Or ... must we index all 3 dimensions into the BKD tree (seems like this could be somewhat wasteful)?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12751815/LUCENE-6699.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6838", "change_description": ": Added IndexSearcher#getQueryCache and #getQueryCachingPolicy.", "change_title": "Add getters for IndexSearcher's query cache and caching policy", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "These can be useful eg. in order to assert that IndexSearcher is using the expected cache.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12766509/LUCENE-6838.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6844", "change_description": ": PayloadScoreQuery can include or exclude underlying span scores\nfrom its score calculations", "change_title": "Add 'includeSpanScore' boolean to PayloadScoreQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "PayloadTermQuery has an 'includeSpanScore' boolean (defaulting to true) which allows clients to either ignore or include the underlying span score before applying payload function scores.  This didn't get ported over to PayloadScoreQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12767330/LUCENE-6844.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6778", "change_description": ": Add GeoPointDistanceRangeQuery, to search for points\nwithin a \"ring\" (beyond a minimum distance and below a maximum\ndistance)", "change_title": "Add GeoPointDistanceRangeQuery support for GeoPointField types", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "GeoPointDistanceQuery currently handles a single point distance. This improvement adds a GeoPointDistanceRangeQuery for supporting use cases such as: find all points between 10km and 20km of a known location.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12767460/LUCENE-6778.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "New Features", "change_id": "LUCENE-6874", "change_description": ": Add a new UnicodeWhitespaceTokenizer to analysis/common\nthat uses Unicode character properties extracted from ICU4J to tokenize\ntext on whitespace. This tokenizer will split on non-breaking\nspace (NBSP), too.", "change_title": "WhitespaceTokenizer should tokenize on NBSP", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "WhitespaceTokenizer uses Character.isWhitespace  to decide what is whitespace.  Here's a pertinent excerpt: It is a Unicode space character (SPACE_SEPARATOR, LINE_SEPARATOR, or PARAGRAPH_SEPARATOR) but is not also a non-breaking space ('\\u00A0', '\\u2007', '\\u202F') Perhaps Character.isWhitespace should have been called isLineBreakableWhitespace? I think WhitespaceTokenizer should tokenize on this.  I am aware it's easy to work around but why leave this trap in by default?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12771994/LUCENE-6874-chartokenizer.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6590", "change_description": ": Query.setBoost(), Query.getBoost() and Query.clone() are gone.\nIn order to apply boosts, you now need to wrap queries in a BoostQuery.", "change_title": "Explore different ways to apply boosts", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "Follow-up from LUCENE-6570: the fact that all queries are mutable in order to allow for applying a boost raises issues since it makes queries bad cache keys since their hashcode can change anytime. We could just document that queries should never be modified after they have gone through IndexSearcher but it would be even better if the API made queries impossible to mutate at all. I think there are two main options: The latter idea is from Robert and I like it a lot given how often I either introduced or found a bug which was due to the boost parameter being ignored. Maybe there are other options, but I think this is worth exploring.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754477/LUCENE-6590.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6716", "change_description": ": SpanPayloadCheckQuery now takes a List<BytesRef> rather than\na Collection<byte[]>.", "change_title": "Improve SpanPayloadCheckQuery API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "SpanPayloadCheckQuery currently takes a Collection<byte[]> to check its payloads against.  This is suboptimal a) because payloads internally use BytesRef rather than byte[] and b) Collection is unordered, but the implementation does actually care about the order in which the payloads appear. We should change the constructor to take a List<BytesRef> instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748655/LUCENE-6716.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6489", "change_description": ": The various span payload queries have been moved to the queries\nsubmodule, and PayloadSpanUtil is now in sandbox.", "change_title": "Move span payloads to sandbox", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "As mentioned on LUCENE-6371: I feel strongly about this and will do the move.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12756229/LUCENE-6489.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6650", "change_description": ": The spatial module no longer uses Filter in any way.  All\nspatial Filters now subclass Query.  The spatial heatmap/facet API\nnow accepts a Bits parameter to filter counts.", "change_title": "Remove dependency of lucene/spatial on oal.search.Filter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "We should try to remove usage of oal.search.Filter in lucene/spatial. I gave it a try but this module makes non-trivial use of filters so I wouldn't mind some help here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12764265/LUCENE-6650.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6301", "change_description": ": org.apache.lucene.search.Filter is now deprecated. You should use\nQuery objects instead of Filters, and the BooleanClause.Occur.FILTER clause in\norder to let Lucene know that a Query should be used for filtering but not\nscoring.", "change_title": "Deprecate Filter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "It will still take time to completely remove Filter, but I think we should start deprecating it now to state our intention and encourage users to move to queries as soon as possible?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12765366/LUCENE-6301.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6939", "change_description": ": SpanOrQuery.addClause is now deprecated, clauses should all be\nprovided at construction time.", "change_title": "BlendedInfixSuggester to support exponential reciprocal BlenderType", "detail_type": "Improvement", "detail_affect_versions": "5.4", "detail_fix_versions": "5.5,6.0", "detail_description": "The orignal BlendedInfixSuggester introduced in LUCENE-5354 has support for: These are used to score documents based on the position of the matched token i.e the closer is the matched term to the beginning, the higher score you get. In some use cases, we need a more aggressive scoring based on the position. That's where the exponential reciprocal comes into play  i.e  coef = 1/Math.pow(position+1, exponent) where the exponent is a configurable variable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12778927/LUCENE-6939.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6855", "change_description": ": CachingWrapperQuery is deprecated and will be removed in 6.0.", "change_title": "Deprecate CachingWrapperQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "I propose that we deprecate CachingWrapperQuery in favour of LRUQueryCache. I think the latter is much more convenient as", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6870", "change_description": ": DisjunctionMaxQuery#add is now deprecated, clauses should all be\nprovided at construction time.", "change_title": "Make DisjunctionMaxQuery immutable", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Just like we did for TermQuery, BooleanQuery, PhraseQuery, ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769733/LUCENE-6870.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6884", "change_description": ": Analyzer.tokenStream() and Tokenizer.setReader() are no longer\ndeclared as throwing IOException.", "change_title": "Analyzer.tokenStream() shouldn't throw IOException", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "I'm guessing that in the past, calling Analyzer.tokenStream() would call TokenStream.reset() somewhere downstream, meaning that we had to deal with IOExceptions.  However, tokenstreams are created entirely lazily now, so this is unnecessary.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770591/LUCENE-6884.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6849", "change_description": ": Expose IndexWriter.flush() method, to move all\nin-memory segments to disk without opening a near-real-time reader\nnor calling fsync", "change_title": "Add IndexWriter API to write segment(s) without refreshing them", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Today, the only way to have IndexWriter free up some heap is to invoke refresh or flush or close it, but these are all quite costly, and do much more than simply \"move bytes to disk\". I think we should add a simple API, e.g. \"move the biggest in-memory segment to disk\" to 1) give more granularity (there could be multiple in-memory segments), and 2) only move bytes to disk (not refresh, not fsync, etc.). This way apps that want to be more careful on how heap is used can have more control.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770779/LUCENE-6849.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "API Changes", "change_id": "LUCENE-6911", "change_description": ": Add correct StandardQueryParser.getMultiFields() method,\ndeprecate no-op StandardQueryParser.getMultiFields(CharSequence[]) method.\n(Christine Poerschke, Mikhail Khludnev, Coverity Scan (via Rishabh Patel))", "change_title": "StandardQueryParser's getMultiFields(CharSequence[] fields) method is a no-op", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "problem summary: details:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12774353/LUCENE-6911.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6708", "change_description": ": TopFieldCollector does not compute the score several times on the\nsame document anymore.", "change_title": "TopFieldCollector sometimes calls Scorer.score() several times on the same doc", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "If the sort spec includes a sort field that needs scores, and if trackDocScores or trackMaxScore is set, then TopFieldCollectors may compute the score several times on the same document, once to check whether the hit is competitive, and once to update maxScore or to set the score on the ScoreDoc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748017/LUCENE-6708.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6720", "change_description": ": ValueSourceScorer, returned from\nFunctionValues.getRangeScorer(), now uses TwoPhaseIterator.", "change_title": "new FunctionRangeQuery, plus ValueSourceScorer improvements", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "This issue provides a new FunctionRangeQuery, which is basically a wrapper around ValueSourceScorer (retrieved from FunctionValues.getRangeScorer).  It replaces ValueSourceFilter in the spatial module.  Solr has a class by the same name which is similar but isn't suitable to being ported. Also, it includes refactorings to the ValueSourceScorer, to include performance enhancements by making it work with the TwoPhaseIterator API. note: I posted this to LUCENE-4251 initially but then felt it's really its own issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748897/LUCENE-6720__FunctionRangeQuery.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6756", "change_description": ": MatchAllDocsQuery now has a dedicated BulkScorer for better\nperformance when used as a top-level query.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6746", "change_description": ": DisjunctionMaxQuery, BoostingQuery and BoostedQuery now create\nsub weights through IndexSearcher so that they can be cached.", "change_title": "Compound queries should create sub weights through IndexSearcher", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "By creating sub weights through IndexSearcher, we give IndexSearcher a chance to add a caching wrapper. We were already doing it for BooleanQuery and ConstantScoreQuery but forgot to also modify DisjunctionMaxQuery, BoostingQuery and BoostedQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12751281/LUCENE-6746.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6754", "change_description": ": Optimized IndexSearcher.count for the cases when it can use\nindex statistics instead of collecting all matches.", "change_title": "Optimize IndexSearcher.count for simple queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "IndexSearcher.count currently always create a collector to compute the number of hits, but it could optimize some queries like MatchAllDocsQuery or TermQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12751544/LUCENE-6754.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6773", "change_description": ": Nested conjunctions now iterate over documents as if clauses\nwere all at the same level.", "change_title": "Always flatten nested conjunctions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "LUCENE-6585 started the work to flatten nested conjunctions, but this only works with approximations. Otherwise a ConjunctionScorer is passed to ConjunctionDISI.intersect, and is not flattened since it is not an instance of ConjunctionDISI.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754637/LUCENE-6773.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6777", "change_description": ": Reuse BytesRef when visiting term ranges in\nGeoPointTermsEnum to reduce GC pressure", "change_title": "Switch GeoPointTermsEnum range list to use a reusable BytesRef", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "GeoPointTermsEnum currently constructs a BytesRef for every computed range, then sorts on this BytesRef.  This adds an unnecessary memory overhead since the TermsEnum only requires BytesRef on calls to nextSeekTerm and accept and the ranges only need to be sorted by their long representation. This issue adds the following two improvements: 1. Lazily compute the BytesRef on demand only when its needed 2. Add a single, transient BytesRef to GeoPointTermsEnum This will further cut back on heap usage when constructing ranges across every segment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755169/LUCENE-6777.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6779", "change_description": ": Reduce memory allocated by CompressingStoredFieldsWriter to write\nstrings larger than 64kb by an amount equal to string's utf8 size.", "change_title": "Reduce memory allocated by CompressingStoredFieldsWriter to write large strings", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "In SOLR-7927, I am trying to reduce the memory required to index very large documents (between 10 to 100MB) and one of the places which allocate a lot of heap is the UTF8 encoding in CompressingStoredFieldsWriter. The same problem existed in JavaBinCodec and we reduced its memory allocation by falling back to a double pass approach in SOLR-7971 when the utf8 size of the string is greater than 64KB. I propose to make the same changes to CompressingStoredFieldsWriter as we made to JavaBinCodec in SOLR-7971.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12756005/LUCENE-6779.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6850", "change_description": ": Optimize BooleanScorer for sparse clauses.", "change_title": "BooleanWeight should not use BS1 when there is a single non-null clause", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "When a disjunction has a single non-null scorer, we still use BS1 for bulk-scoring, which first collects matches into a bit set and then calls the collector. This is inefficient: we should just call the inner bulk scorer directly and wrap the scorer to apply the coord factor (like BooleanTopLevelScorers.BoostedScorer does).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12767760/LUCENE-6850.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6840", "change_description": ": Ordinal indexes for SORTED_SET/SORTED_NUMERIC fields and\naddresses for BINARY fields are now stored on disk instead of in memory.", "change_title": "Put ord indexes of doc values on disk", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Currently we still load monotonic blocks into memory to map doc ids to an offset on disk. Since these data structures are usually consumed sequentially I would like to investigate putting them to disk.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769074/LUCENE-6840.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6878", "change_description": ": Speed up TopDocs.merge.", "change_title": "TopDocs.merge should use updateTop instead of pop / add", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "5.4,6.0", "detail_description": "The function TopDocs.merge uses PriorityQueue in a pattern: pop, update value (ref.hitIndex++), add. JavaDocs for PriorityQueue.updateTop say that using this function instead should be at least twice as fast.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770175/LUCENE-6878.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6885", "change_description": ": StandardDirectoryReader (initialCapacity) tweaks", "change_title": "StandardDirectoryReader (initialCapacity) tweaks", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "proposed patch against trunk to follow", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770631/LUCENE-6885.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6863", "change_description": ": Optimized storage requirements of doc values fields when less\nthan 1% of documents have a value.", "change_title": "Store sparse doc values more efficiently", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "For both NUMERIC fields and ordinals of SORTED fields, we store data in a dense way. As a consequence, if you have only 1000 documents out of 1B that have a value, and 8 bits are required to store those 1000 numbers, we will not require 1KB of storage, but 1GB. I suspect this mostly happens in abuse cases, but still it's a pity that we explode storage requirements. We could try to detect sparsity and compress accordingly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770130/LUCENE-6863.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6892", "change_description": ": various lucene.index initialCapacity tweaks", "change_title": "various lucene.index initialCapacity tweaks", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "patch to follow", "patch_link": "https://issues.apache.org/jira/secure/attachment/12771732/LUCENE-6892.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6276", "change_description": ": Added TwoPhaseIterator.matchCost() which allows to confirm the\nleast costly TwoPhaseIterators first.", "change_title": "Add matchCost() api to TwoPhaseDocIdSetIterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "We could add a method like TwoPhaseDISI.matchCost() defined as something like estimate of nanoseconds or similar. ConjunctionScorer could use this method to sort its 'twoPhaseIterators' array so that cheaper ones are called first. Today it has no idea if one scorer is a simple phrase scorer on a short field vs another that might do some geo calculation or more expensive stuff. PhraseScorers could implement this based on index statistics (e.g. totalTermFreq/maxDoc)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12766405/LUCENE-6276-NoSpans2.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Optimizations", "change_id": "LUCENE-6898", "change_description": ": In the default codec, the last stored field value will not\nbe fully read from disk if the supplied StoredFieldVisitor doesn't want it.\nSo put your largest text field value last to benefit.", "change_title": "Avoid reading last stored field value when StoredFieldVisitor.Status.NO", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "CompressingStoredFieldsReader.visitDocument (line 597) loops through the fields in the input while consulting the StoredFieldVisitor on what to do.  There is a small optimization that could be done on the last loop iteration.  If the visitor returns Status.NO then it should be treated as equivalent to Status.STOP.  As it is now, it will call skipField() which reads needless bytes from the DataInput that won't be used. With this optimization in place, it is advisable to put the largest text field last in sequence – something the user or search platform (e.g. ES/Solr) could do.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12772788/LUCENE-6898.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6905", "change_description": ": Unwrap center longitude for dateline crossing\nGeoPointDistanceQuery.", "change_title": "GeoPointDistanceQuery using wrapped lon for dateline crossing query", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "GeoPointDistanceQuery handles dateline crossing by splitting the Minimum Bounding Rectangle (MBR) into east/west ranges and rewriting to a Boolean SHOULD. PostFiltering is accomplished by calculating the distance from the center point to the candidate point field. Unfortunately the center point is wrapped such that calculating the closest point on the \"circle\" from an eastern point to a western MBR provides incorrect results thus causing false negatives in the range creation. This was caught by a jenkins failure and reproduced in 2 places: GeoPointDistanceTermsEnum and TestGeoRelations", "patch_link": "https://issues.apache.org/jira/secure/attachment/12774370/LUCENE-6905.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6817", "change_description": ": ComplexPhraseQueryParser.ComplexPhraseQuery does not display\nslop in toString().", "change_title": "ComplexPhraseQueryParser.ComplexPhraseQuery does not display slop in toString()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "This one is quite simple (I think) – ComplexPhraseQuery doesn't display the slop factor which, when the result of parsing is dumped to logs, for example, can be confusing. I'm heading for a weekend out of office in a few hours... so in the spirit of not committing and running away (  ), if anybody wishes to tackle this, go ahead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12764002/LUCENE-6817.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6730", "change_description": ": Hyper-parameter c is ignored in term frequency NormalizationH1.", "change_title": "Hyper-parameter c is ignored in term frequency NormalizationH1", "detail_type": "Bug", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.4", "detail_description": "Unlike NormalizationH2, c parameter is not used in term frequency calculation in NormalizationH1.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12749487/LUCENE-6730.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6742", "change_description": ": Lovins & Finnish implementation of SnowballFilter was\nfixed to behave exactly as specified. A bug in the snowball compiler\ncaused differences in output of the filter in comparison to the original\ntest data.  In addition, the performance of those filters was improved\nsignificantly.", "change_title": "Fix SnowballFilter for Lovins & Finnish (and others that use reflection)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "While reviewing the warnings (LUCENE-6740) I noticed the following: The LovinsStemmer and the FinnishStemmer from the snowball package cannot work at all. Indeed Robert Muir added a comment to the test referring to: http://article.gmane.org/gmane.comp.search.snowball/1139 The bug is the following: The Among class looks up the method to call via reflection. But the stemmer compiler creates all these methods as private. As  AccessibleObject#setAccessible() is called nowhere (we also don't want this), the SnowballProgram main class cannot call the method. Unfortunately it completely supresses all reflection errors and returns false. SnowballProgram then thinks, the called private boolean r_A() method returned false and so the whole snowball algorithm breaks. Also the snowball stemmer classes had a bug: methodObject is a static instance of the Stemmer, and Among calls the private method then on wrong object. So when fixing the above, this would fail again (because the stemmer changes a static singleton object, not itsself!). We also need to change the object in SnowballProgram to use \"this\" when calling the method. There are several possibilities to solve the private methods problem: The MethodHandles trick can be done the following way: The second part is done by a new Ant task: ant patch-snowball. Whenever you add a new Snowball stemmer, copy the java file generated by the snowball compiler to the ext directory and call the ant task. It will patch the methodObject declaration (and also add a @SuppressWarnings(\"unused\") for convenience). I reenabled the dictionary tests and the whole stemmer works. There are also no issues with security managers, because we do nothing security sensitive (all is our own stuff, no setAccessible). And finally: LovinsStemmer and FinnishStemmer got insanely fast (and correct, of course).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12750699/LUCENE-6742-Java8.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6783", "change_description": ": Removed side effects from FuzzyLikeThisQuery.rewrite.", "change_title": "FuzzyLikeThisQuery.rewrite should not have side effects", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754604/LUCENE-6783.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6776", "change_description": ": Fix geo3d math to handle randomly squashed planet\nmodels", "change_title": "Randomized planet model shows up additional XYZBounds errors", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Adding randomized PlanetModel construction causes points to be generated inside a shape that are outside XYZBounds.  mikemccand please take note.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754545/LUCENE-6776.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6792", "change_description": ": Fix TermsQuery.toString() to work with binary terms.", "change_title": "TermsQuery.toString() assumes UTF-8 terms", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "As reported by Ruslan Muzhikov, it will give AssertionError on binary terms. We should use Term.toString() which will do the right thing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754948/LUCENE-6792.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5503", "change_description": ": When Highlighter's WeightedSpanTermExtractor converts a\nPhraseQuery to an equivalent SpanQuery, it would sometimes use a slop that is\ntoo low (no highlight) or determine inOrder wrong.", "change_title": "Trivial fixes to WeightedSpanTermExtractor", "detail_type": "Bug", "detail_affect_versions": "4.7", "detail_fix_versions": "5.4", "detail_description": "The conversion of PhraseQuery to SpanNearQuery miscalculates the slop if there are stop words in some cases.  The issue only really appears if there is more than one intervening run of stop words: ab the cd the the ef. I also noticed that the inOrder determination is based on the newly calculated slop, and it should probably be based on the original phraseQuery.getSlop() patch and unit tests on way", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755544/LUCENE-5503.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6790", "change_description": ": Fix IndexWriter thread safety when one thread is\nhandling a tragic exception but another is still committing", "change_title": "Rollback (during tragic exception) should wait for concurrent commit to finish", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Causes this test failure: http://build-eu-00.elastic.co/job/lucene_linux_java8_64_test_only/63025/ This is a regression, caused by LUCENE-6579 ... I remember hitting deadlock on that issue ... I'll try to add back acquiring the commitLock before rollback ... The issue happens if one thread is committing while another thread is handling a tragedy...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755004/LUCENE-6790.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6810", "change_description": ": Upgrade to Spatial4j 0.5 -- fixes some edge-case bugs in the\nspatial module. See", "change_title": "Upgrade to Spatial4j 0.5", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "Spatial4j 0.5 was released a few days ago.  There are some bug fixes, most of which were surfaced via the tests here.  It also publishes the test jar (thanks nknize for that one) and with that there are a couple test utilities here I can remove. https://github.com/locationtech/spatial4j/blob/master/CHANGES.md", "patch_link": "https://issues.apache.org/jira/secure/attachment/12761474/LUCENE-6810_Spatial4j_0_5.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "https://github.com/locationtech/spatial4j/blob/master/CHANGES.md", "change_description": ": Upgrade to Spatial4j 0.5 -- fixes some edge-case bugs in the\nspatial module. See", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6813", "change_description": ": OfflineSorter no longer removes its output Path up\nfront, and instead opens it for write with the\nStandardCopyOption.REPLACE_EXISTING to overwrite any prior file, so\nthat callers can safely use Files.createTempFile for the output.\nThis change also fixes OfflineSorter's default temp directory when\nrunning tests to use mock filesystems so e.g. we detect file handle\nleaks", "change_title": "OfflineSorter.sort shouldn't remove the output Path up front", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "The new BKD tree classes, and NumericRangeTree (just a 1D BKD tree), make heavy use of OfflineSorter to build their data structures at indexing time when the number of indexed documents is biggish. But when I was first building them (LUCENE-6477), I hit a thread safety issue in OfflineSorter, and at that time I just worked around it by creating my own private temp directory each time I need to write a BKD tree. This workaround is sort of messy, and it causes problems with \"pending delete\" files on Windows when we try to remove that temp directory, causing test failures like http://jenkins.thetaphi.de/job/Lucene-Solr-5.x-Windows/5149/ I think instead we should fix the root cause ... i.e. make OfflineSorter thread safe.  It looks like it's simple... Separately I'd like to somehow fix these BKD tests to catch any leaked file handles ... I'm not sure they are today.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12762167/LUCENE-6813.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6813", "change_description": ": RangeTreeWriter was failing to close all file handles\nit opened, leading to intermittent failures on Windows", "change_title": "OfflineSorter.sort shouldn't remove the output Path up front", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "The new BKD tree classes, and NumericRangeTree (just a 1D BKD tree), make heavy use of OfflineSorter to build their data structures at indexing time when the number of indexed documents is biggish. But when I was first building them (LUCENE-6477), I hit a thread safety issue in OfflineSorter, and at that time I just worked around it by creating my own private temp directory each time I need to write a BKD tree. This workaround is sort of messy, and it causes problems with \"pending delete\" files on Windows when we try to remove that temp directory, causing test failures like http://jenkins.thetaphi.de/job/Lucene-Solr-5.x-Windows/5149/ I think instead we should fix the root cause ... i.e. make OfflineSorter thread safe.  It looks like it's simple... Separately I'd like to somehow fix these BKD tests to catch any leaked file handles ... I'm not sure they are today.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12762167/LUCENE-6813.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6826", "change_description": ": Fix ClassCastException when merging a field that has no\nterms because they were filtered out by e.g. a FilterCodecReader", "change_title": "java.lang.ClassCastException: org.apache.lucene.index.TermsEnum$2 cannot be cast to org.apache.lucene.index.MultiTermsEnum when adding indexes", "detail_type": "Bug", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.4,6.0", "detail_description": "We are using addIndexes and FilterCodecReader tricks as part of index migration. Whether FilterCodecReader tricks are required to reproduce this is uncertain, but in any case, when migrating a particular index, I saw this exception: TermsEnum$2 appears to be TermsEnum.EMPTY. The place where it creates it is here: MultiTermsEnum#reset: A quick hack would be for MappedMultiFields to check for TermsEnum.EMPTY specifically before casting, but there might be some way to avoid the cast entirely and that would obviously be a better idea.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6823", "change_description": ": LocalReplicator should use System.nanoTime as its clock\nsource for checking for expiration", "change_title": "Remove use of System.currentTimeMillis() from LocalReplicator", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "LocalReplicator uses System.currentTimeMillis() for session expiry, which is not guaranteed monotonic.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12764778/LUCENE-6823.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6856", "change_description": ": The Weight wrapper used by LRUQueryCache now delegates to the\noriginal Weight's BulkScorer when applicable.", "change_title": "LRUQueryCache.CachingWrapperWeight should delegate the bulk scorer", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "I think we have two issues with LRUQueryCache.CachingWrapperWeight:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12768981/LUCENE-6856.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6858", "change_description": ": Fix ContextSuggestField to correctly wrap token stream\nwhen using CompletionAnalyzer.", "change_title": "Fix ContextSuggestField to correctly wrap token stream", "detail_type": "Bug", "detail_affect_versions": "5.4,6.0", "detail_fix_versions": "5.4,6.0", "detail_description": "Currently, when a ContextSuggestField is analyzed with a completion analyzer, it incorrectly uses the completion token stream as an input to it's PrefixTokenFilter instead of using the underlying token stream, before wrapping the token stream as a completion token stream.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769150/LUCENE-6858.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6872", "change_description": ": IndexWriter handles any VirtualMachineError, not just OOM,\nas tragic.", "change_title": "IndexWriter OOM handling should be any VirtualMachineError", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "IndexWriter is defensive in this case: this error could come from any unexpected place. But its superclass VirtualMachineError is the correct one: \"Thrown to indicate that the Java Virtual Machine is broken or has run out of resources necessary for it to continue operating.\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769953/LUCENE-6872.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6814", "change_description": ": PatternTokenizer no longer hangs onto heap sized to the\nmaximum input string it's ever seen, which can be a large memory\n\"leak\" if you tokenize large strings with many threads across many\nindices", "change_title": "PatternTokenizer indefinitely holds heap equal to max field it has ever tokenized", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Caught by Alex Chow in this Elasticsearch issue: https://github.com/elastic/elasticsearch/issues/13721 Today, PatternTokenizer reuses a single StringBuilder, but it doesn't free its heap usage after tokenizing is done.  We can either stop reusing, or ask it to .trimToSize when we are done ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12762101/LUCENE-6814.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6888", "change_description": ": Explain output of map() function now also prints default value", "change_title": "explain of map() Function does not show default value", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "The explain output from the map(x,min,max,target,default) function does not print default.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12771017/LUCENE-6888.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6899", "change_description": ": Upgrade randomizedtesting to 2.3.1.", "change_title": "Upgrade randomizedtesting to 2.3.1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "This has a bunch of internal and some external improvements, overview here: https://github.com/randomizedtesting/randomizedtesting/releases", "patch_link": "https://issues.apache.org/jira/secure/attachment/12772816/LUCENE-6899.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6478", "change_description": ": Test execution can hang with java.security.debug.", "change_title": "Test execution can hang with java.security.debug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "As reported by Robert: Hangs the test runner. The same problem appears to be present in ES builds too. It seems like some kind of weird stream buffer problem, the security framework seems to be writing to the native descriptors directly. Will have to dig (deep...).", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6862", "change_description": ": Upgrade of RandomizedRunner to version 2.2.0.", "change_title": "Upgrade of RandomizedRunner to version 2.2.0", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769374/LUCENE-6862.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6857", "change_description": ": Validate StandardQueryParser with NOT operator\nwith-in parantheses.", "change_title": "Validate StandardQueryParser with NOT operator with-in parentheses.", "detail_type": "Test", "detail_affect_versions": "5.3", "detail_fix_versions": "4.10.5,5.4,6.0", "detail_description": "Provide test case to validate LUCENE-6249,  which validates NOT in parentheses. e.g: lottery (NOT ticket) lottery (-ticket) lottery AND (NOT ticket) +lottery +(-ticket)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769221/LUCENE-6857.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6827", "change_description": ": Use explicit capacity ArrayList instead of a LinkedList\nin MultiFieldQueryNodeProcessor.", "change_title": "Use explicit capacity ArrayList instead of a LinkedList in MultiFieldQueryNodeProcessor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12765163/LUCENE-6827.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6812", "change_description": ": Upgrade RandomizedTesting to 2.1.17.", "change_title": "Upgrade RandomizedTesting to 2.1.17", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6174", "change_description": ": Improve \"ant eclipse\" to select right JRE for building.", "change_title": "Improve \"ant eclipse\" to select right JRE for building", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Whenever I run \"ant eclipse\" the setting choosing the right JVM is lost and has to be reassigned in the project properties. In fact the classpath generator writes a new classpath file (as it should), but this onl ycontains the \"default\" entry: Instead it should preserve something like: We can either path this by a Ant property via command line or user can do this with \"lucene/build.properties\" or per user. An alternative would be to generate the name \"jdk1.8.0_25\" by guessing from ANT's \"java.home\". If this name does not exist in eclipse it would produce an error and user would need to add the correct JDK. I currently have the problem that my Eclipse uses Java 7 by default and whenever I rebuild the eclipse project, the change to Java 8 in trunk is gone. When this is fixed, I could easily/automatically have the \"right\" JDK used by eclipse for trunk (Java 8) and branch_5x (Java 7).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12749822/LUCENE-6174.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6417", "change_description": ",", "change_title": "Upgrade ANTLR to version 4.5", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "I would like to upgrade ANTLR from 3.5 to 4.5.  This version adds several features that will improve the existing grammars.  The main improvement would be the allowance of left-hand recursion in grammar rules which will reduce the number of rules significantly for expressions. This change will require some code refactoring to the existing expressions work.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12749461/LUCENE-6417-cleanups.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6830", "change_description": ",", "change_title": "Upgrade ANTLR to version 4.5.1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Simple upgrade to ANTLR 4.5.1 which includes numerous bug fixes: https://github.com/antlr/antlr4/releases/tag/4.5.1 Note this does not change the grammar itself, only small pieces of the generated code.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12766901/LUCENE-6830.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6729", "change_description": ": Upgrade ASM used in expressions module to version 5.0.4.", "change_title": "Upgrade ASM version to 5.0.4 (expressions / Solr's TIKA)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "The expressions module currently uses ASM 4 for generating class files. We should upgrade to ASM 5.0.4. In addition, with that version we can create Java 8 class files on trunk. There is a clash with Apache TIKA, which uses the same old 4.x version. But ASM 5 is still compatible to ASM 4, if you pass the old ASM version to your own visitors. But on long term TIKA should upgrade, too, because currently it cannot parse Java 8 class files. ASM 5.0 also fixes some bugs with Java 7 class files, so we should really upgrade.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12749479/LUCENE-6729.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6738", "change_description": ": remove IndexWriterConfig.[gs]etIndexingChain", "change_title": "remove IndexWriterConfig.[gs]etIndexingChain", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "github pull request with proposed code change to follow. see also LUCENE-6571 (which concerns Javadoc errors/warnings) for context.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6755", "change_description": ": more tests of ToChildBlockJoinScorer.advance", "change_title": "more tests of ToChildBlockJoinScorer.advance", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "I recently helped diagnose some strange errors with ToChildBlockJoinQuery in an older version of Solr which lead me to realize that the problem seemed to have been fixed by LUCENE-6593 – however the tests Adrien added in that issue focused specifically the interaction of ToChildBlockJoinScorer with with the (fairly new) aproximations support in Scorers (evidently that was trigger that caused Adrien to investigate and make the fixes). However, in my initial diagnoses / testing, there were at least 2 (non aproximation based) situations where the old code was problematic: As mentioned, Adrien's changes in LUCENE-6593 seemed to fix both of these problematic situations, but I'm opening this issue to track the addition of some new tests to explicitly cover these situations to protect us against future regression.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6571", "change_description": ": fix some private access level javadoc errors and warnings", "change_title": "Javadoc error when run in private access level", "detail_type": "Bug", "detail_affect_versions": "5.2", "detail_fix_versions": "5.4,6.0", "detail_description": "Javadoc error when run in private access level.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12739827/LUCENE-6571.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6768", "change_description": ": AbstractFirstPassGroupingCollector.groupSort private member\nis not needed.", "change_title": "AbstractFirstPassGroupingCollector.groupSort private member is not needed", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "patch with proposed change to follow. (SOLR-2072 could result in AbstractFirstPassGroupingCollector changes also but from glancing through the patches the groupSort member would still not be needed (but could of course be added back if/when needed))", "patch_link": "https://issues.apache.org/jira/secure/attachment/12752964/LUCENE-6768.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6761", "change_description": ": MatchAllDocsQuery's Scorers do not expose approximations\nanymore.", "change_title": "MatchAllDocsQuery should not expose approximations", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "This is a relic from when queries had to deal with deleted docs themselves: MatchAllDocsQuery used to return an iterator that matched everything as an approximation and applied live docs in the confirmation phase. But now that live docs are checked on top, it should just returns an efficient Scorer that matches every document.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12753502/LUCENE-6761.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6775", "change_description": ",", "change_title": "Improve MorfologikFilterFactory to allow arbitrary dictionaries from ResourceLoader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Followup issue for LUCENE-6774: The filter ctor already allows to pass any dictionary to the filter, but you have no chance to configure this through the Factory (CustomAnalyzer/Solr/Elasticsearch/...). This will add 2 parameters to the factory (exclusive with the dictionary string specifying language, default \"pl\"), to load FSA (dictionary) and corresponding property file (metadata/featureData). This dictionary could be placed, e.g. in Solr's conf dir and loaded, because this would be done via ResourceLoader. Alternatively the language could still be passed, but must be part of JAR file distribution. Currently this defaults to \"pl\" at the moment and plain Lucene does not allow more, unless you add own JAR files. So practically, the parameter is useless for a pure, uncustomized Lucene-Impl.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754963/LUCENE-6775.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6833", "change_description": ",", "change_title": "Upgrade morfologik to version 2.0.1, simplify MorfologikFilter's dictionary lookup", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "This is a follow-up to Uwe's work on LUCENE-6774. This patch updates the code to use Morfologik stemming version 2.0.1, which removes the \"automatic\" lookup of classpath-relative dictionary resources in favor of an explicit InputStream or URL. So the user code is explicitly responsible to provide these resources, reacting to missing files, etc. There were no other \"default\" dictionaries in Morfologik other than the Polish dictionary so I also cleaned up the filter code from a number of attributes that were, to me, confusing. This patch is not backward compatible, but it attempts to provide useful feedback on initialization: if the removed attributes were used, it points at this JIRA issue, so it should be clear what to change and how.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12765546/LUCENE-6833.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6797", "change_description": ": Make GeoCircle an interface and use a factory to create\nit, to eventually handle degenerate cases", "change_title": "Geo3d circle construction could benefit from its own factory", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "GeoCircles need special handling for whole-world situations and for single point situations.  It would be better to have a factory that constructed appropriate instance types based on the parameters than try to fold everything into one class.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755234/LUCENE-6797.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6800", "change_description": ": Use XYZSolidFactory to create XYZSolids", "change_title": "Regularize how Geo3d XYZSolids are managed, as was done for GeoCircle", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "The instantiation of XYZSolid-family objects is currently done through a factory method in GeoAreaFactory.  For consistency, there should be the following:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755381/LUCENE-6800.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6798", "change_description": ": Geo3d now models degenerate (too tiny) circles as a\nsingle point", "change_title": "Add geo3d support for zero-width circles", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "The current GeoCircleFactory can't make zero-width circles.  We should fix that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755292/LUCENE-6798.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6770", "change_description": ": Add javadocs that FSDirectory canonicalizes the path.", "change_title": "FSDirectory ctor should use getAbsolutePath instead of getRealPath for directory", "detail_type": "Improvement", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.4,6.0", "detail_description": "After upgrade from 4.1 to 5.2.1 I found that one of our test failed. Appeared the guilty was FSDirectory that converts given Path to Path.getRealPath. As result the test will fail: Path p = Paths.get(\"/var/lucene_store\"); FSDirectory d = new FSDirectory(p); assertEquals(p.toString(), d.getDirectory().toString()); It because /var/lucene_store is a symlink and  Path directory =path.getRealPath();  resolves it to /private/var/lucene_store I think this is bad design decision because \"direcrory\" isn't just internal state but is exposed in a public interface and \"getDirectory()\" is widely used to initialize other components. It should use paths.getAbsolutePath() instead. build and \"ant test\" were successful after fix.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754945/LUCENE-6770.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6795", "change_description": ": Fix various places where code used\nAccessibleObject#setAccessible() without a privileged block. Code\nwithout a hard requirement to do reflection were rewritten. This\nmakes Lucene and Solr ready for Java 9 Jigsaw's module system, where\nreflection on Java's runtime classes is very restricted.", "change_title": "Remove all accessClassInPackage permissions / remove uses AccessibleObject#setAccessible() to make ready for Java 9 Jigsaw", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "With jigsaw builds this stuff is not allowed, its no longer an option of security manager or not. So we should remove these permissions and fix the test leaks, crappy code, remove hacks, etc. If the hack is really somehow needed for some special reason (e.g. well known case like mmap), it needs to gracefully handle not being able to do this, the code and tests should still \"work\" if it cannot do the hack. Otherwise there will be problems for java 9.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12755582/LUCENE-6795-forbids.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6467", "change_description": ": Simplify Query.equals.", "change_title": "Simplify Query.equals()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "Remove this == other test in Query.equals().", "patch_link": "https://issues.apache.org/jira/secure/attachment/12730822/LUCENE-6474.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6845", "change_description": ": SpanScorer is now merged into Spans", "change_title": "Merge Spans and SpanScorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "SpanScorer and Spans currently share the burden of scoring span queries, with SpanScorer delegating to Spans for most operations.  Spans is essentially a Scorer, just with the ability to iterate through positions as well, and no SimScorer to use for scoring.  This seems overly complicated.  We should merge the two classes into one.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12767820/LUCENE-6845_norenames.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6887", "change_description": ": DefaultSimilarity is deprecated, use ClassicSimilarity for equivilent behavior,\nor consider switching to BM25Similarity which will become the new default in Lucene 6.0", "change_title": "5x: backport ClassicSimilarity, mark DefaultSimilarity deprecated & update javadocs to mention ClassicSim vs. BM25Sim", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "LUCENE-6789 changed the default Similarity on trunk to BM25, and renamed DefaultSimiliarity to ClassicSimilarity for backcompat w/o confusion about the \"default\" name. But nothing related to this change was committed to the 5x branch. On the 5x branch we should:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770869/LUCENE-6887.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6893", "change_description": ": factor out CorePlusQueriesParser from CorePlusExtensionsParser", "change_title": "factor out CorePlusQueriesParser from CorePlusExtensionsParser", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "proposed change (patch against trunk to follow): before: after: motivation:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12772205/LUCENE-6893.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6902", "change_description": ": Don't retry to fsync files / directories; fail\nimmediately.", "change_title": "Fail fsync immediately", "detail_type": "Improvement", "detail_affect_versions": "5.3.1", "detail_fix_versions": "5.4,6.0", "detail_description": "While analysing a build issue in Elasticsearch I stumpled upon org.apache.lucene.util.IOUtils.fsync. It has a retry loop in fsync whenever an IOException occurs. However, there are lots of instances where a retry is not useful, e.g. when a channel has been closed, a ClosedChannelException is thrown and IOUtils#fsync still tries to fsync multiple times on the closed channel. After bringing the issue to Robert's attention, he even opted for removing the retry logic entirely for fsyncing. Please find attached a patch that removes the retry logic.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12773559/ioutils-fsync-fail-fast.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Other", "change_id": "LUCENE-6801", "change_description": ": Clarify JavaDocs of PhraseQuery that it in fact supports terms\nat the same position (as does MultiPhraseQuery), treated like a conjunction.\nAdded test.", "change_title": "PhraseQuery incorrectly advertises it supports terms at the same position", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "The following in PhraseQuery has been here since Sept 15th 2004 (by \"goller\"): Of course this isn't true; it's why we have MultiPhraseQuery.  Yet we even allow you to have consecutive terms with the same positions.  We shouldn't allow that; we should throw an exception.  For my own sanity, I modified a simple MultiPhraseQuery test to use PhraseQuery instead and of course it didn't work.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12773185/LUCENE_6801.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Build", "change_id": "LUCENE-6732", "change_description": ": Improve checker for invalid source patterns to also\ndetect javadoc-style license headers. Use Groovy to implement the\nchecks instead of plain Ant.", "change_title": "Improve validate-source-patterns in build.xml (e.g., detect invalid license headers!!)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Today I enabled warnings analysis on Policeman Jenkins. This scans the build log for warnings by javac and reports them in statistics, together with source file dumps. When doing that I found out that someone added again a lot of \"invalid\" license headers using /** instead a simple comment. This causes javadocs warnings under some circumstances, because /** is start of javadocs and not a license comment. I then tried to fix the validate-source-patterns to detect this, but due to a bug in ANT, the <containsregexp/> filter is applied per line (although it has multiline matching capabilities!!!). So I rewrote our checker to run with groovy. This also has some good parts:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12750077/LUCENE-6732-v2.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Build", "change_id": "LUCENE-6594", "change_description": ": Update forbiddenapis to 2.0.", "change_title": "Add java.time forbidden-apis to trunk (update to forbiddenapis 2.0)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "The current version of forbiddenapis misses to add the new java.time APIs of Java 8, Some of the methods use default Locale or default Timezone. Until a new version of forbidden-apis is released, I would like to add those APIs to the Trunk (Java 8) base.txt signatures list. We already have some code in trunk using java.time (IndexWriter logging), so we should take care of that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12764457/LUCENE-6594.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Tests", "change_id": "LUCENE-6752", "change_description": ": Add Math#random() to forbiddenapis.", "change_title": "include Math.random() into forbiddenAPI", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "Math.random() should be included into forbiddenAPI", "patch_link": "https://issues.apache.org/jira/secure/attachment/12751684/LUCENE-6752.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-6742", "change_description": ": The Lovins & Finnish implementation of SnowballFilter\nwere fixed to now behave exactly like the original Snowball stemmer.\nIf you have indexed text using those stemmers you may need to reindex.", "change_title": "Fix SnowballFilter for Lovins & Finnish (and others that use reflection)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "While reviewing the warnings (LUCENE-6740) I noticed the following: The LovinsStemmer and the FinnishStemmer from the snowball package cannot work at all. Indeed Robert Muir added a comment to the test referring to: http://article.gmane.org/gmane.comp.search.snowball/1139 The bug is the following: The Among class looks up the method to call via reflection. But the stemmer compiler creates all these methods as private. As  AccessibleObject#setAccessible() is called nowhere (we also don't want this), the SnowballProgram main class cannot call the method. Unfortunately it completely supresses all reflection errors and returns false. SnowballProgram then thinks, the called private boolean r_A() method returned false and so the whole snowball algorithm breaks. Also the snowball stemmer classes had a bug: methodObject is a static instance of the Stemmer, and Among calls the private method then on wrong object. So when fixing the above, this would fail again (because the stemmer changes a static singleton object, not itsself!). We also need to change the object in SnowballProgram to use \"this\" when calling the method. There are several possibilities to solve the private methods problem: The MethodHandles trick can be done the following way: The second part is done by a new Ant task: ant patch-snowball. Whenever you add a new Snowball stemmer, copy the java file generated by the snowball compiler to the ext directory and call the ant task. It will patch the methodObject declaration (and also add a @SuppressWarnings(\"unused\") for convenience). I reenabled the dictionary tests and the whole stemmer works. There are also no issues with security managers, because we do nothing security sensitive (all is our own stuff, no setAccessible). And finally: LovinsStemmer and FinnishStemmer got insanely fast (and correct, of course).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12750699/LUCENE-6742-Java8.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6772", "change_description": ": MultiCollector now catches CollectionTerminatedException and\nremoves the collector that threw this exception from the list of sub\ncollectors to collect.", "change_title": "MultiCollector should catch CollectionTerminatedException", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "If you wrap two collectors in a MultiCollector and one of them terminates early, then it will also make the other one terminate since MultiCollector propagates the CollectionTerminatedException.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12753743/LUCENE-6772.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6784", "change_description": ": IndexSearcher's query caching is enabled by default. Run\nindexSearcher.setQueryCache(null) to disable.", "change_title": "Enable query caching by default", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.4", "detail_description": "Now that our main queries have become immutable, I would like to revisit enabling the query cache by default.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754656/LUCENE-6784.patch", "patch_content": "none"}
{"library_version": "5.4.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6305", "change_description": ": BooleanQuery.equals and hashcode do not depend on the order of\nclauses anymore.", "change_title": "BooleanQuery.equals should ignore clause order", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "BooleanQuery.equals is sensitive to the order in which clauses have been added. So for instance \"+A +B\" would be considered different from \"+B +A\" although it generates the same matches and scores.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12765802/LUCENE-6305.patch", "patch_content": "none"}

{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5085", "change_description": ": MorfologikFilter will no longer stem words marked as keywords", "change_title": "MorfologikFilter shoudn't stem words marked as keyword", "detail_type": "Bug", "detail_affect_versions": "4.2.1", "detail_fix_versions": "4.4,6.0", "detail_description": "I added \"agd\" as keyword using solr.KeywordMarkerFilterFactory I would be able to add synonyms after solr.MorfologikFilterFactory:  agd => lodówka, zamrażarka, chłodziarka, piekarnik, etc. It's not possible right now. All words (even keywords) are threated same way.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4955", "change_description": ": NGramTokenFilter now emits all n-grams for the same token at the\nsame position and preserves the position length and the offsets of the\noriginal token.", "change_title": "NGramTokenFilter increments positions for each gram", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "NGramTokenFilter increments positions for each gram rather for the actual token which can lead to rather funny problems especially with highlighting. if this filter should be used for highlighting is a different story but today this seems to be a common practice in many situations to highlight sub-term matches. I have a test for highlighting that uses ngram failing with a StringIOOB since tokens are sorted by position which causes offsets to be mixed up due to ngram token filter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580631/LUCENE-4955.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4955", "change_description": ": NGramTokenizer now emits n-grams in a different order\n(a, ab, b, bc, c) instead of (a, b, c, ab, bc) and doesn't trim trailing\nwhitespaces.", "change_title": "NGramTokenFilter increments positions for each gram", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "NGramTokenFilter increments positions for each gram rather for the actual token which can lead to rather funny problems especially with highlighting. if this filter should be used for highlighting is a different story but today this seems to be a common practice in many situations to highlight sub-term matches. I have a test for highlighting that uses ngram failing with a StringIOOB since tokens are sorted by position which causes offsets to be mixed up due to ngram token filter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580631/LUCENE-4955.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5042", "change_description": ": The n-gram and edge n-gram tokenizers and filters now correctly\nhandle supplementary characters, and the tokenizers have the ability to\npre-tokenize the input stream similarly to CharTokenizer.", "change_title": "Improve NGramTokenizer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Now that we fixed NGramTokenizer and NGramTokenFilter to not produce corrupt token streams, the only way to have \"true\" offsets for n-grams is to use the tokenizer (the filter emits the offsets of the original token). Yet, our NGramTokenizer has a few flaws, in particular: Since we already broke backward compatibility for it in 4.4, I'd like to also fix these issues before we release.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12586881/LUCENE-5042.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4967", "change_description": ": NRTManager is replaced by\nControlledRealTimeReopenThread, for controlling which requests must\nsee which indexing changes, so that it can work with any\nReferenceManager", "change_title": "Absorb NRTManager entirely into a separate reopen thread class", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "I think NRTManager can be drastically simplified by moving all of its logic into a new reopen thread class.  All logic for waiting for a specific generation and reopening at different rates would live in this class. This would fully decouple the \"wait for generation X to be visible\" from which particular ReferenceManager impl you're using, which would make it possible to use the controlled consistency approach of NRTManager with any managers (e.g. SearcherTaxonomyManager).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580914/LUCENE-4967.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4973", "change_description": ": SnapshotDeletionPolicy no longer requires a unique\nString id", "change_title": "SnapshotDeletionPolicy should not require an id", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "The id is unnecessary and just adds complexity: SDP can just return the IndexCommit, and when you want to release you pass back the IndexCommit.  PersistentSDP can expose release(long gen).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581503/LUCENE-4973.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4946", "change_description": ": The internal sorting API (SorterTemplate, now Sorter) has been\ncompletely refactored to allow for a better implementation of TimSort.", "change_title": "Refactor SorterTemplate", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "When working on TimSort (LUCENE-4839), I was a little frustrated of not being able to add galloping support because it would have required to add new primitive operations in addition to compare and swap. I started working on a prototype that uses inheritance to allow some sorting algorithms to rely on additional primitive operations. You can have a look at https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts (but beware it is a prototype and still misses proper documentation and good tests). I think it would offer several advantages: If you are interested in comparing these implementations with Arrays.sort, there is a Benchmark class in src/examples. What do you think?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581687/LUCENE-4946.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4963", "change_description": ": Some TokenFilter options that generate broken TokenStreams have\nbeen deprecated: updateOffsets=true on TrimFilter and\nenablePositionIncrements=false on all classes that inherit from\nFilteringTokenFilter: JapanesePartOfSpeechStopFilter, KeepWordFilter,\nLengthFilter, StopFilter and TypeTokenFilter.", "change_title": "Deprecate broken TokenFilter constructors", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "We have some TokenFilters which are only broken with specific options. This includes: I think we should deprecate these behaviors in 4.4 and remove them in trunk.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581078/LUCENE-4963.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4963", "change_description": ": In order not to take position increments into account in\nsuggesters, you now need to call setPreservePositionIncrements(false) instead\nof configuring the token filters to not increment positions.", "change_title": "Deprecate broken TokenFilter constructors", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "We have some TokenFilters which are only broken with specific options. This includes: I think we should deprecate these behaviors in 4.4 and remove them in trunk.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581078/LUCENE-4963.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-3907", "change_description": ": EdgeNGramTokenizer now supports maxGramSize > 1024, doesn't trim\nthe input, sets position increment = 1 for all tokens and doesn't support\nbackward grams anymore.", "change_title": "Improve the Edge/NGramTokenizer/Filters", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "Our ngram tokenizers/filters could use some love.  EG, they output ngrams in multiple passes, instead of \"stacked\", which messes up offsets/positions and requires too much buffering (can hit OOME for long tokens).  They clip at 1024 chars (tokenizers) but don't (token filters).  The split up surrogate pairs incorrectly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581988/LUCENE-3907.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-3907", "change_description": ": EdgeNGramTokenFilter does not support backward grams and does\nnot update offsets anymore.", "change_title": "Improve the Edge/NGramTokenizer/Filters", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "Our ngram tokenizers/filters could use some love.  EG, they output ngrams in multiple passes, instead of \"stacked\", which messes up offsets/positions and requires too much buffering (can hit OOME for long tokens).  They clip at 1024 chars (tokenizers) but don't (token filters).  The split up surrogate pairs incorrectly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581988/LUCENE-3907.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4981", "change_description": ": PositionFilter is now deprecated as it can corrupt token stream\ngraphs. Since it main use-case was to make query parsers generate boolean\nqueries instead of phrase queries, it is now advised to use\nQueryParser.setAutoGeneratePhraseQueries(false) (for simple cases) or to\noverride QueryParser.newFieldQuery.", "change_title": "Deprecate PositionFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "According to the documentation (http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PositionFilterFactory), PositionFilter is mainly useful to make query parsers generate boolean queries instead of phrase queries although this problem can be solved at query parsing level instead of analysis level (eg. using QueryParser.setAutoGeneratePhraseQueries). So given that PositionFilter corrupts token graphs (see TestRandomChains), I propose to deprecate it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12583465/LUCENE-4981.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5018", "change_description": ": CompoundWordTokenFilterBase and its children\nDictionaryCompoundWordTokenFilter and HyphenationCompoundWordTokenFilter don't\nupdate offsets anymore.", "change_title": "Never update offsets in CompoundWordTokenFilterBase", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "CompoundWordTokenFilterBase and its children DictionaryCompoundWordTokenFilter and HyphenationCompoundWordTokenFilter update offsets. This can make OffsetAttributeImpl trip an exception when chained with other filters that group tokens together such as ShingleFilter, see http://www.gossamer-threads.com/lists/lucene/java-dev/196376?page=last.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12584956/LUCENE-5018.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5015", "change_description": ": SamplingAccumulator no longer corrects the counts of the sampled\ncategories. You should set TakmiSampleFixer on SamplingParams if required (but\nnotice that this means slower search).", "change_title": "Unexpected performance difference between SamplingAccumulator and StandardFacetAccumulator", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "I have an unexpected performance difference between the SamplingAccumulator and the StandardFacetAccumulator. The case is an index with about 5M documents and each document containing about 10 fields. I created a facet on each of those fields. When searching to retrieve facet-counts (using 1 CountFacetRequest), the SamplingAccumulator is about twice as fast as the StandardFacetAccumulator. This is expected and a nice speed-up. However, when I use more CountFacetRequests to retrieve facet-counts for more than one field, the speeds of the SampingAccumulator decreases, to the point where the StandardFacetAccumulator is faster. Is this behaviour normal? I did not expect it, as the SamplingAccumulator needs to do less work? Some code to show what I do:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585027/LUCENE-5015.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4933", "change_description": ": Replace ExactSimScorer/SloppySimScorer with just SimScorer. Previously\nthere were 2 implementations as a performance hack to support tableization of\nsqrt(), but this caching is removed, as sqrt is implemented in hardware with modern\njvms and its faster not to cache.", "change_title": "SweetSpotSimilarity doesnt override tf(float)", "detail_type": "Bug", "detail_affect_versions": "2.0.0", "detail_fix_versions": "4.4,6.0", "detail_description": "This means its scoring is not really right: it only applies to term queries and exact phrase queries, but not e.g. sloppy phrase queries and spans. As far as I can tell, its had this bug all along.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578623/LUCENE-4933.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5038", "change_description": ": MergePolicy now has a default implementation for useCompoundFile based\non segment size and noCFSRatio. The default implemantion was pulled up from\nTieredMergePolicy.", "change_title": "Don't call MergePolicy / IndexWriter during DWPT Flush", "detail_type": "Improvement", "detail_affect_versions": "4.3,6.0", "detail_fix_versions": "4.4,6.0", "detail_description": "We currently consult the indexwriter -> merge policy to decide if we need to write CFS or not which is bad in many ways. I wonder if we can use a simple boolean for this in the IWC and get away with not consulting merge policy. This would simplify concurrency a lot here already.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12587607/LUCENE-5038.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5063", "change_description": ": FieldCache.get(Bytes|Shorts), SortField.Type.(BYTE|SHORT) and\nFieldCache.DEFAULT_(BYTE|SHORT|INT|LONG|FLOAT|DOUBLE)_PARSER are now\ndeprecated. These methods/types assume that data is stored as strings although\nLucene has much better support for numeric data through (Int|Long)Field,\nNumericRangeQuery and FieldCache.get(Int|Long)s.", "change_title": "Allow GrowableWriter to store negative values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "For some use-cases, it would be convenient to be able to store negative values in a GrowableWriter, for example to use it in FieldCache: The first term is the minimum value and one could use a GrowableWriter to store deltas between this minimum value and the current value. (The need for negative values comes from the fact that maxValue - minValue might be larger than Long.MAX_VALUE.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12588423/LUCENE-5063.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5078", "change_description": ": TfIDFSimilarity lets you encode the norm value as any arbitrary long.\nAs a result, encode/decodeNormValue were made abstract with their signatures changed.\nThe default implementation was moved to DefaultSimilarity, which encodes the norm as\na single-byte value.", "change_title": "Allow TfIdfSimilarity implementations to encode norm values into more than a single byte", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Continuation from here: http://lucene.markmail.org/message/jtwit3pwu5oiqr2h.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589613/LUCENE-5078.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4890", "change_description": ": QueryTreeBuilder.getBuilder() only finds interfaces on the\nmost derived class.", "change_title": "QueryTreeBuilder.getBuilder() only finds interfaces on the most derived class", "detail_type": "Bug", "detail_affect_versions": "2.9,2.9.1,2.9.2,2.9.3,2.9.4,3.0,3.0.1,3.0.2,3.0.3,3.1,3.2,3.3,3.4,3.5,3.6,3.6.1,3.6.2", "detail_fix_versions": "3.6.3,4.4,6.0", "detail_description": "QueryBuilder implementations registered with QueryTreeBuilder.setBuilder() are not recognized by QueryTreeBuilder.getBuilder() if they are registered for an interface implemented by a superclass. Registering them for a concrete query node class or an interface implemented by the most-derived class do work.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12584817/LUCENE-4890_2013_05_25.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4997", "change_description": ": Internal test framework's tests are sensitive to previous\ntest failures and tests.failfast.", "change_title": "Internal test framework's tests are sensitive to previous test failures and tests.failfast.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4955", "change_description": ": NGramTokenizer now supports inputs larger than 1024 chars.", "change_title": "NGramTokenFilter increments positions for each gram", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "NGramTokenFilter increments positions for each gram rather for the actual token which can lead to rather funny problems especially with highlighting. if this filter should be used for highlighting is a different story but today this seems to be a common practice in many situations to highlight sub-term matches. I have a test for highlighting that uses ngram failing with a StringIOOB since tokens are sorted by position which causes offsets to be mixed up due to ngram token filter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580631/LUCENE-4955.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4959", "change_description": ": Fix incorrect return value in\nSimpleNaiveBayesClassifier.assignClass.", "change_title": "Incorrect return value from SimpleNaiveBayesClassifier.assignClass", "detail_type": "Bug", "detail_affect_versions": "4.2.1,6.0", "detail_fix_versions": "None", "detail_description": "The local copy of BytesRef referenced by foundClass is affected by subsequent TermsEnum.iterator.next() calls as the shared BytesRef.bytes changes. If a term \"test\" gives a good match and a next term in the terms collection is \"classification\" with a lower match score then the return result will be \"clas\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580740/LUCENE-4959.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4972", "change_description": ": DirectoryTaxonomyWriter created empty commits even if no changes\nwere made.", "change_title": "DirectoryTaxonomyWriter makes a commit even if no changes were made", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Now that IndexWriter allows committing changes even if the only change is setCommitData, DirectoryTaxonomyWriter creates empty commits because whenever you call commit/close, it sets as commitData the indexEpoch, thereby creating unnecessary commit points. I think that DirTaxoWriter should track if the index is dirty ... or preferably get that from IndexWriter (i.e. getChangeCount or something). I'll create a test case exposing the bug and then fix DTW.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581594/LUCENE-4972.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-949", "change_description": ": AnalyzingQueryParser can't work with leading wildcards.", "change_title": "AnalyzingQueryParser can't work with leading wildcards.", "detail_type": "Bug", "detail_affect_versions": "2.2", "detail_fix_versions": "4.4,6.0", "detail_description": "The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards: protected Query getWildcardQuery(String field, String termStr) throws ParseException \t{ \t\tString useTermStr = termStr; \t\tString leadingWildcard = null; \t\tif (\"*\".equals(field)) boolean hasLeadingWildcard = (useTermStr.startsWith(\"*\") || useTermStr.startsWith(\"?\")) ? true : false; if (!getAllowLeadingWildcard() && hasLeadingWildcard) \t\t\tthrow new ParseException(\"'*' or '?' not allowed as first character in WildcardQuery\"); if (getLowercaseExpandedTerms()) if (hasLeadingWildcard) List tlist = new ArrayList(); \t\tList wlist = new ArrayList(); \t\t/* else tmpBuffer.append(chars[i]); \t\t} \t\tif (isWithinToken) else // get Analyzer from superclass and tokenize the term \t\tTokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr)); \t\torg.apache.lucene.analysis.Token t; int countTokens = 0; \t\twhile (true) \t\t{ \t\t\ttry catch (IOException e) if (t == null) if (!\"\".equals(t.termText())) \t\t\t{ \t\t\t\ttry catch (IndexOutOfBoundsException ioobe) } \t\t} \t\ttry catch (IOException e) if (countTokens != tlist.size()) if (tlist.size() == 0) else if (tlist.size() == 1) \t\t{ \t\t\tif (wlist.size() == 1) \t\t\t{ \t\t\t\t/* sb.append((String) tlist.get(0)); \t\t\t\tsb.append(wlist.get(0).toString()); \t\t\t\treturn super.getWildcardQuery(field, sb.toString()); \t\t\t} \t\t\telse } \t\telse \t\t{ \t\t\t/* for (int i = 0; i < tlist.size(); i++) return super.getWildcardQuery(field, sb.toString()); \t\t} \t}", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581853/LUCENE-949.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4980", "change_description": ": Fix issues preventing mixing of RangeFacetRequest and\nnon-RangeFacetRequest when using DrillSideways.", "change_title": "Can't use DrillSideways with both RangeFacetRequest and non-RangeFacetRequest", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "I tried to combine these two and there were several issues:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582109/LUCENE-4980.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4996", "change_description": ": Ensure DocInverterPerField always includes field name\nin exception messages.", "change_title": "DocInverterPerField to log which field throws exceptions", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "One of ours fields seems to have a problem that didn't result in an exception before. It seems one of my filters doesn't deal with posIncAttr properly but Lucene did not log which of my numerous fields was the problem. This patch includes the field name in the exception.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582621/LUCENE-4996-trunk.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4992", "change_description": ": Fix constructor of CustomScoreQuery to take FunctionQuery\nfor scoringQueries. Instead use QueryValueSource to safely wrap arbitrary\nqueries and use them with CustomScoreQuery.", "change_title": "ArrayOutOfBoundsException in BooleanScorer2", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.4,6.0", "detail_description": "Seeing following exception in BooleanScorer2 in our production system: Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 2147483647 \tat org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:312) \tat org.apache.lucene.queries.CustomScoreQuery$CustomScorer.score(CustomScoreQuery.java:324) \tat org.apache.lucene.search.DisjunctionMaxScorer.score(DisjunctionMaxScorer.java:84) \tat org.apache.lucene.search.TopScoreDocCollector$InOrderTopScoreDocCollector.collect(TopScoreDocCollector.java:47) \tat org.apache.lucene.search.Scorer.score(Scorer.java:64) \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:605) \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:482) \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:438) \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281) \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582797/LUCENE-4992.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5016", "change_description": ": SamplingAccumulator returned inconsistent label if asked to\naggregate a non-existing category. Also fixed a bug in RangeAccumulator if\nsome readers did not have the requested numeric DV field.", "change_title": "Sampling can break FacetResult labeling", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "When sampling FacetResults, the TopKInEachNodeHandler is used to get the FacetResults. This is my case: A FacetResult is returned (which matches a FacetRequest) from the StandardFacetAccumulator. The facet has 0 results. The labelling of the root-node seems incorrect. I know, from the StandardFacetAccumulator, that the rootnode has a label, so I can use that one. Currently the recursivelyLabel method uses the taxonomyReader.getPath() to retrieve the label. I think we can skip that for the rootNode when there are no children (and gain a little performance on the way too?)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585268/LUCENE-5016.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5028", "change_description": ": Remove pointless and confusing doShare option in FST's\nPositiveIntOutputs", "change_title": "doShare is pointless in PositiveIntOutputs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "We recently use this in oal.core.fst.PositiveIntOutputs to indicate whether to share outputs. The comment mentioned 'with doShare=false, in some case this may result in a smaller FST'. However, this is not intuitive, as for long type, we always have the smallest output reduced to NO_OUTPUT, thus the smallest one is 'moved' towards root, and no extra output is created. However, if there are many many small outputs around root arcs, when we share outputs, a large output might be pushed into the root arcs. When root arcs are packed as fixed-array, yes the size of FST is increased. But, I suppose this should invoke other intuitive heuristics, instead of the confusing 'doShare'? Besides, this only exist in PositiveIntOutputs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585708/LUCENE-5028.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5032", "change_description": ": Fix IndexOutOfBoundsExc in PostingsHighlighter when\nmulti-valued fields exceed maxLength", "change_title": "PostingsHighlighter throws IndexOutOfBounds exception when using multivalued fields and get to maxLength", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "When using PostingsHighlighter with multi-valued fields, if the sum of the lengths of the fields is more than maxLength the highlighter throws an IndexOutOfBoundsException. I got to this error using Solr 4.3", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585867/LUCENE-5032.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4933", "change_description": ": SweetSpotSimilarity didn't apply its tf function to some\nqueries (SloppyPhraseQuery, SpanQueries).", "change_title": "SweetSpotSimilarity doesnt override tf(float)", "detail_type": "Bug", "detail_affect_versions": "2.0.0", "detail_fix_versions": "4.4,6.0", "detail_description": "This means its scoring is not really right: it only applies to term queries and exact phrase queries, but not e.g. sloppy phrase queries and spans. As far as I can tell, its had this bug all along.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12578623/LUCENE-4933.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5033", "change_description": ": SlowFuzzyQuery was accepting too many terms (documents) when\nprovided minSimilarity is an int > 1", "change_title": "SlowFuzzyQuery appears to fail with edit distance >=3 in some cases", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "Levenshtein edit btwn \"monday\" and \"montugu\" should be 4.  The following shows a query with \"sim\" set to 3, and there is a hit. public void testFuzzinessLong2() throws Exception", "patch_link": "https://issues.apache.org/jira/secure/attachment/12586500/LUCENE-5033.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5045", "change_description": ": DrillSideways.search did not work on an empty index.", "change_title": "DrillSideways.search yields IllegalArgEx if given IndexReader is empty", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "DrillSideways.create() creates either TopScoreDocCollector or TopFieldCollector with numHits = min(topN, reader.maxDoc()). When the reader is empty, these collectors throw IllegalArgEx that numHits should be > 0. While this is a correct behavior on their part, I think the behavior in DS is wrong. It's an optimization to ask for min(topN, reader.maxDoc(), i.e. if it just delegated topN all was well. So if we'd like to keep the optimization, we should at least make it min(topN,maxDoc+1), because there's nothing technically wrong from running a search against an empty index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12586943/LUCENE-5045.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4995", "change_description": ": CompressingStoredFieldsReader now only reuses an internal buffer\nwhen there is no more than 32kb to decompress. This prevents from running\ninto out-of-memory errors when working with large stored fields.", "change_title": "Remove the strong reference of CompressingStoredFieldsReader on the decompression buffer", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "CompressingStoredFieldsReader has a strong reference on the buffer it uses for decompression. Although it makes the reader able to reuse this buffer, this can trigger high memory usage in case some documents are very large. Creating this buffer on demand would help give memory back to the JVM.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12586939/LUCENE-4995.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5062", "change_description": ": If the spatial data for a document was comprised of multiple\noverlapping or adjacent parts then a CONTAINS predicate query might not match\nwhen the sum of those shapes contain the query shape but none do individually.\nA flag was added to use the original faster algorithm.", "change_title": "Spatial CONTAINS is sometimes incorrect for overlapped indexed shapes", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4", "detail_description": "If the spatial data for a document is comprised of multiple overlapping or adjacent parts, it might fail to match a query shape when doing the CONTAINS predicate when the sum of those shapes contain the query shape but none do individually.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12588403/LUCENE-5062_Spatial_CONTAINS_with_overlapping_shapes.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4971", "change_description": ": Fixed NPE in AnalyzingSuggester when there are too many\ngraph expansions.", "change_title": "NPE in AnalyzingSuggester", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.4,6.0", "detail_description": "Setting maxGraphExpansions > 0 with a lot of expansions (e.g. due to synonyms). Set<IntsRef> paths = toFiniteStrings(surfaceForm, ts2a); paths may be null, so maxAnalyzedPathsForOneInput = Math.max(maxAnalyzedPathsForOneInput, paths.size()) may end with NPE", "patch_link": "https://issues.apache.org/jira/secure/attachment/12588870/LUCENE-4971.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5080", "change_description": ": Combined setMaxMergeCount and setMaxThreadCount into one\nsetter in ConcurrentMergePolicy: setMaxMergesAndThreads.  Previously these\nsetters would not work unless you invoked them very carefully.", "change_title": "CMS setters cannot work unless you setMaxMergeCount before you setMaxThreadCount", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "but: So you must call them in a magical order. I think we should nuke these setters and just have a CMS(int,int)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12590134/LUCENE-5080.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5068", "change_description": ": QueryParserUtil.escape() does not escape forward slash.", "change_title": "QueryParserUtil.escape() does not escape forward slash", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.4,6.0", "detail_description": "QueryParserUtil.escape() and QueryParser.escape() have different implementations. Most important, the former omit escaping forward slash (\"/\"). This again caused errors in the queryparser when a query ended with forward slash.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5103", "change_description": ": A join on A single-valued field with deleted docs scored too few\ndocs.", "change_title": "join on single-valued field with deleted docs scores too few docs", "detail_type": "Bug", "detail_affect_versions": "4.3.1", "detail_fix_versions": "4.4", "detail_description": "TermsIncludingScoreQuery has an inner class SVInnerScorer used when the \"to\" side of a join is single-valued.  This has a nextDocOutOfOrder() method that is faulty when there are deleted documents, and a document that is deleted is matched by the join.  It'll terminate with NO_MORE_DOCS prematurely.  Interestingly, it appears MVInnerScorer (multi-valued) was coded properly to not have this problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12591963/LUCENE-5103_join_livedocs_bug.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5090", "change_description": ": Detect mismatched readers passed to\nSortedSetDocValuesReaderState and SortedSetDocValuesAccumulator.", "change_title": "SSDVA should detect a mismatch in the SSDVReaderState", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "This is trappy today: every time you open a new reader, you must create a new SSDVReaderState (this computes the seg -> global ord mapping), and pass that to SSDVA. But if this gets messed up (e.g. you pass an old SSDVReaderState) it will result in confusing AIOOBE, or silently invalid results.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12590715/LUCENE-5090.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5120", "change_description": ": AnalyzingSuggester modifed it's FST's cached root arc if payloads\nare used and the entire output resided on the root arc on the first access. This\ncaused subsequent suggest calls to fail.", "change_title": "AnalyzingSuggester might modify it's FST's cached root arc if payloads are used", "detail_type": "Bug", "detail_affect_versions": "4.4,6.0", "detail_fix_versions": "4.4,6.0", "detail_description": "if payloads are used with AnalyzingSuggester and the payload is entirely stored on a cached root arc AnalyzingSuggester modifies the payload BytesRef.length during lookup causing assertions to trip and subsequent requests fail with an negative array index", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593064/LUCENE-5120.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-4936", "change_description": ": Improve numeric doc values compression in case all values share\na common divisor. In particular, this improves the compression ratio of dates\nwithout time when they are encoded as milliseconds since Epoch. Also support\nTABLE compressed numerics in the Disk codec.", "change_title": "docvalues date compression", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "DocValues fields can be very wasteful if you are storing dates (like solr's TrieDateField does if you enable docvalues) and don't actually need all the precision: e.g. \"date-only\" fields like date of birth with no time component, time fields without milliseconds precision, and so on. Ideally we'd compute GCD of all the values to save space (numberOfTrailingZeros is not really enough here), but i think we should at least look for values like 86400000, 3600000, and 1000 to be practical.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12579869/LUCENE-4936.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-4951", "change_description": ": DrillSideways uses the new Scorer.cost() method to make\nbetter decisions about which scorer to use internally.", "change_title": "DrillSidewaysScorer should use Scorer.cost instead of its own heuristic", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Today it does the \"first docID\" trick to guess the cost of the baseQuery, which is silly now that we have cost API.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580027/LUCENE-4951.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-4976", "change_description": ": PersistentSnapshotDeletionPolicy writes its state to a\nsingle snapshots_N file, and no longer requires closing", "change_title": "PersistentSnapshotDeletionPolicy should save to a single file", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Today it creates a single-document Lucene index, and calls commit() after each snapshot/release. I think we can just use a single file instead, and remove Closeable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581637/LUCENE-4976.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-5035", "change_description": ": Compress addresses in FieldCacheImpl.SortedDocValuesImpl more\nefficiently.", "change_title": "FieldCacheImpl.SortedDocValuesImpl should compress addresses to term bytes more efficiently", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Each ordinal in SortedDocValuesImpl has a corresponding address to find its location in the big byte[] to support lookupOrd() Today this uses GrowableWriter with absolute addresses. But it would be much better to use MonotonicAppendingLongBuffer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12586312/LUCENE-5035.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-4941", "change_description": ": Sort \"from\" terms only once when using JoinUtil.", "change_title": "JoinUtil's TermsQuery should sort terms only once.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "The sorting of the 'from' terms occurs as often as the number of segments. This only needs to happen once.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12579303/LUCENE-4941.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-5050", "change_description": ": Close the stored fields and term vectors index files as soon as\nthe index has been loaded into memory to save file descriptors.", "change_title": "CompressingStoredFieldsReader should close the index file as soon as it has been read", "detail_type": "Improvement", "detail_affect_versions": "4.3.1", "detail_fix_versions": "4.4,6.0", "detail_description": "Although CompressingStoredFieldsReader loads the stored fields index into memory, it only closes the index file in close(). Closing at the end of the constructor should help save file descriptors. The same idea applies to CompressingTermVectorsReader.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12587064/LUCENE-5050.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Optimizations", "change_id": "LUCENE-5086", "change_description": ": RamUsageEstimator now uses official Java 7 API or a proprietary\nOracle Java 6 API to get Hotspot MX bean, preventing AWT classes to be\nloaded on MacOSX.", "change_title": "RamUsageEstimator causes AWT classes to be loaded by calling ManagementFactory#getPlatformMBeanServer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Yea, that type of day and that type of title . Since the last update of Java 6 on OS X, I started to see an annoying icon pop up at the doc whenever running elasticsearch. By default, all of our scripts add headless AWT flag so people will probably not encounter it, but, it was strange that I saw it when before I didn't. I started to dig around, and saw that when RamUsageEstimator was being loaded, it was causing AWT classes to be loaded. Further investigation showed that actually for some reason, calling ManagementFactory#getPlatformMBeanServer now with the new Java version causes AWT classes to be loaded (at least on the mac, haven't tested on other platforms yet). There are several ways to try and solve it, for example, by identifying the bug in the JVM itself, but I think that there should be a fix for it in Lucene itself, specifically since there is no need to call #getPlatformMBeanServer to get the hotspot diagnostics one (its a heavy call...). Here is a simple call that will allow to get the hotspot mxbean without using the #getPlatformMBeanServer method, and not causing it to be loaded and loading all those nasty AWT classes:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12590966/LUCENE-5086-branch4x.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5085", "change_description": ": MorfologikFilter will no longer stem words marked as keywords", "change_title": "MorfologikFilter shoudn't stem words marked as keyword", "detail_type": "Bug", "detail_affect_versions": "4.2.1", "detail_fix_versions": "4.4,6.0", "detail_description": "I added \"agd\" as keyword using solr.KeywordMarkerFilterFactory I would be able to add synonyms after solr.MorfologikFilterFactory:  agd => lodówka, zamrażarka, chłodziarka, piekarnik, etc. It's not possible right now. All words (even keywords) are threated same way.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5064", "change_description": ": Added PagedMutable (internal), a paged extension of\nPackedInts.Mutable which allows for storing more than 2B values.", "change_title": "Add PagedMutable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "In the same way that we now have a PagedGrowableWriter, we could have a PagedMutable which would behave just like PackedInts.Mutable but would support more than 2B values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12588359/LUCENE-5064.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4766", "change_description": ": Added a PatternCaptureGroupTokenFilter that uses Java regexes to\nemit multiple tokens one for each capture group in one or more patterns.", "change_title": "Pattern token filter which emits a token for every capturing group", "detail_type": "New Feature", "detail_affect_versions": "4.1", "detail_fix_versions": "4.4,6.0", "detail_description": "The PatternTokenizer either functions by splitting on matches, or allows you to specify a single capture group.  This is insufficient for my needs. Quite often I want to capture multiple overlapping tokens in the same position. I've written a pattern token filter which accepts multiple patterns and emits tokens for every capturing group that is matched in any pattern. Patterns are not anchored to the beginning and end of the string, so each pattern can produce multiple matches. For instance a pattern like : when matched against: would produce the tokens: Multiple patterns can be applied, eg these patterns could be used for camelCase analysis: When matched against the string \"letsPartyLIKEits1999_dude\", they would produce the tokens: If no token is emitted, the original token is preserved.  If the preserveOriginal flag is true, it will output the full original token (ie \"letsPartyLIKEits1999_dude\") in addition to any matching tokens (but in this case, if a matching token is identical to the original, it will only emit one copy of the full token). Multiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand. This is my first Java code, so apologies if I'm doing something stupid.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580262/LUCENE-4766.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4952", "change_description": ": Expose control (protected method) in DrillSideways to\nforce all sub-scorers to be on the same document being collected.\nThis is necessary when using collectors like\nToParentBlockJoinCollector with DrillSideways.", "change_title": "DrillSideways should expose \"scoreSubDocsAtOnce\" control", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "I hit this when running a ToParentBlockJoinCollector/Query under DrillSideways ... the problem is ToParentBlockJoinCollector.collect expects that all sub-scorers are positioned on the docID being collected, but DrillSideways sometimes scores with a in-order BooleanScorer-like scorer that advances each sub-scorer in chunks ... this breaks ToParentBlockJoinCollector. This is the same issue as LUCENE-2686, where apps that want to peek at the sub-scorers from their collector need those sub-scorers to all be \"on\" the current docID being collected... One way to \"fix\" this would be to switch based on Collector.acceptsDocsOutOfOrder() ... but that's really a hack ... it only \"works\" for BooleanQuery because BooleanScorer is an out-of-order scorer (well and because we fixed all BS2s to keep sub-scorers positioned on the doc being collected in LUCENE-3505). But if for example we added MUST clauses back into BooleanScorer (which I think we should!) then it could easily score those queries in-order.  Really we need another boolean (scoreSubDocsAtOnce or something) to Weight.scorer... but that's a big change... I think for this issue I'll just add an expert protected method to DrillSideways that returns this boolean, and an app could subclass to override.  Apps that \"know\" they are using queries/collectors like ToParentBlockJoinQuery/Collector must subclass and override ... DrillSideways already has other expert methods that you subclass & override.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580068/LUCENE-4952.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "SOLR-4761", "change_description": ": Add SimpleMergedSegmentWarmer, which just initializes terms,\nnorms, docvalues, and so on.", "change_title": "add option to plug in mergedsegmentwarmer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "This is pretty expert, but can be useful in some cases. We can also provide a simple minimalist implementation that just ensures datastructures are primed so the first queries aren't e.g. causing norms to be read from disk etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580565/SOLR-4761.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4964", "change_description": ": Allow arbitrary Query for per-dimension drill-down to\nDrillDownQuery and DrillSideways, to support future dynamic faceting\nmethods", "change_title": "Allow custom drill-down sub-queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Today the facet module indexes a term for each facet added to a document, and DrillDown/SidewaysQuery assume this by creating a TermQuery, or OR of TermQuery, for each dimension the app drills down on. I think we should relax this and allow an [expert] arbitrary query to drill down on a given dimension ... e.g., this can enable future dynamic faceting methods, or custom app drill-down methods. It's easy for DrillDownQuery to do this, but requires generalization in DrillSideways, basically just reviving the first approach on LUCENE-4748.  This approach is somewhat slower, but more general ... it will keep using the current method as an optimization when it applies. This should also fix the possible performance regression from LUCENE-4952 when scoreSubDocsAtOnce is true, by using the MinShouldMatchSumScorer in that case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580844/LUCENE-4964.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4966", "change_description": ": Add CachingWrapperFilter.sizeInBytes()", "change_title": "Add CachingWrapperFilter.sizeInBytes()", "detail_type": "Improvement", "detail_affect_versions": "4.4,6.0", "detail_fix_versions": "4.4,6.0", "detail_description": "I think it's useful to be able to check how much RAM a given CWF is using ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12580909/LUCENE-4966.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4965", "change_description": ": Add dynamic (no taxonomy index used) numeric range\nfaceting to Lucene's facet module", "change_title": "Add dynamic numeric range faceting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "The facet module today requires the app to compute the hierarchy at index time, eg a timestamp field might use a year/month/day hierarchy. While this gives great performance, since it minimizes the search-time computation, sometimes it's unfortunately useful/necessary to do things entirely at search time, like Solr does. E.g. I'm playing with a prototype Lucene search for Jira issues and I'd like to add a drill down+sideways for \"Updated in past day, 2 days, week, month\" etc.  But because time is constantly advancing, doing this at index time is a not easy ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581043/LUCENE-4965.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4979", "change_description": ": LiveFieldFields can work with any ReferenceManager, not\njust ReferenceManager<IndexSearcher>", "change_title": "LiveFieldValues should accept any ReferenceManager", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Today it requires ReferenceManager<IndexSearcher> but it doesn't rely on that at all (it just forwards that IndexSearcher to the subclass's lookup method).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581832/LUCENE-4979.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4975", "change_description": ": Added a new Replicator module which can replicate index\nrevisions between server and client.", "change_title": "Add Replication module to Lucene", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "I wrote a replication module which I think will be useful to Lucene users who want to replicate their indexes for e.g high-availability, taking hot backups etc. I will upload a patch soon where I'll describe in general how it works.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12582864/LUCENE-4975.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5022", "change_description": ": Added FacetResult.mergeHierarchies to merge multiple\nFacetResult of the same dimension into a single one with the reconstructed\nhierarchy.", "change_title": "Add FacetResult.mergeHierarchies", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "When you DrillSideways on a hierarchical dimension, and especially when you OR multiple drill-downs together, you get several FacetResults back, one for each category you drill down on. So for example, if you want to drill-down on Date/2010 OR Date/2011/May, the FacetRequests that you need to create (to get the sideways effect) are: Date/, Date/2010, Date/2011 and Date/2011/May. Date/ is because you want to get sideways counts as an alternative to Date/2010, and Date/2011 in order to get months count as an alternative to Date/2011/May. That results in 4 FacetResult objects. Having a utility which merges all FacetResults of the same dimension into a single hierarchical one will be very useful for e.g. apps that want to display the hierarchy. I'm thinking of FacetResult.mergeHierarchies which takes a List<FacetResult> and returns the merged ones, one FacetResult per dimension.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585174/LUCENE-5022.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5026", "change_description": ": Added PagedGrowableWriter, a new internal packed-ints structure\nthat grows the number of bits per value on demand, can store more than 2B\nvalues and supports random write and read access.", "change_title": "PagedGrowableWriter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "We already have packed data structures that support more than 2B values such as AppendingLongBuffer and MonotonicAppendingLongBuffer but none of them supports random write-access. We could write a PagedGrowableWriter for this, which would essentially wrap an array of GrowableWriters.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585787/LUCENE-5026.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5025", "change_description": ": FST's Builder can now handle more than 2.1 billion\n\"tail nodes\" while building a minimal FST.", "change_title": "Allow more than 2.1B \"tail nodes\" when building FST", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "We recently relaxed some of the limits for big FSTs, but there is one more limit I think we should fix.  E.g. Aaron hit it in building the world's biggest FST: http://aaron.blog.archive.org/2013/05/29/worlds-biggest-fst/ The issue is NodeHash, which currently uses a GrowableWriter (packed ints impl that can grow both number of bits and number of values): it's indexed by int not long. This is a hash table that's used to share suffixes, so we need random get/put on a long index of long values, i.e. this is logically a long[]. I think one simple way to do this is to make a \"paged\" GrowableWriter... Along with this we'd need to fix the hash codes to be long not int.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12585631/LUCENE-5025.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5063", "change_description": ": FieldCache.DEFAULT.get(Ints|Longs) now uses bit-packing to save\nmemory.", "change_title": "Allow GrowableWriter to store negative values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "For some use-cases, it would be convenient to be able to store negative values in a GrowableWriter, for example to use it in FieldCache: The first term is the minimum value and one could use a GrowableWriter to store deltas between this minimum value and the current value. (The need for negative values comes from the fact that maxValue - minValue might be larger than Long.MAX_VALUE.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12588423/LUCENE-5063.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5079", "change_description": ": IndexWriter.hasUncommittedChanges() returns true if there are\nchanges that have not been committed.", "change_title": "allow IndexWriter user to tell if there are uncommitted changes.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "IndexWriter already currently tracks if there are uncommitted changes.  We should expose this somehow... perhaps a boolean hasUncommittedChanges()?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589776/LUCENE-5079.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "SOLR-4565", "change_description": ": Extend NorwegianLightStemFilter and NorwegianMinimalStemFilter\nto handle \"nynorsk\"", "change_title": "Extend NorwegianMinimalStemFilter to handle \"nynorsk\"", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Norway has two official languages, both called \"Norwegian\", namely Bokmål (nb_NO) and Nynorsk (nn_NO). The NorwegianMinimalStemFilter and NorwegianLightStemFilter today only works with the largest of the two, namely Bokmål. Propose to incorporate \"nn\" support through a new \"vaiant\" config option:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589902/SOLR-4565.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5087", "change_description": ": Add getMultiValuedSeparator to PostingsHighlighter, for cases\nwhere you want a different logical separator between field values. This can\nbe set to e.g. U+2029 PARAGRAPH SEPARATOR if you never want passes to span\nvalues.", "change_title": "add PostingsHighlighter.getGapSeparator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Currently this is hardcoded to a space. But in some situations for a multi-valued field (e.g. authors field), its convenient to treat each value discretely. See LUCENE-2603 for example. So for such a field its nice if you can override and specify something else like U+2029 PARAGRAPH SEPARATOR.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12590425/LUCENE-5087.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-5013", "change_description": ": Added ScandinavianFoldingFilterFactory and\nScandinavianNormalizationFilterFactory", "change_title": "ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory", "detail_type": "New Feature", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "This filter is an augmentation of output from ASCIIFoldingFilter, it discriminate against double vowels aa, ae, ao, oe and oo, leaving just the first one. blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj räksmörgås == ræksmørgås == ræksmörgaos == raeksmoergaas == raksmorgas Caveats: Since this is a filtering on top of ASCIIFoldingFilter äöåøæ already has been folded down to aoaoae when handled by this filter it will cause effects such as: bøen -> boen -> bon åene -> aene -> ane I find this to be a trivial problem compared to not finding anything at all. Background: Swedish åäö is in fact the same letters as Norwegian and Danish åæø and thus interchangeable in when used between these languages. They are however folded differently when people type them on a keyboard lacking these characters and ASCIIFoldingFilter handle ä and æ differently. When a Swedish person is lacking umlauted characters on the keyboard they consistently type a, a, o instead of å, ä, ö. Foreigners also tend to use a, a, o. In Norway people tend to type aa, ae and oe instead of å, æ and ø. Some use a, a, o. I've also seen oo, ao, etc. And permutations. Not sure about Denmark but the pattern is probably the same. This filter solves that problem, but might also cause new.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12584933/LUCENE-5013.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "New Features", "change_id": "LUCENE-4845", "change_description": ": AnalyzingInfixSuggester finds suggestions based on\nmatches to any tokens in the suggestion, not just based on pure\nprefix matching.", "change_title": "Add AnalyzingInfixSuggester", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Our current suggester impls do prefix matching of the incoming text against all compiled suggestions, but in some cases it's useful to allow infix matching.  E.g, Netflix does infix suggestions in their search box. I did a straightforward impl, just using a normal Lucene index, and using PostingsHighlighter to highlight matching tokens in the suggestions. I think this likely only works well when your suggestions have a strong prior ranking (weight input to build), eg Netflix knows the popularity of movies.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589142/LUCENE-4845.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "API Changes", "change_id": "LUCENE-5077", "change_description": ": Make it easier to use compressed norms. Lucene42NormsFormat takes\nan overhead parameter, so you can easily pass a different value other than\nPackedInts.FASTEST from your own codec.", "change_title": "make it easier to use compressed norms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "Lucene42DVConsumer's ctor takes acceptableOverheadRatio, so that you can tradeoff time/space, and we pass PackedInts.FASTEST so we always use 8 bits per value. But the class is package private, so if I want to make my own NormsFormat and pass e.g. PackedInts.COMPACT, I can't ... I think we should make this class public / @experimental?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589513/LUCENE-5077.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "API Changes", "change_id": "LUCENE-5097", "change_description": ": Analyzer now has an additional tokenStream(String fieldName,\nString text) method, so wrapping by StringReader for common use is no\nlonger needed. This method uses an internal reuseable reader, which was\npreviously only used by the Field class.", "change_title": "Add utility method to Analyzer: public final TokenStream tokenStream(String fieldName,String text)", "detail_type": "Bug", "detail_affect_versions": "4.3.1", "detail_fix_versions": "4.4,6.0", "detail_description": "It might be a good idea to remove tons of useless code from tests: Most people use TokenStreams and Analyzers by only passing a String, wrapped by a StringReader. It would make life easier, if Analyzer would have an additional public (and final!!!) method that simply does the wrapping with StringReader by itsself. It might maybe not even needed to throw IOException (not sure)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12591231/LUCENE-5097.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "API Changes", "change_id": "LUCENE-4542", "change_description": ": HunspellStemFilter's maximum recursion level is now configurable.", "change_title": "Make RECURSION_CAP in HunspellStemmer configurable", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "4.4,6.0", "detail_description": "Currently there is  private static final int RECURSION_CAP = 2; in the code of the class HunspellStemmer. It makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex. it costs 36ms to stem long sentence in latvian for recursion_cap=2 and 5 ms for recursion_cap=1). It would be nice to be able to tune this number as needed. AFAIK this number (2) was chosen arbitrary. (it's a first issue in my life, so please forgive me any mistakes done).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552642/Lucene-4542-javadoc.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Build", "change_id": "LUCENE-4987", "change_description": ": Upgrade randomized testing to version 2.0.10:\nTest framework may fail internally due to overly aggresive J9 optimizations.", "change_title": "Test framework may fail internally under J9 (some serious JVM exclusive-section issue).", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "This was reported by Shai. The runner failed with an exception: The problem is that this is impossible because the code around JUnit4.java:809 looks like this: and the contract on Guava's EventBus states that: I wrote a simple snippet of code that does it in a loop and indeed, two threads can appear in the critical section at once. This is not reproducible on Hotspot and only appears to be the problem on J9/1.7/Windows (J9 1.6 works fine). I'll provide a workaround in the runner (an explicit monitor seems to be working) but this is some serious J9 issue.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Build", "change_id": "LUCENE-5043", "change_description": ": The eclipse target now uses the containing directory for the\nproject name.  This also enforces UTF-8 encoding when files are copied with\nfiltering.", "change_title": "Eclipse project name change - autogenerated", "detail_type": "Improvement", "detail_affect_versions": "4.3", "detail_fix_versions": "4.4,6.0", "detail_description": "The eclipse project name (created by 'ant eclipse') for most of the 4.x versions comes up as \"lucene_solr_branch_4x\" ... which causes a few problems.  Recently I needed to take a look at a particular class in Solr 4.2.1, 4.3.0, and branch_4x.  I couldn't load all three projects into Eclipse at the same time, because they have the same project name.  Even if I could have, it would have been very confusing.  I would like to improve this situation for the future. I have a couple of ideas right up front, both of which seem like reasonable ways to go: 1) Use the directory name, similar to what IntelliJ Idea does. 2) Use the \"fakeReleaseVersion\" property in the central build.xml.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12586570/LUCENE-5043.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Build", "change_id": "LUCENE-5055", "change_description": ": \"rat-sources\" target now checks also build.xml, ivy.xml,\nforbidden-api signatures, and parts of resources folders.", "change_title": "rat-sources target is missing build and ivy xml files, also resources folders", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "The check for copyright notices in files isn't checking all files, only source/test files.  A couple modules xml files are missing this (join, spatial, and custom-tasks.xml).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12587594/LUCENE-5055.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Build", "change_id": "LUCENE-5072", "change_description": ": Automatically patch javadocs generated by JDK versions\nbefore 7u25 to work around the frame injection vulnerability (CVE-2013-1571,\nVU#225657).", "change_title": "Fix frame injection bug in javadocs generated with Java 6 (and Java 7 prior u25)", "detail_type": "Bug", "detail_affect_versions": "4.3.1", "detail_fix_versions": "4.4,6.0", "detail_description": "The Apache Infra / Security team posted to all committers: Hi All, Oracle has announced [1], [2] a frame injection vulnerability in Javadoc generated by Java 5, Java 6 and Java 7 before update 22. [...] Please take the necessary steps to fix any currently published Javadoc and to ensure that any future Javadoc published by your project does not contain the vulnerability. The announcement by Oracle includes a link to a tool that can be used to fix Javadoc without regeneration. The infrastructure team is investigating options for preventing the publication of vulnerable Javadoc. The issue is public and may be discussed freely on your project's dev list. Thanks, Mark (ASF Infra) I fixed all published Javadocs on http://lucene.apache.org (for all historic releases where we have public available Javadocs on the web page). The mail also notes that we should not publish javadocs with this javadocs problem in the future. Unfortunately the release manager has to use the latest Java 7u25 version (released 2 days) ago. This would be fine for Lucene trunk (which is Java 7 only). But when we generate Javadocs JARs for Lucene 3 and 4, we cannot use Java 7 (to build the official release) because the javadocs would contain e.g. AutoCloaseable interface unless we use a JDK 6 or 5 bootclasspath (like we do for web pages). We also want the lucene/solr-*-javadoc.jar files to be correct, but those are built with Java 5 (3.x) or Java 6 (4.x). Unfortunately Oracle does not relaese a newer JDK 5 or JDK 6, so its impossible to do a release. But Oracle publishes the binary and source code of a \"fix tool\", that can be run on top of a tree of HTML files, patching all broken files (and only those). You can run it theoretically on the root folder of your harddisk - I did this on the whole lucene.apache.org web site. Robert Muir and I were looking for a IVY-compatible solution (the original Oracle tool cannot be automatically downloaded by IVY, as Oracle's website sets cookies and requests license confirmations). We found the following GITHUB project by olamy/karianna: https://github.com/AdoptOpenJDK/JavadocUpdaterTool As soon as they release the JAR file officially on Maven, we can download it with IVY and use it. This is a Maven Plugin, but it still contains the original source code of Oracle's tool, so we can execute it as ANT task after loading the JAR with IVY's coordinates: <java fork=\"false\" class=\"...\"/> In the GITHUB project description they note that you need JDK7 to use the tool, but this is no longer true, the -source/-target is Java 5 now, so we can run it easily. I will add the required tasks in common-build.xml's javadoc macro so it post-processes all javadocs and patches vulnerable files. If you build javadocs with a recent JDK, it would do nothing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589327/LUCENE-5072.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Tests", "change_id": "LUCENE-4901", "change_description": ": TestIndexWriterOnJRECrash should work on any\nJRE vendor via Runtime.halt().", "change_title": "TestIndexWriterOnJRECrash should work on any JRE vendor via Runtime.halt()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4,6.0", "detail_description": "I successfully compiled Lucene 4.2 with IBM. Then ran unit tests with the nightly option set to \"true\" The test case TestIndexWriterOnJRECrash was skipped returning \"IBM Corporation JRE not supported\": [junit4:junit4] Suite: org.apache.lucene.index.TestIndexWriterOnJRECrash [junit4:junit4] IGNOR/A 0.28s | TestIndexWriterOnJRECrash.testNRTThreads [junit4:junit4]    > Assumption #1: IBM Corporation JRE not supported. [junit4:junit4] Completed in 0.68s, 1 test, 1 skipped", "patch_link": "https://issues.apache.org/jira/secure/attachment/12583307/LUCENE-4901.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in runtime behavior", "change_id": "LUCENE-5038", "change_description": ": New segments written by IndexWriter are now wrapped into CFS\nby default. DocumentsWriterPerThread doesn't consult MergePolicy anymore\nto decide if a CFS must be written, instead IndexWriterConfig now has a\nproperty to enable / disable CFS for newly created segments.", "change_title": "Don't call MergePolicy / IndexWriter during DWPT Flush", "detail_type": "Improvement", "detail_affect_versions": "4.3,6.0", "detail_fix_versions": "4.4,6.0", "detail_description": "We currently consult the indexwriter -> merge policy to decide if we need to write CFS or not which is bad in many ways. I wonder if we can use a simple boolean for this in the IWC and get away with not consulting merge policy. This would simplify concurrency a lot here already.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12587607/LUCENE-5038.patch", "patch_content": "none"}
{"library_version": "4.4.0", "change_type": "Changes in runtime behavior", "change_id": "LUCENE-5107", "change_description": ": Properties files by Lucene are now written in UTF-8 encoding,\nUnicode is no longer escaped. Reading of legacy properties files with\n\\u escapes is still possible.", "change_title": "Convert all Properties#store() and load() to use UTF-8 charset", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "Followup of LUCENE-5106: This needs to be changed and the forbidden signatures changed to disallow InputStream/OutputStream and allow Reader/Writer only.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12592032/LUCENE-5107-4.4.patch", "patch_content": "none"}

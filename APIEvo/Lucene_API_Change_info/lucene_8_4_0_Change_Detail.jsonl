{"library_version": "8.4.0", "change_type": "API Changes", "change_id": "LUCENE-9029", "change_description": ": Deprecate SloppyMath toRadians/toDegrees in favor of Java Math.", "change_title": "Deprecate SloppyMath toRadians/toDegrees in favor of Java Math", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "This change follows a TODO left in SloppyMath to remove toRadians/toDegrees since from Java 9 forward Math toRadians/toDegrees is now identical. Since these methods/constants are public, deprecation messages are added to each one. Internally, in Lucene, all instances of the SloppyMath versions are replaced with the standard Java Math versions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12984291/LUCENE-9029.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "New Features", "change_id": "LUCENE-8620", "change_description": ": Add CONTAINS support for LatLonShape and XYShape.", "change_title": "Add CONTAINS support for LatLonShape", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Currently the only spatial operation that cannot be performed using LatLonShape is CONTAINS. This issue will add such capability by tracking if an edge of a generated triangle from the Tessellator is an edge of the polygon.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955253/LUCENE-8620.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9002", "change_description": ": Skip costly caching clause in LRUQueryCache if it makes the query\nmany times slower.", "change_title": "query caching leads to absurdly slow queries", "detail_type": "Improvement", "detail_affect_versions": "7.7.2,8.2", "detail_fix_versions": "None", "detail_description": "Description We have dozens of ES clusters(based on Lucene) for metric scenarios. Most of the queries are like this: host_ip:10.10.10.10 AND timestamp:[2019-10-01 00:00:00 TO 2019-10-05 23:59:59]. And we frequently encounter some absurdly slow queries. Solution For a long time range query(e.g. 5 days), each range query will consume tens of megabytes of memory and spend hundreds of milliseconds to cache, but the benefits are not obvious. And those large cache entries will cause frequent cache eviction. So it's better to  skip the caching action directly when large range query appears with a selective lead iterator.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9006", "change_description": ": WordDelimiterGraphFilter's catenateAll token is now ordered before any token parts, like WDF did.", "change_title": "Ensure WordDelimiterGraphFilter always emits catenateAll token early", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Ideally, the first token of WDGF is the preserveOriginal (if configured to emit), and the second should be the catenateAll (if configured to emit).  The deprecated WDF does this but WDGF can sometimes put the first other token earlier when there is a non-emitted candidate sub-token. Example input \"8-other\" when only generateWordParts and catenateAll – not generateNumberParts.  WDGF internally sees the '8' but moves on.  Ultimately, the \"other\" token and the catenated \"8other\" will appear at the same internal position, which by luck fools the sorter to emit \"other\" first.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9028", "change_description": ": introducing Intervals.multiterm()", "change_title": "Public method for MultiTermIntervalSource", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Right now we have prefix and widlcard for multiterm Intervals. Sometimes it's necessary to provide terms set in a more generic way as automaton. As a benefit we can handle it more efficient rather than or over terms. What do you think?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12984305/LUCENE-9028.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9018", "change_description": ": ConcatenateGraphFilter now has a configurable separator.", "change_title": "Separator for ConcatenateGraphFilterFactory", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "I would like to have an option to choose a separator to use for token concatenation. Currently ConcatenateGraphFilterFactory can use only \"\\u001F\" symbol.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12985615/LUCENE-9018.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9036", "change_description": ": ExitableDirectoryReader may interupt scaning over DocValues", "change_title": "ExitableDirectoryReader to interrupt DocValues as well", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "This allow to make AnalyticsComponent and json.facet sensitive to time allowed. Does it make sense? Is it enough to check on DV creation ie per field/segment or it's worth to check every Nth doc?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12986052/LUCENE-9036.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9062", "change_description": ": QueryVisitor now has a consumeTermsMatching() method, allowing queries\nthat match a class of terms to pass a ByteRunAutomaton matching those that class\nback to the visitor.", "change_title": "Extend QueryVisitor to support multi-term queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Currently, QueryVisitor only allows queries to report that they consume a fixed set of terms.  For multi-term queries, however, they don't know which terms in an index they're going to match until rewrite time.  Current users of this API get round this by using instanceof checks in a `visitLeaf()` method, but this is clunky and does not adapt well to user-defined queries. We should extend QueryVisitor so that queries can report that they consume a class of terms matching an automaton, in addition to individual terms.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Improvements", "change_id": "LUCENE-9073", "change_description": ": IntervalQuery to respond field on toString() and explain()", "change_title": "IntervalQuery to respond field on toString and explain", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "It seems like IntervalQuery doesnt' expose the field whether on toString() nor via explain(). I think it's worth to return field for better visibility. WDYT, romseygeek ?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12987270/LUCENE-9073.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-8928", "change_description": ": When building a kd-tree for dimensions n > 2, compute exact bounds for an inner node every N splits\nto improve the quality of the tree. N is defined by SPLITS_BEFORE_EXACT_BOUNDS which is set to 4.", "change_title": "BKDWriter could make splitting decisions based on the actual range of values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Currently BKDWriter assumes that splitting on one dimension has no effect on values in other dimensions. While this may be ok for geo points, this is usually not true for ranges (or geo shapes, which are ranges too). Maybe we could get better indexing by re-computing the range of values on each dimension before making the choice of the split dimension?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-8992", "change_description": ": TopFieldCollector and TopScoreDocCollector can now share minimum scores across leaves\nconcurrently.", "change_title": "Share minimum score across segments in concurrent search", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.4", "detail_description": "As a follow up of  LUCENE-8978 we should share the minimum score in concurrent search for top field collectors that sort on relevance first. The same logic should be applicable with the only condition that the primary sort is by relevance.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-8932", "change_description": ": BKDReader's index is now stored off-heap when the IndexInput is\nan instance of ByteBufferIndexInput.", "change_title": "Allow BKDReader packedIndex to be off heap", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "This change modifies BKDReader to read the packedIndex bytes off heap rather than load them all on heap at a single time. Questions for discussion: Using luceneutils IndexAndSearchOpenStreetMaps present the following test results: with -box -points (patch) READER MB: 1.1345596313476562 BEST M hits/sec: 73.34277344984474 BEST QPS: 74.63011169783009 with -box -points (original) READER MB: 1.7249317169189453 BEST M hits/sec: 73.77125157623486 BEST QPS: 75.06611062353801 with -nearest 10 -points (patch) READER MB: 1.1345596313476562 BEST M hits/sec: 0.013586298373879497 BEST QPS: 1358.6298373879497 with -nearest 10 -points (original) READER MB: 1.7249317169189453 BEST M hits/sec: 0.01445208197367343 BEST QPS: 1445.208197367343 with -box -geo3d (patch) READER MB: 1.1345596313476562 BEST M hits/sec: 39.84968715299074 BEST QPS: 40.54914292796736 with -box -geo3d (original) READER MB: 1.7456226348876953 BEST M hits/sec: 40.45051734329004 BEST QPS: 41.160519101846695", "patch_link": "https://issues.apache.org/jira/secure/attachment/12983956/LUCENE-8932.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-9024", "change_description": ": IntroSelector now falls back to the median of medians algorithm\ninstead of sorting when the maximum recursion level is exceeded, providing\nbetter worst-case runtime.", "change_title": "Optimize IntroSelector for worst case", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "There is a TODO in IntroSelector.java to use the median of medians algorithm instead of HeapSort for the worst case, as median of medians offers a better time complexity. I've discussed this with jpountz and he has agreed to review my work on this.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-8920", "change_description": ": The denser arcs of FST now index labels with a bitset in order\nto provide near constant time access.", "change_title": "Reduce size of FSTs due to use of direct-addressing encoding", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Some data can lead to worst-case ~4x RAM usage due to this optimization. Several ideas were suggested to combat this on the mailing list: I think we can improve thesituation here by tracking, per-FST instance, the size increase we're seeing while building (or perhaps do a preliminary pass before building) in order to decide whether to apply the encoding. we could also make the encoding a bit more efficient. For instance I noticed that arc metadata is pretty large in some cases (in the 10-20 bytes) which make gaps very costly. Associating each label with a dense id and having an intermediate lookup, ie. lookup label > id and then id>arc offset instead of doing label->arc directly could save a lot of space in some cases? Also it seems that we are repeating the label in the arc metadata when array-with-gaps is used, even though it shouldn't be necessary since the label is implicit from the address?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-9027", "change_description": ": Use SIMD instructions to decode postings.", "change_title": "SIMD-based decoding of postings lists", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "rcmuir has been mentioning the idea for quite some time that we might be able to write the decoding logic in such a way that Java would use SIMD instructions. More recently paul.masurel wrote a blog post that raises the point that Lucene could still do decode multiple ints at once in a single instruction by packing two ints in a long and we've had some discussions about what we could try in Lucene to speed up the decoding of postings. This made me want to look a bit deeper at what we could do. Our current decoding logic reads data in a byte[] and decodes packed integers from it. Unfortunately it doesn't make use of SIMD instructions and looks like this. I confirmed by looking at the generated assembly that if I take an array of integers and shift them all by the same number of bits then Java will use SIMD instructions to shift multiple integers at once. This led me to writing this implementation that tries as much as possible to shift long sequences of ints by the same number of bits to speed up decoding. It is indeed faster than the current logic we have, up to about 2x faster for some numbers of bits per value. Currently the best implementation I've been able to come up with combines the above idea with the idea that Paul mentioned in his blog that consists of emulating SIMD from Java by packing multiple integers into a long: 2 ints, 4 shorts or 8 bytes. It is a bit harder to read but gives another speedup on top of the above implementation. I have a JMH benchmark available in case someone would like to play with this and maybe even come up with an even faster implementation. It is 2-2.5x faster than our current implementation for most numbers of bits per value. I'm copying results here: The thing that is a bit frustrating is that the best throughputs are obtained on a ByteBuffer that is configured to use the little endian byte order (which is the native byte order of my machine) while Java/Lucene default to big endian. So if we want that kind of throughput we'll probably need to add ways to read data in the native byte order in the IndexInput API.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-9049", "change_description": ": Remove FST cached root arcs now redundant with labels indexed by bitset.\nThis frees some on-heap FST space.", "change_title": "Remove FST cachedRootArcs now redundant with direct-addressing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "With LUCENE-8920 FST most often encodes top level nodes with direct-addressing (instead of array for binary search). This probably made the cachedRootArcs redundant. So they should be removed, and this will reduce the code.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12986441/LUCENE-9049.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Optimizations", "change_id": "LUCENE-9045", "change_description": ": Do not use TreeMap/TreeSet in BlockTree and PerFieldPostingsFormat.", "change_title": "Do not use TreeMap/TreeSet in BlockTree and PerFieldPostingsFormat", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "TreeMap/TreeSet is a heavy structure designed to dynamically sort keys. It's iterator is much less performant than a list iterator. We should not use it when we don't need the sorting capability once built. And this is the case in BlockTreeTermsReader and PerFieldPostingsFormat. We need a Map and to sort keys at building time. But once built, we don't need to sort anymore, we can use a simple list for iteration efficiency.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9001", "change_description": ": Fix race condition in SetOnce.", "change_title": "Race condition in SetOnce", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "There is race condition in SetOnce that can cause code below fail with NullPointerException: This is caused by 2 separate write operations - 1 for set marker field and 1 for actual object. So it's possible that marker is already set to true (causing AlreadySetException on second write attempt) but object is still not set (causing NullPointerException). This can be avoided by using single AtomicReference instead to serve both purposes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9030", "change_description": ": Fix WordnetSynonymParser behaviour so it behaves similar to\nSolrSynonymParser.", "change_title": "Solr- and WordnetSynonymParser behaviour differs", "detail_type": "Bug", "detail_affect_versions": "8.2", "detail_fix_versions": "9.0,8.4", "detail_description": "Equivalent synonyms are showing up with different token types and ordering depending on whether the Solr format or the Wordnet format is used. A synonym set like \"woods, wood, forest\" in Solr format leads to the following token stream (term and type) when analyzing the term \"forest\": \"forest\"/word, \"woods\"/SYNONYM, \"wood\" /SYNONYM  The following set in Wordnet format should give the same output (all terms are in the same synset), however all tokens are of type SYNONYM here and the original input token \"forest\" isn't the first one: synonyms.txt: Token stream (term/type) when an woods\"/SYNONYM, \"wood\" /SYNONYM, \"forest\"/SYNONYM I don't think this is intentional and is confusing (especially because the \"original\" input token type gets lost). I saw that the way the synsets are added to the SynonymMap in the respective parsers differes and have a PR that changes this.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9054", "change_description": ": Fix reproduceJenkinsFailures.py to not overwrite junit XML files when retrying", "change_title": "reproduceJenkinsFailures.py usage in the Lucene-Solr-repro jenkins job under reports number of failures", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.4", "detail_description": "Our reproduceJenkinsFailures.py script as used by the https://builds.apache.org/job/Lucene-Solr-repro/ runs the tests multiple times, overwriting the same junit TEST-*.xml test result files each time, causing the jenkins job to under report how many times the various test(s) fail.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12986449/LUCENE-9054.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9031", "change_description": ": UnsupportedOperationException on MatchesIterator.getQuery()", "change_title": "UnsupportedOperationException on highlighting Interval Query", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "When UnifiedHighlighter highlights Interval Query it encounters UnsupportedOperationException.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12986590/LUCENE-9031.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8996", "change_description": ": maxScore was sometimes missing from distributed grouped responses.", "change_title": "maxScore is sometimes missing from distributed grouped responses", "detail_type": "Bug", "detail_affect_versions": "5.3", "detail_fix_versions": "9.0,8.4", "detail_description": "This issue occurs when using the grouping feature in distributed mode and sorting by score. Each group's docList in the response is supposed to contain a maxScore entry that hold the maximum score for that group. Using the current releases, it sometimes happens that this piece of information is not included: Looking into the issue, it comes from the fact that if a shard does not contain a document from that group, trying to merge its maxScore with real maxScore entries from other shards is invalid (it results in NaN). I'm attaching a patch containing a fix.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12983895/LUCENE-8996.03.patch", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9055", "change_description": ": Fix the detection of lines crossing triangles through edge points.", "change_title": "Line2D misses crossings through triangle edge points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "when a line crosses a triangle through an edge point, the crossing is not detected.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9103", "change_description": ": Disjunctions can miss some hits in some rare conditions.", "change_title": "WANDScorer can miss some hits", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "I found the issue while building the release candidate for 8.4. There is a rare bug in WANDScorer that occurs when a block is ignored because the sum of the maximum scores is less than the minimum competitive score. In that case WANDScorer advances until it finds a block where the sum of the maximum scores is greater than the minimum competitive score. Then it pops clauses from the tail until the maximum score of the tail gets smaller than the minimum competitive score, advances these clauses beyond the current target, and treats the smallest doc ID as the next candidate. This is where the bug lies: sometimes this candidate will be beyond the current block and WANDScorer won't update maximum scores of other clauses. In this case it might skip over hits thinking that they can't have a competitive score when in fact they could.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Other", "change_id": "LUCENE-8979", "change_description": ": Code Cleanup: Use entryset for map iteration wherever possible. - Part 2", "change_title": "Code Cleanup: Use entryset for map iteration wherever possible - part 2 possible.", "detail_type": "Improvement", "detail_affect_versions": "8.2", "detail_fix_versions": "8.4", "detail_description": "For some reason, not all issues had been picked up in the previous ticket. This should be all of them. My bad I suppose.  ==========================  Simple, non-important code cleanup. Again, to clarify, please don't bother yourself with this ticket on company time, on personal time you could be working on something that makes you money or improves the product for your feature personally.  This entire ticket is an afterthough. A look back at the code base that most people don't have the time for.  ================  While true that using `entrySet()` is really only an improvement for traversing a TreeMap(at least that's how it was in JDK8), it's a good practice in general to use it over keySet(), if you then use that keyset to do an extra lookup to get the value as well as the key.  So that's what this ticket is.  All changes were done automatically via Intellij's built-in code analysis.  Putting this on LUCENE because code both in lucene and solr was changed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Other", "change_id": "LUCENE-8746", "change_description": ": Refactor EdgeTree - Introduce a Component tree that represents the tree of components (e.g polygons).\nEdge tree is now just a tree of edges.", "change_title": "Make EdgeTree (aka ComponentTree) support different type of components", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Currently the class EdgeTree is a bit confusing as it is in reality a tree of components. The inner class Edge is the one that builds a tree of edges which is used by Polygon2D and Line2D to represent their structure. Here is proposed: 1) Create a new class called ComponentTree which is in fact the current EdgeTree 2) Modify EdgeTree to be in fact the inner class Edge 3) Extract a Component interface so we can have different types of components in the same tree. This allow us to support heterogeneous trees of components. 4) Make Polygon2D and Line2D instance of the component interface. 4) With this change, LatLonShapePolygonQuery and LatLonShapeLineQuery can be replaced with one LatLonShapeComponentQuery.   ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Other", "change_id": "LUCENE-8994", "change_description": ": Code Cleanup - Pass values to list constructor instead of empty constructor followed by addAll().", "change_title": "Code Cleanup - Pass values to list constructor instead of empty constructor followed by addAll().", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Trivial ticket, if your time is precious, feel free to skip.  In case of passing a collection into another, it's more efficient to pass the already existing collection into the constructor of the new one. It can usually deal with it more efficiently.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Other", "change_id": "LUCENE-9046", "change_description": ": Fix wrong example in Javadoc of TermInSetQuery", "change_title": "Fix wrong example in Javadoc of TermInSetQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "There is a wrong example in Javadoc of TermInSetQuery. This patch will be merged to the master and 8.x branch.  Before After", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Other", "change_id": "LUCENE-8983", "change_description": ": Add sandbox PhraseWildcardQuery to control multi-terms expansions in a phrase.", "change_title": "PhraseWildcardQuery - new query to control and optimize wildcard expansions in phrase", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "A generalized version of PhraseQuery, built with one or more MultiTermQuery that provides term expansions for multi-terms (one of the expanded terms must match). Its main advantage is to control the total number of expansions across all MultiTermQuery and across all segments. This query is similar to MultiPhraseQuery, but it handles, controls and optimizes the multi-term expansions. This query is equivalent to building an ordered SpanNearQuery with a list of SpanTermQuery and SpanMultiTermQueryWrapper.  But it optimizes the multi-term expansions and the segment accesses.  It first resolves the single-terms to early stop if some does not match. Then it expands each multi-term sequentially, stopping immediately if one does not match. It detects the segments that do not match to skip them for the next expansions. This often avoid expanding the other multi-terms on some or even all segments. And finally it controls the total number of expansions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Other", "change_id": "LUCENE-9067", "change_description": ": Polygon2D#contains() is now thread safe.", "change_title": "Polygon2D#contains is not thread safe", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "We hit the following test error:   The problem is that Polygon2D is not thread safe but Lucene assumes that multiple scorers from the same weight can be consumed concurrently. I that case two threads accessing the contains method on the same polygon can lead to bad answers.    ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.4.0", "change_type": "Build", "change_id": "LUCENE-9041", "change_description": ": Upgrade ecj to 3.19.0 to fix sporadic precommit javadoc issues", "change_title": "Upgrade ecj to 3.19.0 to fix sporadic precommit javadoc issues", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Lucene/Solr dev mail list post: http://mail-archives.apache.org/mod_mbox/lucene-dev/201911.mbox/%3CCAJU9nmhzmvg1mWPup9%2Bg3V%3Dsbz18M2DLO-3asEqaUCQpcZHiYA%40mail.gmail.com%3E", "patch_link": "https://issues.apache.org/jira/secure/attachment/12985548/LUCENE-9041.patch", "patch_content": "none"}

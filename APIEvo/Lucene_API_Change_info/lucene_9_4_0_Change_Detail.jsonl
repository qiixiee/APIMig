{"library_version": "9.4.0", "change_type": "API Changes", "change_id": "LUCENE-10577", "change_description": ": Add VectorEncoding to enable byte-encoded HNSW vectors", "change_title": "Enable quantization of HNSW vectors to 8 bits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The KnnVectorField api handles vectors with 4-byte floating point values. These fields can be used (via KnnVectorsReader) in two main ways: 1. The VectorValues iterator enables retrieving values 2. Approximate nearest -neighbor search The main point of this addition was to provide the search capability, and to support that it is not really necessary to store vectors in full precision. Perhaps users may also be willing to retrieve values in lower precision for whatever purpose those serve, if they are able to store more samples. We know that 8 bits is enough to provide a very near approximation to the same recall/performance tradeoff that is achieved with the full-precision vectors. I'd like to explore how we could enable 4:1 compression of these fields by reducing their precision. A few ways I can imagine this would be done: 1. Provide a parallel byte-oriented API. This would allow users to provide their data in reduced-precision format and give control over the quantization to them. It would have a major impact on the Lucene API surface though, essentially requiring us to duplicate all of the vector APIs. 2. Automatically quantize the stored vector data when we can. This would require no or perhaps very limited change to the existing API to enable the feature. I've been exploring (2), and what I find is that we can achieve very good recall results using dot-product similarity scoring by simple linear scaling + quantization of the vector values, so long as  we choose the scale that minimizes the quantization error. Dot-product is amenable to this treatment since vectors are required to be unit-length when used with that similarity function. Even still there is variability in the ideal scale over different data sets. A good choice seems to be max(abs(min-value), abs(max-value)), but of course this assumes that the data set doesn't have a few outlier data points. A theoretical range can be obtained by 1/sqrt(dimension), but this is only useful when the samples are normally distributed. We could in theory determine the ideal scale when flushing a segment and manage this quantization per-segment, but then numerical error could creep in when merging. I'll post a patch/PR with an experimental setup I've been using for evaluation purposes. It is pretty self-contained and simple, but has some drawbacks that need to be addressed: 1. No automated mechanism for determining quantization scale (it's a constant that I have been playing with) 2. Converts from byte/float when computing dot-product instead of directly computing on byte values I'd like to get people's feedback on the approach and whether in general we should think about doing this compression under the hood, or expose a byte-oriented API. Whatever we do I think a 4:1 compression ratio is pretty compelling and we should pursue something.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "New Features", "change_id": "LUCENE-10654", "change_description": ": Add new ShapeDocValuesField for LatLonShape and XYShape.", "change_title": "New companion doc value format for LatLonShape and XYShape field types", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "XYDocValuesField provides doc value support for XYPoint.  LatLonDocValuesField provides docvalue support for LatLonPoint. However, neither LatLonShape nor XYShape currently have a docvalue format.  This lack of doc value support for shapes means facets, aggregations, and IndexOrDocValues queries are currently not possible for Shape field types. This gap needs be closed in lucene. To support IndexOrDocValues queries along with various geometry aggregations and facets, the ability to compute the spatial relation with the doc value is needed. This is straightforward with XYPoint and LatLonPoint since the doc value encoding is nothing more than a simple 2D integer encoding of the x,y and lat,lon dimensional components. Accomplishing the same with a naive integer encoded binary representation for N-vertex shapes would be costly. ComponentTree already provides an efficient in memory structure for quickly computing spatial relations over Shape types based on a binary tree of tessellated triangles provided by the Tessellator. Furthermore, this tessellation is already computed at index time. If we create an on-disk representation of ComponentTree 's binary tree of tessellated triangles and use this as the doc value binaryValue format we will be able to efficiently compute spatial relations with this binary representation and achieve the same facet/aggregation result over shapes as we can with points today (e.g., grid facets, centroid, area, etc).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "New Features", "change_id": "LUCENE-10629", "change_description": ": Support match set filtering with a query in MatchingFacetSetCounts.", "change_title": "Add fastMatchQuery param to MatchingFacetSetCounts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "Some facet counters, like RangeFacetCounts, allow the user to pass in a fastMatchQuery parameter in order to quickly and efficiently filter out documents in the passed in match set. We should create this same parameter in MatchingFacetSetCounts as well.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "New Features", "change_id": "LUCENE-10633", "change_description": ": SortField#setOptimizeSortWithIndexedData and\nSortField#getOptimizeSortWithIndexedData were introduced to provide\nan option to disable sort optimization for various sort fields.", "change_title": "Dynamic pruning for queries sorted by SORTED(_SET) field", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "LUCENE-9280 introduced the ability to dynamically prune non-competitive hits when sorting by a numeric field, by leveraging the points index to skip documents that do not compare better than the top of the priority queue maintained by the field comparator. However queries sorted by a SORTED(_SET) field still look at all hits, which is disappointing. Could we leverage the terms index to skip hits?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "New Features", "change_id": "GITHUB#912", "change_description": ": Support for Java 19 foreign memory support was added. Applications started\nwith command line parameter \"java --enable-preview\" will automatically use the new\nforeign memory API of Java 19 to access indexes on disk with MMapDirectory. This is\nan opt-in feature and requires explicit Java command line flag! When enabled, Lucene logs\na notice using java.util.logging. Please test thoroughly and report bugs/slowness to Lucene's\nmailing list. When the new API is used, MMapDirectory will mmap Lucene indexes in chunks of\n16 GiB (instead of 1 GiB) and indexes closed while queries are running can no longer crash\nthe JVM.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "INFO: This is a followup of #518 : It's the same code base, but with API changes from JDK 19 applied This is just a draft PR for a first insight on memory mapping improvements in JDK 19+. Some background information: Starting with JDK-14, there is a new incubating module \"jdk.incubator.foreign\" that has a new, not yet stable API for accessing off-heap memory (and later it will also support calling functions using classical MethodHandles that are located in libraries like .so or .dll files). This incubator module has several versions: This new preview API more or less overcomes several problems: @uschindler had many discussions with the team at OpenJDK and finally with the third incubator, we have an API that works with Lucene. It was very fruitful discussions (thanks to @mcimadamore !) With the third incubator we are now finally able to do some tests (especially performance). The code basically just modifies MMapDirectory to use LONG instead of INT for the chunk size parameter. In addition it adds MemorySegmentIndexInput that is a copy of our ByteBufferIndexInput (still there, but unused), but using MemorySegment instead of ByteBuffer behind the scenes. It works in exactly the same way, just the try/catch blocks for supporting EOFException or moving to another segment were rewritten. It passes all tests and it looks like you can use it to read indexes. The default chunk size is now 16 GiB (but you can raise or lower it as you like; tests are doing this). Of course you can set it to Long.MAX_VALUE, in that case every index file is always mapped to one big memory mapping. My testing with Windows 10 have shown, that this is not a good idea!!! . Huge mappings fragment address space over time and as we can only use like 43 or 46 bits (depending on OS), the fragmentation will at some point kill you. So 16 GiB looks like a good compromise: Most files will be smaller than 6 GiB anyways (unless you optimize your index to one huge segment). So for most Lucene installations, the number of segments will equal the number of open files, so Elasticsearch huge user consumers will be very happy. The sysctl max_map_count may not need to be touched anymore. In addition, this implements readLongs in a better way than @jpountz did (no caching or arbitrary objects). The new foreign-vector APIs will in future also be written with MemorySegment in its focus. So you can allocate a vector view on a MemorySegment and let the vectorizer fully work outside java heap inside our mmapped files! :-)_ It would be good if you could checkout this branch and try it in production. According to speed tests it should be as fast as MMAPDirectory, partially also faster because less switching between byte-buffers is needed. With recent optimizations also long -based absolute access in loops should be faster. But be aware: It would be good to get some benchmarks, especially by @rmuir or @mikemccand . Take your time and enjoy the complexity of setting this up! ;-)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Improvements", "change_id": "LUCENE-10592", "change_description": ": Build HNSW Graph on indexing.", "change_title": "Should we build HNSW graph on the fly during indexing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "Currently, when we index vectors for KnnVectorField, we buffer those vectors in memory and on flush during a segment construction we build an HNSW graph.  As building an HNSW graph is very expensive, this makes flush operation take a lot of time. This also makes overall indexing performance quite unpredictable (as the number of flushes are defined by memory used, and the presence of concurrent searches), e.g. some indexing operations return almost instantly while others that trigger flush take a lot of time. Building an HNSW graph on the fly as we index vectors allows to avoid this problem, and spread a load of HNSW graph construction evenly during indexing. This will also supersede LUCENE-10194", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Improvements", "change_id": "LUCENE-10207", "change_description": ": TermInSetQuery can now provide a ScoreSupplier with cost estimation, making it\nusable in IndexOrDocValuesQuery.", "change_title": "Make TermInSetQuery usable with IndexOrDocValuesQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "IndexOrDocValuesQuery is very useful to pick the right execution mode for a query depending on other bits of the query tree. We would like to be able to use it to optimize execution of TermInSetQuery. However IndexOrDocValuesQuery only works well if the \"index\" query can give an estimation of the cost of the query without doing anything expensive (like looking up all terms of the TermInSetQuery in the terms dict). Maybe we could implement it for primary keys (terms.size() == sumDocFreq) by returning the number of terms of the query? Another idea is to multiply the number of terms by the average postings length, though this could be dangerous if the field has a zipfian distribution and some terms have a much higher doc frequency than the average. romseygeek and I were discussing this a few weeks ago, and more recently mikemccand and gsmiller again independently. So it looks like there is interest in this. Here is an email thread where this was recently discussed: https://lists.apache.org/thread.html/re3b20a486c9a4e66b2ca4a2646e2d3be48535a90cdd95911a8445183%40%3Cdev.lucene.apache.org%3E.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13035418/LUCENE-10207_multitermquery.patch", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Improvements", "change_id": "LUCENE-10216", "change_description": ": Use MergePolicy to define and MergeScheduler to trigger the reader merges\nrequired by addIndexes(CodecReader[]) API.", "change_title": "Add concurrency to addIndexes(CodecReader…) API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "main,9.4", "detail_description": "I work at Amazon Product Search, and we use Lucene to power search for the e-commerce platform. I’m working on a project that involves applying metadata+ETL transforms and indexing documents on n different indexing boxes, combining them into a single index on a separate reducer box, and making it available for queries on m different search boxes (replicas). Segments are asynchronously copied from indexers to reducers to searchers as they become available for the next layer to consume. I am using the addIndexes API to combine multiple indexes into one on the reducer boxes. Since we also have taxonomy data, we need to remap facet field ordinals, which means I need to use the addIndexes(CodecReader…) version of this API. The API leverages SegmentMerger.merge() to create segments with new ordinal values while also merging all provided segments in the process. This is however a blocking call that runs in a single thread. Until we have written segments with new ordinal values, we cannot copy them to searcher boxes, which increases the time to make documents available for search. I was playing around with the API by creating multiple concurrent merges, each with only a single reader, creating a concurrently running 1:1 conversion from old segments to new ones (with new ordinal values). We follow this up with non-blocking background merges. This lets us copy the segments to searchers and replicas as soon as they are available, and later replace them with merged segments as background jobs complete. On the Amazon dataset I profiled, this gave us around 2.5 to 3x improvement in addIndexes() time. Each call was given about 5 readers to add on average. This might be useful add to Lucene. We could create another addIndexes() API with a boolean flag for concurrency, that internally submits multiple merge jobs (each with a single reader) to the ConcurrentMergeScheduler, and waits for them to complete before returning. While this is doable from outside Lucene by using your thread pool, starting multiple addIndexes() calls and waiting for them to complete, I felt it needs some understanding of what addIndexes does, why you need to wait on the merge and why it makes sense to pass a single reader in the addIndexes API. Out of box support in Lucene could simplify this for folks a similar use case.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Improvements", "change_id": "GITHUB#11715", "change_description": ": Add Integer awareness to RamUsageEstimator.sizeOf", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Add RamUsageEstimator.sizeOf(Integer) Improve estimation for both Long and Integer to not count VM Cache values.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Optimizations", "change_id": "LUCENE-10661", "change_description": ": Reduce memory copy in BytesStore.", "change_title": "Reduce memory copy in BytesStore", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "This is derived from LUCENE-10627(https://github.com/apache/lucene/pull/987) AND LUCENE-10657(https://github.com/apache/lucene/pull/1034) The abstract method copyBytes in DataOutput have to copy from input to a copyBuffer and then write into BytesStore.blocks, which is called in FST initialization read from metaIn. Although, this copy bytes only a few bytes (in the testscase only 3-10 bytes), i think we can save this memory copy, just save the DataOutput.copyBytes to create new copyBuffer with 16384 bytes", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Optimizations", "change_id": "GITHUB#1020", "change_description": ": Support #scoreSupplier and small optimizations to DocValuesRewriteMethod.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I'm coming back to work on LUCENE-10207 , and one thing I found is that DocValuesRewriteMethod doesn't support scoreSupplier . Having support for this is necessary for LUCENE-10207 to avoid unnecessary work if a DV-rewritten query is used within an IndexOrDocValuesQuery . This change just adds the scoreSupplier support along with a small optimization around singleton doc values. Separating that out from the rest of LUCENE-10207 will help keep PRs small, and I think this bit should be pretty straight-forward and non-controversial :)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Optimizations", "change_id": "LUCENE-10633", "change_description": ": Added support for dynamic pruning to queries sorted by a string\nfield that is indexed with terms and SORTED or SORTED_SET doc values.", "change_title": "Dynamic pruning for queries sorted by SORTED(_SET) field", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "LUCENE-9280 introduced the ability to dynamically prune non-competitive hits when sorting by a numeric field, by leveraging the points index to skip documents that do not compare better than the top of the priority queue maintained by the field comparator. However queries sorted by a SORTED(_SET) field still look at all hits, which is disappointing. Could we leverage the terms index to skip hits?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Optimizations", "change_id": "LUCENE-10627", "change_description": ": Using ByteBuffersDataInput reduce memory copy on compressing data.", "change_title": "Using ByteBuffersDataInput reduce memory copy on compressing data", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "Code: https://github.com/apache/lucene/pull/987 I see When Lucene Do flush and merge store fields, need many memory copies: When Lucene CompressingStoredFieldsWriter do flush documents, it needs many memory copies: With Lucene90 using LZ4WithPresetDictCompressionMode: With Lucene90 using DeflateWithPresetDictCompressionMode:  I think we can use CompositeByteBuf to reduce temp memory copies:  I write a simple mini benchamrk in test code (link ): LZ4WithPresetDict run Capacity:41943040(bytes) , iter 10times: Origin elapse:5391ms , New elapse:5297ms DeflateWithPresetDict run Capacity:41943040(bytes), iter 10times: Origin elapse:115ms, New elapse:12ms   And I run runStoredFieldsBenchmark with doc_limit=-1: shows:  ---------{}UPDATE{}---------  I try to reuse ByteBuffersDataInput to reduce memory copy because it can get from ByteBuffersDataOutput.toDataInput.  and it could reduce this complexity （PR） BUT i am not sure whether can change Compressor interface compress input param from byte[] to ByteBuffersDataInput. If change this interface like, it increased the backport code like, however if we change the interface with ByteBuffersDataInput, we can optimize memory copy into different compress algorithm code. Also, i found we can do more memory copy reduce in CompressingStoredFieldsWriter.copyOneDoc like and CompressingTermVectorsWriter.flush (like) I think this commit just reduce memory copy, so we not only use one benchmark time metric but also use jvm gc time to see the improvement. so i try to add StatisticsHelper into StoredFieldsBenchmark.(code) so at latest commit:  i do the runStoredFieldsBenchmark with jvm StatisticsHelper it shows as following:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Optimizations", "change_id": "GITHUB#1062", "change_description": ": Optimize TermInSetQuery when a term is present that matches all docs in a segment.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change introduces an optimization to TermInSetQuery when a term is present that matches all docs in a segment.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10663", "change_description": ": Fix KnnVectorQuery explain with multiple segments.", "change_title": "KnnVectorQuery explain incorrect when multiple segments", "detail_type": "Bug", "detail_affect_versions": "9.0,9.1,9.2", "detail_fix_versions": "9.4", "detail_description": "If there are multiple segments. KnnVectorQuery explain has a bug in locating docid. This is because the docid in explain, which is the docBase without the segment. In KnnVectorQuery.DocAndScoreQuery docs docid is increased in each segment of the docBase. The two docid are not in the same dimension. So, in the 'DocAndScoreQuery.explain', needs to be added with the segment's docBase.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10673", "change_description": ": Improve check of equality for latitudes for spatial3d GeoBoundingBox", "change_title": "Spatial3d fails constructing a legal bounding box", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "The issue can be reproduced with the following test: this currently fails with the following error:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10678", "change_description": ": Fix potential overflow when building a BKD tree with more than 4 billion points. The overflow\noccurs when computing the partition point.", "change_title": "computing the partition point on a BKD tree merge can overflow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.11.3,9.4,9.3.1", "detail_description": "I just discover a bad bug in the BKD tree when doing merges. Before calling the BKDTreeRadix selector we need to compute the partition point which is dome multiplying two integers. If the partition point is > Integer.MAX_VALUE then it will overflow. https://github.com/apache/lucene/blob/35ca2d79f73c6dfaf5e648fe241f7e0b37084a90/lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter.java#L2021 ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10644", "change_description": ": Facets#getAllChildren testing should ignore child order.", "change_title": "Facets#getAllChildren testing should ignore child order", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "Our javadoc for Facets#getAllChildren explicitly calls out that callers should make no assumptions about child ordering, but a number of our own unit tests turn around and make that assumption. I ran into this when recently trying an optimization that would result in a different child ordering for getAllChildren, and found a number of unit tests that started failing. I'll upload a list of what I found failing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10665", "change_description": ",", "change_title": "Deadlock in AnalysisSPILoader", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Loading different TokenFilter/Tokenizer/CharFilter from different threads is causing deadlock. To reproduce use the below code: public static void main(String[] args) Took the thread dump, and found that thread1 gets stuck while calling ensureClassInitialized(LowerCaseFilterFactory) possibly because thread2 is holding some lock on the same class.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11701", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Loading different TokenFilter/Tokenizer/CharFilter from different threads is causing deadlock. To reproduce use the below code: Took the thread dump, and found that thread1 gets stuck while calling ensureClassInitialized(LowerCaseFilterFactory) possibly because thread2 is holding some lock on the same class. Migrated from LUCENE-10665 by Jasir KT The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10674", "change_description": ": Ensure BitSetConjDISI returns NO_MORE_DOCS when sub-iterator exhausts.", "change_title": "BitSetConjunctionDISI iterators fall out of sync when lead doc >= minlength of bitsets", "detail_type": "Bug", "detail_affect_versions": "9.0,9.1,9.2,9.3", "detail_fix_versions": "None", "detail_description": "In BitSetConjunctionDISI.doNext() if the lead doc is greater than or equal to the length of any of the BitSets, NO_MORE_DOCS is returned. On subsequent calls to BitSetConjunctionDISI.docId(), the lead's docID which is not exhausted will be returned. I think this could be fixed by calling lead.advance(NO_MORE_DOCS) before returning NO_MORE_DOCS in doNext.  Related issue: https://issues.apache.org/jira/browse/LUCENE-9541", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11794", "change_description": ": Guard FieldExistsQuery against null pointers", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "FieldExistsQuery checks if there are points for a certain field, and then retrieves the corresponding point values. When all documents that had points for a certain field have been deleted from a certain segments, as well as merged away, field info may report that there are points yet the corresponding point values are null. The same can happen when terms are accessed. With this change we add null checks to FieldExistsQuery. Long term, we will likely want to prevent this situation from happening. Relates #11393", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Build", "change_id": "GITHUB#11720", "change_description": ": Upgrade randomizedtesting to 2.8.1 (potential fix for odd wall clock - related\ntimeout failures).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Hopefully mitigates wall-clock related problems (timeouts at weird stack traces) mentioned in #7687 . The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Build", "change_id": "LUCENE-10669", "change_description": ": The build should be more helpful when generated resources are touched", "change_title": "The build should be more helpful when generated resources are touched", "detail_type": "Improvement", "detail_affect_versions": "10.0(main)", "detail_fix_versions": "9.4", "detail_description": "As per discussion at https://github.com/apache/lucene/pull/1016, it'd be good if a build failure could point at the sources and generated files of the task for which checksums are mismatched (signaling either modified templates or accidentally modified generated files).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.4.0", "change_type": "Other", "change_id": "LUCENE-10559", "change_description": ": Add Prefilter Option to KnnGraphTester", "change_title": "Add preFilter/postFilter options to KnnGraphTester", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.4", "detail_description": "We want to be able to test the efficacy of pre-filtering in KnnVectorQuery: if you (say) want the top K nearest neighbors subject to a constraint Q, are you better off over-selecting (say 2K) top hits and then filtering (post-filtering), or incorporating the filtering into the query (pre-filtering). How does it depend on the selectivity of the filter? I think we can get a reasonable testbed by generating a uniform random filter with some selectivity (that is consistent and repeatable). Possibly we'd also want to try filters that are correlated with index order, but it seems they'd be unlikely to be correlated with vector values in a way that the graph structure would notice, so random is a pretty good starting point for this.", "patch_link": "none", "patch_content": "none"}

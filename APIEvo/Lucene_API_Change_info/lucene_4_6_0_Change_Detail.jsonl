{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-4906", "change_description": ": PostingsHighlighter can now render to custom Object,\nfor advanced use cases where String is too restrictive", "change_title": "PostingsHighlighter's PassageFormatter should allow for rendering to arbitrary objects", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "For example, in a server, I may want to render the highlight result to JsonObject to send back to the front-end. Today since we render to string, I have to render to JSON string and then re-parse to JsonObject, which is inefficient... Or, if (Rob's idea we make a query that's like MoreLikeThis but it pulls terms from snippets instead, so you get proximity-influenced salient/expanded terms, then perhaps that renders to just an array of tokens or fragments or something from each snippet.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12602804/LUCENE-4906.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5133", "change_description": ": Changed AnalyzingInfixSuggester.highlight to return\nObject instead of String, to allow for advanced use cases where\nString is too restrictive", "change_title": "AnalyzingInfixSuggester should return structured highlighted results instead of single String per result", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Today it renders to an HTML string (<b>..</b> for hits) in protected methods that one can override to change the highlighting, but this is hard/inefficient to use for search servers that want to e.g. return JSON representation of the highlighted result. This is the same issue as LUCENE-4906 (PostingsHighlighter) but for AnalyzingInfixSuggester's highlights instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594249/LUCENE-5133.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5207", "change_description": ",", "change_title": "lucene expressions module", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Expressions are geared at defining an alternative ranking function (e.g. incorporating the text relevance score and other field values/ranking signals). So they are conceptually much more like ElasticSearch's scripting support (http://www.elasticsearch.org/guide/reference/modules/scripting/) than solr's function queries. Some additional notes:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603240/LUCENE-5207.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5334", "change_description": ",", "change_title": "Add Namespaces to Expressions Javascript Compiler", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I would like to add the concept of namespaces to the expressions javascript compiler using '.' as the operator. Example of namespace usage in functions: AccurateMath.sqrt(field) FastMath.sqrt(field) Example of namespace usage in variables: location.latitude location.longitude", "patch_link": "https://issues.apache.org/jira/secure/attachment/12612662/LUCENE-5334.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5180", "change_description": ": ShingleFilter now creates shingles with trailing holes,\nfor example if a StopFilter had removed the last token.", "change_title": "ShingleFilter should make shingles from trailing holes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "When ShingleFilter hits a hole, it uses _ as the token, e.g. bigrams for \"the dog barked\", if you have a StopFilter removing the, would be: \"_ dog\", \"dog barked\". But if the input ends with a stopword, e.g. \"wizard of\", ShingleFilter fails to produce \"wizard _\" due to LUCENE-3849 ... once we fix that I think we should fix ShingleFilter to make shingles for trailing holes too ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603234/LUCENE-5180.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5219", "change_description": ": Add support to SynonymFilterFactory for custom\nparsers.", "change_title": "Make SynonymFilterFactory format attribute pluggable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "It would be great to allow custom synonym formats to work with SynonymFilterFactory.  There is already a comment in the code to make it pluggable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12604003/LUCENE-5219.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5235", "change_description": ": Tokenizers now throw an IllegalStateException if the\nconsumer does not call reset() before consuming the stream. Previous\nversions throwed NullPointerException or ArrayIndexOutOfBoundsException\non best effort which was not user-friendly.", "change_title": "throw illegalstate from Tokenizer (instead of NPE/IIOBE) if reset not called", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "We added these best effort checks, but it would be much better if we somehow gave a clear exception... this comes up often", "patch_link": "https://issues.apache.org/jira/secure/attachment/12604423/LUCENE-5235.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5240", "change_description": ": Tokenizers now throw an IllegalStateException if the\nconsumer neglects to call close() on the previous stream before consuming\nthe next one.", "change_title": "additional safety in Tokenizer state machine", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "We can add a simple check for this easily now in setReader. I found a few bugs, and fixed all except TrieTokenizer in solr (I am lost here... somewhere i have a patch to remove this thing).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12605809/LUCENE-5240.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5214", "change_description": ": Add new FreeTextSuggester, to predict the next word\nusing a simple ngram language model.  This is useful for the \"long\ntail\" suggestions, when a primary suggester fails to find a\nsuggestion.", "change_title": "Add new FreeTextSuggester, to handle \"long tail\" suggestions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "The current suggesters are all based on a finite space of possible suggestions, i.e. the ones they were built on, so they can only suggest a full suggestion from that space. This means if the current query goes outside of that space then no suggestions will be found. The goal of FreeTextSuggester is to address this, by giving predictions based on an ngram language model, i.e. using the last few tokens from the user's query to predict likely following token. I got the idea from this blog post about Google's suggest: http://googleblog.blogspot.com/2011/04/more-predictions-in-autocomplete.html This is very much still a work in progress, but it seems to be working.  I've tested it on the AOL query logs, using an interactive tool from luceneutil to show the suggestions, and it seems to work well. It's fun to use that tool to explore the word associations... I don't think this suggester would be used standalone; rather, I think it'd be a fallback for times when the primary suggester fails to find anything.  You can see this behavior on google.com, if you type \"the fast and the \", you see entire queries being suggested, but then if the next word you type is \"burning\" then suddenly you see the suggestions are only based on the last word, not the entire query. It uses ShingleFilter under-the-hood to generate the token ngrams; once LUCENE-5180 is in it will be able to properly handle a user query that ends with stop-words (e.g. \"wizard of \"), and then stores the ngrams in an FST.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12605966/LUCENE-5214.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5251", "change_description": ": New DocumentDictionary allows building suggesters via\ncontents of existing field, weight and optionally payload stored\nfields in an index", "change_title": "New Dictionary Implementation for Suggester consumption", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "With the vast array of new suggester, It would be nice to have a dictionary implementation that could feed the suggesters terms, weights and (optionally) payloads from the lucene index. The idea of this dictionary implementation is to grab stored documents from the index and use user-configured fields for terms, weights and payloads. use-case: If you have a document with three fields then using this implementation would enable you to have a suggester for product_name using the weight of product_popularity_score and return you the payload of product_id, with which you can do further processing on (example: construct a url etc).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607297/LUCENE-5251.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5261", "change_description": ": Add QueryBuilder, a simple API to build queries from\nthe analysis chain directly, or to make it easier to implement\nquery parsers.", "change_title": "add simple API to build queries from analysis chain", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Currently this is pretty crazy stuff. Additionally its duplicated in like 3 or 4 places in our codebase (i noticed it doing LUCENE-5259) We can solve that duplication, and make it easy to simply create queries from an analyzer (its been asked on the user list), as well as make it easier to build new queryparsers.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607542/LUCENE-5261.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5270", "change_description": ": Add Terms.hasFreqs, to determine whether a given field\nindexed per-doc term frequencies.", "change_title": "add Terms.hasFreqs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "While working on LUCENE-5268, I realized we have hasPositions/Offsets/Payloads methods in Terms but not hasFreqs ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607818/LUCENE-5270.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5269", "change_description": ": Add CodepointCountFilter.", "change_title": "TestRandomChains failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5.1,4.6,6.0", "detail_description": "One of EdgeNGramTokenizer, ShingleFilter, NGramTokenFilter is buggy, or possibly only the combination of them conspiring together.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607709/LUCENE-5269.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5294", "change_description": ": Suggest module: add DocumentExpressionDictionary to\ncompute each suggestion's weight using a javascript expression.", "change_title": "Suggester Dictionary implementation that takes expressions as term weights", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "It would be nice to have a Suggester Dictionary implementation that could compute the weights of the terms consumed by the suggester based on an user-defined expression (using lucene's expression module). It could be an extension of the existing DocumentDictionary (which takes terms, weights and (optionally) payloads from the stored documents in the index). The only exception being that instead of taking the weights for the terms from the specified weight fields, it could compute the weights using an user-defn expression, that uses one or more NumicDocValuesField from the document. Example:   let the document have Then this implementation could be used with an expression of     \"0.2*product_popularity + 0.8*product_profit\" to determine the weights of the terms for the corresponding documents (optionally along with a payload (product_id))", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609096/LUCENE-5294.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5274", "change_description": ": FastVectorHighlighter now supports highlighting against several\nindexed fields.", "change_title": "Teach fast FastVectorHighlighter to highlight \"child fields\" with parent fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "I've been messing around with the FastVectorHighlighter and it looks like I can teach it to highlight matches on \"child fields\".  Like this query: foo:scissors foo_exact:running would highlight foo like this: <em>running</em> with <em>scissors</em> Where foo is stored WITH_POSITIONS_OFFSETS and foo_plain is an unstored copy of foo a different analyzer and its own WITH_POSITIONS_OFFSETS. This would make queries that perform weighted matches against different analyzers much more convenient to highlight. I have working code and test cases but they are hacked into Elasticsearch.  I'd love to Lucene-ify if you'll take them.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609315/LUCENE-5274.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5304", "change_description": ": SingletonSortedSetDocValues can now return the wrapped\nSortedDocValues", "change_title": "SingletonSortedSetDocValues should allow for getting back the wrapped instance", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "This idea was mentioned by Robert on LUCENE-5300 Some codecs or FieldCache impls use SingletonSortedSetDocValues when a field which is supposed to be multi-valued is actually single-valued. By having a getter on this class to get back the wrapped SortedDocValues instance, we could add more specialization (which often already exists, eg. Solr's DocValuesFacets already have a specialized impl for SortedDocValues).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12610070/LUCENE-5304.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-2844", "change_description": ": The benchmark module can now test the spatial module. See\nspatial.alg", "change_title": "benchmark geospatial performance based on geonames.org", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "See comments for details. In particular, the original patch \"benchmark-geo.patch\" is fairly different than LUCENE-2844.patch", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609887/LUCENE-2844_spatial_benchmark.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5302", "change_description": ": Make StemmerOverrideMap's methods public", "change_title": "Make StemmerOverrideMap methods public", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "StemmerOverrideFilter is configured with an FST-based map that you can build at construction time from a list of entries.  Building this FST offline and loading it directly as a bytestream makes construction a lot quicker, but you can't do that conveniently at the moment as all the methods of StemmerOverrideMap are package-private.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609873/LUCENE-5302.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5296", "change_description": ": Add DirectDocValuesFormat, which holds all doc values\nin heap as uncompressed java native arrays.", "change_title": "Add DirectDocValuesFormat", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Indexes values to disk but at search time it loads/accesses the values via simple java arrays (i.e. no compression).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609419/LUCENE-5296.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5189", "change_description": ": Add IndexWriter.updateNumericDocValues, to update\nnumeric DocValues fields of documents, without re-indexing them.", "change_title": "Numeric DocValues Updates", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "In LUCENE-4258 we started to work on incremental field updates, however the amount of changes are immense and hard to follow/consume. The reason is that we targeted postings, stored fields, DV etc., all from the get go. I'd like to start afresh here, with numeric-dv-field updates only. There are a couple of reasons to that: I have some working patch already which I'll upload next, explaining the changes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12610287/LUCENE-5189-4x.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5298", "change_description": ": Add SumValueSourceFacetRequest for aggregating facets by\na ValueSource, such as a NumericDocValuesField or an expression.", "change_title": "Allow aggregating facets by any ValueSource", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Facets can be aggregated today by counting them, aggregating by summing their association values, or summing the scores of the documents. Applications can write their own FacetsAggregator to compute the value of the facet in other ways. Following the new expressions module, I think it will be interesting to allow aggregating facets by arbitrary expressions, e.g. _score * sqrt(price) where 'price' is an NDV field. I'd like to explore allowing any ValueSource to be passed in and write a ValueSourceFacetsAggregator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12611820/LUCENE-5298.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5323", "change_description": ": Add .sizeInBytes method to all suggesters (Lookup).", "change_title": "Add sizeInBytes to Suggester.Lookup", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "It would be nice to have a sizeInBytes() method added to Suggester.Lookup interface. This would allow users to estimate the size of the in-memory data structure created by various suggester implementation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12611798/LUCENE-5323.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5312", "change_description": ": Add BlockJoinSorter, a new Sorter implementation that makes sure\nto never split up blocks of documents indexed with IndexWriter.addDocuments.", "change_title": "Block-join-friendly index sorting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "It could be useful to have a block-join-friendly sorter implementation that doesn't break index-time blocks:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12610598/LUCENE-5312.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "New Features", "change_id": "LUCENE-5297", "change_description": ": Allow to range-facet on any ValueSource, not just\nNumericDocValues fields.", "change_title": "Allow rang faceting on any ValueSource", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Today RangeAccumulator assumes the ranges should be read from a NumericDocValues field. Would be good if we can modify it, or introduce a new ValueSourceAccumulator which allows you to range-facet on any ValueSource, e.g. one that is generated from an expression.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12611932/LUCENE-5297.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5272", "change_description": ": OpenBitSet.ensureCapacity did not modify numBits, causing\nfalse assertion errors in fastSet.", "change_title": "OpenBitSet.ensureCapacity does not modify numBits", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "It's a simple bug, reproduced by this simple test: The problem is that numBits which is used only for assrets isn't modified by ensureCapacity and so the next fastSet trips the assert. I guess we should also fix ensureCapacityWords and test it too. I may not be able to fix this until Sunday though, so if anyone wants to fix it before (maybe it can make it into 4.5.1), feel free.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12608220/LUCENE-5272.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5303", "change_description": ": OrdinalsCache did not use coreCacheKey, resulting in\nover caching across multiple threads.", "change_title": "OrdinalsCache should use reader.getCoreCacheKey()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "I'm doing some facet performance tests, and I tried using the CachedOrdsCountingFacetsAggregator to cache the decoded ords per doc X field ... but noticed it was generating way too many cache entries, because it's currently using the NDV instance as the cache key. NDV instances are thread-private so this results in way too many entries in the cache.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609915/LUCENE-5303.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5307", "change_description": ": Fix topScorer inconsistency in handling QueryWrapperFilter\ninside ConstantScoreQuery, which now rewrites to a query removing the\nobsolete QueryWrapperFilter.", "change_title": "Inconsistency between Weight.scorer documentation and ConstantScoreQuery on top of a Filter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Weight.scorer states that if topScorer == true, Scorer.collect will be called and that otherwise Scorer.nextDoc/advance will be called. This is a problem when ConstantScoreQuery is used on top of a QueryWrapperFilter:  1. ConstantScoreWeight  calls getDocIdSet on the filter to know which documents to collect.  2. QueryWrapperFilter.getDocIdSet returns a Scorer created with topScorer == false so that nextDoc/advance are supported.  3. But then ConstantScorer.score(Collector) has the following optimization: So the filter iterator is a scorer which was created with topScorer = false but ParentScorer ends up using its score(Collector) method, which is illegal. (I found this out because AssertingSearcher has some checks to make sure Scorers are used accordingly to the value of topScorer.) I can imagine several fixes, including: but I'm not sure which one is the best one. What do you think?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12610391/LUCENE-5307.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5330", "change_description": ": IndexWriter didn't process all internal events on\n#getReader(), #close() and #rollback() which causes files to be\ndeleted at a later point in time. This could cause short-term disk\npollution or OOM if in-memory directories are used.", "change_title": "IndexWriter doesn't process all events on getReader / close / rollback", "detail_type": "Bug", "detail_affect_versions": "4.5,6.0", "detail_fix_versions": "4.6,6.0", "detail_description": "IndexWriter misses to apply all pending events in getReader() as well as close() / rollback(). This can lead to files that never get deleted or only very very late. If you are using RAM Directories for instance this quickly fills up a huge amount of memory. It might not be super critical in production and it also doesn't cause any data loss or index corruption.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12612263/LUCENE-5330.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5342", "change_description": ": Fixed bulk-merge issue in CompressingStoredFieldsFormat which\ncreated corrupted segments when mixing chunk sizes.\nLucene41StoredFieldsFormat is not impacted.", "change_title": "CompressingStoredFieldsFormat's bulk merge should be disabled when reader and writer don't have the same chunk size", "detail_type": "Bug", "detail_affect_versions": "4.5,4.5.1", "detail_fix_versions": "4.6", "detail_description": "LUCENE-5188 changed the way data is compressed when single documents exceed the chunk size, meaning that it is not legal anymore to bulk merge if writer and reader have different chunk sizes. This bug only happens when mixing two instances of CompressingStoredFieldsFormat that have the same compression mode but different chunk sizes, and when there are documents that exceed any of the chunk sizes. So the default codec is not impacted since it has always had the same chunk size (16KB).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12614078/LUCENE-5342.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5222", "change_description": ": Add SortField.needsScores(). Previously it was not possible\nfor a custom Sort that makes use of the relevance score to work correctly\nwith IndexSearcher when an ExecutorService is specified.", "change_title": "TestExpressionSorts fails sometimes when using expression returning score", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Jenkins picked this up.  Repeat with: It appears to have to do with scoring, as removing the score sort from the original sorts causes the tests to pass.  If you remove the possible discrepancy between doDocScores and docMaxScore params to searcher.search, then the test gets farther before failing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603852/LUCENE-5222.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5275", "change_description": ": Change AttributeSource.toString() to display the current\nstate of attributes.", "change_title": "Fix AttributeSource.toString()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Its currently just Object.toString, e.g.: org.apache.lucene.analysis.en.PorterStemFilter@8a32165c But I think we should make it more useful, to end users trying to see what their chain is doing, and to make SOPs easier when debugging: Proposed output:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607849/LUCENE-5275.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5277", "change_description": ": Modify FixedBitSet copy constructor to take an additional\nnumBits parameter to allow growing/shrinking the copied bitset. You can\nuse FixedBitSet.clone() if you only need to clone the bitset.", "change_title": "Modify FixedBitSet copy constructor to take numBits to allow grow/shrink the new bitset", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "FixedBitSet copy constructor is redundant the way it is now â€“ one can call FBS.clone() to achieve that (and indeed, no code in Lucene calls this ctor). I think it will be useful to add a numBits parameter to that method to allow growing/shrinking the new bitset, while copying all relevant bits from the passed one.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607951/LUCENE-5277.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5260", "change_description": ": Use TermFreqPayloadIterator for all suggesters; those\nsuggesters that can't support payloads will throw an exception if\nhasPayloads() is true.", "change_title": "Make older Suggesters more accepting of TermFreqPayloadIterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "As discussed in https://issues.apache.org/jira/browse/LUCENE-5251, it would be nice to make the older suggesters accepting of TermFreqPayloadIterator and throw an exception if payload is found (if it cannot be used). This will also allow us to nuke most of the other interfaces for BytesRefIterator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12608146/LUCENE-5260.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5280", "change_description": ": Rename TermFreqPayloadIterator -> InputIterator, along\nwith associated suggest/spell classes.", "change_title": "Rename TermFreqPayloadIterator -> SuggestionIterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "The current name is cumbersome, and annoying to change whenever we add something to the iterator (in this case payloads).  Since we are breaking it anyway in 4.6, I think we should take the opportunity to find a better name.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12608236/LUCENE-5280.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5157", "change_description": ": Rename OrdinalMap methods to clarify API and internal structure.", "change_title": "Refactoring MultiDocValues.OrdinalMap to clarify API and internal structure.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "I refactored MultiDocValues.OrdinalMap, removing one unused parameter and renaming some methods to more clearly communicate what they do. Also I renamed subIndex references to segmentIndex.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12596083/LUCENE-5157.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5313", "change_description": ": Move preservePositionIncrements from setter to ctor in\nAnalyzing/FuzzySuggester.", "change_title": "Add \"preservePositionIncrements\" to AnalyzingSuggester and FuzzySuggester constructors", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "It would be convenient to have \"preservePositionIncrements\" in the suggesters constructor, rather than having a setPreservePositionIncrements method. That way it could be nicely used with the factory model already used by Solr.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12610992/LUCENE-5313.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5321", "change_description": ": Remove Facet42DocValuesFormat. Use DirectDocValuesFormat if you\nwant to load the category list into memory.", "change_title": "Remove Facet42DocValuesFormat", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "The new DirectDocValuesFormat is nearly identical to Facet42DVF, except that it stores the addresses in direct int[] rather than PackedInts. On LUCENE-5296 we measured the performance of DirectDVF vs Facet42DVF and it improves perf for some queries and have negligible effect for others, as well as RAM consumption isn't much worse. We should remove Facet42DVF and use DirectDVF instead. I also want to rename Facet46Codec to FacetCodec. There's no need to refactor the class whenever the default codec changes (e.g. from 45 to 46) since it doesn't care about the actual Codec version underneath, it only overrides the DVF used for the facet fields. FacetCodec should take the DVF from the app (so e.g. the facet/ module doesn't depend on codecs/) and be exposed more as a utility Codec rather than a real, versioned, Codec.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12611761/LUCENE-5321.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "API Changes", "change_id": "LUCENE-5324", "change_description": ": AnalyzerWrapper.getPositionIncrementGap and getOffsetGap can now\nbe overridden.", "change_title": "Make AnalyzerWrapper.get(Offset|PositionIncrement)Gap non-final", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "It can sometimes be useful to reconfigure the position and offset gaps of an existing analyzer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12611827/LUCENE-5324.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Optimizations", "change_id": "LUCENE-5225", "change_description": ": The ToParentBlockJoinQuery only keeps tracks of the the child\ndoc ids and child scores if the ToParentBlockJoinCollector is used.", "change_title": "ToParentBlockJoinQuery don't accumulate the child doc ids and scores if ToParentBlockJoinCollector is not used", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "The BlockJoinScorer temporarily saves the child docids and scores in two arrays (pendingChildDocs/pendingChildScores) for the current block (parent/child docs) being processed. This is only need for ToParentBlockJoinCollector and in the case that this collector isn't used then these two arrays shouldn't be used as well. I've seen cases where only the ToParentBlockJoinQuery is used and there are many child docs (100k and up), in that case these two arrays are a waste of resources.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603813/LUCENE-5225.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Optimizations", "change_id": "LUCENE-5236", "change_description": ": EliasFanoDocIdSet now has an index and uses broadword bit\nselection to speed-up advance().", "change_title": "Use broadword bit selection in EliasFanoDecoder", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "Try and speed up decoding", "patch_link": "https://issues.apache.org/jira/secure/attachment/12608222/LUCENE-5236.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Optimizations", "change_id": "LUCENE-5266", "change_description": ": Improved number of read calls and branches in DirectPackedReader.", "change_title": "Optimization of the direct PackedInts readers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "Given that the initial focus for PackedInts readers was more on in-memory readers (for storing stuff like the mapping from old to new doc IDs at merging time), I never spent time trying to optimize the direct readers although it could be beneficial now that they are used for disk-based doc values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12607764/LUCENE-5266.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Optimizations", "change_id": "LUCENE-5300", "change_description": ": Optimized SORTED_SET storage for fields which are single-valued.", "change_title": "SORTED_SET could use SORTED encoding when the field is actually single-valued", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "It would be nice to detect when a SORTED_SET field is single-valued in order to optimize storage.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12609877/LUCENE-5300.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Documentation", "change_id": "LUCENE-5211", "change_description": ": Better javadocs and error checking of 'format' option in\nStopFilterFactory, as well as comments in all snowball formated files\nabout specifying format option.", "change_title": "StopFilterFactory docs do not advertise/explain hte \"format\" option", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.6,6.0", "detail_description": "StopFilterFactory supports a \"format\" option for controlling wether \"getWordSet\" or \"getSnowballWordSet\" is used to parse the file, but this option is not advertised and people can be confused by looking at the example stopword files include in the releases (some of which are in the snoball format w/ \"|\" comments) and try to use them w/o explicitly specifying format=\"snowball\" and silently get useless stopwords (that include the \"| comments\" as literal portions of hte stopwrds. we need to better document the use of \"format\" and consider updating all of the example stopword files we ship that are in the snowball format with a note about the need to use format=\"snowball\" with those files. The StopFilterFactory builds a CharArraySet directly from the raw lines of the supplied words file. This causes a problem when using the stop word files supplied with the Solr/Lucene distribution. In particular, the comments in those files get added to the CharArraySet. A line like this... ceci           |  this Should result in the string \"ceci\" being added to the CharArraySet, but \"ceci           |  this\" is what actually gets added. Workaround: Remove all comments from stop word files you are using. Suggested fix: The StopFilterFactory should strip any comments, then strip trailing whitespace. The stop word files supplied with the distribution should be edited to conform to the supported comment format.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603683/LUCENE-5211.code.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5235", "change_description": ": Sub classes of Tokenizer have to call super.reset()\nwhen implementing reset(). Otherwise the consumer will get an\nIllegalStateException because the Reader is not correctly assigned.\nIt is important to never change the \"input\" field on Tokenizer\nwithout using setReader(). The \"input\" field must not be used\noutside reset(), incrementToken(), or end() - especially not in\nthe constructor.", "change_title": "throw illegalstate from Tokenizer (instead of NPE/IIOBE) if reset not called", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "We added these best effort checks, but it would be much better if we somehow gave a clear exception... this comes up often", "patch_link": "https://issues.apache.org/jira/secure/attachment/12604423/LUCENE-5235.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5204", "change_description": ": Directory doesn't have default implementations for\nLockFactory-related methods, which have been moved to BaseDirectory. If you\nhad a custom Directory implementation that extended Directory, you need to\nextend BaseDirectory instead.", "change_title": "Make Directory easier to wrap", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "We have a few DirectoryWrapper implementations such as RateLimitedDirectoryWrapper and MockDirectoryWrapper. However, the Directory class is not straightforward to wrap since it already has logic for getting and setting the lock factory, so wrappers need to decide whether they should forward lock handling to the delegate or handle it themselves. I would like to move the locking logic out of the Directory class and to have a base FilterDirectory that could be extended by other directory wrapper impls.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12602754/LUCENE-5204.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Build", "change_id": "LUCENE-5283", "change_description": ": Fail the build if ant test didn't execute any tests\n(everything filtered out).", "change_title": "Fail the build if ant test didn't execute any tests (everything filtered out).", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "This should be an optional setting that defaults to 'false' (the build proceeds).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12736240/LUCENE-5283.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Build", "change_id": "LUCENE-5249", "change_description": ",", "change_title": "All Lucene/Solr modules should use the same dependency versions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "markrmiller@gmail.com wrote on the dev list: I'd like it for some things if we actually kept the versions somewhere else - for instance, Hadoop dependencies should match across the mr module and the core module. Perhaps we could define versions for dependencies across multiple modules that should probably match, in a prop file or ant file and use sys sub for them in the ivy files. For something like Hadoop, that would also make it simple to use Hadoop 1 rather than 2 with a single sys prop override. Same with some other depenencies.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12606017/LUCENE-5162.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Build", "change_id": "LUCENE-5257", "change_description": ",", "change_title": "Lock down centralized versioning of ivy dependencies", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "LUCENE-5249 introduced centralized versioning of 3rd party dependencies and converted all ivy.xml files across Lucene/Solr to use this scheme.  But there is nothing preventing people from ignoring this setup and (intentionally or not) introducing non-centralized dependency versions. SOLR-3664 discusses the problem of out-of-sync 3rd party dependencies between Lucene/Solr modules.  Centralized versioning makes synchronization problems less likely but not impossible. One fairly simple way to ensure that all modules use the same version of 3rd party deps would be to require that all deps in ivy.xml would have to use the rev=\"${/org/name}\" syntax, via a validation script. The problem remains that there may eventually be a requirement to use different 3rd party libs in different modules.  Any form of lockdown here should allow for this possibility.  Hoss's suggestion from a conversation on #lucene IRC earlier today:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12606744/LUCENE-5257.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Build", "change_id": "LUCENE-5273", "change_description": ": Binary artifacts in Lucene and Solr convenience binary\ndistributions accompanying a release, including on Maven Central,\nshould be identical across all distributions.", "change_title": "Binary artifacts in Lucene and Solr convenience binary distributions accompanying a release, including on Maven Central, should be identical across all distributions", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "As mentioned in various issues (e.g. LUCENE-3655, LUCENE-3885, SOLR-4766), we release multiple versions of the same artifact: binary Maven artifacts are not identical to the ones in the Lucene and Solr binary distributions, and the Lucene jars in the Solr binary distribution, including within the war, are not identical to the ones in the Lucene binary distribution.  This is bad. It's (probably always?) not horribly bad, since the differences all appear to be caused by the build re-creating manifests and re-building jars and the Solr war from their constituents at various points in the release build process; as a result, manifest timestamp attributes, as well as archive metadata (at least constituent timestamps, maybe other things?), differ each time a jar is rebuilt.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12608221/LUCENE-5273-slowdown-fixed.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Build", "change_id": "LUCENE-4753", "change_description": ": Run forbidden-apis Ant task per module. This allows more\nimprovements and prevents OOMs after the number of class files\nraised recently.", "change_title": "Make forbidden API checks per-module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.6", "detail_description": "After the forbidden API checker was released separately from Lucene as a Google Code project (forked and improved), including Maven support, the checks on Lucene should be changed to work per-module. The reason for this is: The improved checker is more picky about e.g. extending classes that are forbidden or overriding methods and calling super.method() if they are on the forbidden signatures list. For these checks, it is not enough to have the class files and the rt.jar, you need the whole classpath. The forbidden APIs 1.0 now by default complains if classes are missing from the classpath. It is very hard with the module architecture of Lucene/Solr, to make a uber-classpath, instead the checks should be done per module, so the default compile/test classpath of the module can be used and no crazy path statements with */.jar are needed. This needs some refactoring in the exclusion lists, but the Lucene checks could be done by a macro in common-build, that allows custom exclusion lists for specific modules. Currently, the \"strict\" checking is disabled for Solr, so the checker only complains about missing classes but does not fail the build: I added almost all missing jars, but those do not seem to be in the solr part of the source tree (i think they are only copied when building artifacts). With making the whole thing per module, we can use the default classpath of the module which makes it much easier.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12613072/LUCENE-4753.patch", "patch_content": "none"}
{"library_version": "4.6.0", "change_type": "Tests", "change_id": "LUCENE-5278", "change_description": ": Fix MockTokenizer to work better with more regular expression\npatterns. Previously it could only behave like CharTokenizer (where a character\nis either a \"word\" character or not), but now it gives a general longest-match\nbehavior.", "change_title": "MockTokenizer throws away the character right after a token even if it is a valid start to a new token", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.6,6.0", "detail_description": "MockTokenizer throws away the character right after a token even if it is a valid start to a new token.  You won't see this unless you build a tokenizer that can recognize every character like with new RegExp(\".\") or RegExp(\"...\"). Changing this behaviour seems to break a number of tests.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12608125/LUCENE-5278.patch", "patch_content": "none"}

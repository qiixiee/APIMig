{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7703", "change_description": ": SegmentInfos now record the major Lucene version at index\ncreation time.", "change_title": "Record the version that was used at index creation time", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "SegmentInfos already records the version that was used to write a commit and the version that was used to write the oldest segment in the index. In addition to those, I think it could be useful to record the Lucene version that was used to create the index. I think it could help with:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12853981/LUCENE-7703.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7756", "change_description": ": LeafReader.getMetaData now exposes the index created version as\nwell as the oldest Lucene version that contributed to the segment.", "change_title": "Only record the major that was used to create the index rather than the full version", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "LUCENE-7703 added information about the Lucene version that was used to create the index to the segment infos. But since there is a single creation version, it means we need to reject calls to addIndexes that can mix indices that have different creation versions, which might be seen as an important regression by some users. So I have been thinking about only recording the major version that was used to create the index, which is still very valuable information and would allow us to accept calls to addIndexes when all merged indices have the same major version. This looks like a better trade-off to me.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12861724/LUCENE-7756.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7854", "change_description": ": The new TermFrequencyAttribute used during analysis\nwith a custom token stream allows indexing custom term frequencies", "change_title": "Indexing custom term frequencies", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "When you index a field with IndexOptions.DOCS_AND_FREQS, Lucene will store just the docID and term frequency (how many times that term occurred in that document) for all documents that have a given term. We compute that term frequency by counting how many times a given token appeared in the field during analysis. But it can be useful, in expert use cases, to customize what Lucene stores as the term frequency, e.g. to hold custom scoring signals that are a function of term and document (this is my use case).  Users have also asked for this before, e.g. see https://stackoverflow.com/questions/26605090/lucene-overwrite-term-frequency-at-index-time. One way to do this today is to stuff your custom data into a byte[] payload.  But that's quite inefficient, forcing you to index positions, and pay the overhead of retrieving payloads at search time. Another approach is \"token stuffing\": just enumerate the same token N times where N is the custom number you want to store, but that's also inefficient when N gets high. I think we can make this simple to do in Lucene.  I have a working version, using my own custom indexing chain, but the required changes are quite simple so I think we can add it to Lucene's default indexing chain? I created a new token attribute, TermDocFrequencyAttribute, and tweaked the indexing chain to use that attribute's value as the term frequency if it's present, and if the index options are DOCS_AND_FREQS for that field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12870798/LUCENE-7854.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7866", "change_description": ": Add a new DelimitedTermFrequencyTokenFilter that allows to\nmark tokens with a custom term frequency (", "change_title": "Add TokenFilter to add custom term frequency (like DelimitedPayloadTokenFilter)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "This is a followup of LUCENE-7854. This will add a simple TokenFilter like DelimitedPayloadTokenFilter that can be used to index a custom term frequency: \"token|5\" will be index token \"token\" with a term freq of 5. The effect is the same as adding the token 5 times by a \"repeat token filter\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12871707/LUCENE-7866.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7854", "change_description": ": Add a new DelimitedTermFrequencyTokenFilter that allows to\nmark tokens with a custom term frequency (", "change_title": "Indexing custom term frequencies", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "When you index a field with IndexOptions.DOCS_AND_FREQS, Lucene will store just the docID and term frequency (how many times that term occurred in that document) for all documents that have a given term. We compute that term frequency by counting how many times a given token appeared in the field during analysis. But it can be useful, in expert use cases, to customize what Lucene stores as the term frequency, e.g. to hold custom scoring signals that are a function of term and document (this is my use case).  Users have also asked for this before, e.g. see https://stackoverflow.com/questions/26605090/lucene-overwrite-term-frequency-at-index-time. One way to do this today is to stuff your custom data into a byte[] payload.  But that's quite inefficient, forcing you to index positions, and pay the overhead of retrieving payloads at search time. Another approach is \"token stuffing\": just enumerate the same token N times where N is the custom number you want to store, but that's also inefficient when N gets high. I think we can make this simple to do in Lucene.  I have a working version, using my own custom indexing chain, but the required changes are quite simple so I think we can add it to Lucene's default indexing chain? I created a new token attribute, TermDocFrequencyAttribute, and tweaked the indexing chain to use that attribute's value as the term frequency if it's present, and if the index options are DOCS_AND_FREQS for that field.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12870798/LUCENE-7854.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7868", "change_description": ": Multiple threads can now resolve deletes and doc values\nupdates concurrently, giving sizable speedups in update-heavy\nindexing use cases", "change_title": "Use multiple threads to apply deletes and DV updates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Today, when users delete documents or apply doc values updates, IndexWriter buffers them up into frozen packets and then eventually uses a single thread (BufferedUpdatesStream.applyDeletesAndUpdates) to resolve delete/update terms to docids.  This thread also holds IW's monitor lock, so it also blocks refresh, merges starting/finishing, commits, etc. We have heavily optimized this part of Lucene over time, e.g. LUCENE-6161, LUCENE-2897, LUCENE-2680, LUCENE-3342, but still, it's a single thread so it can't use multiple CPU cores commonly available now. This doesn't affect append-only usage, but for update-heavy users (me!) this can be a big bottleneck, and causes long stop-the-world hangs during indexing. I have an initial exploratory patch to make these lookups concurrent, without holding IW's lock, so that when a new packet of deletes is pushed, which happens when we flush a new segment, we immediately use that same indexing thread to and resolve the deletions. This is analogous to when we made segment flushing concurrent (LUCENE-3023), just for deletes and DV updates as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12873688/LUCENE-7868.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7823", "change_description": ": Pure query based naive bayes classifier using BM25 scores", "change_title": "Have a naive bayes classifier which uses plain BM25 scores instead of plain frequencies", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "SimpleNaiveBayesClassifier users term frequencies with add one smoothing to calculate likelihood and just tf for prior. Given Lucene has switched to BM25 it would be better to have a different impl which uses BM25  scoring as a probability measure of both prior and likelihood.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7838", "change_description": ": Knn classifier based on fuzzified term queries", "change_title": "Add a knn classifier based on fuzzified term queries", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "FLT mixes fuzzy and MLT, in the context of Lucene based classification it might be useful to add such a fuzziness to a dedicated KNN classifier (based on FLT queries).", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "New Features", "change_id": "LUCENE-7855", "change_description": ": Added advanced options of the Wikipedia tokenizer to its factory.", "change_title": "Add support for WikipediaTokenizer's advanced options", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "The advanced parameters of the WikipediaTokenizer should be added to the factory.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12870295/LUCENE-7855.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-2605", "change_description": ": Classic QueryParser no longer splits on whitespace by default.\nUse setSplitOnWhitespace(true) to get the old behavior.", "change_title": "queryparser parses on whitespace", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2", "detail_description": "The queryparser parses input on whitespace, and sends each whitespace separated term to its own independent token stream. This breaks the following at query-time, because they can't see across whitespace boundaries: Its also rather unexpected, as users think their charfilters/tokenizers/tokenfilters will do the same thing at index and querytime, but in many cases they can't. Instead, preferably the queryparser would parse around only real 'operators'.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816326/LUCENE-2605-dont-split-by-default.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7369", "change_description": ": Similarity.coord and BooleanQuery.disableCoord are removed.", "change_title": "Remove coordination factors from scores", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Splitting LUCENE-7347 into smaller tasks.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12815759/LUCENE-7369.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7368", "change_description": ": Removed query normalization.", "change_title": "Remove queryNorm", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Splitting LUCENE-7347 into smaller tasks.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816660/LUCENE-7368.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7355", "change_description": ": AnalyzingQueryParser has been removed as its functionality has\nbeen folded into the classic QueryParser.", "change_title": "Leverage MultiTermAwareComponent in query parsers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "MultiTermAwareComponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. However, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816841/LUCENE-7355.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7407", "change_description": ": Doc values APIs have been switched from random access\nto iterators, enabling future codec compression improvements.", "change_title": "Explore switching doc values to an iterator API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "I think it could be compelling if we restricted doc values to use an iterator API at read time, instead of the more general random access API we have today: This idea has come up many in the past, e.g. LUCENE-7253 is a recent example, and very early iterations of doc values started with exactly this However, it's a truly enormous change, likely 7.0 only.  Or maybe we could have the new iterator APIs also ported to 6.x side by side with the deprecate existing random-access APIs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12829030/LUCENE-7407.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7475", "change_description": ": Norms now support sparsity, allowing to pay for what is\nactually used.", "change_title": "Sparse norms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Even though norms now have an iterator API, they are still always dense in practice since documents that do not have a value get assigned 0 as a norm value.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12831926/LUCENE-7475.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7494", "change_description": ": Points now have a per-field API, like doc values.", "change_title": "Explore making PointValues a per-field API like doc values", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "This is a follow-up to LUCENE-7491. Maybe we could simplify things a bit by changing LeafReader.getPointValues() to LeafReader.getPointValues(String fieldName) and removing all String fieldName parameters from PointValues?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12833729/LUCENE-7494.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7410", "change_description": ": Cache keys and close listeners have been refactored in order\nto be less trappy. See IndexReader.getReaderCacheHelper and\nLeafReader.getCoreCacheHelper.", "change_title": "Make cache keys and closed listeners less trappy", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "IndexReader currently exposes getCoreCacheKey(), getCombinedCoreAndDeletesKey(), addCoreClosedListener() and addReaderClosedListener(). They are typically used to manage resources whose lifetime needs to mimic the lifetime of segments/indexes, typically caches. I think this is trappy for various reasons: When maintaining a cache, entries are added to the cache based on the cache key and then evicted using the cache key that is given back by the close listener, so it is very important that both keys are the same. But if a filter reader happens to delegate get*Key() and not add*ClosedListener() or vice-versa then there is potential for a memory leak since the closed listener will be called on a different key and entries will never be evicted from the cache. The expectation of using the core cache key is that it will not change in case of deletions, but this is only true on SegmentReader and LeafReader impls that delegate to it. Other implementations such as composite readers or parallel leaf readers use the same key for \"core\" and \"combined core and deletes\". An application might want to either expose more (with a ParrallelReader or MultiReader) or less information (by filtering fields/docs that can be seen) depending on the user who is logged in. In that case the application would typically maintain a DirectoryReader and then wrap it per request depending on the logged user and throw away the wrapper once the request is completed. The problem is that these wrappers have their own cache keys and the application may build something costly and put it in a cache to throw it away a couple milliseconds later. I would rather like for such readers to have a way to opt out from caching on order to avoid this performance trap. The keys that are exposed are plain java.lang.Object instances, which requires caches to look like a Map<Object, ?> which makes it very easy to either try to get, put or remove on the wrong object since any object would be accepted.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12851201/LUCENE-7410.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-6819", "change_description": ": Index-time boosts are not supported anymore. As a replacement,\nindex-time scoring factors should be indexed into a doc value field and\ncombined at query time using eg. FunctionScoreQuery.", "change_title": "Deprecate index-time boosts?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.5,7.0", "detail_description": "Follow-up of this comment: https://issues.apache.org/jira/browse/LUCENE-6818?focusedCommentId=14934801&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14934801 Index-time boosts are a very expert feature whose behaviour is tight to the Similarity impl. Additionally users have often be confused by the poor precision due to the fact that we encode values on a single byte. But now we have doc values that allow you to encode any values the way you want with as much precision as you need so maybe we should deprecate index-time boosts and recommend to encode index-time scoring factors into doc values fields instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12855185/LUCENE-6819-wip.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7734", "change_description": ": FieldType's copy constructor was widened to accept any IndexableFieldType.", "change_title": "FieldType copy constructor should accept IndexableFieldType", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "FieldType is a concrete implementation of IndexableFieldType.  It has a copy-constructor but it demands a FieldType.  It should accept  IndexableFieldType.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12856863/LUCENE_7734.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7701", "change_description": ": Grouping collectors have been refactored, such that groups are\nnow defined by a GroupSelector implementation.", "change_title": "Refactor grouping collectors", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Grouping currently works via abstract collectors, which need to be overridden for each way of defining a group - currently we have two, 'term' (based on SortedDocValues) and 'function' (based on ValueSources).  These collectors all have a lot of repeated code, and means that if you want to implement your own group definitions, you need to override four or five different classes. This would be easier to deal with if instead the 'group selection' code was abstracted out into a single interface, and the various collectors were changed to concrete implementations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12861307/LUCENE-7701.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7741", "change_description": ": DoubleValuesSource now has an explain() method", "change_title": "Add explain() to DoubleValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "As noted on LUCENE-7737, it would be nice to be able to get an explanation for DoubleValues values, particularly for things like complex spatial distance calculations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12866846/LUCENE-7741.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7815", "change_description": ": Removed the PostingsHighlighter; you should use the UnifiedHighlighter\ninstead, which derived from the UH.  WholeBreakIterator and\nCustomSeparatorBreakIterator were moved to UH's package.", "change_title": "Remove PostingsHighlighter in 7.0", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "The UnifiedHighlighter is derived from the PostingsHighlighter, which should be quite obvious to anyone who cares to look at them.  There is no feature in the PH that is not also present in the UH.  The PH is marked as lucene.experimental so we may remove it in 7.0.  The upgrade path is pretty easy given the API similarity.  By removing the PH, the goal is to ease maintenance.  Some issues lately have been applicable to both of these highlighters which is annoying to apply twice.  In one case I forgot to.  And of course there is user confusion by having both. What I propose to do in this issue is move CustomSeparatorBreakIterator and WholeBreakIterator out of the postingshighlight package into the uhighlight package (or perhaps add a common or util should future highlighters need them?).  Then of course remove postingshighlight package.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869394/LUCENE_7815_Remove_PostingsHighlighter.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7850", "change_description": ": Removed support for legacy numerics.", "change_title": "Remove legacy numerics", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Legacy numerics have been deprecated since 6.0, we should remove them. As discussed on the dev list, we could move them to Solr so that Solr can support them for an additional major version.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869682/LUCENE-7850.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7500", "change_description": ": Removed abstract LeafReader.fields(); instead terms(fieldName)\nhas been made abstract, fomerly was final.  Also, MultiFields.getTerms\nwas optimized to work directly instead of being implemented on getFields.", "change_title": "Remove LeafReader.fields()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Fields seems like a pointless intermediary between the LeafReader and Terms. Why not have LeafReader.getTerms(fieldName) instead? One loses the ability to get the count and iterate over indexed fields, but it's not clear what real use-cases are for that and such rare needs could figure that out with FieldInfos. mikemccand pointed out that we'd probably need to re-introduce a TermVectors class since TV's are row-oriented not column-oriented.  IMO they should be column-oriented but that'd be a separate issue. (p.s. I'm lacking time to do this w/i the next couple months so if someone else wants to tackle it then great)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12872404/LUCENE_7500_Remove_LeafReader_fields.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7872", "change_description": ": TopDocs.totalHits is now a long.", "change_title": "TopDocs.totalHits should be a long", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Even though a single index cannot have more than 2B documents, TopDocs.merge might merge TopDocs instances that have more than 2B documents in total.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12873214/LUCENE-7872.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7868", "change_description": ": IndexWriterConfig.setMaxBufferedDeleteTerms is\nremoved.", "change_title": "Use multiple threads to apply deletes and DV updates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Today, when users delete documents or apply doc values updates, IndexWriter buffers them up into frozen packets and then eventually uses a single thread (BufferedUpdatesStream.applyDeletesAndUpdates) to resolve delete/update terms to docids.  This thread also holds IW's monitor lock, so it also blocks refresh, merges starting/finishing, commits, etc. We have heavily optimized this part of Lucene over time, e.g. LUCENE-6161, LUCENE-2897, LUCENE-2680, LUCENE-3342, but still, it's a single thread so it can't use multiple CPU cores commonly available now. This doesn't affect append-only usage, but for update-heavy users (me!) this can be a big bottleneck, and causes long stop-the-world hangs during indexing. I have an initial exploratory patch to make these lookups concurrent, without holding IW's lock, so that when a new packet of deletes is pushed, which happens when we flush a new segment, we immediately use that same indexing thread to and resolve the deletions. This is analogous to when we made segment flushing concurrent (LUCENE-3023), just for deletes and DV updates as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12873688/LUCENE-7868.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7877", "change_description": ": PrefixAwareTokenStream is replaced with ConcatenatingTokenStream", "change_title": "Replace PrefixAwareTokenStream", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "PrefixAwareTokenStream/PrefixAndSuffixAwareTokenStream use the deprecated and broken Token class, which means they will not work with custom attributes.  We should fix/replace them.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12873494/LUCENE-7877.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7867", "change_description": ": The deprecated Token class is now only available in the test\nframework", "change_title": "Remove deprecated Token class", "detail_type": "Improvement", "detail_affect_versions": "7.0", "detail_fix_versions": "7.0", "detail_description": "The Token class has been sitting around deprecated since 2.9.  We should nuke it already.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12874079/LUCENE-7867.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7723", "change_description": ": DoubleValuesSource enforces implementation of equals() and\nhashCode()", "change_title": "LongValuesSource/DoubleValuesSource impls should implement equals/hashCode", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Given that we embed values sources in queries and sorts, from which we expect that they have working equals/hashCode, we should also implement equals/hashCode on LongValuesSource/DoubleValuesSource impls.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12871599/LUCENE-7723.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7737", "change_description": ": The spatial-extras module no longer has a dependency on the\nqueries module.  All uses of ValueSource are either replaced with core\nDoubleValuesSource extensions, or with the new ShapeValuesSource and\nShapeValuesPredicate classes", "change_title": "Remove spatial-extras dependency on queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "The spatial-extras module uses ValueSources for a number of different purposes, requiring a dependency on the queries module.  I'd like to try using core-only interfaces here instead, allowing us to remove the dependency", "patch_link": "https://issues.apache.org/jira/secure/attachment/12874362/LUCENE-7737.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7892", "change_description": ": Doc-values query factory methods have been renamed so that their\nname contains \"slow\" in order to cleary indicate that they would usually be a\nbad choice.", "change_title": "LatLonDocValuesField methods should be clearly marked as slow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0,8.0", "detail_description": "It is very trappy that LatLonDocValuesField has stuff like newBoxQuery/newDistanceQuery. Users bring this up on the user list and are confused as to why the resulting queries are slow. Here, we hurt the typical use case, to try to slightly speed up an esoteric one (sparse stuff). Its a terrible tradeoff for the API. If we truly must have such slow methods in the public API, then they should have slow in their name.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "API Changes", "change_id": "LUCENE-7899", "change_description": ": FieldValueQuery is renamed to DocValuesFieldExistsQuery", "change_title": "Rename FieldValueQuery to DocValuesFieldExistsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0,8.0", "detail_description": "I don't think we have a query today to efficiently test whether a doc values field exists (has any value) for each document in the index? Now that we use iterators to access doc values, this should be an efficient query: we can return the DISI we get for the doc values. ElasticSearch indexes its own field to record which field names occur in a document, so it's able to do \"exists\" for any field (not just doc values fields), but I think doc values fields we can just get \"for free\". I haven't started on this ... just wanted to open the issue first for discussion.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12875737/LUCENE-7899.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7626", "change_description": ": IndexWriter will no longer accept broken token offsets", "change_title": "IndexWriter shouldn't accept broken offsets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "I think we should do this in 7.0 (not 6.x). Long ago we stopped accepting broken offsets (where the start offset for a token is before the start offset of the last token) in postings (LUCENE-4127), but we are still lenient with term vectors. I think we should also check for term vectors: this would let users know that their analysis chain is producing offsets that cannot be used properly at search time.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12847311/LUCENE-7626.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7859", "change_description": ": Spatial-extras PackedQuadPrefixTree bug that only revealed itself\nwith the new pointsOnly optimizations in", "change_title": "PackedQuadCell.getTokenBytesNoLeaf bug when null BytesRef", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "PackedQuadCell.getTokenBytesNoLeaf and PackedQuadCell.getTokenBytesWithLeaf have a couple issues that were exposed in LUCENE-7845. One is that the WithLeaf version actually modifies the state (long term) which it shouldn't. Another is that if the BytesRef is null, it can compute a different result than if it's not null, which is definitely wrong. It appears that these problems are only revealing themselves in practice with the LUCENE-7845 optimization since there has been no PackedQuad test  failure before, and RandomSpatialOpFuzzyPrefixTreeTest really hammers the prefix tree.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7845", "change_description": ": Spatial-extras PackedQuadPrefixTree bug that only revealed itself\nwith the new pointsOnly optimizations in", "change_title": "spatial RPT optimization when query by point or common date range", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "If the query to an RPT index is a 2D point, or if using NumerBrangePrefixTreeStrategy / DateRangePrefixTree (Solr DateRangeField) if the query is a grid cell (a common date range unit like some particular day), then we can do some optimizations, especially if the data is pointsOnly.  If the data is pointsOnly the strategy can return a TermQuery, if the data isn't then we can at least tweak the prefixGridScanLevel.  This is motivated by two scenarios: This development was funded by the Harvard Center for Geographic Analysis as part of the HHypermap project", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869333/LUCENE_7845_query_by_point_optimization.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7871", "change_description": ": fix false positive match in BlockJoinSelector when children have no value, introducing\nwrap methods accepting children as DISI. Extracting ToParentDocValues", "change_title": "false positive match BlockJoinSelector[SortedDV] when child value is absent", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0,8.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12874636/LUCENE-7871.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7914", "change_description": ": Add a maximum recursion level in automaton recursive\nfunctions (Operations.isFinite and Operations.topsortState) to prevent\nlarge automaton to overflow the stack", "change_title": "Add safeguards to RegExp.toAutomaton and Operations", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0,8.0", "detail_description": "When creating an automaton from a regexp, some operators can create more states than maxDeterminizedStates: The example above <b>creates</b> a single path with 10k states already determinized so maxDeterminizedStates is not checked.  Some operations on automaton like Operations.isFinite or Operations.topoSortStates are recursive and the maximum level of recursion depends on the longest path in the automaton. So a large automaton like above can exceed java's stack. In most of the cases we are covered by maxDeterminizedStates but there will always be adversarial cases where a large automaton is created from a small input so I think we should also have safeguards in the recursive methods. I've attached a patch that adds a max recursion level to Operations.isFinite and Operations.topoSortStates in order to limit stack overflows. The limit is set to 1000 so any automaton with a path bigger than 1000 would throw an IllegalStateException. The patch also uses maxDeterminizedStates to limit the number of states that a repeat operator can create and throw a TooComplex..Exception when this limit is reached. Finally the patch adds the ability to skip Operations.isFinite on AutomatonQuery and uses this as an optimization for PrefixQuery that uses infinite automatons only.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12879843/LUCENE-7914.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7864", "change_description": ": IndexMergeTool is not using intermediate hard links (even\nif possible).", "change_title": "IndexMergeTool is not using intermediate hard links (even if possible)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "The current code is wrapping source directories with HardlinkCopyDirectoryWrapper; it should be the other way around (target directory should be wrapped so that it creates links to source segments).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12871552/LUCENE-7864.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7956", "change_description": ": Fixed potential stack overflow error in ICUNormalizer2CharFilter.", "change_title": "ICUNormalizer2CharFilter may fail with a StackOverFlow error", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "ICUNormalizer2CharFilter recursively fills up a buffer of 128 chars until the last char is neither a surrogate nor inert. On large inputs of non inert characters, this can cause a stack overflow error.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12885347/LUCENE-7956.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7963", "change_description": ": Remove useless getAttribute() in DefaultIndexingChain that\ncauses performance drop, introduced by", "change_title": "Useless getAttribute() in DefaultIndexingChain causes performance drop", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "7.0", "detail_description": "Some background: We have spotted a significant drop in indexing throughput on machines with a large number of cores (see https://github.com/elastic/elasticsearch/issues/26339 for details). We finally managed to isolate the behavior in a JMH \"microbenchmark\". Here is the output of running that benchmark with 32 threads with JMH's perfasm profiler (also Oracle Development Studio finds the same hotspot): You can see that a significant amount of time is spent in mov $0x7f4926a81d88,%rcx. It turns out that this maps to the following line in Java (DefaultIndexingChain$PerField::invert): Which is - luckily - unused (and removed by the attached patch). We have verified the impact of the change with an Elasticsearch macrobenchmark which indexes 165,346,692 documents (~ 74GB) with 24 clients concurrently. The reported numbers are median indexing throughput during that benchmark against a single Elasticsearch node: Details about the benchmark setup etc. are mentioned in the Elasticsearch issue #26339 if you're interested. Unfortunately, it is beyond me why writing this register to main memory takes such a long time and why C2 did not eliminate this line as dead code to begin with.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12886049/0001-Remove-unused-variable-in-DefaultIndexingChain.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7626", "change_description": ": Remove useless getAttribute() in DefaultIndexingChain that\ncauses performance drop, introduced by", "change_title": "IndexWriter shouldn't accept broken offsets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "I think we should do this in 7.0 (not 6.x). Long ago we stopped accepting broken offsets (where the start offset for a token is before the start offset of the last token) in postings (LUCENE-4127), but we are still lenient with term vectors. I think we should also check for term vectors: this would let users know that their analysis chain is producing offsets that cannot be used properly at search time.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12847311/LUCENE-7626.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Improvements", "change_id": "LUCENE-7489", "change_description": ": Better storage of sparse doc-values fields with the default\ncodec.", "change_title": "Improve sparsity support of Lucene70DocValuesFormat", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Like Lucene70NormsFormat, it should be able to only encode actual values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12833124/LUCENE-7489.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Improvements", "change_id": "LUCENE-7730", "change_description": ": More accurate encoding of the length normalization factor\nthanks to the removal of index-time boosts.", "change_title": "Better encode length normalization in similarities", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Now that index-time boosts are gone (LUCENE-6819) and that indices record the version that was used to create them (for backward compatibility, LUCENE-7703), we can look into storing the length normalization factor more efficiently.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12868733/LUCENE-7730.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Improvements", "change_id": "LUCENE-7901", "change_description": ": Original Highlighter now eagerly throws an exception if you\nprovide components that are null.", "change_title": "original/default Highlighter's constructor should throw if passed null args", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0,8.0", "detail_description": "The default/original Highlighter has a constructor that takes several parameters, including an Encoder.  If per chance these are null, we'll throw an exception later at highlight time but it's nicer to get an exception eagerly.  It will also avoid the LUCENE-6979 trap.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12877519/LUCENE-7901.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Improvements", "change_id": "LUCENE-7841", "change_description": ": Normalize Ò to Ð³ in Ukrainian analyzer.", "change_title": "Normalize ґ to г in Ukrainian analyzer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "Letter ґ was re-introduced into Ukrainian alphabet in 1990 and many Ukrainian texts don't use this letter consistently so the search will benefit if we normalize it to г.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7416", "change_description": ": BooleanQuery optimizes queries that have queries that occur both\nin the sets of SHOULD and FILTER clauses, or both in MUST/FILTER and MUST_NOT\nclauses.", "change_title": "BooleanQuery rewrite optimizations", "detail_type": "Improvement", "detail_affect_versions": "7.0", "detail_fix_versions": "7.0", "detail_description": "A couple of BooleanQuery rewrites / optimizations. First, as discussed on the user group, a BooleanQuery with a query that is both a SHOULD and a FILTER can be rewritten as a single MUST query, but care must be taken to decrement minShouldMatch by 1. Another case is if a query is both required (MUST or FILTER) and MUST_NOT at the same time, it can be converted to a MatchNoDocsQuery (although I haven't discussed this yet so hopefully I'm not missing something!).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12825500/LUCENE-7146-MatchAllMustNot.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7506", "change_description": ": FastTaxonomyFacetCounts should use CPU in proportion to\nthe size of the intersected set of hits from the query and documents\nthat have a facet value, so sparse faceting works as expected", "change_title": "FastTaxonomyFacetCounts should use ConjunctionDISI", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "jpountz proposed this patch, so that in sparse cases the facet counting is fast (more in proportion to the size of the intersected set).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12834136/LUCENE-7506.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7519", "change_description": ": Add optimized APIs to compute browse-only top level\nfacets", "change_title": "Optimize computing browse-only facets for taxonomy and sorted set methods", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "For the \"browse facets\" use case, where logically you run MatchAllDocsQuery and then compute facet hits, we can optimize this case for both SortedSetDocValuesFacets and FastTaxonomyFacetCounts so that we don't use the query DISI at all and rather just pull from the doc values iterator using nextDoc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12834810/LUCENE-7519.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7589", "change_description": ": Numeric doc values now have the ability to encode blocks of\nvalues using different numbers of bits per value if this proves to save\nstorage.", "change_title": "Prevent outliers from raising the number of bits of everyone with numeric doc values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Today we encode entire segments with a single number of bits per value. It was done this way because it was faster, but it also means a single outlier can significantly increase the space requirements. I think we should have protection against that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12842582/LUCENE-7589.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7845", "change_description": ": Enhance spatial-extras RecursivePrefixTreeStrategy queries when the\nquery is a point (for 2D) or a is a simple date interval (e.g. 1 month).  When\nthe strategy is marked as pointsOnly, the results is a TermQuery.", "change_title": "spatial RPT optimization when query by point or common date range", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "If the query to an RPT index is a 2D point, or if using NumerBrangePrefixTreeStrategy / DateRangePrefixTree (Solr DateRangeField) if the query is a grid cell (a common date range unit like some particular day), then we can do some optimizations, especially if the data is pointsOnly.  If the data is pointsOnly the strategy can return a TermQuery, if the data isn't then we can at least tweak the prefixGridScanLevel.  This is motivated by two scenarios: This development was funded by the Harvard Center for Geographic Analysis as part of the HHypermap project", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869333/LUCENE_7845_query_by_point_optimization.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7874", "change_description": ": DisjunctionMaxQuery rewrites to a BooleanQuery when tiebreaker is set to 1.", "change_title": "DisjunctionMaxQuery could rewrite to BooleanQuery when tiebreaker == 1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "If all disjuncts are considered equal in terms of scoring, rewriting to a BooleanQuery would not modify the score and could bring some speed up with the bulk scorer optims.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12872239/LUCENE-7874.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7828", "change_description": ": Speed up range queries on range fields by improving how we\ncompute the relation between the query and inner nodes of the BKD tree.", "change_title": "Improve PointValues visitor calls when all docs in a leaf share a value", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "When all the docs in a leaf node have the same value, range queries can waste a lot of processing if the node itself returns CELL_CROSSES_QUERY when compare() is called, in effect performing the same calculation in visit(int, byte[]) over and over again.  In the case I'm looking at (very low cardinality indexed LongRange fields), this causes something of a perfect storm for performance.  PointValues can detect up front if a given node has a single value (because it's min value and max value will be equal), so this case should be fairly simple to identify and shortcut.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12870757/LUCENE-7828.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7923", "change_description": ": Removed FST.Arc.node field (unused).", "change_title": "Remove FST.Arc.node field (not used anywhere)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12881008/LUCENE-7923.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7328", "change_description": ": Remove LegacyNumericEncoding from GeoPointField.", "change_title": "Remove LegacyNumericEncoding from GeoPointField", "detail_type": "Improvement", "detail_affect_versions": "7.0", "detail_fix_versions": "None", "detail_description": "6.0 deprecated legacy NumericEncoding in GeoPointField this issue completely removes the encoding for 7.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809466/LUCENE-7328.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7360", "change_description": ": Remove Explanation.toHtml()", "change_title": "Remove Explanation.toHtml()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "This seems to be something of a relic.  It's still used in Solr, but I think it makes more sense to move it directly into the ExplainAugmenter there rather than having it in Lucene itself.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12814143/LUCENE-7360.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7681", "change_description": ": MemoryIndex uses new DocValues API", "change_title": "Remove LegacyDocValues implementations from MemoryIndex", "detail_type": "Improvement", "detail_affect_versions": "7.0", "detail_fix_versions": "7.0", "detail_description": "MemoryIndex in master is using the LegacyDocValue wrappers.  We should replace these with plain 7.0-style iterators instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12851406/LUCENE-7681.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7753", "change_description": ": Make fields static when possible.", "change_title": "Findbugs: mark certain final fields static", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "http://findbugs.sourceforge.net/bugDescriptions.html#SS_SHOULD_BE_STATIC", "patch_link": "https://issues.apache.org/jira/secure/attachment/12860920/LUCENE-7753.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7540", "change_description": ": Upgrade ICU to 59.1", "change_title": "Upgrade ICU to 58.1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "ICU is up to 58.1, but our ICU analysis components currently use 56.1, which is ~1 year old by now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12869513/LUCENE-7540.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7852", "change_description": ": Correct copyright year(s) in lucene/LICENSE.txt file.", "change_title": "out-of-date Copyright year(s) on NOTICE.txt files?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12870417/LUCENE-7852.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7719", "change_description": ": Generalized the UnifiedHighlighter's support for AutomatonQuery\nfor character & binary automata. Added AutomatonQuery.isBinary.", "change_title": "UnifiedHighlighter doesn't handle some AutomatonQuery's with multi-byte chars", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "In MultiTermHighlighting, a CharacterRunAutomaton is being created that takes the result of AutomatonQuery.getAutomaton that in turn is byte oriented, not character oriented.  For ASCII terms, this is safe but it's not for multi-byte characters.  This is most likely going to rear it's head with a WildcardQuery, but due to special casing in MultiTermHighlighting, PrefixQuery isn't affected.  Nonetheless it'd be nice to get a general fix in so that MultiTermHighlighting can remove special cases for PrefixQuery and TermRangeQuery (both subclass AutomatonQuery). AFAICT, this bug was likely in the PostingsHighlighter since inception.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12862120/LUCENE_7719.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7873", "change_description": ": Due to serious problems with context class loaders in several\nframeworks (OSGI, Java 9 Jigsaw), the lookup of Codecs, PostingsFormats,\nDocValuesFormats and all analysis factories was changed to only inspect the\ncurrent classloader that defined the interface class (lucene-core.jar).\nSee MIGRATE.txt for more information!", "change_title": "Remove context classloader from SPI lookups by default", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "As discussed on LUCENE-7870, we should really remove the context class loader from Lucene's SPI lookup (NamedSPLoader, SPIClassIterator, AnalysisSPI stuff). My idea would be (as stated before): Get rid of the Context Classloader in SPI lookups! Lucene never uses it, it is just there for backwards compatibility. The current setup of SPI does not work with modules of Java 9 and it does not work with stuff in completely different classloaders. So OSGI fails in any case, if you have lucene-core.jar and lucene-backwards-codecs.jar as OSGI modules, because both would use different loaders. The context loader won't help. The problem is that we may break some apps that rely on the context loader traversal. In my opinion, we may add a system property that is read on setup of NamedSPILoader / SPIClassIterator that can be set to true (e.g. lucene.useContextLoaderForSPI, defaulting to false). This may fix legacy apps and new apps would only traverse the classloader that loaded lucene-core.jar. For Java 9 and \"Lucene as Java 9 module\") we have to refactor this anyways, becaue we need to respect module-info,java and look for SPI exports. FYI: Context class loaders were the worst idea ever in Java. I personally hate them and I would do anything - just to make them disappear from the spec! When drinking beers with Mark Reinhold in Brussels, I am always reminding him about this together with the inability to unmap byte buffers... Unfortunately the sysprop approach is the only way to handle this as this must be done very early on JVM/Lucene setup. If somebody called Codec.forName() its too late to change the default... But I am fine with using a sysprop with AccessController.doPrivileged(), as this is only required for those legacy WEBAPP stuff. Normal applications should see the META-INF files on the application classloader anyways.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12873644/LUCENE-7873.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7883", "change_description": ": Lucene no longer uses the context class loader when resolving\nresources in CustomAnalyzer or ClassPathResourceLoader. Resources are only\nresolved against Lucene's class loader by default. Please use another builder\nmethod to change to a custom classloader.", "change_title": "Remove references to Thread#getContextClassLoader() from Lucene/Solr codebase", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "As discussed in LUCENE-7873, the use of Thread.currentThread().getContextClassLoader() is in most cases a bug caused by sloppy usage of ClassLoader APIs and should be replaced by getClass().getClassLoader(). In Lucene we only have the ClassPathResourceLoader, but this one can be solved by removing the \"default\" constructor. It only affects CustomAnalyzer, that should also be extended in Lucene 7. The uses in Solr and its test are all to be replaced by getClass().getClassLoader() (except some workaround with clustering using a try-finally). This is in most cases historical code, when Solr was a web application to workaround some buggy webapp containers. In the current state, the code is \"just wrong\". Finally, we should add java.lang.Thread#getContextClassLoader() to forbidden-apis. I'd like to get this into Lucene 7, as it affects some APIs in Lucene.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12873742/LUCENE-7883.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-5822", "change_description": ": Convert README to Markdown", "change_title": "Create a markdown formatted README file for Lucene/Solr", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "We have a minimal plain text readme file right now. Github is very popular these days and we are even accepting pull requests from there. I think we should add a proper markdown formatted README file which not only talks about the features of Lucene/Solr but also include a short tutorial on both Lucene and Solr.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12875322/LUCENE-5822-addemdum.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7773", "change_description": ": Remove unused/deprecated token types from StandardTokenizer.", "change_title": "Remove unused/deprecated token types from StandardTokenizer", "detail_type": "Improvement", "detail_affect_versions": "6.5", "detail_fix_versions": "7.0,7.1,8.0", "detail_description": "StandardTokenizer does not recognize e-mail, company etc. This issue removes those token types.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12875990/LUCENE-7773.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7800", "change_description": ": Remove code that potentially rethrows checked exceptions\nfrom methods that don't declare them (\"sneaky throw\" hack).", "change_title": "Remove code that potentially rethrows checked exceptions from methods that don't declare them (\"sneaky throw\" hack)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "For a long time I considered the \"sneaky\" throw hack to be a nice way of coding around some of Java's limitations (especially with invoking methods via reflection or method handles), but with time I started to see how it can be potentially dangerous and is nearly always confusing. If you have a Java method and its signature doesn't indicate the possibility of a checked exception you, as a programmer, simply don't expect it to happen. Never. So, for example, you could write: and consider the code above to be absolutely bullet-proof in terms of handling exceptions. Unfortunately with sneaky throws anywhere in the \"luceneApi\" this is no longer the case – you can receive a checked exception that will simply fall through and hit some code frame above. So I suggest we remove sneaky throw from the core entirely. It only exists in two places – private methods inside Snowball programs invoked via method handles (these don't even declare checked exceptions so I assume they can't occur) and AttributeFactory – here there is a real possibility somebody could declare an attribute class's constructor that does throw an unchecked exception. In that case I think it is more reasonable to wrap it in a RuntimeException than rethrow it as original. Alternatively, we can modify the signature of createAttributeInstance and getStaticImplementation to declare some kind of checked exception (either a wrapper or even a Throwable), but I see little reason for it and it'd change the API.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12868234/LUCENE-7800.patch", "patch_content": "none"}
{"library_version": "7.0.0", "change_type": "Other", "change_id": "LUCENE-7876", "change_description": ": Avoid calls to LeafReader.fields() and MultiFields.getFields()\nthat are trivially replaced by LeafReader.terms() and MultiFields.getTerms()", "change_title": "Avoid needless calls to LeafReader.fields and MultiFields.getFields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.0", "detail_description": "In LUCENE-7500 we're removing LeafReader.fields for 7.x.  Here in this issue for 6.x and 7.x we simply avoid calling this method (and also MultiFields.getFields) when there is an obvious replacement for LeafReader.terms(field) (and MultiFields.getTerms).  Any absolutely non-trivial changes are occurring in LUCENE-7500.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12872405/LUCENE_7876_avoid_leafReader_fields.patch", "patch_content": "none"}

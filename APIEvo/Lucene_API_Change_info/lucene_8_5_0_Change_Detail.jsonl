{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9093", "change_description": ": Not an API change but a change in behavior of the UnifiedHighlighter's LengthGoalBreakIterator that will\nyield Passages sized a little different due to the fact that the sizing pivot is now the center of the first match and\nnot its left edge.", "change_title": "Unified highlighter with word separator never gives context to the left", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "When using the unified highlighter with hl.bs.type=WORD, I am not able to get context to the left of the matches returned; only words to the right of each match are shown.  I see this behaviour on both Solr 6.4 and Solr 7.1. Without context to the left of a match, the highlighted snippets are much less useful for understanding where the match appears in a document. As an example, using the techproducts data with Solr 7.1, given a search for \"apple\", highlighting the \"features\" field: http://localhost:8983/solr/techproducts/select?hl.fl=features&hl=on&q=apple&hl.bs.type=WORD&hl.fragsize=30&hl.method=unified I see this snippet: \"<em>Apple</em> Lossless, H.264 video\" Note that \"Apple\" is anchored to the left.  Compare with the original highlighter: http://localhost:8983/solr/techproducts/select?hl.fl=features&hl=on&q=apple&hl.fragsize=30 And the match has context either side: \", Audible, <em>Apple</em> Lossless, H.264 video\" (To complicate this, in general I am not sure that the unified highlighter is respecting the hl.fragsize parameter, although SOLR-9935 suggests support was added.  I included the hl.fragsize param in the unified URL too, but it's making no difference unless set to 0.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12989051/LUCENE-9093.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9116", "change_description": ": PostingsWriterBase and PostingsReaderBase no longer support\nsetting a field's metadata via a `long[]`.", "change_title": "Simplify postings API by removing long[] metadata", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "The postings API allows to store metadata about a term either in a long[] or in a byte[]. This is unnecessary as all information could be encoded in the byte[], which is what most codecs do in practice.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9116", "change_description": ": The FSTOrd postings format has been removed.", "change_title": "Simplify postings API by removing long[] metadata", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "The postings API allows to store metadata about a term either in a long[] or in a byte[]. This is unnecessary as all information could be encoded in the byte[], which is what most codecs do in practice.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-8369", "change_description": ": Remove obsolete spatial module.", "change_title": "Remove the spatial module as it is obsolete", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The \"spatial\" module is at this juncture nearly empty with only a couple utilities that aren't used by anything in the entire codebase – GeoRelationUtils, and MortonEncoder.  Perhaps it should have been removed earlier in LUCENE-7664 which was the removal of GeoPointField which was essentially why the module existed.  Better late than never.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929126/LUCENE-8369.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-8621", "change_description": ": Refactor LatLonShape, XYShape, and all query and utility classes to core.", "change_title": "Move LatLonShape and XYShape out of sandbox", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "LatLonShape has matured a lot over the last months, I'd like to start thinking about moving it out of sandbox so that it doesn't stay there for too long like what happened to LatLonPoint. I am pretty happy with the current encoding. To my knowledge, we might just need to do a minor modification because of  LUCENE-8620. XYShape and foundation classes will also need to be refactored.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9218", "change_description": ": XY geometries API works in float space.", "change_title": "XYGeometries should use floats instead of doubles", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "XYGeometries (XYPolygon, XYLine, XYRectangle & XYPoint) are a bit counter-intuitive. Where most of them are initialised using floats, when returning those values, they are returned as doubles. In addition XYRectangle seems to work on doubles. In this issue it is proposed to harmonise those classes to only work on floats. As these classes were just move to core and they have not been released, it should be ok to change its interfaces.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9212", "change_description": ": Intervals.multiterm() takes CompiledAutomaton rather than plain Automaton", "change_title": "Intervals.multiterm() should take a CompiledAutomaton", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "LUCENE-9028 added a `multiterm` factory method for intervals that accepts an arbitrary Automaton, and converts it internally into a CompiledAutomaton.  This isn't necessarily correct behaviour, however, because Automatons can be defined in both binary and unicode space, and there's no way of telling which it is when it comes to compiling them.  In particular, for automatons produced by FuzzyTermsEnum, we need to convert them to unicode before compilation. The `multiterm` factory should just take `CompiledAutomaton` directly, and we should deprecate the methods that take `Automaton` and remove in master.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9150", "change_description": ": Restore support for dynamic PlanetModel in spatial3d.", "change_title": "Restore support for dynamic PlanetModel in Geo3D", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "LUCENE-7072 removed dynamic planet model support in Geo3D. This was logical at the time (given the state of Lucene and spatial projections and coordinate reference systems). Since then, however, there have been a lot of new developments within the OGC community around Coordinate Reference Systems, Dynamic Coordinate Reference Systems, and Updated ISO Standards. It would be useful for Geo3D (and eventually LatLon*) to support different geographic datums to make lucene a viable option for indexing/searching in different spatial reference systems (e.g., more accurately computing query shape relations to BKD's internal nodes using datum consistent with the spatial projection). This would also provide an alternative to other limitations of the LatLon*/XY* implementation (e.g., pole/dateline crossing, quantization of small polygons). I'd like to propose keeping the current WGS84 static datum as the default for Geo3D but adding back the constructors to accept custom planet models. Perhaps this could be listed as an \"expert\" API feature?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "API Changes", "change_id": "LUCENE-9171", "change_description": ": QueryBuilder.newTermQuery() and .newSynonymQuery() now take boost parameters.", "change_title": "Synonyms Boost by Payload", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "I have been working in the additional capability of boosting queries by terms payload through a parameter to enable it in Lucene Query Builder. This has been done targeting the Synonyms Query. It is parametric, so it meant to see no difference unless the feature is enabled. Solr has its bits to comply thorugh its SynonymsQueryStyles", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "New Features", "change_id": "LUCENE-8903", "change_description": ": Add LatLonShape and XYShape point query.", "change_title": "Add LatLonShape point query", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Adds a query to LatLonShape that filters by a provided point.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "New Features", "change_id": "LUCENE-8707", "change_description": ": Add LatLonShape and XYShape distance query.", "change_title": "Add LatLonShape distance query", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Add a query to LatLonShape that filters by distance from a provided point.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "New Features", "change_id": "LUCENE-9238", "change_description": ": New XYPointField field and Queries for indexing, searching and sorting\ncartesian points.", "change_title": "XYPointField: Adapt LatLonPoint field type, and queries to non-geo shapes", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "This issue will explore generalizing the LatLonPoint field and LatLonPoint queries to non geospatial (cartesian) coordinate systems so lucene can provide the index & search capability for general cartesian point / non GIS type use cases.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9149", "change_description": ": Increase data dimension limit in BKD.", "change_title": "Increase data dimension limit in BKD", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "LUCENE-8496 added selective indexing; the ability to designate the first K <= N dimensions for driving the construction of the BKD internal nodes. Follow on work stored the \"data dimensions\" for only the leaf nodes and only the \"index dimensions\" are stored for the internal nodes. While maxPointsInLeafNode is still important for managing the BKD heap memory footprint (thus we don't want this to get too large), I'd like to propose increasing the MAX_DIMENSIONS limit (to something not too crazy like 16; effectively doubling the index dimension limit) while maintaining the MAX_INDEX_DIMENSIONS at 8. Doing this will enable us to encode higher dimension data within a lower dimension index (e.g., 3D tessellated triangles as a 10 dimension point using only the first 6 dimensions for index construction) ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12991264/LUCENE-9149.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9102", "change_description": ": Add maxQueryLength option to DirectSpellchecker.", "change_title": "Add maxQueryLength option to DirectSpellchecker", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Attempting to spellcheck some long query terms can trigger org.apache.lucene.util.automaton.TooComplexToDeterminizeException. This change (previously discussed in SOLR-13190) adds a maxQueryLength option to DirectSpellchecker so that Lucene can be configured to not attempt to spellcheck terms over a specified length. PR: https://github.com/apache/lucene-solr/pull/1103 Dependent Solr issue: SOLR-14131", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9091", "change_description": ": UnifiedHighlighter HTML escaping should only escape essentials", "change_title": "UnifiedHighlighter HTML escaping should only escape essentials", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "The unified highlighter does not use the org.apache.lucene.search.highlight.SimpleHTMLEncoder through org.apache.solr.highlight.HtmlEncoder. It has the HTML escaping feature re-implemented and embedded in the org.apache.lucene.search.uhighlight.DefaultPassageFormatter. The HTML escaping done by the unified highlighter escapes characters that do not need it. This makes the result payload 50%+ more heavy with no benefit. Here is a highlight snippet using the original highlighter: Here is the same highlight snippet using the unified highlighter: Maybe I'm missing the point why this is done the way it is. If this behaviour is desired for some use-case it should be a separate encoder, and the HTML encoder should only escape the necessary characters. Affects all versions of Lucene-Solr since the addition of the UnifiedHighlighter. Here are the lines where the escaping are implemented differently: ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12988720/LUCENE-9091.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9105", "change_description": ": UniformSplit postings format detects corrupted index and better handles IO exceptions.", "change_title": "UniformSplit postings format should detect corrupted index", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "BlockTree postings format has some checks when reading index metadata to detect index corruption. UniformSplit should have the same. Additionally UniformSplit has assertions in BlockReader that should be runtime checks to also detect index corruption (this case has been encountered in production environment).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9106", "change_description": ": UniformSplit postings format allows extension of block/line serializers.", "change_title": "UniformSplit postings format should allow extension of block/line serializers.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Currently UniformSplit postings format has static read methods for block / line / header. So it is not possible to extend them to change slightly the format. By introducing non-static serializers it will become possible to extend easily the format to make changes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9093", "change_description": ": UnifiedHighlighter's LengthGoalBreakIterator has a new fragmentAlignment option to better center the\nfirst match in the passage. Also the sizing point now pivots at the center of the first match term and not its left\nedge. This yields Passages that won't be identical to the previous behavior.", "change_title": "Unified highlighter with word separator never gives context to the left", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "When using the unified highlighter with hl.bs.type=WORD, I am not able to get context to the left of the matches returned; only words to the right of each match are shown.  I see this behaviour on both Solr 6.4 and Solr 7.1. Without context to the left of a match, the highlighted snippets are much less useful for understanding where the match appears in a document. As an example, using the techproducts data with Solr 7.1, given a search for \"apple\", highlighting the \"features\" field: http://localhost:8983/solr/techproducts/select?hl.fl=features&hl=on&q=apple&hl.bs.type=WORD&hl.fragsize=30&hl.method=unified I see this snippet: \"<em>Apple</em> Lossless, H.264 video\" Note that \"Apple\" is anchored to the left.  Compare with the original highlighter: http://localhost:8983/solr/techproducts/select?hl.fl=features&hl=on&q=apple&hl.fragsize=30 And the match has context either side: \", Audible, <em>Apple</em> Lossless, H.264 video\" (To complicate this, in general I am not sure that the unified highlighter is respecting the hl.fragsize parameter, although SOLR-9935 suggests support was added.  I included the hl.fragsize param in the unified URL too, but it's making no difference unless set to 0.)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12989051/LUCENE-9093.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9153", "change_description": ": Allow WhitespaceAnalyzer to set a maxTokenLength other than the default of 255", "change_title": "Lucene Query parser append space if query length is greater than 255", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Hello Everyone   I am working with Lucene 4.7.1 When parsing query using Lucene query parser. If query length is greater than 255 bytes, it returns query with space appended after every 255 bytes, which is causing further issues in my project.   Can you please let me know why the term (parsed query contain Arraylist<Term>) max length is 255 bytes. Why space is appended in between the query?   I will really appreciate it if someone can help me with this. Do let me know if you have not understood my query and require some reference   For analysis,  Please check QueryBuilder.java class which has method createFieldQuery(....)", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9152", "change_description": ": Improve line intersections with polygons when they are touching from the outside.", "change_title": "Improve line intersections from polygons", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Currently we always check triangle intersection in polygons without considering the boundary. This is not totally right as it might miss an intersection if a polygon and a triangle are touching each other.  The proposal is the following: *  if there is no points of the triangle inside the polygon, then consider the boundary * If all points are inside the polygon, then do not consider the boundary.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9123", "change_description": ": Add new JapaneseTokenizer constructors with discardCompoundToken option that controls whether\nthe tokenizer emits original (compound) tokens when the mode is not NORMAL.", "change_title": "JapaneseTokenizer with search mode doesn't work with SynonymGraphFilter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.5", "detail_description": "JapaneseTokenizer with `mode=search` or `mode=extended` doesn't work with both of SynonymGraphFilter and SynonymFilter when JT generates multiple tokens as an output. If we use `mode=normal`, it should be fine. However, we would like to use decomposed tokens that can maximize to chance to increase recall. Snippet of schema: An synonym entry that generates error: The following is an output on console:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12991495/LUCENE-9123.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9253", "change_description": ": KoreanTokenizer now supports custom dictionaries(system, unknown).", "change_title": "Support custom dictionaries in KoreanTokenizer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.5", "detail_description": "KoreanTokenizer does not support custom dictionaries(system, unknown) now, even though Nori provides DictionaryBuilder that creates custom dictionary. In the current state, it is very difficult for Nori users to use a custom dictionary. Therefore, we need to open a new constructor that uses it. Kuromoji is already supported(LUCENE-8971) that, and I referenced it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Improvements", "change_id": "LUCENE-9171", "change_description": ": QueryBuilder can now use BoostAttributes on input token streams to selectively\nboost particular terms or synonyms in parsed queries.", "change_title": "Synonyms Boost by Payload", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "I have been working in the additional capability of boosting queries by terms payload through a parameter to enable it in Lucene Query Builder. This has been done targeting the Synonyms Query. It is parametric, so it meant to see no difference unless the feature is enabled. Solr has its bits to comply thorugh its SynonymsQueryStyles", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9211", "change_description": ": Add compression for Binary doc value fields.", "change_title": "Adding compression to BinaryDocValues storage", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "While SortedSetDocValues can be used today to store identical values in a compact form this is not effective for data with many unique values. The proposal is that BinaryDocValues should be stored in LZ4 compressed blocks which can dramatically reduce disk storage costs in many cases. The proposal is blocks of a number of documents are stored as a single compressed blob along with metadata that records offsets where the original document values can be found in the uncompressed content. There's a trade-off here between efficient compression (more docs-per-block = better compression) and fast retrieval times (fewer docs-per-block = faster read access for single values). A fixed block size of 32 docs seems like it would be a reasonable compromise for most scenarios. A PR is up for review here https://github.com/apache/lucene-solr/pull/1234", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-4702", "change_description": ": Better compression of terms dictionaries.", "change_title": "Terms dictionary compression", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "I've done a quick test with the block tree terms dictionary by replacing a call to IndexOutput.writeBytes to write suffix bytes with a call to LZ4.compressHC to test the peformance hit. Interestingly, search performance was very good (see comparison table below) and the tim files were 14% smaller (from 150432 bytes overall to 129516). Only queries which are very terms-dictionary-intensive got a performance hit (Fuzzy, Fuzzy2, Respell, Wildcard), other queries including Prefix3 behaved (surprisingly) well. Do you think of it as something worth exploring?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566942/LUCENE-4702.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9228", "change_description": ": Sort dvUpdates in the term order before applying if they all update a\nsingle field to the same value. This optimization can reduce the flush time by around\n20% for the docValues update user cases.", "change_title": "Sort docValues updates by terms before applying", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.5", "detail_fix_versions": "9.0,8.5", "detail_description": "If all DVUpdates update a single field to the same value, then we can apply these updates in the term order instead of the request order as both will yield the same result. This optimization allows us to iterate the term dictionary faster and de-duplicate updates.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9245", "change_description": ": Reduce AutomatonTermsEnum memory usage.", "change_title": "Reduce AutomatonTermsEnum memory usage", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Reduce AutomatonTermsEnum memory usage. It uses a long[] with size = number of automaton states, which may be large. Instead we can reduce to short[] with slightly faster performance. (I tried a FixedBitSet, but it requires to clear and this impacts negatively performance)", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9237", "change_description": ": Faster UniformSplit intersect TermsEnum.", "change_title": "Faster TermsEnum intersect for UniformSplit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "New version of TermsEnum intersect for UniformSplit. It is 75% more efficient than the previous version for FuzzyQuery. Compared to BlockTree IntersectTermsEnum:  When I debugged thoroughly to understand what was the limitation of the previous approach we had (to compute the common prefix between two consecutive block keys in the FST), I saw that actually for all FuzzyQuery the common prefix matched so we entered all blocks.  I realized that the FuzzyQuery automaton accepts many variations for the prefix, and the common prefix was not long enough to allow us to filter correctly. I looked at what VarGapFixedInterval did. It jumped all the time after each term to find the next target term accepted by the automaton. And this was sufficiently efficient thanks to a vital optimization that compared the target term to the immediate following term, to actually not jump most of the time. So I applied the same idea to compute the next accepted term and jump, but now with a first condition based on the number of consecutively rejected terms, and by anticipating the comparison of the accepted term with the immediate next term. This is the main factor of the improvement. We leverage also other optimizations that speed up the automaton validation of each sequential term in the block.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9068", "change_description": ": FuzzyQuery builds its Automaton up-front", "change_title": "Build FuzzyQuery automata up-front", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "FuzzyQuery builds a set of levenshtein automata (one for each possible edit distance) at rewrite time, and passes them between different TermsEnum invocations using an attribute source.  This seems a bit needlessly complicated, and also means that things like visiting a query end up building the automata again.  We should instead build the automata at query construction time, which is how AutomatonQuery does it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9113", "change_description": ": Faster merging of SORTED/SORTED_SET doc values.", "change_title": "Speed up merging doc values terms dictionaries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "The default DocValuesConsumer#mergeSortedField and DocValuesConsumer#mergeSortedSetField implementations create a merged view of the doc values producers to merge. Unfortunately, it doesn't override termsEnum(), whose default implementation of next() increments the ordinal and calls lookupOrd() to retrieve the term. Currently, lookupOrd() doesn't take advantage of its current position, and would seek to the block start and then call next() up to 16 times to go to the desired term. While there are discussions to optimize lookups to take advantage of the current ord (LUCENE-8836), it shouldn't be required for merging to be efficient and we should instead make next() call next() on its sub enums.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9125", "change_description": ": Optimize Automaton.step() with binary search and introduce Automaton.next().", "change_title": "Improve Automaton.step() with binary search and introduce Automaton.next()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Implement the existing todo in Automaton.step() (lookup a transition from a source state depending on a given label) to use binary search since the transitions are sorted. Introduce new method Automaton.next() to optimize iteration & lookup over all the transitions of a state. This will be used in RunAutomaton constructor and in MinimizationOperations.minimize().", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Optimizations", "change_id": "LUCENE-9147", "change_description": ": The index of stored fields and term vectors in now off-heap.", "change_title": "Move the stored fields index off-heap", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Now that the terms index is off-heap by default, it's almost embarrassing that many indices spend most of their memory usage on the stored fields index or the term vectors index, which are much less performance-sensitive than the terms index. We should move them off-heap too?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9084", "change_description": ": Fix potential deadlock due to circular synchronization in AnalyzingInfixSuggester", "change_title": "circular synchronization wait (potential deadlock) in AnalyzingInfixSuggester", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "I created a pull request on github for this: https://github.com/apache/lucene-solr/pull/1064 Detailed code (snippets and links) are in the sections after this overview (section *Detailed Code* and *This Patch's Code*). Method ensureOpen() is synchronized (acquires this) and its body contains a synchronized (searcherMgrLock) block (i.e., then acquires searcherMgrLock). Method ensureOpen() is called two times from public methods add() and update(). A thread calling public methods add() or update() will acquire locks in order: Public method build() has a synchronized (searcherMgrLock) block in which it calls method add(). Method add(), as described above, calls method ensureOpen(). Therefore, a thread calling public method build() will acquire locks in order: 2 threads can acquire locks in different order which may cause a circular wait (deadlock). I do not know which threads call these methods, but there is a lot of synchronization in these methods and in this file, so I think these methods must be called concurrently. One thread can acquire: this -> searcherMgrLock (the first order above) and the other thread can acquire: searcherMgrLock -> this (the second order above). Note how the above 2 orders lead to a circular wait. Method ensureOpen() is synchronized and its body contains a synchronized (searcherMgrLock): https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L371-L379 Method ensureOpen() is called two times from public methods add() and update(): https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L394-L395 https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L406-L407 Public method build() has a synchronized (searcherMgrLock) block in which it calls method add(): https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L278-L310 Method add() is the same one I linked above. Note that method ensureOpen() (inlined above) is the only place (method or synchronization block) that is synchronized on this. All the other synchronizations in this file are on searcherMgrLock. This CR removes the synchronized on this (again, being the only synchronized on this, we can do this change safely). And moves synchronized (searcherMgrLock) a few lines above, to protect the entire code (that otherwise was protected by synchronized on this). The above breaks the lock cycle I described earlier. The fix looks big because it changes indentation. But only one line is moved by a few lines up. I.e., from this: To this: Here are all the places where synchronized (searcherMgrLock) appears in this file (and again, no other synchronized on other objects is done): I.e., doing the synchronization like above is safe and consistent with the rest of the file.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12987797/LUCENE-9084.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9115", "change_description": ": NRTCachingDirectory no longer caches files of unknown size.", "change_title": "NRTCachingDirectory may put large files in the cache", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "NRTCachingDirectory assumes that the length of a file to write is 0 if there is no merge info or flush info. This is not correct as there are situations when Lucene might write very large files that have neither of them, for instance:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9144", "change_description": ": Fix error message on OneDimensionBKDWriter when too many points are added to the writer.", "change_title": "Error message on OneDimensionBKDWriter is wrong when adding too many points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "The error message for the 1D BKD writer when adding too many points is wrong because: 1) It uses pointCount (which is always 0 at that point) instead of valueCount 2) It concatenate the numbers as a string instead of adding them.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9135", "change_description": ": Make UniformSplit FieldMetadata counters long.", "change_title": "UniformSplit FieldMetadata counters should all be long", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Currently UniformSplit FieldMetadata stores sumDocFreq, numTerms, sumTotalTermFreq as int which is incorrect. The fix is to make them long. The postings format will be compatible since those counters are currently written as VInt and they will be read as VLong (and then written as VLong afterwards).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9200", "change_description": ": Fix TieredMergePolicy to use double (not float) math to make its merging decisions, fixing\na corner-case bug uncovered by fun randomized tests", "change_title": "TieredMergePolicy's test fails with OB1 error after \"toning down\" (randomizing)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.5", "detail_description": "I tried to reduce the overhead of MergePolicy simulation tests. Especially TieredMergePolicy's testSimulateUpdates is one of the slowest lucene tests. As a workaround it is NIGHTLY but we should fix that. It should \"behave\" on a developer machine. As a part of of trying to improve this the fixed number of documents exercised by the test was changed from 10 million to use \"atLeast\" so that it would scale bigger in jenkins but be fast on your local machine. As well in the base class, the randomization is \"tweaked\" so that it generally runs efficiently, but still exercises corner cases. Unfortunately TieredMP hates these changes and will randomly (under beasting) fail with an OB1 error: Steps to reproduce: 1. Apply the patch to master (commit hash 7382375d8ab7c1a9eb111433db784218b739cb7a) 2. ./gradlew -p lucene/core test --tests TestTieredMergePolicy.testSimulateUpdates -Dtests.seed=E79E5C317D63A1E9:73780B8AD33B297D", "patch_link": "https://issues.apache.org/jira/secure/attachment/12992406/LUCENE-9200.patch", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9099", "change_description": ": Unordered and Ordered interval queries now correctly handle\nrepeated subterms - ordered intervals could supply an 'extra' minimized\ninterval, resulting in odd matches when combined with eg CONTAINS queries;\nand unordered intervals would match duplicate subterms on the same position,\nso an query for UNORDERED(foo, foo) would match a document containing 'foo'\nonly once.", "change_title": "Correctly handle repeats in ordered and unordered intervals", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "If you have repeating intervals in an ordered or unordered interval source, you currently get somewhat confusing behaviour: It is possible to deal with the unordered case when building sources by rewriting duplicates to nested ORDERED clauses, so that UNORDERED(a, b, c, a, b) becomes UNORDERED(ORDERED(a, a), ORDERED(b, b), c), but this then breaks MAXGAPS filtering. We should try and fix this within intervals themselves.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9250", "change_description": ": Add support for Circle2d#intersectsLine around the dateline.", "change_title": "LatLonShape Circle queries do not detect crossings over dateline", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "This was actually an oversight in LUCENE-8707. The original PR did support crossing over the dateline added in b59b24d but it was lost during a refactor of the classes. It should be added again.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9243", "change_description": ": Add fudge factor when creating a bounding box of a XYCircle.", "change_title": "TestXYPointDistanceSort failure: point was within the distance , but the bbox doesn't contain it", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Reproduce: I had a look and this error is similar to LUCENE-7143. The solution should be similar, add a fudge factor to the bounding box of a circle so we make sure we include all points that are at the specified distance.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9239", "change_description": ": Circle2D#WithinTriangle detects properly if a triangle is Within distance.", "change_title": "TestLatLonMultiPolygonShapeQueries error with CIRCLE queries", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "The failure can be reproduced with: The error message:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9251", "change_description": ": Fix bug in the polygon tessellator where edges with different value on #isEdgeFromPolygon\nwere bot filtered out properly.", "change_title": "Polygon tessellator fails to detect some collinear points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "A user of Elasticsearch has reported a tessellation error in a valid polygon. The reported polygon is quite complex but after digging a bit, the problem is the tesselator fails to detect some colinearities. In particular, in complex tessellation we can end up with two equal edges with different flag in isEdgeFromPolygon. Still we should be able to remove that co-linearity.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9263", "change_description": ": Fix wrong transformation of distance in meters to radians in Geo3DPoint.", "change_title": "Geo3D distance query computes wrongly the radius", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "This I side effect of LUCENE-9150, the transformation of radius in meters to radians is totally wrong as it does not take into account the mean radius of the earth.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Other", "change_id": "LUCENE-9109", "change_description": ": Backport some changes from master (except StackWalker) to improve\nTestSecurityManager", "change_title": "Use Java 9+ StackWalker to implement TestSecurityManager's detection of JVM exit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This is just a small improvement in Lucene/Solr master (Java 11) to detect exit of JVM in our test framework. There are other places in Lucene that use ineffective ways to inspect the stack trace. This one optimizes the implementation of TestSecurityManager#checkExit(status) to disallow all JVM exits outside of the official test runner by using StackWalker. In addition this needs no additional permissions, because we do not instruct StackWalker to fetch all crazy stuff like Class instances of stack elements. The way how this works is: Walk through stack trace: This can only be commited to master (9.0), as it requires Java 9.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Other", "change_id": "LUCENE-9110", "change_description": ": Backport refactored stack analysis in tests to use generalized\nLuceneTestCase methods", "change_title": "Use StackWalker in tests instead of iterating through StackTraceElement arrays", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.5", "detail_description": "Followup of LUCENE-9109: There are a lot of tests (especially around IndexWriter) that look into stack traces to inject failures or check that some methods were called in their call stack. This issue will refactor all those tests by adding a few methods to LuceneTestCase that make it easy to verify if some method call/class is in stack trace. On master (Java 11) we can use StackWalker to do this checks, which has a speedup of sometimes >>2 times (depending on how deep you dive into call stack). There are a few tests (only 3) that do more complex stack trace analysis. Those should be refactored at some point. For now I added a deprecated method to get the whole StackTrace in Java 11, which is still 2 times faster than using an Exception. For branch 8.x i will apply the same patch, just the LuceneTestCase methods use the old \"Java 8\" way to inspect stack trace using the thread's stack trace (which is very expensive). So this issue is mainly about refactoring the tests to use a common method pattern to check the existence of stack frames. One important thing is: Using StackWalker makes sure that the stack is \"correct\". Stacks from Thread or Exception may miss some frames, as it does not deoptimize the code. So depending on JVMs and optimizations (e.g. Graal), call stacks may change if we still use old code for analysis. This is no longer an issue for Java 8, but may be in future.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Other", "change_id": "LUCENE-9141", "change_description": ": Simplify LatLonShapeXQuery API by adding a new abstract class called LatLonGeometry. Queries are\nexecuted with input objects that extend such interface.", "change_title": "Simplify LatLonShapeXQuery API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "We currently have three query objects that are the same with the only difference of the Component2D object they contain:  LatLonShapePolygonQuery LatLonShapeLineQuery LatLonShapePointQuery  This is a bit of a burden whenever you want to make changes on those classes as you need to do it for every one. I wonder if we can merge all these three queries into one LatLonShapeComponent2DQuery. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Other", "change_id": "LUCENE-9194", "change_description": ": Simplify XYShapeXQuery API by adding a new abstract class called XYGeometry. Queries are\nexecuted with input objects that extend such interface.", "change_title": "Simplify XYShapeXQuery API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Similar to what was done in LUCENE-9141 simplify XYShape queries.  This change will allow as well to make most of the internal geo classes package private. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Other", "change_id": "LUCENE-9096", "change_description": ": Simplification of CompressingTermVectorsWriter#flushOffsets.", "change_title": "Implementation of CompressingTermVectorsWriter.flushOffsets can be simpler", "detail_type": "Improvement", "detail_affect_versions": "8.2", "detail_fix_versions": "8.5", "detail_description": "In CompressingTermVectorsWriter.flushOffsets,  we count sumPos and sumOffsets by the way we always use the position - previousPos,  it can be summarized like this: If we should simplify it: position5-position1 ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.5.0", "change_type": "Other", "change_id": "LUCENE-9225", "change_description": ": Rectangle extends LatLonGeometry so it can be used in a geometry collection.", "change_title": "Rectangle should extend LatLonGeometry", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "Rectangle class is the only geometry class that do not extend LatLonGeometry. This is because we have an specialise query for rectangles that works on the encoded space (very similar to what LatLonPoint is doing). It would be nice if Rectangle could implement LatLonGeometry, so in cases where a bounding box is part of a complex geometry, it can fall back to Component2D objects. The idea is to move the specialise logic in Rectangle2D inside the specialised LatLonBoundingBoxQuery and rename the current XYRectangle2D to Rectangle2D.", "patch_link": "none", "patch_content": "none"}

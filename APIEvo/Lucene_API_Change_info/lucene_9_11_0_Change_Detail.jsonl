{"library_version": "9.11.0", "change_type": "API Changes", "change_id": "GITHUB#13145", "change_description": ": Deprecate ByteBufferIndexInput as it will be removed in Lucene 10.0.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is preparation for Lucene 10:ByteBufferIndexInput was made public for unknown reason (it should have been private). This officially deprecates the class as it will be removed in Lucene 10 (see other PR: TODO)There was a bug in NRTSuggester which wasn't able to detect MemorySegmentIndexInput for loading FST off-heap. This hack was already pssoibly the reason why the input was made public.This PR fixes both issues for Lucene 9.11.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "API Changes", "change_id": "GITHUB#13422", "change_description": ": an explicit dependency on the HPPC library is removed in favor of an internal repackaged copy in oal.internal.hppc. If you relied on HPPC as a transitive dependency, you'll have to add it to your project explicitly. The HPPC classes now bundled in Lucene core are internal and will have restricted access in future releases, please do not use them.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "It remains to move the hppc fork to oal.internal.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13125", "change_description": ": Recursive graph bisection is now supported on indexes that have blocks, as long as they configure a parent field via `IndexWriterConfig#setParentField`.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is similar to the work we did on supporting index sorting on indexes that have blocks, but for index reordering this time.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#12915", "change_description": ": Add new token filters for Japanese sutegana (æ¨ã¦ä»®å). This introduces JapaneseHiraganaUppercaseFilter and JapaneseKatakanaUppercaseFilter.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Sutegana (捨て仮名) is small letter of hiragana and katakana in Japanese. In the old Japanese text, sutegana (捨て仮名) is not used unlikely to modern one. For example: So it's meaningful to normalize sutegana to normal (uppercase) characters if we search against the corpus which includes old Japanese text such as patents, legal documents, contract policies, etc. This pull request introduces 2 token filters: so that user can use either one separately. Each. filter make all the sutegana (small characters) into normal kana (uppercase character) to normalize the token. This transformation must be done as token filter. There have already been MappingCharFilter , but if we apply this character filter to normalize sutegana, it will impact to tokenization and it is not expected.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13196", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a first idea how we can use Panama Foreign to pass madvise() hints to the kernel when mapping memory segments. The code looks up the function pointer from stdlib (libc) on Linux and Macos (untested, but should work) and then invokes madvise() for all MemorySegments we have mmapped when the following is true: Interestingly it works without any extra parameters to command line (at least in Java 21). This is a draft only to do some performance tests and extend the IOContext interpretation to try out more possibilities. The current \"readOnce => MADV_SEQUENTIAL\" is just an example as this is the main issue: We merge segments and don't want the soon to be trashed segments be sticky in RAM. MADV_SEQUENTIAL instructs kernel to forget about the mappings and also do readahead which helps during merging.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13222", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This switches the following files to ReadAdvice.RANDOM : I hesitated using IOContext.RANDOM on terms, since they have a random access pattern when running term queries, but a more sequential access pattern when running multi-term queries. I erred on the conservative side and did not switch them to IOContext.RANDOM for now. For simplicity, I'm only touching the current codec, not previous codecs. There are also some known issues:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13197", "change_description": ": Expand support for new scalar bit levels for HNSW vectors. This includes 4-bit vectors and an option to compress them to gain a 50% reduction in memory usage.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR is a culmination of some various streams of work: The idea of further scalar quantization gives users the choice between: I didn't add more panama vector APIs as I think trying to micro-optimize int4 for anything other than dot-product was a fools errand. Additionally, I only focused on ARM. I experimented with trying to get better performance on other architectures, but didn't get very far, so I fall back to dotProduct.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13268", "change_description": ": Add ability for UnifiedHighlighter to highlight a field based on combined matches from multiple fields.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Add ability to UnifiedHighlighter to combine matches from multiple fields to highlight a single field. FastVectorHighlighter for a long time has an option to highlight a single field based on matches from several fields. But UnifiedHighlighter was missing this option. This adds this ability.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13288", "change_description": ": Make HNSW and Flat storage vector formats easier to extend with new FlatVectorScorer interface. Add new Hnsw format for binary quantized vectors.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Instead of making a separate thing pluggable inside of the FieldFormat, this instead keeps the vector similarities as they are, but allows a custom scorer to be provided to the FlatVector storage used by HNSW. This idea is akin to the compression extensions we have. But in this case, its for vector scorers. To show how this would work in practice, I took the liberty of adding a new HnswBitVectorsFormat in the sandbox module. A larger part of the change is a refactor of the RandomAccessVectorValues<T> to remove the <T> . Nothing actually uses that any longer, and we should instead rely on well defined classes and stop relying on casting with generics (yuck).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13181", "change_description": ": Add new VectorScorer interface to vector value iterators. This allows for vector codecs to supply simpler and more optimized vector scoring when iterating vector values directly.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "With quantized vectors, and with current vectors, we separate out the \"scoring\" vs. \"iteration\", requiring the user to always iterate the raw vectors and provide their own similarity function. While this is flexible, it creates frustration in: This significantly hampers support for true exact kNN search. I see two options on providing this: Folks who might be interested: @jpountz @msokolov @mccullocht", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13414", "change_description": ": Counts are always available in the result when using taxonomy facets.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Taxonomy facets always have counts since #12966 . We add a count field to LabelAndValue so that users can retrieve those counts. I propose we go ahead with this PR for now and then follow-up with one of the options above (or another if anyone has ideas) for 10x. Addresses #11282", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "New Features", "change_id": "GITHUB#13445", "change_description": ": Add new option when calculating scalar quantiles. The new option of setting `confidenceInterval` to `0` will now dynamically determine the quantiles through a grid search over multiple quantiles calculated by multiple intervals.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When int4 scalar quantization was merged, it added a new way to dynamically calculate quantiles. However, when that was merged, I inadvertently changed the default behavior, where a null confidenceInterval would actually calculate the dynamic quantiles instead of doing the previous auto-setting to 1 - 1/(dim + 1) . This commit formalizes the dynamic quantile calculate through setting the confidenceInterval to 0 , and preserves the previous behavior for null confidenceIntervals so that users upgrading will not see different quantiles than they would expect.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13092", "change_description": ": `static final Map` constants have been made immutable", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13041", "change_description": ": TokenizedPhraseQueryNode code cleanup", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13087", "change_description": ": Changed `static final Set` constants to be immutable. Among others it affected ScandinavianNormalizer.ALL_FOLDINGS set with public access.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "For the mutable \"global\" Sets changed Collections.synchronizedSet() to ConcurrentHashMap.newKeySet() for better performance.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13155", "change_description": ": Hunspell: allow ignoring exceptions on duplicate ICONV/OCONV mappings", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13156", "change_description": ": Hunspell: don't proceed with other suggestions if we found good REP ones", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This makes \"rep\" and \"ph\" from TestHunspellRepositoryTestCases pass on latest revisions of the original Hunspell (after hunspell/hunspell@ b88f9ea )", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13066", "change_description": ": Support getMaxScore of DisjunctionSumScorer for non top level scoring clause", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Same as #13043 . WANDScorer BlockMaxConjunctionScorer doesn't work for disjunctions within disjunctions conjunctions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13124", "change_description": ": MergeScheduler can now provide an executor for intra-merge parallelism. The first implementation is the ConcurrentMergeScheduler and the Lucene99HnswVectorsFormat will use it if no other executor is provided.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Opening this PR for discussion. I took a stab at #12740 The idea is this: If this direction seems good, I can finish cleaning up things and write some tests. @zhaih @jpountz y'all might be interested here.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13239", "change_description": ": Upgrade icu4j to version 74.2.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Current ICU version is ancient (70.1): Upgrade to the latest 74.2 release supporting Unicode 15.1 Practically, because testEmojiFromTheFuture() passes, we don't behave that out-of-date to end-users. But the algorithms and code here get maintained, improved, etc, bugs get fixed, we should upgrade. It would also be confusing for 10.0 to support older unicode version than the JDK itself.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13202", "change_description": ": Early terminate graph and exact searches of AbstractKnnVectorQuery to follow timeout set from IndexSearcher#setTimeout(QueryTimeout).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "#927 added timeout support to IndexSearcher#search where we wrap a scorer in a TimeLimitingBulkScorer which periodically checks whether a query timed out while collecting docs in fixed chunks However as #11677 points out, this does not apply to query rewrites -- an important use case being vector searches ( AbstractKnnVectorQuery ), where the bulk of computations (HNSW graph searches) happen in #rewrite If we had set a timeout, and HNSW searches turned out too expensive -- we would not return results even after the search completes (because the timeout is checked before attempting to score docs -- at which point we have all possible results, but they aren't returned) In this PR, we're wrapping the KnnCollector in another one which additionally checks for QueryTimeout#shouldExit in the KnnCollector#earlyTerminated function. If this timeout is hit, we immediately return empty results Also extended this to exact searches, where we check for a timeout before scoring each document", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#12966", "change_description": ": Move most of the responsibility from TaxonomyFacets implementations to TaxonomyFacets itself. This reduces code duplication and enables future development.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a large change, refactoring most of the taxonomy facets code and changing internal behavior, without changing the API. There are specific API changes this sets us up to do later, e.g. retrieving counts from aggregation facets.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13362", "change_description": ": Add sub query explanations to DisjunctionMaxQuery, if the overall query didn't match.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "#13357 states that it's useful to have the explanations of the sub queries of DisjuncationMaxQuery present, even if the document didn't match. Considering that other queries like CoveringQuery also include explanations, if the document didn't match I've adjusted the logic according to the issue's proposal. Also added tests for explain (match and no match case). I've also adjusted CheckHits#verifyExplanation to only check sub explanations, if the overall explanation returned a hit and scores need to be checked.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13385", "change_description": ": Add Intervals.noIntervals() method to produce an empty IntervalsSource. (Aniketh Jain, Uwe Schindler, Alan Woodward))", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "A bug was reported in OpenSearch which caused Interval Queries containing sub-query in rules to fail with Sub-iterators of ConjunctionDISI are not on the same document if an no_match query was generated after going through the search analyser. A similar issue was found in Lucene a few years back and was fixed by this PR I am proposing to make NO_INTERVALS present in Lucene as public so it can be used by it's clients like OpenSearch rather than creating a clone of the same.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13276", "change_description": ": UnifiedHighlighter: new 'passageSortComparator' option to allow sorting other than offset order.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "FieldHighlighter always sorts the final selected passages based on startOffset, but this may not align with the user's intentions. For example, in the case of Solr's multiValue fields, the order might just be a simple listing of items without any significance. In such cases, the order in which passages appear is not important. It might be more effective to maintain sorting by score in these situations. Therefore, it seems necessary to allow for the final sorting criteria to be determined in accordance with the user's intentions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Improvements", "change_id": "GITHUB#13429", "change_description": ": Hunspell: speed up \"compress\"; minimize the number of the generated entries; don't even consider \"forbidden\" entries anymore", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13306", "change_description": ": Use RWLock to access LRUQueryCache to reduce contention.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Elasticsearch (which based on lucene) can automatically infer types for users with its dynamic mapping feature. When users index some low cardinality fields, such as gender / age / status... they often use some numbers to represent the values, while ES will infer these fields as long, and ES uses BKD as the index of long fields. Just as #541 said, when the data volume grows, building the result set of low-cardinality fields will make the CPU usage and load very high even if we use a boolean query with filter clauses for low-cardinality fields. One reason is that it uses a ReentrantLock to limit accessing LRUQueryCache. QPS and costs of their queries are often high,  which often causes trying locking failures when obtaining the cache, resulting in low concurrency in accessing the cache. So I replace the ReentrantLock with a ReentrantReadWriteLock. I only use the read lock when I need to get the cache for a query, I benchmarked this optimization by mocking some random LongPoint and querying them with one PointInSetQuery with bool filter. I think this change can help filter queries that need to query low-cardinality fields.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13252", "change_description": ": Replace handwritten loops compare with Arrays.compareUnsigned in SegmentTermsEnum.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Similar to SegmentTermsEnumFrame , we can use Arrays.compareUnsigned instead of iterating compare, in SegmentTermsEnum.seekCeil and SegmentTermsEnum.seekExact .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#12996", "change_description": ": Reduce ArrayUtil#grow in decompress.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13115", "change_description": ": Short circuit queued flush check when flush on update is disabled", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Small change which reduces locking of write threads when flush on update is disabled by short circuiting a check Resolves #13079", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13085", "change_description": ": Remove unnecessary toString() / substring() calls to save some String allocations", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Removed unnecessary toString() / substring() calls to avoid extra Strings being allocated", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13121", "change_description": ": Speedup multi-segment HNSW graph search for diversifying child kNN queries. Builds on", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This adds multi-leaf optimizations for diversified children collector. This means as children vectors are collected within a block join, we can share information between leaves to speed up vector search. To make this happen, I refactored the multi-leaf collector slightly. Now, instead of inheriting from TopKnnCollector , we inject a inner collector.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#12962", "change_description": ": Speedup multi-segment HNSW graph search for diversifying child kNN queries. Builds on", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "A second implementation of #12794 using Queue instead of MaxScoreAccumulator. Speedup concurrent multi-segment HNWS graph search by exchanging the global top scores  collected so far across segments. These global top scores set the minimum threshold that candidates need to pass to be considered. This allows earlier stopping for segments that don't have good candidates.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13184", "change_description": ": Make the HitQueue size more appropriate for KNN exact search", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently, when performing KNN exact search, we consistently set the HitQueue size to k . However, there may be instances where the number of candidates is actually lower than k .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13199", "change_description": ": Speed up dynamic pruning by breaking point estimation when threshold get exceeded.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Typically, we estimate the point value count to compare to a threshold and all we need is just a boolean which represents whether the point count is greater than this threshold. This PR proposes to parse the threshold into the intersect logic and break the recursion when the threshold is exceeded. Dynamic pruning is a case that heavily using estimate point count so i run luceneutil for it. Here is the benchmark result on wikimedium10m : M2 Chip Intel Chip PS: When profiling i noticed that PointTree construction cost a lot so i tried to make it reusable, this optimization also contributed to this speed-up.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13203", "change_description": ": Speed up writeGroupVInts", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change uses VarHandles instead of BytesRefBuilder#append to speed up writeGroupVInts , which is more aligned with group-varint generally implementation. Main: PR:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13224", "change_description": ": Use singleton for all-zeros DirectMonotonicReader.Meta", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Having a single block of all zeros is a fairly common case that is using a lot of heap for duplicate instances in some use-cases in ES. => read a singleton for it to save the duplication", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13232", "change_description": ": Introduce singleton for PackedInts.NullReader of size 256", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Size 256 is very common here through the monotonic long values default page size. In ES we're seeing many MB O(10M) of duplicate instances of this size relatively quickly. => adding a singleton for it to save some heap", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#11888", "change_description": ": Binary search the BlockTree terms dictionary entries when all suffixes have the same length in a leaf block, speeding up cases like primary key lookup on an id field when all ids are the same length.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fix #11722 's bug.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13149", "change_description": ": Made PointRangeQuery faster, for some segment sizes, by reducing the amount of virtual calls to IntersectVisitor::visit(int).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Instead of calling IntersectVisitor.visit for each doc in the readDelta16 and readInts32 methods, create a DocIdSetIterator and call IntersectVisitor.visit(DocIdSetIterator) instead. This seems to make Lucene faster at some sorting and range querying tasks - I saw 35-45% reduction in execution time . In learnt this through running this benchmark setup by Elastic: https://github.com/elastic/elasticsearch-opensearch-benchmark . The hypothesis is that this is due to fewer virtual calls being made - once per BKD leaf, instead of once per document. Note that this is only measurable if the readInts methods have been called with at least 3 implementation of the IntersectVisitor interface - otherwise the JIT inlining takes away the virtual call. In real life Lucene deployments, I would judge that it is very likely that at least 3 implementations are used. For more details on method etc, there are details in this blog post: https://blunders.io/posts/es-benchmark-4-inlining I tried benchmarking this with luceneutil, but did not see any significant change with the default benchmark - I suspect that I'm using the wrong luceneutil tasks to see any major difference. Which luceneutils benchmarks should I be using for these changes?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#12966", "change_description": ": FloatTaxonomyFacets can now collect values into a sparse structure, like IntTaxonomyFacets already could.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a large change, refactoring most of the taxonomy facets code and changing internal behavior, without changing the API. There are specific API changes this sets us up to do later, e.g. retrieving counts from aggregation facets.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13284", "change_description": ": Per-field doc values and knn vectors readers now use a HashMap internally instead of a TreeMap.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Our per-field vector and doc-values readers use TreeMap s but don't rely on the iteration order, so these TreeMap s can be replaced with more CPU/RAM-efficient HashMap s. The per-field postings reader stays on a TreeMap since it relies on the iteration order.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13321", "change_description": ": Improve compressed int4 quantized vector search by utilizing SIMD inline with the decompression process.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This updates the int4 dot-product comparison to have an optimized one for when one of the vectors are compressed (the most common search case). This change actually makes the compressed search on ARM faster than the uncompressed. However, on AVX512/256, it still slightly slower than uncompressed, but it still much faster now with this optimization than before (eagerly decompressing). This optimized is tied tightly with how the vectors are actually compressed and stored, consequently, I added a new scorer that is within the lucene99 codec. So, this gives us 8x reduction over float32, well more than 2x faster queries than float32, and no need to rerank as the recall and accuracy are excellent. Here are some lucene-util numbers over CohereV3 at 1024 dimensions: New compressed numbers on ARM Compared with uncompressed on ARM: Here are some JMH numbers as well (note, I am excluding odd number of indices as these don't support compression). NOTE: PackedUnpacked is eagerly decompressing the vectors and then using dot-product, what is occurring now. ARM: AVX512:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#12408", "change_description": ": Lazy initialization improvements for Facets implementations when there are segments with no hits to count.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change proposes some faceting optimizations for situations where there are no hits to facet over. While this seems like an odd scenario, at Amazon product search we actually have some situations where this can become common (sparse queries that don't have results in some segments, or have no results altogether). You could argue that the calling code should handle this optimization and avoid faceting altogether if there are no hits, but that only solves for the case of no results, not no results in some segments. I think it's also nice to move this optimization into the faceting module so that every user doesn't have to do this themselves. Curious what people think of this idea. Happy to hear feedback, counterarguments, etc. :) This change covers: Left for a future iteration (I'll open a follow up issue if we move forward with this one):", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13327", "change_description": ": Reduce memory usage of field maps in FieldInfos and BlockTree TermsReader.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Two goals: 1- Reduce the memory usage of field maps when there are many fields. FieldInfos construtor is refactored to build the byNumber array in a more efficient way, avoiding array growing and copies. Use a primitive IntObjectHashMap to reduce the memory usage compared to an HashMap in Lucene90BlockTreeTermsReader. 2- Add new IntObjectHashMap in the existing small hppc fork. Leverage this PR to show an example use-case. It hopefully can be reused later for other use-cases.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13339", "change_description": ": Add a MemorySegment Vector scorer - for scoring without copying on-heap", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Add a MemorySegment Vector scorer - for scoring without copying on-heap. The vector scorer loads values directly from the backing memory segment when available. Otherwise, if the vector data spans across segments, or is a query vector, the scorer copies the vector data on-heap. A benchmark shows ~2x performance improvement of this scorer over the default copy-on-heap scorer. The benchmark need a little more scrutiny and evaluation on different platforms. Here's the results on a Max M2: The scorer currently only operates on vectors with an element size of byte , since loading vector data from float[] (the fallback), is only supported in JDK 22. We can evaluate if and how to support floats separately. See https://bugs.openjdk.org/browse/JDK-8318678 The vector scorer is implicitly tied to the Panama Vector Utils implementation - you can only have a Memory segment scorer if the Panama vector implementation is present. There is a little room for improvement in how these things are initialised and structured.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13368", "change_description": ": Replace Map<Integer, Object> by primitive IntObjectHashMap.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Also replace some Map<Integer, Integer> by IntIntHashMap, if they don't rely on null value. The goal is to gain globally some memory, maybe some perf on some spots that call the map intensively, with a replacement that does not seem to bring complexity. Most of the time it consists in changing a field type to the primitive map, and the call to the map constructor. If some areas shouldn't be modified, we can exclude them from the replacement.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13392", "change_description": ": Replace Map<Long, Object> by primitive LongObjectHashMap.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No functional changes, only replacements by primitve maps. Adds LongObjectHashMap and LongIntHashMap to the org.apache.lucene.util.hppc package, with some refactoring. Adds a dependency to com.carrotsearch.hppc to the join and spatial modules. This dependency is already present in the facet module. This way these packages can use primitive hash map with float or double values. The changes in the JoinUtil class are a good example of usage of primitive structures. This highly reduces the auto-boxing around primitives.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13400", "change_description": ": Replace Set<Integer> by IntHashSet and Set<Long> by LongHashSet.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Add IntHashSet and LongHashSet to the HPPC fork. Use them to replace usages of Set of Integer or Long. Refactor a bit the forked HPPC classes, add tests. On the way I discovered a small bug in HPPC HashMap, that I'll fix also in HPPC. Rename lucene.document.LongHashSet to DocValuesLongHashSet. This specific hash set open-addressing implementation is specific to DocValues, with optimizations for sorted elements provided in the constructor. I didn't replace it by HPPC LongHashSet, but I renamed it. This PR modifies slightly QueryParser.jj. I regenerated QueryParser.java using the gradle task.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13406", "change_description": ": Replace List<Integer> by IntArrayList and List<Long> by LongArrayList.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Add IntArrayList and LongArrayList to the HPPC fork. Use them to replace usages of List of Integer or Long. No functional changes, only optimization for less auto-boxing and more compact structures. Also, no public API changes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Optimizations", "change_id": "GITHUB#13420", "change_description": ": Replace Map<Character> by CharObjectHashMap and Set<Character> by CharHashSet.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is my last round of adding primitive structures to the HPPC fork, and using them to gain memory on various locations. No functional change, only optimization. In JapaneseKatakanaUppercaseFilter, in addition to replacing the generic structure, this PR also optimizes the character replacement in the CharTermAttribute.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13105", "change_description": ": Fix ByteKnnVectorFieldSource & FloatKnnVectorFieldSource to work correctly when a segment does not contain any docs with vectors", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Like most field types in lucene, Knn(Byte|Float)VectorField classes do not require a field value for every document -- they support use cases where some documents are indexed w/o a field value. The (Byte|Float)KnnVectorFieldSource implementations generally follow the pattern of most field based ValueSource impls in terms of how they behave when some documents do not have a value in the specified field ( TestKnnVectorSimilarityFunctions even checks for this in various tests using the knnFloatField3 and knnByteField3 fields which only exist in one document). But these value sources break if/when a user gets unlucky and a segment exists where none of the documents have value in the vector field (and by extension: when querying an empty index) This is due to the following check... This problem is trivial to reproduce reliably by adding an iw.commit(); call to TestKnnVectorSimilarityFunctions between indexing the two documents. (The RandomIndexWriter should eventually trigger it in this test even w/o modifications .. not sure if it ever has in jenkins?) The appropriate way to implement this type of check is to follow the pattern in the DocValues.getFoo() methods, that use a checkField(...) method to inspect the FieldInfos from the LeafReader for consistency w/expectations if reader.getFooDocValues() returns null. checkField(...) assumes everything is fine if no FieldInfo exist for this field in this reader -- using an \"empty\" impl in it's place. I'm attaching a patch that fixes these ValueSource classes, by returning a VectorFieldFunction that wraps DocIdSetIterator.empty() when vectorValues is null, as long as a new VectorEncoding method checkField(reader, fieldName) does not throw an exception (Adding checkField directly to VectorEncoding seemed like the most straightforward place to put this ... not sure if there is a more appropriate class?) I also made the new checkField method throw IllegalStateException instead of IllegalArgumentException to be consistent with other \"is the index field type consistent with the type of query you are trying to do?\" checks in the code base (like DocValues.checkField(...) and updated TestKnnVectorSimilarityFunctions accordingly. Patch: VectorFieldSource.fix-missing-check.patch.txt No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13017", "change_description": ": Fix DV update files referenced by merge will be deleted by concurrent flush.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR aims to address issue #13015 . A more detailed explanation of the issue and the reasoning behind the fix can be found in the report link above.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13145", "change_description": ": Detect MemorySegmentIndexInput correctly in NRTSuggester.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is preparation for Lucene 10: This PR fixes both issues for Lucene 9.11.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13154", "change_description": ": Hunspell GeneratingSuggester: ensure there are never more than 100 roots to process", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13162", "change_description": ": Fix NPE when LeafReader return null VectorValues", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "LeafReader#getXXXVectorValues may return null value. Reproduction : Output :", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13169", "change_description": ": Fix potential race condition in DocumentsWriter & DocumentsWriterDeleteQueue", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "13127 is failing due to seqNo being larger than maxSeqNo on close() . maxSeqNo is set during DocumentsWriterDeleteQueue#advanceQueue , synchronized on self. It utilizes getLastSequenceNumber() , which reads the AtomicLong for seqNo . However, DocumentsWriterDeleteQueue#getNextSequenceNumber() is not synchronized on self. Meaning after (or before) reading the AtomicLong , it could be incremented by this method. DocumentsWriterDeleteQueue#getNextSequenceNumber() is called in other synchronized methods, the one external one being DocumentsWriter#getNextSequenceNumber , which is synchronized on self (e.g. DocumentsWriter). When calling DocumentsWriterFlushControl#markFullFlush , neither the DocumentsWriterDeleteQueue nor DocumentsWriterDeleteQueue are locked, and it is possible for documentsWriter.getNextSequenceNumber() to be called after documentsWriter.deleteQueue.advanceQueue but before documentsWriter.resetDeleteQueue . This commit moves the documentsWriter.deleteQueue.advanceQueue call into the already synchronized documentsWriter.resetDeleteQueue to ensure that getNextSequenceNumber cannot be called while the queue is being reset. This is admittedly difficult to fully test. But I have seen many failures in continuous testing. So, if this commit does fix that test, I would expect those to disappear. closes : #13127", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13204", "change_description": ": Fix equals/hashCode of IOContext.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "It also modernizes it. Actualy this class should be changed to be \"record\", then equals/hashCode is autogenerated and more performant by using invokedynamic. This was found by @rmuir in #13196", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13206", "change_description": ": Subtract deleted file size from the cache size of NRTCachingDirectory.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The size of deleted files is not subtracted from the cache size of NRTCachingDirectory. As a consequence, the cache eventually appears to be full preventing new files from being cached despite there being available memory since files no longer consume memory once they are deleted. There is a weakness in the fix if the file is concurrently deleted in another thread while being closed but I don't think this pathologic use case deserves the additional synchronization it would require.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12966", "change_description": ": Aggregation facets no longer assume that aggregation values are positive.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a large change, refactoring most of the taxonomy facets code and changing internal behavior, without changing the API. There are specific API changes this sets us up to do later, e.g. retrieving counts from aggregation facets.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13356", "change_description": ": Ensure negative scores are not returned from scalar quantization scorer.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Depending on how we quantize and then scale, we can edge down below 0 for dotproduct scores. This is exceptionally rare, I have only seen it in extreme circumstances in tests (with random data and low dimensionality).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13366", "change_description": ": Disallow NaN and Inf values in scalar quantization and better handle extreme cases.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In testing, it was noticed that tiny vectors (<= 2 dims) could cause weird behavior in the quantizer. Additionally, we should fail any quantization that ends up with NaN or Inf values.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13369", "change_description": ": Fix NRT opening failure when soft deletes are enabled and the document fails to index before a point field is written", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The issue outlines the problem. When we have point value dimensions, segment core readers assume that there will be point files. However, when allowing soft deletes and a document fails indexing failed before a point field could be written, this assumption fails. Consequently, the NRT fails to open. I tried many different ways of fixing this issue. So, I settled on always flushing a point file if the field info says there are point fields, even if there aren't any docs in the buffer. I am happy to consider other options. closes #13353", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13378", "change_description": ": Fix points writing with no values", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit updates the writer to handle the case where there are no values.Previously (before #13369), there was a check that there were some points values before trying to write, this is no longer the case. The code in writeFieldNDims has an assumption that the values is not empty - an empty values will result in calculating a negative number of splits, and a negate array size to hold the splits. The fix is trivial, return null when values is empty - null is an allowable return value from this method. Note: writeField1Dim is able to handle an empty values. After this change both the newly failing test, TestIndexWriterExceptions2.testBasics, and the test added for #13369, testExceptionJustBeforeFlushWithPointValues, pass successfully several thousands of times. No new test is added, as testBasics already covers this. closes #13377", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13374", "change_description": ": Fix bug in SQ when just a single vector present in a segment", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit fixes a corner case in the ScalarQuantizer when just a single vector is present. I ran into this when updating a test that previously passed successfully with Lucene 9.10 but fails in 9.x. The score error correction is calculated to be NaN, as there are no score docs or variance.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13376", "change_description": ": Fix integer overflow exception in postings encoding as group-varint.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Closes : #13373 This exception occurs because a negative  integer value stores as  positive long.  In line 376, after a long value << 1 , if the sign bit of the integer value is 1, it will be a  negative number as integer, but a positive numbers as long, when we stores this value as positive long, it would cause Math.toIntExact to throw ArithmeticException exception.  lucene/lucene/core/src/java/org/apache/lucene/codecs/lucene99/Lucene99PostingsWriter.java Lines 373 to 379       in f12e489  POC code: TODO:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13421", "change_description": ": Fixes TestOrdinalMap.testRamBytesUsed for multiple default PackedInts.NullReader instances.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "closes #13372 TestOrdinalMap#testRamBytesUsed was failing for the case when there are multiple Default instances for PackedInts.NullReader. Currently, we return 0 when the instance is built with default size i.e. 256. Ref: org.apache.lucene.util.packed.PackedInts.NullReader#ramBytesUsed But this was not accounted for in the Accumulator being used. Made changes to skip the object if the reader is of default static instance. Passes failing UT. ./gradlew :lucene:core:test --tests \"org.apache.lucene.index.TestOrdinalMap.testRamBytesUsed\" -Ptests.seed=55680AD930225FC5 -Ptests.nightly=true", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Other", "change_id": "GITHUB#13068", "change_description": ": Replace numerous `brToString(BytesRef)` copies with a `ToStringUtils` method", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I noticed there are brToString() methods in 16 different classes. What do you think of providing a shared implementation in the ToStringUtils class? If you find the proposal useful I will proceed with removing the other copies. Otherwise the ToStringUtils class could probably be deleted - the two methods there byteArray() and longHex() are not used anywhere.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Other", "change_id": "GITHUB#13077", "change_description": ": Add public getter for SynonymQuery#field", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Since all the query terms must have the same field, the field value is exposed anyway via but it's cleaner if one could instead use", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.11.0", "change_type": "Other", "change_id": "GITHUB#13393", "change_description": ": Add support for reloading the SPI for KnnVectorsFormat class", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Lucene uses SPI to get the instance for various classes like Codec, KNNVectorsFormat etc. Currently Codec class provide a way to reload the SPIs by providing an interface which takes a ClassLoader and reload the SPIs. Ref: https://github.com/apache/lucene/blob/branch_9_10/lucene/core/src/java/org/apache/lucene/codecs/Codec.java#L126-L137 but similar functionality is not present in the KNNVectorsFormat.(I checked main branch and branch_9_10 too). Ref: https://github.com/apache/lucene/blob/branch_9_10/lucene/core/src/java/org/apache/lucene/codecs/KnnVectorsFormat.java I am not sure if this is a miss or there is some other way to reload the SPI of KNNVectorsFormat class. What I am looking for here is to add the support for reload SPI function in KNNVectorsFormat class too so that external libraries/application can load their own KNNVectorsFormat. I am willing to pick up this issue and contribute back. @benwtrent please let me know your thoughts on this The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}

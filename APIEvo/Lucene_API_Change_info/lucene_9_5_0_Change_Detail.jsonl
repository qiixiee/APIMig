{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12093", "change_description": ": Deprecate support for UTF8TaxonomyWriterCache and changed default to LruTaxonomyWriterCache.\nPlease use LruTaxonomyWriterCache instead.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As discussed in PR #12013 , deprecating support for UTF8TaxonomyWriterCache in branch_9x. Addresses #12000", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11998", "change_description": ": Add new stored fields and termvectors interfaces: IndexReader.storedFields()\nand IndexReader.termVectors(). Deprecate IndexReader.document() and IndexReader.getTermVector().\nThe new APIs do not rely upon ThreadLocal storage for each index segment, which can greatly\nreduce RAM requirements when there are many threads and/or segments.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently the stored fields and term vectors apis on the index are \"stateless\". Unlike the other parts of the APIs, users can't call any iterators/enumerators, only: Instead of adding any real iterator, ThreadLocal s were added on each segment behind-the-scenes to prevent from having to clone() the stored fields or term vectors reader on every document. For example, this caching could reduce the amount of NIOFSDirectory buffer refills and other associated overhead. But the old API from a previous time, seems to only gets worse these days, because the implementations are more complicated and do block-compression, dictionaries, etc. In some cases, the cached resources can grow to extremely large amounts (see @luyuncheng writeup in #11987 as an example) These per-segment threadlocals can cause memory issues if you have tons of segments, tons of threads, or especially both. Seems plenty of java developers can't help but run into it. I propose we deprecate these APIs: And replace the functionality with these APIs: Instead of lucene internally caching a reader per-thread-per-segment, a user can get one themselves e.g. per-search: It will re-use the datastructures across a search if someone has thousands and thousands of hits, but avoid the ThreadLocal pain. The deprecated APIs still work the same way, and still use ThreadLocals. This way apps can safely migrate to the new APIs at their convenience. Once all the deprecations are fixed, then no ThreadLocals will be created any more, and the app can enjoy the RAM savings. All code and tests have been moved to the new API in this PR, after backporting to 9.x, we can commit this patch to remove the deprecated apis / threadlocal support completely: nukeDeprecated.patch.txt", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11742", "change_description": ": MatchingFacetSetsCounts#getTopChildren now properly returns \"top\" children instead\nof all children.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "MatchingFacetSetsCounts#getTopChildren is currently just delegating to #getAllChildren , which isn't really the correct thing to do. We should properly implement \"top children.\" No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11772", "change_description": ": Removed native subproject and WindowsDirectory implementation from lucene.misc. Recommendation:\nuse MMapDirectory implementation on Windows.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Having native code complicates the gradle build and causes bugs. The Direct-IO directory no longer needs native code, it uses JDK APIs. So the only thing left is WindowsDirectory. I'm not sure this thing is really faster than the JDK. If i remember, the synchronization that causes the issues was in windows. Honestly, users should use MmapDirectory on windows IMO. So maybe net/net it is better for our windows support to remove this thing? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11804", "change_description": ": FacetsCollector#collect is no longer final, allowing extension.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I'd like to propose removing the final restriction on FacetsCollector#collect to allow extension. I have a use-case where I'd like to be able to throw a CollectionTerminatedException from a FacetsCollector after collecting a specified number of hits (this is a runtime optimization where we're OK faceting over a subset of all actual matches). Being able to extend collect would make this much simpler to achieve.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11761", "change_description": ": TieredMergePolicy now allowed a maximum allowable deletes percentage of down to 5%, and the default\nmaximum allowable deletes percentage is changed from 33% to 20%.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I'm an engineer at Amazon Search and we have been experimenting with more aggressively getting rid of deleted documents. We use TieredMergePolicy and we would like to set TieredMergePolicy#deletesPctAllowed to be lower than the current limit of 20% . I was wondering why this limit was set in place. I'm sure I could be missing some context here. Maybe we could keep the limits in place but allow users to explicitly remove the checks? Any information would be much appreciated, thanks! The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11822", "change_description": ": Configure replicator PrimaryNode replia shutdown timeout.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Adds a configurable timeout for PrimaryNode waiting for remotes to close. The default matches existing behavior. In the long run, maybe this wait loop goes away entirely, but I elected to make the most compatible change first. Fixes #11674", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11930", "change_description": ": Added IOContext#LOAD for files that are a small fraction of the\ntotal index size and heavily accessed with a random access pattern. Some\nDirectory implementations may choose to load files that use this IOContext in\nmemory to provide stronger guarantees on query latency.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR introduces a new LOAD IOContext for files that are a small fraction of the total index size and are expected to be accessed in a random-access fashion. This may be leveraged by some Directory implementations to load such files into physical memory (e.g. Java heap) in order to provide stronger guarantees on query latency.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11941", "change_description": ": QueryBuilder#add and #newSynonymQuery methods now take a `field` parameter,\nto avoid possible exceptions when building queries from an empty term list.  The helper\nTermAndBoost class now holds a BytesRef rather than a Term.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "QueryBuilder#newSynonymQuery takes an array of TermAndBoost objects as a parameter and uses the field of the first term in the array as it's field.  However, there are cases where this array may be empty, which will result in an ArrayOutOfBoundsException. This commit reworks QueryBuilder so that TermAndBoost contains plain BytesRefs, and passes the field as a separate parameter.  This guards against accidental calls to newSynonymQuery with an empty list - in this case, an empty synonym query is generated rather than an exception.  We also refactor SynonymQuery itself to hold BytesRefs rather than Terms, which needlessly repeat the field for each one. Fixes #11864", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11961", "change_description": ": VectorValues#EMPTY was removed as this instance was not\nnecessary and also illegal as it reported a number of dimensions equal to\nzero.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This instance is illegal as it reports a number of dimensions equal to zero.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11962", "change_description": ": VectorValues#cost() now delegates to VectorValues#size().", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "VectorValues have a cost() method that reports an approximate number of documents that have a vector, but also a size() method that reports the accurate number of vectors in the field. Since KNN vectors only support single-valued fields we should enforce that cost() returns the size() .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11984", "change_description": ": Improved TimeLimitBulkScorer to check the timeout at exponantial rate.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Increase the timeout check inside TimeLimitBulkScorer at exponential rate. Fix #11676", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12004", "change_description": ": Add new KnnByteVectorQuery for querying vector fields that are encoded as BYTE. Removes the ability to\nuse KnnVectorQuery against fields encoded as BYTE", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is the first commit of a much larger refactor. The overall goal is to separate the concerns of byte vectors and float vectors. Making their usage and APIs clearer for users. This first step adds a new KnnByteVectorQuery and only allows it to be used against fields that have the BYTE encoding. Additionally, the original KnnVectorQuery can only be used against fields that have the FLOAT32 encoding. this partially addresses: #11963", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#11997", "change_description": ": Introduce IntField, LongField, FloatField and DoubleField.\nThese new fields index both 1D points and sorted numeric doc values and\nprovide best performance for filtering and sorting.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit adds new IndexableFields that index both points and doc values at once. Closes #11199", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12066", "change_description": ": Retire/deprecate instance method MMapDirectory#setUseUnmap().\nLike the new setting for MemorySegments, this feature is enabled by default and\ncan only be disabled globally by passing the following sysprop on Java command line:\n\"-Dorg.apache.lucene.store.MMapDirectory.enableUnmapHack=false\"", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is the next step towards MemorySegments: Currently you can disable the unmap hack with an instance setter on MMapDirectory. This makes no sense at all, because it either works or not. If it should be disabled, it needs to be global. This is the main reason why I want to change it here: The code, especially around the provider uses crazy checks and passes per-instance booleans around, just to have it separate configurable. In addition as the setting will go away likely with Lucene 10 / Java 21, it is good to deprecate it now, so we don't have to live forever with this crap. When adding MemorySegment support, I added a system property to disable usage of Java 19 project Panama in PR #12062 so to make it consistent, the same is done here. Instead of a per instance configuration, this can be disabled from outside. It is also not a good idea to make a setting like this configurable from inside the JVM, so a system operator can enable/disable it based on the used JDK. When we remove the byte buffer index input, the sysprop will silently disappear, the same for the memory segment one. So theres no backwards compatibility break. In Lucene 9.x the instance setter will be deprecated (to make code still compile), but it will refuse to change the setting and throws UOE that refers to the system property. Once this was backported to 9.x branch, I will remove the deprecated methods on main branch in a followup commit (without PR).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12038", "change_description": ": Deprecate non-NRT replication support.\nPlease migrate to org.apache.lucene.replicator.nrt instead.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Lucene's replicator/ module has really two replication APIs: NRT and the older non-NRT. The NRT replication is nice, it is actually JUST an API, hence there's no network support or anything like that. The non-NRT replication has some issues: I'd like to deprecate the non-NRT support in 9.x and remove in 10.x. I suspect anyone using this module is using the newer NRT mode? If anyone is still using the legacy non-NRT mode, please let me know on this issue and give me your IP address, so I can try to pop a shell. Please see #11381 for more information (I copied the description from there). I opened the issue about a year ago and there has not been any negative feedback. Closes #11381", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12087", "change_description": ": Move DocValuesNumbersQuery from sandbox to NumericDocValuesField#newSlowSetQuery\nand SortedNumericDocValuesField#newSlowSetQuery. IntField, LongField, FloatField, and DoubleField\nimplement newSetQuery with best-practice use of IndexOrDocValuesQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Clean up this query a bit, and move it around to support: This complements the existing docvalues-based range queries, with a set query. Later we can hook this into IntField/LongField/FloatField/DoubleField via IndexOrDocValuesQuery. In general cleanup was not a big deal, involves: Relates to #12028", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12064", "change_description": ": Create new KnnByteVectorField, ByteVectorValues and KnnVectorsReader#getByteVectorValues(String)\nthat are specialized for byte-sized vectors, and clarify the public API by making a clear distinction\nbetween classes that produce and read float vectors and those that produce and read byte vectors.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This completes the refactoring as described in: #11963 This commit: These refactors are difficult to split up any further.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12101", "change_description": ": Remove VectorValues#binaryValue(). Vectors should only be\naccessed through their high-level representation, via\nVectorValues#vectorValue().", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This method tries to expose an encoded view of vectors, but we shouldn't have this part of our user-facing API. With this change, the way vectors are encoded is entirely on the codec.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "API Changes", "change_id": "GITHUB#12105", "change_description": ": Deprecate KnnVectorField in favour of KnnFloatVectorField,\nKnnVectoryQuery in favour of KnnFloatVectorQuery, and LeafReader#getVectorValues\nin favour of LeafReader#getFloatVectorValues.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We recently introduced KnnByteVectorField , KnnByteVectorQuery and ByteVectorValues , the corresponding float variants of the same classes don't follow the same naming convention: KnnVectorField , KnnVectoryQuery and VectorValues . Ideally their names would reflect that they are the float variant of the vector field, vector query and vector values. This PR aims at clarifying this in the public facing API, by deprecating the current float classes in favour of new ones that are their exact copy but follow the same naming conventions as the byte ones. As a result, LeafReader#getVectorValues is also deprecated in favour of newly introduced getFloatVectorValues method that returns FloatVectorValues . I opened a single PR to gather feedback first. I can also split it up in multiple PRs if we want to move forward in this direction. Relates to #11963", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "GITHUB#11795", "change_description": ": Add ByteWritesTrackingDirectoryWrapper to expose metrics for bytes merged, flushed, and overall\nwrite amplification factor.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I recently opened another issue to lower the allowable delete percentage in TieredMergePolicy here . One of the concerns that came up in the discussion was the write amplification factor. The goal of this issue is to allow Lucene users to easily track the write amplification factor in their use cases. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "GITHUB#11929", "change_description": ": MMapDirectory gives more granular control on which files to\npreload.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This enables configuring preloading on MMapDirectory based on the file name as well as the IOContext that is used to open the file.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "GITHUB#11999", "change_description": ": MemoryIndex now supports stored fields.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "GITHUB#11997", "change_description": ": Add IntField, LongField, FloatField and DoubleField: easy to\nuse numeric fields that perform well both for filtering and sorting.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit adds new IndexableFields that index both points and doc values at once. Closes #11199", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "GITHUB#12033", "change_description": ": Support for Java 19 foreign memory support is now enabled by default,\nno need to pass \"--enable-preview\" on the command line. If exactly Java 19 is used,\nMMapDirectory will mmap Lucene indexes in chunks of 16 GiB (instead of 1 GiB) and\nindexes closed while queries are running can no longer crash the JVM.\nTo disable this feature, pass the following sysprop on Java command line:\n\"-Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\"", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "After many complaints that you need to use --enable-preview to use the new MMapDirectory with Java 19 (especially Elasticsearch peope where afraid that passing this flag enables other \"unwanted features\" in the JDK), this PR makes the class file exposed by javac \"real java 19 production class files\", by removing the preview flag from the class file. Actually this is exactly according to spec: With this PR we use MMapDirectory by default on exactly Java 19. On Java 20 it prints at moment a warning (like before). I will soon provide another MR-JAR variant for Java 20 (there were some significant API changes, so we need a separate SourceSet and compiler). If nobody objects, I will merge this around Xmas. I will add a CHANGES.txt entry mentioning this.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "GITHUB#11869", "change_description": ": RangeOnRangeFacetCounts added, supporting numeric range \"relationship\" faceting over docvalue-stored\nranges.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We currently have LongRangeFacetCounts and DoubleRangeFacetCounts , which counts facets based on doc values points that fall into a given list of range. It would be nice to have a corresponding RangeOnRangeFacetCounts that count facets based on indexed ranges ( LongRangeDocValuesField for example) given a list of ranges. We can let the user supply a RangeFieldQuery#QueryType to determine how the range is counted (like in LongRangeSlowRangeQuery . I know that currently, the RangeFacetCounts pack the provided range into a data structure to enable faster counting, but I think for the first iteration we could probably skip that optimization and just do a simple linear scan. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "New Features", "change_id": "LUCENE-10626", "change_description": "Hunspell: add tools to aid dictionary editing:\nanalysis introspection, stem expansion and stem/flag suggestion", "change_title": "Hunspell: add tools to aid dictionary editing: analysis introspection, stem expansion and stem/flag suggestion", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The following tools would be nice to have when editing and appending an existing dictionary: 1. See how Hunspell analyzes a given word, with all the involved affix flags: `Hunspell.analyzeSimpleWord` 2. See all forms that the given stem can produce with the given flags: `Hunspell.expandRoot`, `WordFormGenerator.expandRoot` 3. Given a number of word forms, suggest a stem and a set of flags that produce these word forms: `Hunspell.compress`, `WordFormGenerator.compress`.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#11785", "change_description": ": Improve Tessellator performance by delaying calls to the method\n#isIntersectingPolygon", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This method iterates over all the remaining edges of a polygons to check if a given edge intersects any of them .Currently the method is called when curing local intersections or splitting the polygon  which is iterating over the polygon edges so it is potentially a O(n^2) on the edges of the polygon. The calls are performed on a big conditional but currently the calls are not done in the last position. So just moving the call to the last position brings a very nice performance improvement. For example for the polygons shared on #11777 : FE-24544446.txt : without change:  542.682 seconds with change: 229.524 seconds ORG-24132378.txt : without change: too long, I did not have patience to let it finish. with change:  1416.57 seconds The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#687", "change_description": ": speed up IndexSortSortedNumericDocValuesRangeQuery#BoundedDocIdSetIterator\nconstruction using bkd binary search.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "first we add common function for caller to binary search in bkdPointTree. Generally for log data, when indexd sort in ascend order by @timestamp field, when we want to run count aggregation query to find the count of document in many time interval, we can use the binary search to find out the min/max docId between on  time interval, and the doc count=max docId- min docId +1 also  in this pr, we speed up IndexSortSortedNumericDocValuesRangeQuery#BoundedDocSetIdIterator construction   if the field is with bkd index and index sort by ascend order. #11461", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#11985", "change_description": ": ExitableTerms to override Terms#getMin and Terms#getMax in order to avoid\niterating through the terms when the wrapped implementation caches such values.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "ExitableTerms should not iterate through the terms to retrieve min and max when the wrapped implementation has the values cached (e.g. OrdsFieldReader, FieldReader) I am not entirely sure which other implementations of FilterTerms should do the same, possibly all of them? Maybe we'll want to make a breaking change and make the methods abstract like they are in PointValues ?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#11860", "change_description": ": Improve storage efficiency of connections in the HNSW graph that Lucene uses for\nvector search.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Vector search is much faster when the graph can fit in memory. Consequently, improvements in vector storage can translate to faster searches on larger graphs. One area of size reduction is node connections. Currently, they are stored as regular int values, but per connection there are usually fewer connections than required to store in an int . This commit proposes storing node connections within the graph with using delta encoding and vint . This requires storing the cumulative memory offsets for each node in each layer. The impact of the savings increases as the number of connections M increases. Here are some numbers (datasets are the ones referenced in ann-benchmarks): Reduction varies from 50%, to close to 90%! This change has no perceived impact on indexing. This is mainly because the cost of indexing is dominated by the distance calculations. Additionally, there is no impact on query latency. In fact, I have noticed a slight increase in QPS (not a dramatic change mind you). closes : #11830", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#12008", "change_description": ": Clean up LongRange#verifyAndEncode logic to remove unnecessary NaN checks.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Minor tweak to logic that appears to have been copy/pasted from DoubleRange .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#12003", "change_description": ": Minor cleanup/improvements to IndexSortSortedNumericDocValuesRangeQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR contains some minor cleanup I thought might be useful after recently spending a little time looking at this code.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#12016", "change_description": ": Upgrade lucene/expressions to use antlr 4.11.1", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Signed-off-by: Andriy Redko andriy.redko@aiven.io The Apache Lucene is using quite old version of ANTLR 4.5.1-1. By itself, it is not a showstopper, but more profound issue is that some ANTLR 3.x bits are used [1]. Since ANTLR 4.10.x (or even earlier), the compatibility layer with 3.x release line has been dropped in 4.x (see please [2]), which makes Apache Lucene impossile to be used with recent ANTLR 4.10.x+ releases [3]. The sample exception is below. [1] https://github.com/apache/lucene/blob/main/lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptLexer.java#L189 [2] antlr/antlr4@ c68e127 [3] opensearch-project/OpenSearch#4546 Closes #11788", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#12034", "change_description": ": Remove null check in IndexReaderContext#leaves() usages", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "IndexReaderContext#leaves() never returns a null value, so we can safely remove the null checks from the method calls. We should also update the method documentation to make that clear.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Improvements", "change_id": "GITHUB#12070", "change_description": ": Compound file creation is no longer subject to merge throttling.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "ConcurrentMergeScheduler uses the rate at which a merge writes bytes as a proxy for CPU usage, in order to prevent merging from disrupting searches too much. However creating compound files are lightweight CPU-wise and do not need throttling. Closes #12068", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11726", "change_description": ": Indexing term vectors on large documents could fail due to\ntrying to apply a dictionary whose size is greater than the maximum supported\nwindow size for LZ4.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When indexing term vectors for a very large document, the automatic computation of the dictionary size based on the overall size of the block might yield a size that exceeds the maximum window size that is supported by LZ4. This commit addresses the issue by automatically taking the minimum of the result of this computation and the maximum window size (64kB).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11768", "change_description": ": Taxonomy and SSDV faceting now correctly breaks ties by preferring smaller ordinal\nvalues.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There are a number of places in Facets implementations where getTopChildren is incorrectly handling count/value ties. The behavior should prefer smaller ordinals when counts/values are equal, but it's not always doing that. Some tests were also incorrect and needed to be updated.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11907", "change_description": ": Fix latent casting bugs in BKDWriter.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit fixes a latent casting bug where int multiplication could roll-over to the negatives. new byte[Math.toIntExact(numSplits * config.bytesPerDim)]; toIntExact does nothing from what I understand. Since both numSplits and config.bytesPerDim are already int values, their multiplication will be an int multiplication that will return an int . Consequently, Math.toIntExact does no work here and we can still get negative numbers. I will venture to assume that the original author actually wanted Math.multiplyExact(numSplits, config.bytesPerDim) which will multiply the two values and throw if there is overflow.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11954", "change_description": ": Remove QueryTimeout#isTimeoutEnabled method and move check to caller.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This removes the method QueryTimeout#isTimeoutEnabled and moves the responsibility to ensure timeout is not null to the caller. Initially I went with the approach to allow QueryTimeout to be null (and removing #isTimeoutEnabled ) and allow wrapping TimeLimitingBulkScorer and ExitableDirectoryReader with null timeouts which is equivalent of timeout not enabled as this makes work easy for the caller but giving a thought again it made more sense to only wrap the classes if there is an actual timeout and moving the responsibility to ensure timeout is configured to caller method as mentioned in the issue. Closes #11914", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11950", "change_description": ": Fix NPE in BinaryRangeFieldRangeQuery variants when the queried field doesn't exist\nin a segment or is of the wrong type.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This fixes a bug where variants of BinaryRangeFieldRangeQuery will result in an NPE if the field doesn't exist in a segment.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11990", "change_description": ": PassageSelector now has a larger minimum size for its priority queue,\nso that subsequent passage merges don't mean that we return too few passages in\ntotal.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "PassageScorer uses a priority queue of size maxPassages to keep track of which highlighted passages are worth returning to the user.  Once all passages have been collected, we go through and merge overlapping passages together, but this reduction in the number of passages is not compensated for by re-adding the highest-scoring passages that were pushed out of the queue by passages which have been merged away. This commit increases the size of the priority queue to try and account for overlapping passages that will subsequently be merged together.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11986", "change_description": ": Fix algorithm that chooses the bridge between a polygon and a hole when there is\ncommon vertex.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "A user of Elasticsearch reported a few polygons that are failing tessellation, for example: They all seems to fail because they were hitting this part of the tessellation code: lucene/lucene/core/src/java/org/apache/lucene/geo/Tessellator.java Line 451\n      in 0cc6f69  In addition, they all share the same situation where p and p.next were the same vertex. I think that iteration is bogus as as we started in p = connection.next and finish in p , which means p is actually not really consider. I did the changes so all nodes are processed and it seems that all failing polygons worked. No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12020", "change_description": ": Fixes bug whereby very flat polygons can incorrectly contain intersecting geometries.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When performing a search using a shape geometry query of relation type QueryRelation.CONTAINS , it is possible to get a false positive when two geometries intersect, but neither actually contains the other. This happens if the indexed geometry is a polygon that is so flat that one of its triangles is simplified to a single line segment. The bug is that the line segment currently records whether it is part of the external polygon by taking that knowledge from first line segment of the triangle, not necessarily the part of the triangle being retained. This first line segment could be an internal line (part of the triangle, but not the polygon). The simplification code should instead take this knowledge from a line segment that is not being collapsed by the simplification. The consequence of this bug occur later during the contains search when the search query deduces that the geometry is contained because the polygon is not closed. The search does not realise it intersects an outer line of the polygon because that line is not correctly marked as outer. No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12058", "change_description": ": Fix detection of Hotspot in TestRamUsageEstimator so it works with OpenJ9.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This improves the test, which fails with OpenJ9 VMs, due to the following problem:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12046", "change_description": ": Out of boundary in CombinedFieldQuery#addTerm.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12072", "change_description": ": Fix exponential runtime for nested BooleanQuery#rewrite when a\nBooleanClause is non-scoring.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When #672 was introduced, it added many nice rewrite optimizations. However, in the case when there are many multiple nested Boolean queries under a top level Boolean#filter clause, its runtime grows exponentially. The key issue was how the BooleanQuery#rewriteNoScoring redirected yet again to the ConstantScoreQuery#rewrite . This causes BooleanQuery#rewrite to be called again recursively , even though it was previously called in ConstantScoreQuery#rewrite , and THEN BooleanQuery#rewriteNoScoring is called again, recursively. This causes exponential growth in rewrite time based on query depth. The change here hopes to short-circuit that and only grow (near) linearly by calling BooleanQuery#rewriteNoScoring directly, instead if attempting to redirect through ConstantScoreQuery#rewrite . The absolute worst case I was able to test is many nested SHOULD clauses with a depth of 22. This ran for over 7 seconds without my change. With my change it took less than 70ms. I had to cancel the test (without my change) when the depth was 30. It was simply taking too long. With my change, it was around 100ms. closes : #12069", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11807", "change_description": ": Don't rewrite queries in unified highlighter.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Since QueryVisitor added the ability to signal multi-term queries, the query rewrite call in UnifiedHighlighter has been essentially useless, and with more aggressive rewriting this is now causing bugs like #11490 .  We can safely remove this call. Fixes #11490", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12088", "change_description": ": WeightedSpanTermExtractor should not throw UnsupportedOperationException\nwhen it encounters a FieldExistsQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "WeightedSpanTermExtractor will try to rewrite queries that it doesn't know about, to see if they end up as something it does know about and that it can extract terms from.  To support field merging, it rewrites against a delegating leaf reader that does not support getFieldInfos() . FieldExistsQuery uses getFieldInfos() in its rewrite, which means that if one is passed to WeightedSpanTermExtractor, we get an UnsupportedOperationException thrown. This commit makes WeightedSpanTermExtractor aware of FieldExistsQuery, so that it can just ignore it and avoid throwing an exception.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12084", "change_description": ": Same bound with fallbackQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "IndexSortSortedNumericDocValuesRangeQuery should have the same bound with fallbackQuery. According to the comment, my guess it is a typo thing?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12077", "change_description": ": WordBreakSpellChecker now correctly respects maxEvaluations", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "WordBreakSpellChecker has a maxEvaluations config option (default: 1000) which is suppose to be the \"maximum number of word combinations to evaluate\" but the way this setting is used in generateBreakUpSuggestions() makes no sense. The crux of this method is to iteratively loop over each character in the input to consider if splitting at that point in the string produces valid suggestion words on the left & right side of the split.  If the left side is a valid suggestion, then regardless of whether the right side was, recursively process the right side. As it's looping, it compares maxEvaluations to a totalEvaluations counter which is incremented on each iteration, as well as being passed down in each recursive call, breaking out of the loop if totalEvaluations >= maxEvaluations . But it also maintains a thisTimeEvaluations counter, which is only incremented based on the loop iterations, and this is what the method returns -- meaning that even if a nested recursive call exceeds maxEvaluations , the return value it gives back to it's caller is a much smaller number (bounded by the length of the string), and the caller will proceed to do more iterations (and more recursive calls) It's such a weird implementation choice (to explicitly maintain these two variables) that I'm sure my explanation above is hard to make sense of (it seems like it must have been intentional -- but i can't understand why?) and I confused myself 3 times trying to write it. The only way i found to really wrap my head around how \"wrong\" this is was by adding some sysout messages to the methods showing just how much iterating & recursing it does even when maxEvaluations . As a result of this behavior, it's actually possible for WordBreakSpellChecker.suggestWordBreaks(...) to return significantly more suggestions then the setting of maxEvaluations I'm attaching: No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11738", "change_description": ": Optimize MultiTermQueryConstantScoreWrapper when a term is present that matches all\ndocs in a segment.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR brings over an optimization we recently made to TermInSetQuery ( #1062 ) to MultiTermQuery more generally.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11735", "change_description": ": KeywordRepeatFilter + OpenNLPLemmatizer always drops last token of a stream.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Initial issue : KeywordRepeatFilter + OpenNLPLLemmatizer leads to empty token list in case of a single token stream. Steps to re-produce : run TestOpenNLPLemmatizerFilterFactory.testNoBreakWithRepeatKeywordFilter and observe that 0 tokens are returned after processing the text “period”. Underlying issue : opennlp package mishandles sentence boundary detection in general when KeywordRepeatFilter is added. The issue flies under the radar because the tests don’t verify which tokens are processed together as one sentence. Below is a screenshot showing that the last token of the last sentence gets dropped. This is usually not a big deal when that token is punctuation (most of the time) but can become especially problematic when the last bit of text of a stream has no punctuation. For example consider the text \"This is some sentence\". If you pass this on its own into an analysis chain identical to the one configured in TestOpenNLPLemmatizerFilterFactory.testNoBreakWithRepeatKeywordFilter you will see this:  Suggested fix : Linking #11734 as the suggested fix for this. The gist is to use a one-step lookahead when processing the token stream to correctly detect sentence transition in the general case of repeating tokens. I have centralized the inner sentence token loop which had been repeated across the different sentence-aware filters. The suggested fix also removes other seemingly unnecessary conditional branching and tidies up the different OpenNLP filters so they operate more similarly to one another (at least wherever possible) Latest version of lucene running jdk-17 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11771", "change_description": ": KeywordRepeatFilter + OpenNLPLemmatizer sometimes arbitrarily exits token stream.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "KeywordRepeatFilter + OpenNLPLemmatizer leads to arbitrarily early exit of token stream. Steps to reproduce: run this test and notice how no text below this line from the test file gets analyzed. The root cause appears to be an extraneous exit condition that doesn't play nicely with KeywordRepeatFilter. This is related to the bug #11735 and is addressed by #11734 latest version of lucene running jdk-17 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11803", "change_description": ": DrillSidewaysScorer has improved to leverage \"advance\" instead of \"next\" where\npossible, and splits out first and second phase checks to delay match confirmation.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change makes use of advance instead of next where possible and splits out 1st and 2nd phase checking to avoid match confirmation when unnecessary. Note that I only focused on the doQueryFirstScoring implementation here and didn't modify the other two scoring approaches. \"Progress not perfection\" and all that (plus, I think we should strongly consider removing these other two implementations, but we'd want to benchmark to be certain). Unfortunately, luceneutil doesn't have dedicated drill sideways benchmarks, but some benchmarks on our internal software that makes use of drill sideways showed a +2% QPS improvement and no obvious regressions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11828", "change_description": ": Tweak TermInSetQuery \"dense\" optimization to only require all terms present in a\ngiven field to match a term (rather than all docs in a segment). This is consistent with\nMultiTermQueryConstantScoreWrapper.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This changes the optimization present in TermInSetQuery to mimic the one in MultiTermQueryConstantScoreWrapper , bringing parity to the two approaches. More specifically, it optimizes the case where all docs with a value for the referenced field contain a given term (rather than requiring all docs in the segment to contain the term). The solution for MultiTermQueryConstantScoreWrapper was discussed in PR #11738 for reference.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11876", "change_description": ": Use ByteArrayComparator to speed up PointInSetQuery in single dimension case.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR proposes to use ByteArrayComparator to speed up PointInSetQuery#MergePointVisitor", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11880", "change_description": ": Use ByteArrayComparator to speed up BinaryRangeFieldRangeQuery, RangeFieldQuery\nLatLonPointDistanceFeatureQuery and CheckIndex.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "After #11876 I find some other codes are still using Arrays#compareUnAssigned which can be repaced by ByteArrayComparator or ArrayUtil#compareUnsigned4 . This PR replaced some of them that look performance sensitive.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11881", "change_description": ": Further optimize drill-sideways scoring by specializing the single dimension case\nand borrowing some concepts from \"min should match\" scoring.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change further optimizes the doc-at-a-time drill-sideways scoring. There are a couple small tweaks, but the two bigger ideas in here are: Unfortunately, we don't have drill-sideways specific tasks in luceneutil benchmarks, but I was able to benchmark the impact of this change using some benchmarks we have on our Lucene-based application (that specifically stress our use of drill-sideways). In our tests, I observed a throughput increase of 2.4% red-line QPS and a 3.7% average latency reduction (which is mostly dominated by a p50 latency reduction of 3.9% with no noticeable impacts to long-tail latencies).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11884", "change_description": ": Simplify the logic of matchAll() in IndexSortSortedNumericDocValuesRangeQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Could we do this simplification, make it more readable？", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11895", "change_description": ": count() in BooleanQuery could be early quit.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Since all queries are pure disjunctional, we could quit early and make the logic much simpler?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11972", "change_description": ": `IndexSortSortedNumericDocValuesRangeQuery` can now also\noptimize query execution with points for descending sorts.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This generalizes #687 to indexes that are sorted in descending order. The main challenge with descending sorts is that they require being able to compute the last doc ID that matches a value, which would ideally require walking the BKD tree in reverse order, but the API only support moving forward. This is worked around by maintaining a stack of PointTree clones to perform the search.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#12006", "change_description": ": Do ints compare instead of ArrayUtil#compareUnsigned4 in LatlonPointQueries.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In LatLonPointQueries we encode query ints to bytes, and compare bytes by decode bytes back to int in ArrayUtil#compareUnsigned4 . We can directly compare ints instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#12011", "change_description": ": Minor speedup to flushing long postings lists when an index\nsort is configured.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When flushing segments that have an index sort configured, postings lists get loaded into arrays and get reordered according to the index sort. This reordering is implemented with TimSorter , a variant of merge sort. Like merge sort, an important part of TimSorter consists of merging two contiguous sorted slices of the array into a combined sorted slice. This merging can be done either with external memory, which is the classical approach, or in-place, which still runs in linear time but with a much higher factor. Until now we were allocating a fixed budget of maxDoc/64 for doing these merges with external memory. If this is not enough, sorted slices would be merged in place. I've been looking at some profiles recently for an index where a non-negligible chunk of the time was spent on in-place merges. So I would like to propose the following change: So overall memory usage would never be more than 50% higher than what it is today, because TimSorter never needs more than X temporary slots if the postings list doesn't have at least 2X entries, and these 2X entries already get loaded into memory today. And for fields that have short postings, memory usage should actually be lower.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#12017", "change_description": ": Aggressive count in BooleanWeight.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When BooleanQuery is pure disjunction, if at least one clause could match all docs, then we could get right count even though there was other clause whose count is  unknown.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#12079", "change_description": ": Faster merging of 1D points.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "On the NYC taxis dataset on my local machine, switching from Arrays#compareUnsigned to ArrayUtil#getUnsignedComparator yielded a 15% speedup of BKD merging.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#12081", "change_description": ": Small merging speedup on sorted indexes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In the case when an index is sorted on a low-cardinality field, or the index sort order correlates with the order in which documents get ingested, we can optimize SortedDocIDMerger by doing a single comparison with the doc ID on the next sub. This checks covers at the same time whether the priority queue needs reordering and whether the current sub reached NO_MORE_DOCS .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#12078", "change_description": ": Enhance XXXField#newRangeQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Introduce IndexSortSortedNumericDocValuesRangeQuery to IntFiled#newRangeQuery and LongField#newRangeQuery . See more discussion #12074 .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11857", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11859", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We discard entries with NOSUGGEST (and some other) flags anyway, so let's bail out of processing them at an earlier stage. This speeds up suggestions for relatively short German words by about 20% for me.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11893", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Optimizations", "change_id": "GITHUB#11909", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "add NGramFragmentChecker to quickly check whether insertions/replacements produce strings that are even possible in the language", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "GITHUB#11856", "change_description": ": Fix nanos to millis conversion for tests", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fix some wrong division to calculate millis from nanos in tests, after commit that removes System.currentTimeMillis() calls. Follows: fb40a43 Follows: Remove usages of System.currentTimeMillis() from tests #11749 Replace all sec, ms, ns calculations with TimeUnit conversion methods Rename msec and millis to ms to have consistency across the code base", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "LUCENE-10423", "change_description": ": Remove usages of System.currentTimeMillis() from tests.", "change_title": "Remove uses of wall-clock time in codebase", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Followup to LUCENE-10421 Code in the library shouldn't rely on wall-clock time. If you look at all the places doing this, they are basically all bad news. Most tests doing this are \"iterating for some amount of wall-clock time\" which causes them to instead just be non-reproducible. These should be changed to use a fixed number of loop iterations instead. It would really be great to ban this stuff in forbidden apis. It is even in the configuration file, just currently commented out.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "GITHUB#11811", "change_description": ": Upgrade google java format to 1.15.0", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "GITHUB#11834", "change_description": ": Upgrade forbiddenapis to version 3.4.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This updates forbiddenapis to 3.4, released today. This mainly adds Java 19 support. This PR will also uncomment and enable forbiddenapis for the main19 sourceset in :lucene:core .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "LUCENE-10635", "change_description": ": Ensure test coverage for WANDScorer by using a test query.", "change_title": "Ensure test coverage for WANDScorer after additional scorers get added", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This is a follow-up issue from discussions https://github.com/apache/lucene/pull/972#issuecomment-1170684358 & https://github.com/apache/lucene/pull/972#pullrequestreview-1024377641 .  As additional scorers such as BlockMaxMaxscoreScorer get added, some tests in TestWANDScorer that used to test WANDScorer now test BlockMaxMaxscoreScorer instead, reducing test coverage for WANDScorer. We would like to see how we can ensure TestWANDScorer reliably tests WANDScorer, perhaps by initiating the scorer directly inside the tests?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "GITHUB#11752", "change_description": ": Added interface to relate a LatLonShape with another shape represented as Component2D.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "With the release of ShapeDocValues and ShapeDoc Values field( LUCENE-10654 ), it provides various interfaces to create the query and create the different ShapeDoc values. But it doesn't provide the interface to find the relation between a Shape(provided as ShapeDocValues) with another shape. This functionality is already present in the ShapeDocValues , but not exposed to the clients, as the ShapeDocValue class itself is marked as package-private. Expectation: The expectation is to provide this relate functionality to the clients, so that it can be used to find the relations. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "GITHUB#11983", "change_description": ": Make constructors for OffsetFromPositions and OffsetsFromMatchIterator\npublic.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "OffsetsFromMatchIterator and OffsetsFromPositions both have package- private constructors, which makes them difficult to use as components in a separate highlighter implementation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "LUCENE-10546", "change_description": ": Update Faceting user guide.", "change_title": "Update Faceting user guide", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The  facet user guide was written based on 4.1. Since there's been a fair amount of active facet-related development over the last year+, it would be nice to review the guide and see what updates make sense.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Other", "change_id": "GITHUB#12099", "change_description": ": Introduce support in KnnVectorQuery for getters.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Knn Queries are locked currently, it would be beneficial for applications using them to have access to getters and setters. An example is how filter queries are managed in Apache Solr: the processing of pre-filters and post-filters could benefit from opening up the access to such variables. Especially the pre-filter support introduced in Solr 9.1 could get great benefits from being able to set the filter, after the query has been parsed. See: apache/solr#1245 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.5.0", "change_type": "Build", "change_id": "GITHUB#11886", "change_description": ": Upgrade to gradle 7.5.1", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Upgrade gradle to 7.5.1. Switch distribution type to 'bin' instead of 'all' (smaller, excludes documentation and examples - these are irrelevant for us and are available online). gradlew launch scripts are left as they were since we have customized them heavily and they seem to be working fine.", "patch_link": "none", "patch_content": "none"}

{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13469", "change_description": ": Expose FlatVectorsFormat as a first-class format; can be configured using a custom Codec.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "just posting this here for discussion purposes; API is up for discussion", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13612", "change_description": ": Hunspell: add Suggester#proceedPastRep to avoid losing relevant suggestions.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR introduces a way to get a copy of the suggester instance that doesn't stop after encountering acceptable words (after applying REP rules). By default, Hunspell stops when it finds any, but this behavior may not always be desirable, e.g., if we have \"REP i ea\", \"tims\" would be replaced only by \"teams\" and not \"times\", which could also be meant.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13603", "change_description": ": Introduced `IndexSearcher#searchLeaf(LeafReaderContext, Weight, Collector)` protected method to\nfacilitate customizing per-leaf behavior of search without requiring to override\n`search(LeafReaderContext[], Weight, Collector)` which requires overriding the entire loop across the leaves", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There's a couple of places in the codebase where we extend IndexSearcher to customize per leaf behaviour, and in order to do that, we need to override the entire search method that loops through the leaves. A good example is ScorerIndexSearcher. Adding a searchLeaf method that provides the per leaf behaviour makes those cases a little easier to deal with. Also, it paves the road for #13542 where we will want to replace search(List<LeafReaderContext>, Weight, Collector) with something along the lines of search(LeafReaderContextPartition[], Weight, Collector) . Such switch will be made easier by no longer having extensions of IndexSearcher that provide per-leaf behaviour by overriding the former method.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13559", "change_description": ": Add BitSet#nextSetBit(int, int) to get the index of the first set bit in range.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In SparseFixedBitSet.firstDoc, instead of iterating though the entire indices array until non-zero value is found, keep track of max updated index. Use case where it improves performance: In my case, we use SparseFixedBitSet to track and iterate children hits found in ToParentBlockJoinQuery . Iterating through empty indices elements becomes expensive when we do it for each parent docID. Lucene util performance test results might not be great though - so maybe there is better way to achieve similar effect?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13568", "change_description": ": Add DoubleValuesSource#toSortableLongDoubleValuesSource and\nMultiDoubleValuesSource#toSortableMultiLongValuesSource methods.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "@Shradha26 and I are working on a new faceting implementation in the sandbox module. With this, we are proposing the following new features to Lucene’s faceting and aggregation capabilities - For an index with documents that have facet field color and numeric field popularity , today, we can associate the MAX or SUM of popularity with the facets of color using the TaxonomyFloatFacetAssociations implementations. We are limited to computing only one associated value in one iteration per Faceting implementation. But sometimes, we also want to compute multiple values  e.g. SUM(popularity) and MAX(bm25_score). With the new sandbox faceting, we have native support for multiple associated values with each facet without requiring to iterate through the documents multiple times. Current faceting supports computing associated values for Taxonomy fields only. With the new implementation, users will be able to compute associated values for any faceting implementation. Today FacetsCollector#collect only collects docIDs, and facet computations happens in each Facets instance in its own doc ID loop. Some numeric field values may be expensive to compute (e.g. some relevance scores).  In order to re-use the values from these fields of documents to compute the different associated values for different facets, we have to cache values for all docs, which might also be expensive. The new implementation computes facets as it collects the docIDs. I allows us to do the expensive computation just once for a document, and then use this value for all the facets the document belongs to. The current faceting implementation is responsible for retrieving the different facets associated with a document and storing aggregated values or counts across different facets. Due to this, we have different facet implementations to handle each type of aggregated values - e.g IntTaxonomyFacets, FloatTaxonomyFacets etc. In the proposed implementation, we have two new interfaces that help decouple the “faceting” and the “aggregation” logic - As a result, any facet type (taxonomy, numeric ranges, etc) can aggregate any type of values (count only, or any type or number of associated value aggregations) Today, Facets#getTopChildren , getAllChildren , getTopDims etc method can be used to get facet results. This API has some limitations. One limitation is that each Facets implementation has slightly different logic. For example, most implementation sort results by count to get topN, but the ones that compute associations sort by aggregated value (in descending order). They also often have different tie-break algorithms. Some implementations tie break by facet ordinal (e.g. TaxonomyFacets ), some by facet value (e.g. LongValueFacetCounts ), etc. To implement different sort order, clients have to extend these classes and override these methods, which is not always convenient, e.g. some of these classes are package private. We have introduced an OrdinalIterator interface, which has implementations to perform “get children”, “get top N”, “get specific values” operations. Ordinals can be sorted by values, collected by FacetRecorder, by labels, etc. Clients can extend OrdinalsIterator interface to implement custom filtering or sort order. For the sandbox module to work, we had to change DrillSideways to support any type of Collector, not just FacetCollector. The challenge there is that different drill sideways dimension might want to use different Collector types and return different result types ( CollectorManager<C, T> ), and Java generics don't make handling of unknown number of generic types very easy. In the PR it is solved by adding CollectorOwner class which keeps collectors that CollectorManager creates. As a result, DrillSideways doesn't have to manage Collectors and results itself, which allows us to use wildcard type ? and let client handle collector and result types. Computing facets during collection is more expensive, because we need to collect in each searcher slice, and then merge results in CollectorManager#reduce . At the same time, it reduces latency, because initial collection is done in parallel and there is an opportunity to reuse values computed for a document and use for the doc for all the different collectors (drillsideways). Please see SandboxFacetsExample in the demo package for some usage examples.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13568", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "@Shradha26 and I are working on a new faceting implementation in the sandbox module. With this, we are proposing the following new features to Lucene’s faceting and aggregation capabilities - For an index with documents that have facet field color and numeric field popularity , today, we can associate the MAX or SUM of popularity with the facets of color using the TaxonomyFloatFacetAssociations implementations. We are limited to computing only one associated value in one iteration per Faceting implementation. But sometimes, we also want to compute multiple values  e.g. SUM(popularity) and MAX(bm25_score). With the new sandbox faceting, we have native support for multiple associated values with each facet without requiring to iterate through the documents multiple times. Current faceting supports computing associated values for Taxonomy fields only. With the new implementation, users will be able to compute associated values for any faceting implementation. Today FacetsCollector#collect only collects docIDs, and facet computations happens in each Facets instance in its own doc ID loop. Some numeric field values may be expensive to compute (e.g. some relevance scores).  In order to re-use the values from these fields of documents to compute the different associated values for different facets, we have to cache values for all docs, which might also be expensive. The new implementation computes facets as it collects the docIDs. I allows us to do the expensive computation just once for a document, and then use this value for all the facets the document belongs to. The current faceting implementation is responsible for retrieving the different facets associated with a document and storing aggregated values or counts across different facets. Due to this, we have different facet implementations to handle each type of aggregated values - e.g IntTaxonomyFacets, FloatTaxonomyFacets etc. In the proposed implementation, we have two new interfaces that help decouple the “faceting” and the “aggregation” logic - As a result, any facet type (taxonomy, numeric ranges, etc) can aggregate any type of values (count only, or any type or number of associated value aggregations) Today, Facets#getTopChildren , getAllChildren , getTopDims etc method can be used to get facet results. This API has some limitations. One limitation is that each Facets implementation has slightly different logic. For example, most implementation sort results by count to get topN, but the ones that compute associations sort by aggregated value (in descending order). They also often have different tie-break algorithms. Some implementations tie break by facet ordinal (e.g. TaxonomyFacets ), some by facet value (e.g. LongValueFacetCounts ), etc. To implement different sort order, clients have to extend these classes and override these methods, which is not always convenient, e.g. some of these classes are package private. We have introduced an OrdinalIterator interface, which has implementations to perform “get children”, “get top N”, “get specific values” operations. Ordinals can be sorted by values, collected by FacetRecorder, by labels, etc. Clients can extend OrdinalsIterator interface to implement custom filtering or sort order. For the sandbox module to work, we had to change DrillSideways to support any type of Collector, not just FacetCollector. The challenge there is that different drill sideways dimension might want to use different Collector types and return different result types ( CollectorManager<C, T> ), and Java generics don't make handling of unknown number of generic types very easy. In the PR it is solved by adding CollectorOwner class which keeps collectors that CollectorManager creates. As a result, DrillSideways doesn't have to manage Collectors and results itself, which allows us to use wildcard type ? and let client handle collector and result types. Computing facets during collection is more expensive, because we need to collect in each searcher slice, and then merge results in CollectorManager#reduce . At the same time, it reduces latency, because initial collection is done in parallel and there is an opportunity to reuse values computed for a document and use for the doc for all the different collectors (drillsideways). Please see SandboxFacetsExample in the demo package for some usage examples.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13750", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "DrillSideways#search : use bounded wildcard for the list of drill-sideways CollectorManagers Lists are invariant, so current API doesn't allow passing something like List<CollectorManagerImplementation> where class CollectorManagerImplementation implements CollectorManager<A, B> . Invariant makes it so List<CollectorManagerImplementation> does not extend List<CollectorManager<A, B>> . Using bounded wildcard type allows to overcome that.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13737", "change_description": ": Deprecate the FacetsCollector#search utility methods and add new corresponding method to\nFacetsCollectorManager that accept a FacetsCollectorManager as last argument in place of a Collector.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "These can now be replaced by the corresponding methods added to FacetsCollectorManager that accept a FacetsCollectorManager as last argument in place of a Collector This is the backport #13733 , minus the removal of the static methods which get instead deprecated in the 9x branch. Relates to #11041", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "API Changes", "change_id": "GITHUB#13794", "change_description": ": Deprecate BulkScorer#score(LeafCollector collector, Bits acceptDocs) in favour of\nBulkScorer#score(LeafCollector collector, Bits acceptDocs, int min, int max). The method will be removed in the next\nmajor version. Replace usages with the latter, providing 0 as min and DocIdSetIterator.NO_MORE_DOCS as max in case\nthe entire segment should be scored. Subclasses that override the method should instead override its replacement.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We have removed BulkScorer#score(LeafReaderContext, Bits) in main in favour of BulkScorer#score(LeafCollector collector, Bits acceptDocs, int min, int max) as part of #13542 . This commit deprecates the method in 9.x. Internal usages of it are left untouched as there may be subclasses that override it, which we don't want to break in a minor release.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "New Features", "change_id": "GITHUB#13430", "change_description": ": Allow configuring the search concurrency via\nTieredMergePolicy#setTargetSearchConcurrency. This in-turn instructs the\nmerge policy to try to have at least this number of segments on the highest\ntier.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Closes #12877 Adds a minimum number of segments to TieredMergePolicy . This allows to ensure that a minimum search concurrency parallelism is achievable using inter-segment parallelism.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "New Features", "change_id": "GITHUB#13517", "change_description": ": Allow configuring the search concurrency on LogDocMergePolicy\nand LogByteSizeMergePolicy via a new #setTargetConcurrency setter.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This adds the same targetSearchConcurrency parameter to LogMergePolicy that #13430 is adding to TieredMergePolicy . The implementation is simpler here since LogMergePolicy is constrained to only merging adjacent segments. From simulating merging on an index that gets appended 555 equal-size segments (like nightly benchmarks) and a targetSearchConcurrency of 8, I get the following results:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "New Features", "change_id": "GITHUB#13568", "change_description": ": Add sandbox facets module to compute facets while collecting.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "@Shradha26 and I are working on a new faceting implementation in the sandbox module. With this, we are proposing the following new features to Lucene’s faceting and aggregation capabilities - For an index with documents that have facet field color and numeric field popularity , today, we can associate the MAX or SUM of popularity with the facets of color using the TaxonomyFloatFacetAssociations implementations. We are limited to computing only one associated value in one iteration per Faceting implementation. But sometimes, we also want to compute multiple values  e.g. SUM(popularity) and MAX(bm25_score). With the new sandbox faceting, we have native support for multiple associated values with each facet without requiring to iterate through the documents multiple times. Current faceting supports computing associated values for Taxonomy fields only. With the new implementation, users will be able to compute associated values for any faceting implementation. Today FacetsCollector#collect only collects docIDs, and facet computations happens in each Facets instance in its own doc ID loop. Some numeric field values may be expensive to compute (e.g. some relevance scores).  In order to re-use the values from these fields of documents to compute the different associated values for different facets, we have to cache values for all docs, which might also be expensive. The new implementation computes facets as it collects the docIDs. I allows us to do the expensive computation just once for a document, and then use this value for all the facets the document belongs to. The current faceting implementation is responsible for retrieving the different facets associated with a document and storing aggregated values or counts across different facets. Due to this, we have different facet implementations to handle each type of aggregated values - e.g IntTaxonomyFacets, FloatTaxonomyFacets etc. In the proposed implementation, we have two new interfaces that help decouple the “faceting” and the “aggregation” logic - As a result, any facet type (taxonomy, numeric ranges, etc) can aggregate any type of values (count only, or any type or number of associated value aggregations) Today, Facets#getTopChildren , getAllChildren , getTopDims etc method can be used to get facet results. This API has some limitations. One limitation is that each Facets implementation has slightly different logic. For example, most implementation sort results by count to get topN, but the ones that compute associations sort by aggregated value (in descending order). They also often have different tie-break algorithms. Some implementations tie break by facet ordinal (e.g. TaxonomyFacets ), some by facet value (e.g. LongValueFacetCounts ), etc. To implement different sort order, clients have to extend these classes and override these methods, which is not always convenient, e.g. some of these classes are package private. We have introduced an OrdinalIterator interface, which has implementations to perform “get children”, “get top N”, “get specific values” operations. Ordinals can be sorted by values, collected by FacetRecorder, by labels, etc. Clients can extend OrdinalsIterator interface to implement custom filtering or sort order. For the sandbox module to work, we had to change DrillSideways to support any type of Collector, not just FacetCollector. The challenge there is that different drill sideways dimension might want to use different Collector types and return different result types ( CollectorManager<C, T> ), and Java generics don't make handling of unknown number of generic types very easy. In the PR it is solved by adding CollectorOwner class which keeps collectors that CollectorManager creates. As a result, DrillSideways doesn't have to manage Collectors and results itself, which allows us to use wildcard type ? and let client handle collector and result types. Computing facets during collection is more expensive, because we need to collect in each searcher slice, and then merge results in CollectorManager#reduce . At the same time, it reduces latency, because initial collection is done in parallel and there is an opportunity to reuse values computed for a document and use for the doc for all the different collectors (drillsideways). Please see SandboxFacetsExample in the demo package for some usage examples.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "New Features", "change_id": "GITHUB#13678", "change_description": ": Add support JDK 23 to the Panama Vectorization Provider.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit updates the Vectorization Provider to support JDK 23. The API has not changed so the changes minimally bump the major JDK check, and enable the incubating API during testing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "New Features", "change_id": "GITHUB#13689", "change_description": ": Add a new faceting feature, dynamic range facets, which automatically picks a balanced set of numeric\nranges based on the distribution of values that occur across all hits.  For use cases that have a highly variable\nnumeric doc values field, such as \"price\" in an e-commerce application, this facet method is powerful as it allows the\npresented ranges to adapt depending on what hits the query actually matches.  This is in contrast to existing range\nfaceting that requires the application to provide the specific fixed ranges up front.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR enables users to build weighted ranges over numeric fields. The user needs to specify how many ranges they want and sources for the values in the range and their weights. Lucene returns ranges of equal weight, with counts per range. Credit to @gsmiller and @Yuti-G for thinking this up and to @Yuti-G for writing most of this code a few years ago for Amazon product search. Closes #11028", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13475", "change_description": ": Re-enable intra-merge parallelism except for terms, norms, and doc values.\nRelated to", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "@iverase has uncovered a potential issue with intra-merge CMS parallelism. This commit helps expose this problem by forcing tests to use intra-merge parallelism instead of always (well, usually) delegating to a SameThreadExecutorService. When intra-merge parallelism is used, norms, doc_values, stored_values, etc. are all merged in a separate thread than the thread that was used to construct their merge optimized instances. This trips assertions numerous assertions in AssertingCodec.assertThread where we assume that the thread that called getMergeInstance() is also the thread getting the values to merge. I am not 100% sure this is a real problem. Segment merging per field type is still only done in a single thread, we just fork the merging action to a separate thread from the constructed one. Opening as draft for discussion. @jpountz you may be interested in this.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13478", "change_description": ": Re-enable intra-merge parallelism except for terms, norms, and doc values.\nRelated to", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "It was noticed that the CMS intra-merge behavior was not fully tested. In an effort to do this, a change to override when the intra-merge scheduler is used has been drafted. #13475 This PR has exposed many existing assertions that may all be weird testing failures. But a couple are more worrisome. In particular: Will periodically fail with an NPE on the sort : No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13548", "change_description": ": Refactor and javadoc update for KNN vector writer classes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13562", "change_description": ": Add Intervals.regexp and Intervals.range methods to produce  IntervalsSource\nfor regexp and range queries.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We already have convenient functions for contructing IntervalsSource for wildcard and fuzzy functions. This adds functions for regexp and range as well.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13625", "change_description": ": Remove BitSet#nextSetBit code duplication.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "After merging #13559 I noticed an opportunity to remove some redundant code in the nextSetBit implementations.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13285", "change_description": ": Early terminate graph searches of AbstractVectorSimilarityQuery to follow timeout set from\nIndexSearcher#setTimeout(QueryTimeout).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Along similar lines of #13202 , adding timeout support for AbstractVectorSimilarityQuery which performs similarity-based vector searches While the graph search happens inside #scorer , it may go over the configured QueryTimeout and we can early terminate it to return whatever partial results are found.. One inherent benefit we have for exact search is that we return a lazy-loading iterator over all vectors, so this is inherently covered by the TimeLimitingBulkScorer (as opposed to exact search of AbstractKnnVectorQuery which manually goes over all vectors to retain the topK during #rewrite )", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13633", "change_description": ": Add ability to read/write knn vector values to a MemoryIndex.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This adds the capability to write and read knn vectors from a memory index. It currently enforces the lower level codec restriction of only one knn vector per field and it does not support nearest neighbor search (seemed unnecessary to me as it will always be the same doc...). closes : #13584", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#12627", "change_description": ": patch HNSW graphs to improve reachability of all nodes from entry points", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I work for Amazon Retail Product search and we are using Lucene KNN for semantic search of products. We recently noticed that the hnsw graphs generated are not always strongly connected and in worst case scenario some products may be undiscoverable. Connectedness of Hierarchical graph can be complicated, so below I am mentioning my experiment details. I took the Lucene indexes from our production servers and for each segment (hnsw graph) I did following test. At each level graph I took the same entry point, the entry point of HNSW graph, checked how many nodes are reachable from this entrypoint. Note that connectedness at each level was checked independently of other levels. Sample code attached. My observations are as below. Although it is fairly easy to be reproduced with Amazon's internal data, but I found it really hard to reproduce it with random vectors. I will attached a simple main class of my test and the parameters that I tried. I only got some disconnectedness as listed in below table. For higher number of vectors, the execution becomes painfully slow because of hnsw graph creation. I will continue to work to find some  parameters for reproducibilty and also explore open source datasets for the same. DIS-L0 means: %age of disconnected nodes in graph at level-0 of HNSW graph. No direct solution comes to my mind but as per my discussion in this email thread . I will look into the part where some connections are removed to maintain the Max-Conn property (diversity check etc.). Lucene version : 9.7 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13201", "change_description": ": Better cost estimation on MultiTermQuery over few terms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The existing cost estimate for multi-term queries was taking the worst-case scenario (e.g the query matches every term in the field), to provide an upper bound on the cost. In cases where the MultiTermQuery matches a small number of terms, we can provide a tighter estimate by summing the doc frequencies over those terms. Fixes #13029", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13735", "change_description": ": Migrate monitor package usage of deprecated IndexSearcher#search(Query, Collector)\nto IndexSearcher#search(Query, CollectorManager).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Relates to: #12892", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Improvements", "change_id": "GITHUB#13746", "change_description": ": Introduce ProfilerCollectorManager to parallelize search when using ProfilerCollector.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "ProfilerCollector did not have until now a corresponding collector manager. This commit introduces one and switches TestProfilerCollector to use search concurrency and move away from the deprecated search(Query, Collector) method. Note that the collector manager does not support children collectors. Figuring out a generic API for that is rather complicated. Users can always create their own collector manager depending on their specific collector hierarchy. Relates to #12892", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13439", "change_description": ": Avoid unnecessary memory allocation in PackedLongValues#Iterator.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We always allocate a long array of page size for a new PackedLongValues#Iterator instance, which is not necessary when packing a small number of values. this is more evident in the scenario of high-frequency flush operations", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13322", "change_description": ": Implement Weight#count for vector values in the FieldExistsQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Considering only one vector value can be indexed per doc, we can use the Float/ByteVectorValues#size method to implement Weight#count for vector values in the FieldExistsQuery", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13454", "change_description": ": MultiTermQuery returns null ScoreSupplier in cases where\nno query terms are present in the index segment", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "MultiTermQuery return null for ScoreSupplier if there are no terms in an index that match query terms. With the introduction of PR #12156 we saw degradation in performance of bool queries where one of the mandatory clauses is a TermInSetQuery with query terms not present in the field. Before for such cases TermsInSetQuery returned null for ScoreSupplier which would shortcut the whole bool query. This PR adds ability for MultiTermQuery to return null for ScoreSupplier if a field doesn't contain any query terms. Relates to PR #12156", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13431", "change_description": ": Replace TreeMap and use compiled Patterns in Japanese UserDictionary.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Replace TreeMap by a List of Match in the lookup method. There was a todo on the TreeMap which caught my attention. I ran some benchmarks and I see 5% performance improvement, but the gain is more on the memory side where we allocate less intermediate structures and less auto-boxing during the lookup. Use compiled Pattern when reading the dictionary.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#12941", "change_description": ": Don't preserve auxiliary buffer contents in LSBRadixSorter if it grows.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "#12844 introduced upper bounds to array growth to avoid overallocation in cases where we can determine a bound. This task is to go through uses of the grow API, identify other cases where we know the upper bound, and replace with growInRange . The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13175", "change_description": ": Stop double-checking priority queue inserts in some FacetCount classes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There are several occurrences in our code where we check that an incoming value can be inserted in a priority queue and then call insertWithOverflow , which re-does the check. Let's stop checking before calling insertWithOverflow . Credit to @epotyom for noticing this in AbstractSortedSetDocValueFacetCounts . The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13538", "change_description": ": Slightly reduce heap usage for HNSW and scalar quantized vector writers.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This slightly reduces heap utilization for KnnIndex writers by: This does change an experimental API (FlatVectorFormats) slightly to allow reuse.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#12100", "change_description": ": WordBreakSpellChecker.suggestWordBreaks now does a breadth first search, allowing it to return\nbetter matches with fewer evaluations", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Spinning this perf improvement off from #12077 ... #12077 (comment) ... strange to me that this method is essentially doing a \"depth first\" walk of the possible splits, given that it's working a character at a time and the only possible BreakSuggestionSortMethod values start with NUM_CHANGES_THEN_.... it seems like we could get \"better\" results, with lower values of maxEvaluations , and less recursion (even if maxChanges is very large) if the logic was something like: ... Since posting that comment, I realized that because we can inspect the size of the suggestions , and we know that candidates with fewer NUM_CHANGES... are always better, a breadth first approach can can actually short-circuit and return much earlier then I originally suggested, if the suggestions queue is already \"full\" ... The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13582", "change_description": ": Stop requiring MaxScoreBulkScorer's outer window from having at\nleast INNER_WINDOW_SIZE docs.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently MaxScoreBulkScorer requires its \"outer\" window to be at least WINDOW_SIZE . The intuition there was that we should make sure we should use the whole range of the bit set that we are using to collect matches. The downside is that it may force us to use an upper level in the skip list that has worse upper bounds for the scores. luceneutil suggests that this is not a good trade-off: removing this requirement makes Or2Terms2StopWords a bit slower, but OrHighMin and OrHighRare much faster:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13570", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit adds a ref counted shared arena to support aggregating segment files into a single Arena.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13574", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit uses IOContext.READONCE in more places where the index input  is clearly being read once by the thread opening it. We can then enforce that segment files are only opened with READONCE, in the test specific Mock directory wrapper. Much of the changes in this PR update individual test usage, but there is one non-test change to Directory::copyFrom. relates #13570", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13535", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Use a confined Arena for IOContext.READONCE. This change will require inputs opened with READONCE to be consumed and closed on the creating thread. Further testing and assertions will need to be added. relates #13325", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13585", "change_description": ": Lucene912PostingsFormat, the new default postings format, now\nonly has 2 levels of skip data, which are inlined into postings instead of\nbeing stored at the end of postings lists. This translates into better\nperformance for queries that need skipping such as conjunctions.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This updates the postings format in order to inline skip data into postings. This format is generally similar to the current Lucene99PostingsFormat , e.g. it shares the same block encoding logic, but it has a few differences: In general, I found that the fact that skip data is inlined may slow down a bit queries that don't need skip data at all (e.g. CountOrXXX tasks that never advance of consult impacts) and speed up a bit queries that advance by small intervals. The fact that the greatest level only allows skipping 4096 docs at once means that we're slower at advancing by large intervals, but data suggests that it doesn't significantly hurt performance. Phrase queries and term queries sorted by field are slower for reasons that I haven't understood yet. These results were produced on wikibigall without inter-segment concurrency.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13581", "change_description": ": OnHeapHnswGraph no longer allocates a lock for every graph node", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Addresses #13580 by adding a locking wrapper for OnHeapHnswGraph's NeighborArrays, and supplying this when running concurrent merges. With this:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13636", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Our postings use a layout that helps take advantage of Java's auto-vectorization to be reasonably fast to decode. But we can make it a bit faster by using explicit vectorization on MemorySegment: This approach only works when the Directory uses MemorySegmentIndexInput under the hood, ie. MMapDirectory on JDK 21+. The ForUtilBenchmark micro benchmark reports the following results: And luceneutil on wikibigall reports the following (only look at queries with low p-values):", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13658", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This updates file formats to compute prefix sums by summing up 8 deltas per long at the same time if the number of bits per value is 4 or less, and 4 deltas per long at the same time if the number of bits per value is between 5 included and 11 included. Otherwise, we keep summing up 2 deltas per long like today. The PostingDecodingUtil was slightly modified due to the fact that more numbers of bits per value now need to apply different shifts to the input data. E.g. now that we store integers that require 5 bits per value as 16-bit integers under the hood rather than 8, we extract the first values by shifting by 16-5=11, 16-2 5=6 and 16-3 5=1 and then decode tail values from the remaining bit per 16-bit integer. Micro benchmarks suggest a noticeable speedup for prefix sums and bits per value 2, 3, 4 when we now sum up 8 values at once instead of 2, and a minor speedup otherwise. For reference, wikibigall is frequently using 2, 3 or 4 bits per value on blocks of doc deltas of stop words (\"the\", \"a\", \"1\", etc.) Before (without enabling the vector module): After (without enabling the vector module): After (with enabling the vector module - Apple M3 - preferredBitSize=128):", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13587", "change_description": ": Use Max WAND optimizations with ToParentBlockJoinQuery when using ScoreMode.Max", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Updates ToParentBlockJoinQuery to propagate the min competitive score when using ScoreMode.Max", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13742", "change_description": ": Reorder checks in LRUQueryCache#count", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Reorder the checks in LRUQueryCache#count to first check the cached version. Addresses or closes #13717", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Optimizations", "change_id": "GITHUB#13697", "change_description": ": Add a bulk scorer to ToParentBlockJoinQuery, which delegates to the bulk scorer of the child query.\n This should speed up query evaluation when the child query has a specialized bulk scorer, such as disjunctive queries.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "No description provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Changes in runtime behavior", "change_id": "GITHUB#13472", "change_description": ": When an executor is provided to the IndexSearcher constructor, the searcher now executes tasks on the\nthread that invoked a search as well as its configured executor. Users should reduce the executor's thread-count by 1\nto retain the previous level of parallelism. Moreover, it is now possible to start searches from the same executor\nthat is configured in the IndexSearcher without risk of deadlocking. A separate executor for starting searches is no\nlonger required.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When executing N tasks and waiting on the result of all of them, we should only fork N - 1 times and execute one task on the calling thread that is getting blocked anyway. This saves at least one context switch, removes the need for any reentrancy protection, and makes better use of available CPU resources.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13498", "change_description": ": Avoid performance regression by constructing lazily the PointTree in NumericComparator,", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We notice a performance regression in Elasticsearch which we could trace back to the changes in #13199 . This change makes constructing NumericComparators more expensive die to the construction of a PointTree eagerly in the constructor. This is particular noticeable when the PointTree is not used later and  your constructing several of them. This. commit proposes to build the PointTree lazily. The original PR proposed it but it was changed due to simplicity. We are only targeting the 9.x line as main has already diverged and doing it in a different way.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13384", "change_description": ": Fix highlighter to use longer passages instead of shorter individual terms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There is a bug in the DefaultPassageFormatter. It will take an passage even if it results in a shorter or more fragmented passage (highlight). The one scenario I have is when there is a phrase match which overlaps with some term matches. This is demonstrated in the attached unit tests.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13463", "change_description": ": Address bug in MultiLeafKnnCollector causing #minCompetitiveSimilarity to stay artificially low in\nsome corner cases.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Addresses the bug described in GH #13462 Relates to #12962", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13553", "change_description": ": Correct RamUsageEstimate for scalar quantized knn vector formats so that raw vectors are correctly\naccounted for.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I still need to write a test, but wanted to open this PR early. Scalar Quantized vector writer ram usage estimates completely ignores the raw float vectors. Meaning, if you have flush based on ram usage configured, you can easily overshoot that estimate and cause an OOM.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13615", "change_description": ": Correct scalar quantization when used in conjunction with COSINE similarity. Vectors are normalized\nbefore quantization to ensure the cosine similarity is correctly calculated.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When quantizing vectors in a COSINE vector space, we normalize them. However, there is a bug when building the quantizer quantiles and we didn't always use the normalized vectors. Consequently, we would end up with poorly configured quantiles and recall will drop significantly (especially in sensitive cases like int4). closes : #13614", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13627", "change_description": ": Fix race condition on flush for DWPT seqNo generation.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There is a tricky race condition with DWPT threads. It is possible that a flush starts by advancing the deleteQueue (in charge of creating seqNo). Thus, the referenced deleteQueue, there should be a cap on the number of actions left. However, it is possible after the advance, but before the DWPT are actually marked for flush, the DWPT gets freed and taken again to be used. To replicate this extreme behavior, see: https://github.com/apache/lucene/compare/main...benwtrent:lucene:test-replicate-and-debug-13127?expand=1 This commit will prevent DWPT from being added back to the free list if their queue has been advanced. This is because the maxSeqNo for that queue was created accounting only for the current number of active threads. If the thread gets passed out again and still references the already advanced queue, it is possible that seqNo actually advances past the set maxSeqNo . closes : #13127 closes : #13571", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13646", "change_description": ": Fix rare test bug in TestLongValueFacetCounts that was introduced in 9.6.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When backporting GH#13568 I realized we have some incorrect usages of RandomNumbers#randomIntBetween in TestLongValueFacetCounts. We use this in place of Random#nextInt for 9x code to work with jdk11, but #nextInt expects the upper bound to be exclusive while RandomNumbers is inclusive.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13691", "change_description": ": Fix incorrect exponent value in explain of SigmoidFunction.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Coming from opensearch-project/OpenSearch#14921 . This PR fixes the correct value of exponent from pivot in the Sigmoid function.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13703", "change_description": ": Fix bug in LatLonPoint queries where narrow polygons close to latitude 90 don't\nmatch any points due to an Integer overflow.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I found an edge case were given an enough narrow polygon close to latitude, then it will not match any point even where theoretically the point should match. The issue is easy to re produce with the following test: It seems the issue is in the component predicate we are using to speed up this queries which reject those points. This test will be successful when run on an index but fail when run on doc values. No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13641", "change_description": ": Unify how KnnFormats handle missing fields and correctly handle missing vector fields when\nmerging segments.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "It is possible to inappropriately use the knn formats and attempt to merge segments with mismatched field names. None of the formats actually check for null , they just all assume that the field entry exists. I opted to throw an IllegalArgumentException here to keep with the current behavior, but prevent an unexpected NPE. Additionally, this unifies the scalar quantized formats, these checked for null values and returned null. But we should behave uniformly across all formats. closes : #13626", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13519", "change_description": ": 8 bit scalar vector quantization is no longer\nsupported: it was buggy starting in 9.11 (", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Based on some of the benchmarking tests that I ran from OpenSearch, there is a significant drop in recall ( appx. 0.03) for 8 bits irrespective of space type, confidence interval or few other parameters. For the same configuration, the recall for 7 bits is atleast greater than 0.85. As part of quantization , after normalizing each dimension of the vector into [0 to 2^bits - 1] range, we are casting it into byte to bring it into byte range of [-128 to 127]. For 7 bits, we are normalizing each value into 0 to 127 which is already in the byte range. So, there is no rotation or shifting of data. But, for 8 bits any vector dimension which is within 128 to 255 range after normalization, the sign and magnitude changes when it is type casted into byte which leads to a non-uniform shifting or distribution of data. As per my understanding, this is the potential root cause for this huge drop in recall. To validate this, I have updated the quantization code and tested it against L2 space type by linearly shifting (subtracting 128) each dimension after normalizing it into 0 to 255 range such that each dimension is uniformly distributed and within the byte range(finally round it and clip it to handle edge cases) of -128 to 127. With these changes, we get a min. recall of 0.86 for the same configuration. Note - The below pseudo code is not a fix and it is a different quantization technique used to validate the root cause. This works only for L2 spacetype because L2 is shift invariant but other spacetypes like cosinesimil and inner product are not shift invariant. @benwtrent @mikemccand Can you please take a look and confirm if you see this issue when tested with lucene-util? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13197", "change_description": ": 8 bit scalar vector quantization is no longer\nsupported: it was buggy starting in 9.11 (", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR is a culmination of some various streams of work: The idea of further scalar quantization gives users the choice between: I didn't add more panama vector APIs as I think trying to micro-optimize int4 for anything other than dot-product was a fools errand. Additionally, I only focused on ARM. I experimented with trying to get better performance on other architectures, but didn't get very far, so I fall back to dotProduct.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Bug Fixes", "change_id": "GITHUB#13799", "change_description": ": Disable intra-merge parallelism for all structures but kNN vectors.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "After adjusting tests that truly exercise intra-merge parallelism, more issues have arisen. See: #13798 To be risk adverse & due to the soon to be released/freezed Lucene 10 & 9.12, I am reverting all intra-merge parallelism, except for the parallelism when merging HNSW graphs. Merging other structures was never really enabled in a release (we disabled it in a bugfix for Lucene 9.11). While this is frustrating as it seems like we leaving lots of perf on the floor, I am err'ing on the side of safety here. In Lucene 10, we can work on incrementally reenabling intra-merge parallelism. closes : #13798", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Build", "change_id": "GITHUB#13695", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "On main today, with Java 21 on a recent Arch Linux, if I run ./gradlew clean then ./gradlew check, all is good. It passes.But if I run ./gradlew check again, I sometimes (not always -- might take another ./gradlew check) get these false warnings about unreferenced license files:It's sometimes a smaller set of (still false positive) warnings, which is also odd.For the longest time I've been assuming I'm doing something wrong :) But then I noticed @msokolov also sees this sometimes, so it's not just me. Does anyone else see these?[I also raised this dev list discussion about it].", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Build", "change_id": "GITHUB#13696", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Revert changes by #12150 in jar-checks.gradle, because tasks in this file shares internal state between tasks without using files. Because of this all tasks here must always execute together, so they cannot define task outputs. This closes #13695", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.12.0", "change_type": "Other", "change_id": "GITHUB#13720", "change_description": ": Add float comparison based on unit of least precision and use it to stop test failures caused by float\nsummation not being associative in IEEE 754.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "@uschindler 's Windows CI build caught this failing test+seed , and it repros on my Linux dev box too: It looks likely this is an OB1 float ULP issue, maybe wrong/different order of float operations between the test and the facet association code? No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}

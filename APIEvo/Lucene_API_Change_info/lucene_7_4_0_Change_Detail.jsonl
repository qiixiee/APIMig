{"library_version": "7.4.0", "change_type": "Upgrading", "change_id": "LUCENE-8344", "change_description": ": If you are using the AnalyzingSuggester or FuzzySuggester subclass, and if you\nexplicitly use the preservePositionIncrements=false setting (not the default), then you ought\nto rebuild your suggester index. If you don't, queries or indexed data with trailing position\ngaps (e.g. stop words) may not work correctly.", "change_title": "TokenStreamToAutomaton doesn't ignore trailing posInc when preservePositionIncrements=false", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "TokenStreamToAutomaton in Lucene core is used by the AnalyzingSuggester (incl. FuzzySuggester subclass ) and NRT Document Suggester and soon the SolrTextTagger.  It has a setting preservePositionIncrements defaulting to true.  If it's set to false (e.g. to ignore stopwords) and if there is a trailing position increment greater than 1, TS2A will still add position increments (holes) into the automata even though it was configured not to. I'm filing this issue separate from LUCENE-8332 where I first found it.  The fix is very simple but I'm concerned about back-compat ramifications so I'm filing it separately.  I'll attach a patch to show the problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927133/LUCENE-8344.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "API Changes", "change_id": "LUCENE-8242", "change_description": ": IndexSearcher.createNormalizedWeight() has been deprecated.\nInstead use IndexSearcher.createWeight(), rewriting the query first.", "change_title": "Rename IndexSearcher.createNormalizedWeight()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "We don't have Weight normalization since LUCENE-7368, so this method name is just plain wrong.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917944/LUCENE-8242.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "API Changes", "change_id": "LUCENE-8248", "change_description": ": MergePolicyWrapper is renamed to FilterMergePolicy and now\nalso overrides getMaxCFSSegmentSizeMB", "change_title": "Rename MergePolicyWrapper to FilterMergePolicy and override all of MergePolicy", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "MergePolicy.getMaxCFSSegmentSizeMB is final, but the corresponding setter is not, which means that overriding it with anything other than a trivial delegation can only lead to confusion. The patch makes the method final and removes the trivial implementations from MergePolicyWrapper and NoMergePolicy. mikemccand also pointed out that the class name is nonstandard for similar adapter classes in Lucene, which are usually Filter*.java. Personally I was looking for MergePolicyAdapter, but if there is a prevailing convention here around Filter, does it make sense to change this class's name to FilterMergePolicy?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12918993/LUCENE-8248.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "API Changes", "change_id": "LUCENE-8303", "change_description": ": LiveDocsFormat is now only responsible for (de)serialization of\nlive docs.", "change_title": "Make LiveDocsFormat only responsible for (de)serialization of live docs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "We could simplify live docs by only making the format responsible from reading/writing a Bits instance that represents live docs while today the format is also involved to delete documents since it needs to be able to provide mutable bits.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922638/LUCENE-8303.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8309", "change_description": ": Live docs are no longer backed by a FixedBitSet.", "change_title": "Don't use mutable FixedBitSets as live docs Bits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "Simon mentioned this idea first: it would be nice to not expose mutable fixedbitsets as live docs, which makes it easy for consumers of live docs to resurrect some documents by casting to a FixedBitSet and potentially corrupt the index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12923056/LUCENE-8309.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8330", "change_description": ": Detach IndexWriter from MergePolicy. MergePolicy now instead of\nrequiring IndexWriter as a hard dependency expects a MergeContext which\nIndexWriter implements.", "change_title": "Detach IndexWriter from MergePolicy", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "This change introduces a new MergePolicy.MergeContext interface     that is easy to mock and cuts over all instances of IW to MergeContext.     Since IW now implements MergeContext the cut over is straight forward.     This reduces the exposed API available in MP dramatically and allows     efficient testing without relying on IW to improve the coverage and     testability of our MP implementations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12925007/LUCENE-8330.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8200", "change_description": ": Allow doc-values to be updated atomically together\nwith a document. Doc-Values updates now can be used as a soft-delete\nmechanism to all keeping several version of a document or already\ndeleted documents around for later reuse. See \"IW.softUpdateDocument(...)\"\nfor reference.", "change_title": "Allow doc-values to be updated atomically together with a document", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "None", "detail_description": "Today we can only update a document by deleting all previously indexed documents for the given term. In some cases like when deletes are not `final` in the way that documents that are marked as deleted should not be merged away a `soft-delete` is needed which is possible when doc-values updates can be done atomically just like delete and add in updateDocument(s) This change introduces such a soft update that reuses all code paths from deletes to update all previously updated documents for a given term instead of marking it as deleted. This is a spinnoff from LUCENE-8198", "patch_link": "https://issues.apache.org/jira/secure/attachment/12914301/LUCENE-8200.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8197", "change_description": ": A new FeatureField makes it easy and efficient to integrate\nstatic relevance signals into the final score.", "change_title": "Make top-k queries fast when static scoring signals are incorporated into the score", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "Block-max WAND (LUCENE-8135) and some earlier issues made Lucene faster at computing the top-k matches of boolean queries. It is quite frequent that users want to improve ranking and end up scoring with a formula that could look like bm25_score + w * log(alpha + pagerank) (w and alpha being constants, and pagerank being a per-document field value). You could do this with doc values and FunctionScoreQuery but unfortunately this will remove the ability to optimize top-k queries since the scoring formula becomes opaque to Lucene. I'd like to add a new field that allows to store such scoring signals as term frequencies, and new queries that could produce log(alpha + pagerank) as a score. Then implementing the above formula can be done by boosting this query with a boost equal to w and adding this boosted query as a SHOULD clause of a BooleanQuery. This would give Lucene the ability to compute top-k hits faster, especially but not only if the index is sorted by decreasing pagerank.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12914872/LUCENE-8197.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8202", "change_description": ": Add a FixedShingleFilter", "change_title": "Add a FixedShingleFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "In LUCENE-3475 I tried to make a ShingleGraphFilter that could accept and emit arbitrary graphs, while duplicating all the functionality of the existing ShingleFilter.  This ends up being extremely hairy, and doesn't play well with query parsers. I'd like to step back and try and create a simpler shingle filter that can be used for index-time phrase tokenization only.  It will have a single fixed shingle size, can deal with single-token synonyms, and won't emit unigrams.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916093/LUCENE-8202-fixes.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8125", "change_description": ": ICUTokenizer support for emoji/emoji sequence tokens.", "change_title": "emoji sequence support in ICUTokenizer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "trunk,7.4", "detail_description": "uax29 word break rules already know how to handle these correctly, we just need to assign them a token type. This is better than users trying to do this with custom rules (e.g. LUCENE-7916) because they are script-independent (common/inherited).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12905218/LUCENE-8125.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8196", "change_description": ",", "change_title": "Add IntervalQuery and IntervalsSource to expose minimum interval semantics across term fields", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "This ticket proposes an alternative implementation of the SpanQuery family that uses minimum-interval semantics from http://vigna.di.unimi.it/ftp/papers/EfficientAlgorithmsMinimalIntervalSemantics.pdf to implement positional queries across term-based fields.  Rather than using TermQueries to construct the interval operators, as in LUCENE-2878 or the current Spans implementation, we instead use a new IntervalsSource object, which will produce IntervalIterators over a particular segment and field.  These are constructed using various static helper methods, and can then be passed to a new IntervalQuery which will return documents that contain one or more intervals so defined.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916576/LUCENE-8196.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8300", "change_description": ",", "change_title": "Add unordered-distinct IntervalsSource", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "mattweber pointed out on LUCENE-8196 that Intervals.unordered() doesn't check to see if its subintervals overlap, which means that for example Intervals.unordered(Intervals.term(\"a\"), Intervals.term(\"a\")) would match a document with a appearing only once.  This ticket will introduce a new function, Intervals.unordered_distinct(), that ensures that all subintervals within an unordered interval do not overlap.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12925718/LUCENE-8300.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8233", "change_description": ": Add support for soft deletes to IndexWriter delete accounting.\nSoft deletes are accounted for inside the index writer and therefor also\nby merge policies. A SoftDeletesRetentionMergePolicy is added that allows\nto selectively carry over soft_deleted document across merges for retention\npolicies", "change_title": "Add support for soft deletes to IndexWriter delete accounting", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "This change adds support for soft deletes as a fully supported feature by the index writer. Soft deletes are accounted for inside the index writer and therefor also by merge policies. This change also adds a SoftDeletesRetentionMergePolicy that allows users to selectively carry over soft_deleted document across merges for retention policies. The merge policy selects documents that should be kept around in the merged segment based on a user provided query.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917536/LUCENE-8232.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8237", "change_description": ": Add a SoftDeletesDirectoryReaderWrapper that allows to respect\nsoft deletes if the reader is opened form a directory.", "change_title": "Add a SoftDeletesDirectoryReaderWrapper", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "This adds support for soft deletes if the reader is opened form a directory. Today we only support soft deletes for NRT readers, this change allows to wrap existing DirectoryReader with a SoftDeletesDirectoryReaderWrapper to also filter out soft deletes in the case of a non-NRT reader.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12918061/LUCENE-8237.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8229", "change_description": ",", "change_title": "Add a method to Weight to retrieve matches for a single document", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "The ability to find out exactly what a query has matched on is a fairly frequent feature request, and would also make highlighters much easier to implement.  There have been a few attempts at doing this, including adding positions to Scorers, or re-writing queries as Spans, but these all either compromise general performance or involve up-front knowledge of all queries. Instead, I propose adding a method to Weight that exposes an iterator over matches in a particular document and field.  It should be used in a similar manner to explain() - ie, just for TopDocs, not as part of the scoring loop, which relieves some of the pressure on performance.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12918676/LUCENE-8229_small_improvements.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8270", "change_description": ",", "change_title": "Remove MatchesIterator.term()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "As discussed on LUCENE-8268, we don't have a clear use-case for this yet, and it's complicating adding Matches to phrase queries, so let's just remove it for now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12920292/LUCENE-8270.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8249", "change_description": ": Implement Matches API for phrase queries", "change_title": "Add matches to exact PhraseQuery and MultiPhraseQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "ExactPhraseScorer can be rejigged fairly easily to expose a MatchesIterator", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922661/LUCENE-8249.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8246", "change_description": ": Allow to customize the number of deletes a merge claims. This\nhelps merge policies in the soft-delete case to correctly implement retention\npolicies without triggering uncessary merges.", "change_title": "Allow to customize the number of deletes a merge claims", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "With the introduction of soft deletes no every merge claims all documents that are marked as deleted in the segment readers. MergePolicies still need to do accurate accounting in order to select segments for merging and need to decide if segments are merged. This chance allows the merge policy to customize the number of deletes a merge of a segment claims.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12918156/LUCENE-8246.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8231", "change_description": ": A new analysis module (nori) similar to Kuromoji\nbut to handle Korean using mecab-ko-dic and morphological analysis.", "change_title": "Nori, a Korean analyzer based on mecab-ko-dic", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "There is a dictionary similar to IPADIC but for Korean called mecab-ko-dic: It is available under an Apache license here: https://bitbucket.org/eunjeon/mecab-ko-dic This dictionary was built with MeCab, it defines a format for the features adapted for the Korean language. Since the Kuromoji tokenizer uses the same format for the morphological analysis (left cost + right cost + word cost) I tried to adapt the module to handle Korean with the mecab-ko-dic. I've started with a POC that copies the Kuromoji module and adapts it for the mecab-ko-dic. I used the same classes to build and read the dictionary but I had to make some modifications to handle the differences with the IPADIC and Japanese.  The resulting binary dictionary takes 28MB on disk, it's bigger than the IPADIC but mainly because the source is bigger and there are a lot of compound and inflect terms that define a group of terms and the segmentation that can be applied.  I attached the patch that contains this new Korean module called godori nori. It is an adaptation of the Kuromoji module so currently the two modules don't share any code. I wanted to validate the approach first and check the relevancy of the results. I don't speak Korean so I used the relevancy tests that was added for another Korean tokenizer (https://issues.apache.org/jira/browse/LUCENE-4956) and tested the output against mecab-ko which is the official fork of mecab to use the mecab-ko-dic. I had to simplify the JapaneseTokenizer, my version removes the nBest output and the decomposition of too long tokens. I also modified the handling of whitespaces since they are important in Korean. Whitespaces that appear before a term are attached to that term and this information is used to compute a penalty based on the Part of Speech of the token. The penalty cost is a feature added to mecab-ko to handle  morphemes that should not appear after a morpheme and is described in the mecab-ko page: https://bitbucket.org/eunjeon/mecab-ko Ignoring whitespaces is also more inlined with the official MeCab library which attach the whitespaces to the term that follows. I also added a decompounder filter that expand the compounds and inflects defined in the dictionary and a part of speech filter similar to the Japanese that removes the morpheme that are not useful for relevance (suffix, prefix, interjection, ...). These filters don't play well with the tokenizer if it can  output multiple paths (nBest output for instance) so for simplicity I removed this ability and the Korean tokenizer only outputs the best path. I compared the result with mecab-ko to confirm that the analyzer is working and ran the relevancy test that is defined in HantecRel.java included in the patch (written by Robert for another Korean analyzer). Here are the results: I find the results very promising so I plan to continue to work on this project. I started to extract the part of the code that could be shared with the Kuromoji module but I wanted to share the status and this POC first to confirm that this approach is viable. The advantages of using the same model than the Japanese analyzer are multiple: we don't have a Korean analyzer at the moment , the resulting dictionary is small compared to other libraries that use the mecab-ko-dic (the FST takes only 5.4MB) and the Tokenizer prunes the lattice on the fly to select the best path efficiently. The dictionary can be built directly from the godori module with the following command: ant regenerate (you need to create the resource directory (mkdir lucene/analysis/godori/src/resources/org/apache/lucene/analysis/ko/dict) first since the dictionary is not included in the patch). I've also added some minimal tests in the module to play with the analysis.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916846/LUCENE-8231-remap-hangul.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8265", "change_description": ": WordDelimter/GraphFilter now have an option to skip tokens\nmarked with KeywordAttribute", "change_title": "WordDelimiterFilter should pass through terms marked as keywords", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "This will help in cases where some terms containing separator characters should be split, but others should not.  For example, this will enable a filter that identifies things that look like fractions and identifies them as keywords so that 1/2 does not become 12, while doing splitting and joining on terms that look like part numbers containing slashes, eg something like \"sn-999123/1\" might sometimes be written \"sn-999123-1\".", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8297", "change_description": ": Add IW#tryUpdateDocValues(Reader, int, Fields...) IndexWriter can\nupdate doc values for a specific term but this might affect all documents\ncontaining the term. With tryUpdateDocValues users can update doc-values\nfields for individual documents. This allows for instance to soft-delete\nindividual documents.", "change_title": "Add IW#tryUpdateDocValues(Reader, int, Fields...)", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "IndexWriter can update doc values for a specific term but this might     affect all documents containing the term. With tryUpdateDocValues     users can update doc-values fields for individual documents. This allows     for instance to soft-delete individual documents.     The new method shares most of it's code with tryDeleteDocuments.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922188/LUCENE-8297.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8298", "change_description": ": Allow DocValues updates to reset a value. Passing a DV field with a null\nvalue to IW#updateDocValues or IW#tryUpdateDocValues will now remove the value from the\nprovided document. This allows to undelete a soft-deleted document unless it's been claimed\nby a merge.", "change_title": "Allow DocValues updates to reset a value", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "Today once a document has a value in a certain DV field this values     can only be changed but not removed. While resetting / removing a value     from a field is certainly a corner case it can be used to undelete a     soft-deleted document unless it's merged away.     This allows to rollback changes without rolling back to another commitpoint     or trashing all uncommitted changes. In certain cenarios it can be used to     \"repair\" history of documents in distributed systems.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922657/LUCENE-8298.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8273", "change_description": ": ConditionalTokenFilter allows analysis chains to skip particular token\nfilters based on the attributes of the current token. This generalises the keyword\ntoken logic currently used for stemmers and WDF.  It is integrated into\nCustomAnalyzer by using the `when` and `whenTerm` builder methods, and a new\nProtectedTermFilter is added as an example.", "change_title": "Add a ConditionalTokenFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "Spinoff of LUCENE-8265.  It would be useful to be able to wrap a TokenFilter in such a way that it could optionally be bypassed based on the current state of the TokenStream.  This could be used to, for example, only apply WordDelimiterFilter to terms that contain hyphens.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924295/LUCENE-8273-part2-rebased.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8310", "change_description": ": Ensure IndexFileDeleter accounts for pending deletes. Today we fail\ncreating the IndexWriter when the directory has a pending delete. Yet, this\nis mainly done to prevent writing still existing files more than once.\nIndexFileDeleter already accounts for that for existing files which we can\nnow use to also take pending deletes into account which ensures that all file\ngenerations per segment always go forward.", "change_title": "Relax IWs check on pending deletes", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "I recently fixed the check in IW to fail if there are pending deletes. After upgrading to a snapshot I realized the consequences of this check. It made most of our usage of IW to for instance prepare commit metadata, rollback to safe commit-points etc. impossible since we have to now busy wait on top of directory util all deletes are actually gone even though that we can guarantee that our history always goes forward. ie we are truly append-only in the sense of never reusing segment generations. The fix that I made was basically return false from a checkPendingDeletions in a directory wrapper to work around it. I do expect this to happen to a lot of lucene users even if they use IW correctly. My proposal is to make the check in IW a bit more sophisticated and only fail if there are pending deletes that are in the future from a generation perspective. We really don't care about files from the past. My patch checks the segment generation of each pending file which is safe since that is the same procedure we apply in IndexFileDeleter to inc reference etc. and only fail if the pending delete is in the future.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12923505/LUCENE-8310.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-7960", "change_description": ": Add preserveOriginal option to the NGram and EdgeNGram filters.", "change_title": "NGram filters -- preserve the original token when it is outside the min/max size range", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "When ngram or edgengram filters are used, any terms that are shorter than the minGramSize are completely removed from the token stream. This is probably 100% what was intended, but I've seen it cause a lot of problems for users.  I am not suggesting that the default behavior be changed.  That would be far too disruptive to the existing user base. I do think there should be a new boolean option, with a name like keepShortTerms, that defaults to false, to allow the short terms to be preserved.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924302/LUCENE-7960.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8335", "change_description": ": Enforce soft-deletes field up-front. Soft deletes field must be marked\nas such once it's introduced and can't be changed after the fact.", "change_title": "Do not allow changing soft-deletes field", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "None", "detail_description": "Today we do not enforce an index to use a single soft-deletes field. A user can create an index with one soft-deletes field then open an IW with another field or add an index with a different soft-deletes field. This should not be allowed and reported the error to users as soon as possible.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12926255/LUCENE-8335.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "New Features", "change_id": "LUCENE-8332", "change_description": ": New ConcatenateGraphFilter for concatenating all tokens into one (or more\nin the event of a graph input).  This is useful for fast analyzed exact-match lookup,\nsuggesters, and as a component of a named entity recognition system.  This was excised\nout of CompletionTokenStream in the NRT doc suggester.", "change_title": "New ConcatenateGraphFilter (move/rename CompletionTokenStream)", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "Lets move and rename the CompletionTokenStream in the suggest module into the analysis module renamed as ConcatenateGraphFilter. See comments in LUCENE-8323 leading to this idea. Such a TokenStream (or TokenFilter?) has several uses: It will need a factory – a TokenFilterFactory, even though we don't have a TokenFilter based subclass of TokenStream. It appears there is no back-compat concern in it suddenly disappearing from the suggest module as it's marked experimental and it only seems to be public now perhaps due to some technicality (it has package level constructors).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12926180/LUCENE-8332.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8221", "change_description": ": MoreLikeThis.setMaxDocFreqPct can easily int-overflow on larger\nindexes.", "change_title": "MoreLikeThis.setMaxDocFreqPct can easily int-overflow on larger indexes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "The above overflows integer range into negative numbers on even fairly small indexes (for maxPercentage = 75, it happens for just over 28 million documents. We should make the computations on long range so that it doesn't overflow and have a more strict argument validation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916147/LUCENE-8221.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8266", "change_description": ": Detect bogus tiles when creating a standard polygon\nand throw a TileException.", "change_title": "Standard polygon is created with bogus tile", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.4,8.0", "detail_description": "Beasting polygon tests produces the time to time errors due to standard polygons with a bogus tile. We need to improve the checks for those situations and throw a TileException in those cases so we fall back to complex polygons.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12920229/LUCENE-8266.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8234", "change_description": ": Fixed bug in how spatial relationship is computed for\nGeoStandardCircle when it covers the whole world.", "change_title": "GeoStandardCircle can compute wrongly the spatial relationship when covering the whole world", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.4,8.0", "detail_description": "GeoStandardCircle computes the wrong spatial relationship with other shape when it covers the whole world and the provided shape covers the whole world as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917303/LUCENE-8234.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8236", "change_description": ": Filter duplicated points when creating GeoPath shapes\nto avoid creation of bogus planes.", "change_title": "GeoPath behavior with identical points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.4,8.0", "detail_description": "GeoPath has the current behavior: I think the factory should filter out these points, in the same way it is done for GeoPolygon. If this is not the desired behavior then the factory  should throw a consistent IllegalArgumentException in all cases.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917497/LUCENE-8326.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8243", "change_description": ": IndexWriter.addIndexes(Directory[]) did not properly preserve\nindex file names for updated doc values fields", "change_title": "IndexWriter might delete DV update files if addIndices are invovled", "detail_type": "Bug", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "the attached test fails with this output:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917991/LUCENE-8243.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8275", "change_description": ": Push up #checkPendingDeletes to Directory to ensure IW fails if\nthe directory has pending deletes files even if the directory is filtered or\na FileSwitchDirectory", "change_title": "Push up #checkPendingDeletes to Directory", "detail_type": "Bug", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "IndexWriter checks in it's ctor if the incoming directory is an     FSDirectory. If that is the case it ensures that the directory retries     deleting it's pending deletes and if there are pending deletes it will     fail creating the writer. Yet, this check didn't unwrap filter directories     or subclasses like FileSwitchDirectory such that in the case of MDW we     never checked for pending deletes. There are also two places in FSDirectory that first removed the file     that was supposed to be created / renamed to from the pending deletes set     and then tried to clean up pending deletes which excluded the file. These     places now remove the file from the set after the pending deletes are checked.  This caused some test failures lately unfortunately very timing dependent: ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12920625/LUCENE-8275.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8244", "change_description": ": Do not leak open file descriptors in SearcherTaxonomyManager's\nrefresh on exception", "change_title": "SearcherTaxonomyManager.refreshIfNeeded leaks file handles on exception", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "This method first refreshes the main index, and then the taxonomy, but if an exception is hit while refreshing the taxonomy, it fails to close the new reader it opened from the main index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917900/LUCENE-8244.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8305", "change_description": ": ComplexPhraseQuery.rewrite now handles an embedded MultiTermQuery\nthat rewrites to a MatchNoDocsQuery instead of throwing an exception.", "change_title": "ComplexPhraseQuery.rewrite can throw exception RE MatchNoDocsQuery if MTQ sub-clause matches no terms", "detail_type": "Bug", "detail_affect_versions": "6.3", "detail_fix_versions": "7.4", "detail_description": "With Solr v6.3, when I issue this query: http://localhost:8983/solr/BestBuy/select?wt=json&rows=10&q= text:%22maytag~%20(refri~%20OR%20refri*)%20%22&fl=id&hl=true&hl.preserveMulti=false&hl.fragsize=60&hl.fl=nameX,shortDescription,longDescription,artistName,type,manufacturer,department I get this error in the JSON response: ************************************************************* {   \"responseHeader\": {     \"zkConnected\": true,     \"status\": 500,     \"QTime\": 8,     \"params\": {       \"q\": \" text:\\\"maytag~ (refri~ OR refri*) \\\"\",       \"hl\": \"true\",       \"hl.preserveMulti\": \"false\",       \"fl\": \"id\",       \"hl.fragsize\": \"60\",       \"hl.fl\": \"nameX,shortDescription,longDescription,artistName,type,manufacturer,department\",       \"rows\": \"10\",       \"wt\": \"json\"     }   },   \"response\": {     \"numFound\": 2,     \"start\": 0,     \"docs\": [ , ]   },   \"error\": } ************************************************************* I did NOT have this error in Solr v6.1 so something has changed in v6.3 that is causing this error. Steve Rowe thinks it may be related to https://issues.apache.org/jira/browse/LUCENE-7337 Hoss' initial thoughts:  \"i think the root of the issue is that the way those fuzzy and prefix queries are parsed means that they may produce an empty boolean query depending on the contents of the index, and then the new optimization rewrites those empty boolean queries into MatchNoDocsQueries – but the highlighter (which uses hueristics to figure out what to ask each query – based on it's type – what to highlight) doesn't know what to do with that.  i'm really suprised the highlighter throws an error in the \"unexpected query type\" code path instead of just ignorning it.\"", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8287", "change_description": ": Ensure that empty regex completion queries always return no results.", "change_title": "ContextQuery with empty RegexCompletionQuery produces an assertion failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "When an empty RegexCompletionQuery is provided to ContextQuery, the following assertion failure occurs: This is a bit of an edge case, but may be concerning since without assertions enabled, you can go on to access IntsRef indices that are out of bounds. The attached patch provides a reproduction of the issue, as the test case TestContextQuery#testEmptyRegexQuery. Note that to reproduce, Java assertions must be enabled (as in the default configuration for tests). The patch also provides a test case for the normal behavior of an empty RegexCompletionQuery, when it is not wrapped in ContextQuery (TestRegexCompletionQuery#testEmptyRegexQuery). In this case, there is no error, and all suggestions are returned.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922285/LUCENE-8287.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8317", "change_description": ": Prevent concurrent deletes from being applied during full flush.\nFuture deletes could potentially be exposed to flushes/commits/refreshes if the\namount of RAM used by deletes is greater than half of the IW RAM buffer.", "change_title": "TestStressNRT fails with missing document", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12923899/LUCENE-8317.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8320", "change_description": ": Fix WindowsFS to correctly account for rename and hardlinks.", "change_title": "WindowFS#move should consider hard-link when transferring ownership", "detail_type": "Bug", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "None", "detail_description": "The attached test trips an assertion in `WindowFS#onClose`.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924061/LUCENE-8320.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8328", "change_description": ": Ensure ReadersAndUpdates consistently executes under lock.", "change_title": "ReadersAndUpdates#getLatestReader should execute under lock", "detail_type": "Bug", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "None", "detail_description": "It's possible for a merge thread to acquire an index reader which is closed before it can incRef. The problem is that `ReadersAndUpdates#getLatestReader` is executed concurrently without holding lock.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924661/LUCENE-8328.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8325", "change_description": ": Fixed the smartcn tokenizer to not split UTF-16 surrogate pairs.", "change_title": "smartcn analyzer can't handle SURROGATE char", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "This issue is from https://github.com/elastic/elasticsearch/issues/30739 smartcn analyzer can't handle SURROGATE char, Example:    In the above code snippet will output:   and I have created a PATCH to try to fix this, please help review(since smartcn only support GBK char, so it's only just handle it as a single char).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12924679/handle_surrogate_char_for_smartcn_2018-05-23.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8186", "change_description": ": LowerCaseTokenizerFactory now lowercases text in multi-term\nqueries.", "change_title": "CustomAnalyzer with a LowerCaseTokenizerFactory fails to normalize multiterms", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "While working on SOLR-12034, a unit test that relied on the LowerCaseTokenizerFactory failed. After some digging, I was able to replicate this at the Lucene level. Unit test: The problem is that the CustomAnalyzer iterates through the tokenfilters, but does not call the tokenizer, which, in the case of the LowerCaseTokenizer, does the filtering work.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12912951/LUCENE-8186.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8278", "change_description": ": Some end-of-input no-scheme domain-only URL tokens are typed as\n<ALPHANUM> rather than <URL>.", "change_title": "UAX29URLEmailTokenizer is not detecting some tokens as URL type", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "We are using the UAX29URLEmailTokenizer so we can use the token types in our plugins. However, I noticed that the tokenizer is not detecting certain URLs as <URL> but <ALPHANUM> instead. Examples that are not working: But: Examples that work: I have checked this JIRA, and could not find an issue. I have tested this on Lucene (Solr) 6.4.1 and 7.3. Could someone confirm my findings and advise what I could do to (help) resolve this issue?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12925332/LUCENE-8278.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8355", "change_description": ": Prevent IW from opening an already dropped segment while DV updates\nare written.", "change_title": "IW#writeSomeDocValuesUpdates might open a dropped segment", "detail_type": "Bug", "detail_affect_versions": "7.4,7.5,8.0", "detail_fix_versions": "None", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927571/LUCENE-8355.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8344", "change_description": ": TokenStreamToAutomaton (used by some suggesters) was not ignoring a trailing\nposition increment when the preservePositionIncrement setting is false.", "change_title": "TokenStreamToAutomaton doesn't ignore trailing posInc when preservePositionIncrements=false", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "TokenStreamToAutomaton in Lucene core is used by the AnalyzingSuggester (incl. FuzzySuggester subclass ) and NRT Document Suggester and soon the SolrTextTagger.  It has a setting preservePositionIncrements defaulting to true.  If it's set to false (e.g. to ignore stopwords) and if there is a trailing position increment greater than 1, TS2A will still add position increments (holes) into the automata even though it was configured not to. I'm filing this issue separate from LUCENE-8332 where I first found it.  The fix is very simple but I'm concerned about back-compat ramifications so I'm filing it separately.  I'll attach a patch to show the problem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927133/LUCENE-8344.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8357", "change_description": ": FunctionScoreQuery.boostByQuery() and boostByValue() were\nproducing truncated Explanations", "change_title": "Fix Explanations on FunctionScoreQuery.boostByQuery and boostByValue", "detail_type": "Bug", "detail_affect_versions": "7.3,7.3.1", "detail_fix_versions": "7.4,8.0", "detail_description": "As noted in SOLR-12414, the replacements for BoostingQuery and BoostedQuery have truncated explanations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927796/LUCENE-8357.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8360", "change_description": ": NGramTokenFilter and EdgeNGramTokenFilter did not correctly\nset position increments in end()", "change_title": "NGramTokenFilter doesn't set posIncAttribute correctly in end()", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "Found by randomly adding ConditionalTokenFilter to random chains, which calls end() on its delegate when it reaches a token that shouldn't have the delegate applied to it, so that things like position increments and position length are preserved. This is also the case for EdgeNGramTokenFilter", "patch_link": "https://issues.apache.org/jira/secure/attachment/12928124/LUCENE-8360.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8301", "change_description": ": Update randomizedtesting to 2.6.0.", "change_title": "Update randomizedtesting to 2.6.0", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8299", "change_description": ": Geo3D wrapper uses new polygon method factory that gives better\nsupport for polygons with many points (>100).", "change_title": "Geo3D: Change factory method for polygons", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "In LUCENE-8220 it was introduced a new factory method that chooses the best technology for the provided polygon. In particular, this factory provides a better support when creating a polygon with a list of points > 100 and in some situations when tiling of the polygon is not possible.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12922465/LUCENE-8299.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8261", "change_description": ": InterpolatedProperties.interpolate and recursive property\nreferences.", "change_title": "InterpolatedProperties.interpolate and recursive property references", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "InterpolatedProperties is used in lib check tasks in the build file. I occasionally see this: I don't think we ever need to use any group references in those replacements; they should be fixed strings (quoted verbatim)? So Pattern.quoteReplacement would be adequate here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12920455/LUCENE-8261.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8228", "change_description": ": removed obsolete IndexDeletionPolicy clone() requirements from\nthe javadoc.", "change_title": "IndexDeletionPolicy has obsolete clone() requirements in the javadoc", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "PR here, for easier review.  https://github.com/apache/lucene-solr/pull/344", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916606/LUCENE-8228.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8219", "change_description": ": Use a realistic estimate of the number of nodes and links in\n LevensteinAutomaton.java, to save reallocation of arrays.", "change_title": "LevenshteinAutomata should estimate the number of states and transitions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "Currently the toAutomaton() method of the LevenshteinAutomata class uses the default constructor of the Automaton although it exactly knows how many states the automaton will have and can do a reasonable estimation on how many transitions it will need as well. I suggest changing the lines to For my test data this cut down on the total amount of memory needed for int arrays by factor 4. The estimation of \"1 + 2 * editDistance\" should maybe rather be replaced by a value coming from the ParametricDescription used.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8214", "change_description": ": Improve selection of testPoint for GeoComplexPolygon.", "change_title": "Improve selection of testPoint for GeoComplexPolygon", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.4,8.0", "detail_description": "I have been checking the effect of the testPoint on GeoComplexPolygon and it seems performance can change quite a bit depending on the choice. The results with random polygons with 20k points shows that a good choice is to ue the center of mass of the shape. On the worst case the performance is similar to what we have now and the best case is twice as fast for within() and getRelationship() methods. Therefore I would like to propose to use that point whenever possible.  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12915066/LUCENE-8214.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "SOLR-10912", "change_description": ": Add automatic patch validation.", "change_title": "Adding automatic patch validation", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "Proposing introduction of automated patch validation, similar what Hadoop or other Apache projects are using (see link). This would ensure that every patch passes a certain set of criterions before getting approved. It would save time for developer (faster feedback loop), save time for committers (less step to do manually), and would increase quality. Hadoop is currently using Apache Yetus to run validations, which seems to be a good direction to start. This jira could be the board of discussing the preferred solution.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12914632/SOLR-10912.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8122", "change_description": ",", "change_title": "upgrade to icu > 60.2", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4", "detail_description": "Currently we are at version 59.1. There is some change to breakiterator behavior, but I think it simplifies our code. Also our tools needed to be updated to pull some data files from new source code location.  — See comments below for reasons why we are explicitly skipping past 60.2", "patch_link": "https://issues.apache.org/jira/secure/attachment/12904950/LUCENE-8122.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8175", "change_description": ",", "change_title": "ICUTokenizer might return corrupt tokens due to concurrency bug in ICU4J", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "trunk,7.4", "detail_description": "I was digging some test failures with testRandomHugeStrings that occurred since the upgrade to ICU4J 60.2 which happen to boil down to this bug: http://bugs.icu-project.org/trac/ticket/13512 which is fixed but not released yet. In short an int[] is shared across several threads while it shouldn't. As a consequence, ICUTokenizer may sometimes return corrupt tokens. I pinged on the issue to know when a release fixing this bug is expected.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916297/LUCENE-8175.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Other", "change_id": "LUCENE-8291", "change_description": ": Remove QueryTemplateManager utility class from XML queryparser.\nThis class is just a general XML transforming tool (using property files and\nXSLT) and has nothing to do with query parsing. It can easily be implemented\nusing more sophisticated libraries or using XSL transformers from the JDK.\nThis change also removes the Lucene demo webapp to prevent XSS issues in\nuntested/unmaintained code.", "change_title": "Possible security issue when parsing XML documents containing external entity references", "detail_type": "Bug", "detail_affect_versions": "7.2.1", "detail_fix_versions": "7.4,8.0", "detail_description": "It appears that in QueryTemplateManager.java lines 149 and 198 and in DOMUtils.java line 204 XML is parsed without disabling external entity references (XXE). This is described in http://cwe.mitre.org/data/definitions/611.html and possible mitigations are listed here: https://www.owasp.org/index.php/XML_External_Entity_(XXE)_Prevention_Cheat_Sheet All recent versions of lucene are affected.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12923602/LUCENE-8291-2.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Build", "change_id": "LUCENE-7935", "change_description": ": Publish .sha512 hash files with the release artifacts and stop\npublishing .md5 hashes since the algorithm is broken", "change_title": "Release .sha512 hash files with our artifacts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "Currently we are only required to release .md5 hashes with our artifacts, and we also include .sha1 files. It is expected that .sha512 will be required in the future (see https://www.apache.org/dev/release-signing.html#sha1), so why not start generating them right away?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12914027/LUCENE-7935.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Build", "change_id": "LUCENE-8230", "change_description": ": Upgrade forbiddenapis to version 2.5.", "change_title": "Update forbiddenapis to 2.5 (and Gradle to 2.4.15)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.4,8.0", "detail_description": "Forbiddenapis 2.5 was released today. It also adds support for commons-io v2.6, so it might be also of interest for Solr. The main change is support for Java 10.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12916663/LUCENE-8230.patch", "patch_content": "none"}
{"library_version": "7.4.0", "change_type": "Documentation", "change_id": "LUCENE-8238", "change_description": ": Improve WordDelimiterFilter and WordDelimiterGraphFilter javadocs", "change_title": "WordDelimiterFilter javadocs reference nonexistent parameters", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "trunk", "detail_description": "The javadocs for both WDF and WDGF include a pretty detailed discussion about the proper use of the \"combinations\" parameter, but no such parameter exists. I don't know the history here, but it sounds as if the docs might be referring to some previous incarnation of this filter, perhaps in the context of some (now-defunct) Solr configuration. The docs should be updated to reference the actual option names that are provided by the class today.   I've attached a patch", "patch_link": "https://issues.apache.org/jira/secure/attachment/12917613/WDGF.patch", "patch_content": "none"}

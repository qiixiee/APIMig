{"library_version": "6.1.0", "change_type": "New Features", "change_id": "LUCENE-7099", "change_description": ": Add LatLonDocValuesField.newDistanceSort to the sandbox.", "change_title": "add newDistanceSort to sandbox LatLonPoint", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "This field does not support sorting by distance, which is a very common use case. We can add LatLonPoint.newDistanceSort(field, latitude, longitude) which returns a suitable SortField. There are a lot of optimizations esp when e.g. the priority queue gets full to avoid tons of haversin() computations. Also, we can make use of the SortedNumeric data to switch newDistanceQuery/newPolygonQuery to the two-phase iterator api, so they aren't doing haversin() calls on bkd leaf nodes. It should look a lot like LUCENE-7019", "patch_link": "https://issues.apache.org/jira/secure/attachment/12792913/LUCENE-7099.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "New Features", "change_id": "LUCENE-7140", "change_description": ": Add PlanetModel.bisection to spatial3d", "change_title": "Compute a geo3d point that is halfway between two endpoints, measured as an arc distance", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "The need was expressed for a method that finds a midpoint between two other points on the surface.  The midpoint would be at, distance-wise, half the arc distance between the two endpoints.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795401/LUCENE-7140.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "New Features", "change_id": "LUCENE-7069", "change_description": ": Add LatLonPoint.nearest, to find nearest N points to a\nprovided query point", "change_title": "Add LatLonPoint.nearest to find closest indexed point to a given query point", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "KD trees (used by Lucene's new dimensional points) excel at finding \"nearest neighbors\" to a given query point ... I think we should add this to Lucene's sandbox as: I only implemented the 1 nearest neighbor for starters ... I think we can easily generalize this in the future to K nearest. It could also be generalized to more than 2 dimensions, but for now I'm making the class package private in sandbox for just the geo2d (lat/lon) use case. I don't think this should go into 6.0.0, but should go into 6.1: it's a new feature, and we need to wrap up and ship 6.0.0 already", "patch_link": "https://issues.apache.org/jira/secure/attachment/12798739/LUCENE-7069.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "New Features", "change_id": "LUCENE-7234", "change_description": ": Added InetAddressPoint.nextDown/nextUp to easily generate range\nqueries with excluded bounds.", "change_title": "Add InetAddressPoint.nextUp/nextDown", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "This can be useful for dealing with exclusive bounds.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12799751/LUCENE-7234.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "New Features", "change_id": "LUCENE-7300", "change_description": ": The misc module now has a directory wrapper that uses hard-links if\napplicable and supported when copying files from another FSDirectory in\nDirectory#copyFrom.", "change_title": "Add directory wrapper that optionally uses hardlinks in copyFrom", "detail_type": "Improvement", "detail_affect_versions": "6.1", "detail_fix_versions": "6.1,7.0", "detail_description": "Today we always do byte-by-byte copy in Directory#copyFrom. While this is reliable and should be the default, certain situations can be improved by using hardlinks if possible to get constant time copy on OS / FS that support such an operation. Something like this could reside in misc if it's contained enough since it requires LinkPermissions to be set and needs to detect if both directories are subclasses of FSDirectory etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12806106/LUCENE-7300.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "API Changes", "change_id": "LUCENE-7163", "change_description": ": refactor GeoRect, Polygon, and GeoUtils tests to geo\npackage in core", "change_title": "Refactor GeoRect and Polygon to core", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "o.a.l.spatial.util.GeoRect and o.a.l.spatial.util.Polygon are reusable classes across multiple lucene modules. It makes sense for them to be moved to the o.a.l.geo package in the core module so they're exposed across multiple modules. GeoRect should also be refactored to something more straightforward, like Rectangle", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796564/LUCENE-7163.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "API Changes", "change_id": "LUCENE-7152", "change_description": ": Refactor GeoUtils from lucene-spatial package to\ncore", "change_title": "Refactor lucene-spatial GeoUtils to core", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "GeoUtils contains a lot of common spatial mathematics that can be reused across multiple packages. As discussed in LUCENE-7150 this issue will refactor GeoUtils to a new o.a.l.util.geo package in core that can be the home for other reusable spatial utility classes required by field and query implementations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796533/LUCENE-7152.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "API Changes", "change_id": "LUCENE-7141", "change_description": ": Switch OfflineSorter's ByteSequencesReader to\nBytesRefIterator", "change_title": "OfflineSorter shouldn't always forceMerge in the end", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Today it always does a final merge, to collapse all segments into a single segment. But typically the caller is going to re-iterate all values anyway, to go off and build an FST or a BKD tree or something, and so that final forceMerge is often not necessary and the merging can be done on the fly when the caller consumes the result. This is somewhat tricky to do ... I'd like to break it into steps, starting with fixing the ByteSequencesReader API to implement BytesRefIterator instead of its own read(BytesRefBuilder) method as a first step.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795413/LUCENE-7141.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "API Changes", "change_id": "LUCENE-7150", "change_description": ": Spatial3d gets useful APIs to create common shape\nqueries, matching LatLonPoint.", "change_title": "geo3d public APIs should match the 2D apis?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "I'm struggling to benchmark the equivalent to LatLonPoint.newDistanceQuery in the geo3d world. Ideally, I think we'd have a Geo3DPoint.newDistanceQuery?  And it would take degrees, not radians, and radiusMeters, not an angle? And if I index and search using PlanetModel.SPHERE I think it should ideally give the same results as LatLonPoint.newDistanceQuery, which uses haversin.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796311/LUCENE-7150.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "API Changes", "change_id": "LUCENE-7243", "change_description": ": Removed the LeafReaderContext parameter from\nQueryCachingPolicy#shouldCache.", "change_title": "Remove LeafReaderContext from QueryCachingPolicy.shouldCache", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Now that the heuristic to not cache on small segments has been moved to the cache, we don't need the LeafReaderContext in QueryCachingPolicy.shouldCache.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12800194/LUCENE-7243.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "API Changes", "change_id": "LUCENE-7283", "change_description": ": SlowCompositeReaderWrapper and the uninverting package have\nbeen moved to Solr.", "change_title": "Move SlowCompositeReaderWrapper and uninverting package to solr sources", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Spinoff from LUCENE-6766, where we fixed index-time sorting to have first class support in Lucene's ore, and no longer use SlowCompositeReaderWrapper. This is a dangerous, long living class, that tries to pretend a set of N segments is actually just a single segment.  It's a leaky abstraction, has poor performance, and puts undue pressure on the APIs of new Lucene features to try to keep up this illusion. With LUCENE-6766, finally all usage of this class (except for UninvertedReader tests, which should maybe also move out?) has been removed from Lucene, so I think we should move it to Solr.  This may also lead to a solution for LUCENE-7086 since e.g. the class could tap into solr's schema to \"know\" how to handle points fields properly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12805430/LUCENE-7283.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7071", "change_description": ": Reduce bytes copying in OfflineSorter, giving ~10%\nspeedup on merging 2D LatLonPoint values", "change_title": "Can we reeduce excessive byte[] copying in OfflineSorter?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "OfflineSorter, which dimensional points uses heavily in the > 1D case, works by reading one partition, a set of N unsorted values, from disk and sorting it in memory and writing it out again. The sort invokes a provided Comparator on two BytesRef values, each of which is fully copied from the ByteBlockPool, when it could often reference a slice from the pool instead. Another byte[] copy happens when iterating through the sorted values. This is an optimization ... I'm targeting 6.1.0 not 6.0.0!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791667/LUCENE-7071.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7105", "change_description": ",", "change_title": "squeeze a little more perf out of LatLonPoint.newDistanceQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "This query can make use of some of the same optimizations we applied to the distance sort.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12793473/LUCENE-7105.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7215", "change_description": ",", "change_title": "don't invoke full haversin for LatLonPoint.newDistanceQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "For tree traversals and edge cases we still sometimes invoke full haversin (with asin() call and everything). this is not necessary: we just need to compute the exact sort key needed for comparisons. While not a huge optimization, its obviously less work and keeps the overhead of the BKD traversal as low as possible. And it removes the slow asin call from any hot path (its already done for sorting too), with its large tables and so on.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12798597/LUCENE-7215.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7097", "change_description": ": IntroSorter now recurses to 2 * log_2(count) quicksort\nstack depth before switching to heapsort", "change_title": "Can we increase the stack depth before Introsorter switches to heapsort?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "trunk,6.1", "detail_description": "Introsort is a \"safe\" quicksort: it uses quicksort but detects when an adversary is at work and cuts over to heapsort at that point. The description at https://en.wikipedia.org/wiki/Introsort shows the cutover as 2X log_2(N) but our impl (IntroSorter) currently uses just log_2. So I tested using 2X log_2 instead, and I see a decent (~5.6%, from 98.2 sec to 92.7 sec) speedup in the time for offline sorter to sort when doing the force merge of 6.1 LatLonPoints from the London UK benchmark. Is there any reason not to switch?  I know this means 2X the stack required, but since this is log_2 space that seems fine?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12792887/LUCENE-7097.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7115", "change_description": ": Speed up FieldCache.CacheEntry toString by setting initial\nStringBuilder capacity", "change_title": "Speed up FieldCache.CacheEntry toString by setting initial StringBuilder capacity", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "trunk,6.1", "detail_description": "Solr can end up printing a lot of these objects via the JmxMonitoriedMap, see SOLR-8869 and SOLR-6747 as examples. From looking at some profiles, a lot of time and memory are spent resizing the StringBuilder, which doesn't set the initial capacity. On my cluster, the strings are a bit over 200 chars; I set the initial capacity to 250 and ran tests calling toString 1000 times.  Tests consistently show 10-15% improvement when setting the initial capacity.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12794265/LUCENE-7115.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7147", "change_description": ": Improve disjoint check for geo distance query traversal", "change_title": "Improve disjoint check for geo distance query traversal", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "When doing geo distance queries, it is important to avoid traversing subtrees which do not contain any relevant points. We currently have checks which compare the bbox of the query to the bounds of the subtree. However, it is possible for a subtree to overlap the bbox, but still not intersect the query. This issue is to improve that check to avoid unnecessary traversals.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795788/LUCENE-7147.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7153", "change_description": ": GeoPointField and LatLonPoint polygon queries now support\nmultiple polygons and holes, with memory usage independent of\npolygon complexity.", "change_title": "give GeoPoint and LatLonPoint full polygon support", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "These two geo impls have a very limited polygon support that does not support inner rings (holes) or multiple outer rings efficiently. Basically if you want to do this, you are left building crazy logic with booleanquery which will send memory into the gigabytes for a single query, needlessly. For example Russia polygon from geonames is 250KB of geojson and over a thousand outer rings. We should instead support this stuff with the queries themselves, especially it will allow us to implement things more efficiently in the future. I think instead of newPolygonQuery(double[], double[]) it should look like newPolygonQuery(Polygon...). A polygon can be a single outer ring (shape) with 0 or more inner rings (holes). No nesting, you just use multiply polygons if you e.g. have an island. See http://esri.github.io/geometry-api-java/doc/Polygon.html for visuals and examples. I indented their GeoJSON example:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796332/LUCENE-7153.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7159", "change_description": ": Speed up LatLonPoint polygon performance.", "change_title": "improve spatial point/rect vs. polygon performance", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Now that we can query on complex polygons without going OOM (LUCENE-7153), we should do something to address the current üê¢ performance. Currently, we use a basic crossings test (O(n)) for boundary cases. We defer these expensive per-doc checks on boundary cases to a two phase iterator (LUCENE-7019, LUCENE-7109), so that it can be avoided if e.g. excluded by filters, conjunctions, deleted doc, and so on. This is currently important for performance, but basically its shoving the problem under the rug and hoping it goes away. At least for point in poly, there are a number of faster techniques described here: http://erich.realtimerendering.com/ptinpoly/ Additionally I am not sure how costly our \"tree traversal\" (rectangle intersection algorithms). Maybe its nothing to be worried about, but likely it too gets bad if the thing gets complex enough. These don't need to be perfect but need to behave like java's Shape#contains (can conservatively return false), and Shape#intersects (can conservatively return true). Of course, if they are too inaccurate, then things can get slower. In cases of precomputed structures we should also consider memory usage: e.g. we shouldn't make a horrible tradeoff there.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796849/LUCENE-7159.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7211", "change_description": ": Reduce memory & GC for spatial RPT Intersects when the number of\nmatching docs is small.", "change_title": "Spatial RPT Intersects should use DocIdSetBuilder to save memory/GC", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1", "detail_description": "I‚Äôve been continuing some analysis into JVM garbage sources in my Solr index. (5.4, 86M docs/core, 56k 99.9th percentile hit count with my query corpus) After applying SOLR-8922, I find my biggest source of garbage by a literal order of magnitude (by size) is the long[] allocated by FixedBitSet. From the backtraces, it appears the biggest source of FixBitSet creation in my case (by two orders of magnitude) is my use of queries that involve geospatial filtering. Specifically, IntersectsPrefixTreeQuery.getDocIdSet, here: https://github.com/apache/lucene-solr/blob/569b6ca9ca439ee82734622f35f6b6342c0e9228/lucene/spatial-extras/src/java/org/apache/lucene/spatial/prefix/IntersectsPrefixTreeQuery.java#L60 Has this been considered for optimization? I can think of a few paths: 1. Persistent Object pools - FixedBitSet size is allocated based on maxDoc, which presumably changes less frequently than queries are issued. If an existing FixedBitSet were not available from a pool, the worst case (create a new one) would be no worse than the current behavior. The complication would be enforcement around when to return the object to the pool, but it looks like this has some lifecycle hooks already. 2. I note that a thing called a SparseFixedBitSet already exists, and puts considerable effort into allocating smaller chunks only as necessary. Is this not usable for this purpose? How significant is the performance difference? I'd be happy to spend some time on a patch, but I was hoping for a little more data around the current choices before choosing an approach.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12798348/SOLR-8944-Use-DocIdSetBuilder-instead-of-FixedBitSet.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7235", "change_description": ": LRUQueryCache should not take a lock for segments that it will\nnot cache on anyway.", "change_title": "Avoid taking the lock in LRUQueryCache when not necessary", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "LRUQueryCache's CachingWeightWrapper works this way: The potential issue is that this first step always takes the lock, and I have seen a couple cases where indices were small and/or queries were very cheap and this showed up as a bottleneck. On the other hand, we have checks in step 3 that tell the cache to not cache on a particular segment regardless of the query. So I would like to move that part before 1 so that we do not even take the lock in that case. For instance right now we require that segments have at least 10k documents and 3% of all docs in the index to be cached. I just looked at a random index that contains 1.7m documents, and only 4 segments out of 29 met this criterion (yet they contain 1.1m documents: 65% of the total index size). So in the case of that index, we would take the lock 7x less often.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12799933/LUCENE-7235.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7238", "change_description": ": Explicitly disable the query cache in MemoryIndex#createSearcher.", "change_title": "MemoryIndex.createSearcher should disable caching explicitly", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Follow-up of LUCENE-7235: In practice, nothing will be cached with a reasonable cache implementation given the size of the index (a single document). But it would still be better to explicitly disable caching so that we don't eg. take unnecessary locks.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7237", "change_description": ": LRUQueryCache now prefers returning an uncached Scorer than\nwaiting on a lock.", "change_title": "LRUQueryCache should rather not cache than wait on a lock", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "This is an idea Robert just mentioned to me: currently the cache is using a lock to keep various data-structures in sync. It is a pity that you might have contention because of caching. So something we could do would be to not cache when the lock is already taken.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12800018/LUCENE-7237.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7261", "change_description": ",", "change_title": "Speed up LSBRadixSorter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Currently it always does 4 passes over the data (one per byte, since ints have 4 bytes). However, most of the time, we know maxDoc, so we can use this information to do fewer passes when they are not necessary. For instance, if maxDoc is less than or equal to 2^24, we only need 3 passes, and if maxDoc is less than or equals to 2^16, we only need two passes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12801231/LUCENE-7261.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7262", "change_description": ",", "change_title": "Add back the \"estimate match count\" optimization", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Follow-up to my last message on LUCENE-7051: I removed this optimization a while ago because it made things a bit more complicated but did not seem to help with point queries. However the reason why it did not seem to help was that the benchmark only runs queries that match 25% of the dataset. This makes the run time completely dominated by calls to FixedBitSet.set so the call to FixedBitSet.cardinality() looks free. However with slightly sparser queries like the geo benchmark generates (dense enough to trigger the creation of a FixedBitSet but sparse enough so that FixedBitSet.set does not dominate the run time), one can notice speed-ups when this call is skipped.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12801701/LUCENE-7262.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7264", "change_description": ",", "change_title": "Fewer conditionals in DocIdSetBuilder.add", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "As reported in LUCENE-7254, DocIdSetBuilder.add has several conditionals that slow down the construction of the DocIdSet.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12801295/LUCENE-7264.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7258", "change_description": ",", "change_title": "Tune DocIdSetBuilder allocation rate", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "LUCENE-7211 converted IntersectsPrefixTreeQuery to use DocIdSetBuilder, but didn't actually reduce garbage generation for my Solr index. Since something like 40% of my garbage (by space) is now attributed to DocIdSetBuilder.growBuffer, I charted a few different allocation strategies to see if I could tune things more. See here: http://i.imgur.com/7sXLAYv.jpg  The jump-then-flatline at the right would be where DocIdSetBuilder gives up and allocates a FixedBitSet for a 100M-doc index. (The 1M-doc index curve/cutoff looked similar) Perhaps unsurprisingly, the 1/8th growth factor in ArrayUtil.oversize is terrible from an allocation standpoint if you're doing a lot of expansions, and is especially terrible when used to build a short-lived data structure like this one. By the time it goes with the FBS, it's allocated around twice as much memory for the buffer as it would have needed for just the FBS.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12801112/LUCENE-7258-Tune-memory-allocation-rate-for-Intersec.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7299", "change_description": ": Speed up BytesRefHash.sort() using radix sort.", "change_title": "BytesRefHash.sort() should use radix sort?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Switching DocIdSetBuilder to radix sort helped make things significantly faster. We should be able to do the same with BytesRefHash.sort()?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12806136/LUCENE-7299.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Optimizations", "change_id": "LUCENE-7306", "change_description": ": Speed up points indexing and merging using radix sort.", "change_title": "Use radix sort for points too", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Like postings, points make heavy use of sorting at indexing time, so we should try to leverage radix sort too?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12806618/LUCENE-7306.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7127", "change_description": ": Fix corner case bugs in GeoPointDistanceQuery.", "change_title": "remove epsilon-based testing from lucene/spatial", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Currently, the random tests here allow a TOLERANCE and will fail if the error exceeds. But this is not fun to debug! It also keeps the door wide open for bugs to creep in. Alternatively, we can rework the tests like we did for sandbox/ points. This means the test is aware of the index-time quantization and so it can demand exact answers. Its more difficult at first, because even floating point error can cause a failure. It requires us to maybe work through corner cases/rework optimizations. If any epsilons must be added, they can be added to the optimizations themselves (e.g. bounding box) instead of the user's result.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12794798/LUCENE-7127.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7166", "change_description": ": Fix corner case bugs in LatLonPoint/GeoPointField bounding box\nqueries.", "change_title": "fix quantization bugs in LatLonPoint and GeoPointField, remove test leniency", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Currently a few remaining tests (around newRectQuery) are lenient and quantize the query rectangles. This is masking several bugs: 1. Both LatLonPoint and GeoPointField's bbox queries quantize their endpoints incorrectly at query-time, which can e.g. cause it to bring in false positive results 2. Tests have always been lenient about this (either by using epsilons or incorrectly quantizing the query rectangles in tests), hiding the above.  3. Both LatLonPoint and GeoPointField still have rounding issues at quantization. For very special values they do not always consistently round in one direction. 4. Random encoding tests will never find the above issue, hiding it. This is because you need very special double values that the current stuff (e.g. -180 + 360.0 * random().nextDouble() will never find!).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796648/LUCENE-7166.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7168", "change_description": ": Switch to stable encode for geo3d, remove quantization\ntest leniency, remove dead code", "change_title": "Remove geo3d test leniency", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "Today the test hides possible failures by leniently handling quantization issues. We should fix it to do what geo2d tests now do: pre-quantized indexed points, but don't quantize query shapes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12797629/LUCENE-7168.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7301", "change_description": ": Multiple doc values updates to the same document within\none update batch could be applied in the wrong order resulting in\nthe wrong updated value", "change_title": "updateNumericDocValue mixed with updateDocument can cause data loss in some randomized testing", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.2,5.6,6.0.2,6.1,7.0", "detail_description": "SOLR-5944 has been held up by a while due to some extremely rare randomized test failures. Ishan and I have been working on whitling those Solr test failures down, trying to create more isolated reproducable test failures, and i think i've tracked it down to a bug in IndexWriter when the client calls to updateDocument intermixed with calls to updateNumericDocValue AND IndexWriterConfig.setMaxBufferedDocs is very low (i suspect \"how low\" depends on the number of quantity/types of updates ‚Äì but just got something that reproduced, and haven't tried reproducing with higher values of maxBufferedDocs and larger sequences of updateDocument / updateNumericDocValue calls.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12806804/LUCENE-7301.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7312", "change_description": ": Fix geo3d's x/y/z double to int encoding to ensure it always\nrounds down", "change_title": "Geo3dPoint test failure", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "6.1,7.0", "detail_description": "Here's the failure:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12807825/LUCENE-7312.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7132", "change_description": ": BooleanQuery sometimes assigned too-low scores in cases\nwhere ranges of documents had only a single clause matching while\nother ranges had more than one clause matching", "change_title": "BooleanQuery scores can be diff for same docs+sim when using coord (disagree with Explanation which doesn't change)", "detail_type": "Bug", "detail_affect_versions": "5.5", "detail_fix_versions": "5.5.2,6.1,7.0", "detail_description": "Some of the folks reported that sometimes explain's score can be different than the score requested by fields parameter. Interestingly, Explain's scores would create a different ranking than the original result list. This is something users experience, but it cannot be re-produced deterministically.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795042/SOLR-8884.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7286", "change_description": ": Added support for highlighting SynonymQuery.", "change_title": "WeightedSpanTermExtractor.extract() does not recognize SynonymQuery", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "6.1,7.0", "detail_description": "Short description: In WeightedSpanTermExtractor.extract(...)  method there is a long list of supported Queries. There is no SynonymQuery which leads to extractUnknownQuery() that does nothing. It would be really nice to have SynonymQuery covered as well. Long description: I'm trying to highlight an external text using a Highlighter. The query is created by QueryParser. If the created query is simple it works like a charm. The problem is when parsed query contains SynonymQuery ‚Äì it happens, when stemmer returns multiple stems, which is not uncommon for Polish language. Btw. this is my first jira issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809207/LUCENE-7286.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7291", "change_description": ": Spatial heatmap faceting could mis-count when the heatmap crosses the\ndateline and indexed non-point shapes are much bigger than the heatmap region.", "change_title": "HeatmapFacetCounter bug with dateline and large non-point shapes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.2,5.6,6.0.2,6.1", "detail_description": "Jenkins found a test failure today. This reproduces for me (master, java 8): ant test  -Dtestcase=HeatmapFacetCounterTest -Dtests.method=testRandom -Dtests.seed=3EC907D1784B6F23 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/x1/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=is-IS -Dtests.timezone=Europe/Tirane -Dtests.asserts=true -Dtests.file.encoding=UTF-8", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809501/LUCENE_7291.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7333", "change_description": ": Fix test bug where randomSimpleString() generated a filename\nthat is a reserved device name on Windows.", "change_title": "BaseDirectoryTestCase may create temporary files with names not accepted by Windows (e.g. com1, con,...)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "BaseDirectoryTestCase may randomly create files with \"special names\", which are not allowed by certain operating systems, e.g. Windows. See https://msdn.microsoft.com/en-us/library/aa365247.aspx for more info. This is the issue we have seen:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809569/LUCENE-7333.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7295", "change_description": ": TermAutomatonQuery.hashCode calculates Automaton.toDot().hash,\nequivalence relationship replaced with object identity.", "change_title": "TermAutomatonQuery.hashCode calculates Automaton.toDot().hash", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "This is going to be excruciatingly slow? We could at least cache the hash code once computed...", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7277", "change_description": ": Make Query.hashCode and Query.equals abstract.", "change_title": "Make Query.hashCode and Query.equals abstract", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "Custom subclasses of the Query class have the default implementation of hashCode/equals that make all instances of the subclass equal. If somebody doesn't know this it can be pretty tricky to debug with IndexSearcher's query cache on. Is there any rationale for declaring it this way instead of making those methods abstract (and enforcing their proper implementation in a subclass)?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12805588/LUCENE-7277.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7174", "change_description": ": Upgrade randomizedtesting to 2.3.4.", "change_title": "Upgrade randomizedtesting to 2.3.4", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "6.1,7.0", "detail_description": "The new version has better output of static leak detector, so you are able to figure out which field caused the InaccessibleObjectException or AccessControlException. https://github.com/randomizedtesting/randomizedtesting/releases/tag/release%2F2.3.4", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7205", "change_description": ": Remove repeated nl.getLength() calls in\n(Boolean|DisjunctionMax|FuzzyLikeThis)QueryBuilder.", "change_title": "remove repeated nl.getLength() calls in (Boolean|DisjunctionMax|FuzzyLikeThis)QueryBuilder", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.1", "detail_description": "proposed trivial patch against master to follow", "patch_link": "https://issues.apache.org/jira/secure/attachment/12798248/LUCENE-7205.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7210", "change_description": ": Make TestCore*Parser's analyzer choice override-able", "change_title": "Make TestCore*Parser's analyzer choice override-able", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "6.1", "detail_description": "Make TestCore[Plus(Queries|Extensions)]Parser's analyzer choice override-able. (Christine Poerschke, Daniel Collins)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12798526/LUCENE-7210.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7263", "change_description": ": Make queryparser/xml/CoreParser's SpanQueryBuilderFactory\naccessible to deriving classes.", "change_title": "xmlparser: Allow SpanQueryBuilder to be used by derived classes", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "6.1,7.0", "detail_description": "Following on from LUCENE-7210 (and others), the xml queryparser has different factories, one for creating normal queries and one for creating span queries. The former is a protected variable so can be used by derived classes, the latter isn't. This makes the spanFactory a variable that can be used more easily.  No functional changes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12801468/LUCENE-7263.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "SOLR-9109", "change_description": "/", "change_title": "Add support for custom ivysettings.xml", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.1", "detail_description": "Currently solr/lucene/common-build.xml hardcodes file ivy-settings.xml in the ivy.configure task. It means that, unlike all other CDH components that use Ant, Solr does not allow the user to provide a custom ivysettings.xml. In the Cauldron CDH build, we need to make some adjustments in the build process to satisfy CDH-internal build dependencies with artifacts generated locally, rather than download them from repo1.maven.org etc. E.g. all component should use locally-generated avro-snapshot jars instead of publicly released ones, etc. For Ant, we achieve that by giving it a special ivysettings.xml file, that limits artifact downloading to the local on-disk maven repository and Cloudera artifactory server. All CDH components except Solr allow the user to specify a custom ivysettings.xml file by overriding -Divysettings.xml property. We need to add the same feature to Solr. It can be easily achieved by changing several lines in solr/lucene/common-build.xml.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12804270/SOLR-9109.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "SOLR-9121", "change_description": "/", "change_title": "ant precommit fails on ant check-lib-versions", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.1", "detail_description": "e.g.  http://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/16766/", "patch_link": "https://issues.apache.org/jira/secure/attachment/12804412/SOLR-9121-passthrough.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7206", "change_description": ": Improve the ToParentBlockJoinQuery's explain by including the explain\nof the best matching child doc.", "change_title": "nest child query explain into ToParentBlockJoinQuery.BlockJoinScorer.explain(int)", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "6.1,7.0", "detail_description": "Now to parent query match is explained with {{Score based on child doc range from .. to .. }} that's quite useless.  It's proposed to nest child query match explanation from the first matching child document into parent explain. WDYT?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12805550/LUCENE-7206-one-child-with-tests.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Other", "change_id": "LUCENE-7307", "change_description": ": Add getters to the PointInSetQuery and PointRangeQuery queries.", "change_title": "Add getters to PointInSetQuery and PointRangeQuery classes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12807386/LUCENE_7307.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Build", "change_id": "LUCENE-7292", "change_description": ": Use '-release' instead of '-source/-target' during\ncompilation on Java 9+ to ensure real cross-compilation.", "change_title": "Change build system to use \"--release 8\" instead of \"-source/-target\" when invoking javac", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "6.1,6.3,7.0", "detail_description": "Currently we pass -source 1.8 -target 1.8 to javac and javadoc when compiling our source code. We all know that this brings problems, because cross-compiling does not really work. We create class files that are able to run on Java 8, but when it is compiled with java 9, it is not sure that some code may use Java 9 APIs that are not available in Java 8. Javac prints a warning about this (it complains about the bootclasspath not pointing to JDK 8 when used with source/target 1.8). Java 8 is the last version of Java that has this trap. From Java 9 on, instead of passing source and target, the recommended way is to pass a single -release 8 parameter to javac (see http://openjdk.java.net/jeps/247). This solves the bootsclasspath problem, because it has all the previous java versions as \"signatures\" (like forbiddenapis), including deprecated APIs,... everything included. You can find this in the $JAVA_HOME/lib/ct.sym file (which is a ZIP file, so you can open it with a ZIP tool of your choice). In Java 9+, this file also contains all old APIs from Java 6+. When invoking the compiler with -release 8, there is no risk of accidentally using API from newer versions. The migration here is quite simple: As we require Java 8 already, there is (theoretically) no need to pass source and target anymore. It is enough to just pass -release 8 if we detect Java 9 as compiling JVM. Nevertheless I plan to do the following: By this we could theoretically remove the check from smoketester about the compiling JDK (the MANIFEST check), because although compiled with Java 9, the class files were actually compiled against the old Java API from ct.sym file. I will also align the warnings to reenable -Xlint:options.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12805292/LUCENE-7292.patch", "patch_content": "none"}
{"library_version": "6.1.0", "change_type": "Build", "change_id": "LUCENE-7296", "change_description": ": Update forbiddenapis to version 2.1.", "change_title": "Update forbiddenapis to version 2.1", "detail_type": "Improvement", "detail_affect_versions": "7.0", "detail_fix_versions": "7.0", "detail_description": "Forbiddenapis v2.1 was released a few minutes ago. The new version supports:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12805547/LUCENE-7296.patch", "patch_content": "none"}

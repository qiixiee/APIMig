{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7417", "change_description": ": The standard Highlighter could throw an IllegalArgumentException when\ntrying to highlight a query containing a degenerate case of a MultiPhraseQuery with one\nterm.", "change_title": "Highlighting fails for MultiPhraseQuery's with one clause", "detail_type": "Bug", "detail_affect_versions": "5.2.1,5.5.2", "detail_fix_versions": "5.5.4,6.2.1,6.3", "detail_description": "This bug is the same issue as LUCENE-7231, just for MultiPhraseQuery instead of PhraseQuery. The fix is the same as well. To reproduce, change the test that was committed for LUCENE-7231 to use a MultiPhraseQuery. It results in the same error java.lang.IllegalArgumentException: Less than 2 subSpans.size():1 I have a patch including a test against branch_5.x, it just needs to go through legal before I can post it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12827090/multiphrasequery_singleclause_highlighting.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7657", "change_description": ": Fixed potential memory leak in the case that a (Span)TermQuery\nwith a TermContext is cached.", "change_title": "Queries that reference a TermContext can cause a memory leak when they are cached", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4.1,6.5,7.0", "detail_description": "The TermContext class has a reference to the top reader context of the IndexReader that was used to build it. So if you build a (Span)TermQuery that references a TermContext and this query gets cached, then it will keep holding a reference to the index reader, even after the latter gets closed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12849090/LUCENE-7657.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7647", "change_description": ": Made stored fields reclaim native memory more aggressively when\nconfigured with BEST_COMPRESSION. This could otherwise result in out-of-memory\nissues.", "change_title": "CompressingStoredFieldsFormat should reclaim memory more aggressively", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4.1,6.5,7.0", "detail_description": "When stored fields are configured with BEST_COMPRESSION, we rely on garbage collection to reclaim Deflater/Inflater instances. However these classes use little JVM memory but may use significant native memory, so if may happen that the OS runs out of native memory before the JVM collects these unreachable Deflater/Inflater instances. We should look into reclaiming native memory more aggressively.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12848559/LUCENE-7647.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7562", "change_description": ": CompletionFieldsConsumer sometimes throws\nNullPointerException on ghost fields", "change_title": "CompletionFieldsConsumer throws NPE on ghost fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "If you index SuggestField for some field X, but later delete all documents with that field, it can cause a ghost situation where the field infos believes field X exists yet the postings do not. I believe this bug is the root cause of this ES issue: https://github.com/elastic/elasticsearch/issues/21500", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838953/LUCENE-7562.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7547", "change_description": ": JapaneseTokenizerFactory was failing to close the\ndictionary file it opened", "change_title": "JapaneseTokenizerFactory opens dictionary file but never closes it again", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "JapaneseTokenizerFactory opens dictionary file in line 130 InputStream stream = loader.openResource(userDictionaryPath); but never closes it again. This leads to too many open files after after a couple of query executions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-6914", "change_description": ": Fixed DecimalDigitFilter in case of supplementary code points.", "change_title": "DecimalDigitFilter skips characters in some cases (supplemental?)", "detail_type": "Bug", "detail_affect_versions": "5.4", "detail_fix_versions": "5.5.4,6.3,7.0", "detail_description": "Found this while writing up the solr ref guide for DecimalDigitFilter. With input like \"ðŸ™ðŸ¡ðŸ ðŸœ\" (\"Double Struck\" 1984) the filter produces \"1ðŸ¡8ðŸœ\" (1, double struck 9, 8, double struck 4)  add some non-decimal characters in between the digits (ie: \"ðŸ™xðŸ¡xðŸ xðŸœ\") and you get the expected output (\"1x9x8x4\").  This doesn't affect all decimal characters though, as evident by the existing test cases. Perhaps this is an off by one bug in the \"if the original was supplementary, shrink the string\" code path?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12774917/LUCENE-6914.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7440", "change_description": ": Document id skipping (PostingsEnum.advance) could throw an\nArrayIndexOutOfBoundsException exception on large index segments (>1.8B docs)\nwith large skips.", "change_title": "Document skipping on large indexes is broken", "detail_type": "Bug", "detail_affect_versions": "2.2", "detail_fix_versions": "5.5.4,6.2.1,6.3,7.0", "detail_description": "Large skips on large indexes fail. Anything that uses skips (such as a boolean query, filtered queries, faceted queries, join queries, etc) can trigger this bug on a sufficiently large index. The bug is a numeric overflow in MultiLevelSkipList that has been present since inception (Lucene 2.2).  It may not manifest until one has a single segment with more than ~1.8B documents, and a large skip is performed on that segment. Typical stack trace on Lucene7-dev: Typical stack trace on Lucene4.10.3:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12827894/LUCENE-7440.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Bug Fixes", "change_id": "LUCENE-7570", "change_description": ": IndexWriter may deadlock if a commit is running while\nthere are too many merges running and one of the merges hits a\ntragic exception", "change_title": "Tragic events during merges can lead to deadlock", "detail_type": "Bug", "detail_affect_versions": "5.5,7.0", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "When an IndexWriter#commit() is stalled due to too many pending merges, you can get a deadlock if the currently active merge thread hits a tragic event. We hit this bug with Lucene 5.5, but I looked at the code in the master branch and it looks like the deadlock still exists there as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12842472/LUCENE-7570.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Other", "change_id": "LUCENE-6989", "change_description": ": Backport MMapDirectory's unmapping code from Lucene 6.4 to use\nMethodHandles. This allows it to work with Java 9 (EA build 150 and later).", "change_title": "Implement MMapDirectory unmapping for coming Java 9 changes", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.0,6.4", "detail_description": "Originally, the sun.misc.Cleaner interface was declared as \"critical API\" in JEP 260 Unfortunately the decission was changed in favor of a oficially supported java.lang.ref.Cleaner API. A side effect of this change is to move all existing sun.misc.Cleaner APIs into a non-exported package. This causes our forceful unmapping to no longer work, because we can get the cleaner instance via reflection, but trying to invoke it will throw one of the new Jigsaw RuntimeException because it is completely inaccessible. This will make our forceful unmapping fail. There are also no changes in Garbage collector, the problem still exists. For more information see this mailing list thread. This commit will likely be done, making our unmapping efforts no longer working. Alan Bateman is aware of this issue and will open a new issue at OpenJDK to allow forceful unmapping without using the now private sun.misc.Cleaner. The idea is to let the internal class sun.misc.Cleaner implement the Runable interface, so we can simply cast to runable and call the run() method to unmap. The code would then work. This will lead to minor changes in our unmapper in MMapDirectory: An instanceof check and casting if possible. I opened this issue to keep track and implement the changes as soon as possible, so people will have working unmapping when java 9 comes out. Current Lucene versions will no longer work with Java 9.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844298/LUCENE-6989-v3-testFixes.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Build", "change_id": "LUCENE-7543", "change_description": ": Make changes-to-html target an offline operation, by moving the\nLucene and Solr DOAP RDF files into the Git source repository under\ndev-tools/doap/ and then pulling release dates from those files, rather than\nfrom JIRA.", "change_title": "Make changes-to-html target an offline operation", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,5.6,6.0.2,6.1.1,6.2.2,6.3.1,6.4,6.4.1,6.5,7.0", "detail_description": "Currently changes-to-html pulls release dates from JIRA, and so fails when JIRA is inaccessible (e.g. from behind a firewall). SOLR-9711 advocates adding a build sysprop to ignore JIRA connection failures, but I'd rather make the operation always offline. In an offline discussion, hossman advocated moving Lucene's and Solr's doap.rdf files, which contain all of the release dates that the changes-to-html now pulls from JIRA, from the CMS Subversion repository (downloadable from the website at http://lucene.apache.org/core/doap.rdf and http://lucene.apache.org/solr/doap.rdf) to the Lucene/Solr git repository. If we did that, then the process could be entirely offline if release dates were taken from the local doap.rdf files instead of downloaded from JIRA.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12839932/LUCENE-7543-drop-XML-Simple.patch", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Build", "change_id": "LUCENE-7596", "change_description": ": Update Groovy to version 2.4.8 to allow building with Java 9\nbuild 148+. Also update JGit version for working-copy checks. This does not\nfix all issues with Java 9, but allows to build the distribution.", "change_title": "Update Groovy to 2.4.8 in build system", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "The current version of Groovy used by several Ant components is incompatible with Java 9 build 148+. We need to update to 2.4.8 once it is released: http://mail.openjdk.java.net/pipermail/jigsaw-dev/2016-December/010474.html", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.5.4", "change_type": "Build", "change_id": "LUCENE-7651", "change_description": ": Backport (Lucene 6.4.1) fix for Java 8u121 to allow documentation\nbuild to inject \"Google Code Prettify\" without adding Javascript to Javadocs's", "change_title": "Javadocs build fails with Java 8 update 121", "detail_type": "Bug", "detail_affect_versions": "6.4", "detail_fix_versions": "6.4.1,6.5,7.0", "detail_description": "Oracle released the recent Java 8 security update (u121). The Jenkins builds fail with the following error while building the Javadocs: This is caused by the Javascript added to pretty-print code examples. We load this in the page footer \"<bottom>\" parameter. Surely, it will be posisble to simply add the mentioned argument, but this will break builds with earlier Java 8 versions. This is nowhere documented, I haven't seen any documentation about this flag nowhere, so I assume this is a bug in Java. They can't change or add command line parameters in minor updates of Java 8. I will ask on the OpenJDK mailing lists if this is a bug (maybe accidentally backported from Java 9).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12848774/LUCENE-7651.patch", "patch_content": "none"}

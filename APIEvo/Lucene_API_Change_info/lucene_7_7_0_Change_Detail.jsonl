{"library_version": "7.7.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8527", "change_description": ": StandardTokenizer and UAX29URLEmailTokenizer now support Unicode 9.0,\nand provide Unicode UTS#51 v11.0 Emoji tokenization with the \"<EMOJI>\" token type.", "change_title": "Upgrade JFlex to 1.7.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "JFlex 1.7.0, supporting Unicode 9.0, was released recently: http://jflex.de/changelog.html#jflex-1.7.0.  We should upgrade.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12953536/LUCENE-8527.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Build", "change_id": "LUCENE-8611", "change_description": ": Update randomizedtesting to 2.7.2, JUnit to 4.12, add hamcrest-core\ndependency.", "change_title": "Update randomizedtesting to 2.7.2, JUnit to 4.12, add hamcrest-core dependency", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.7", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12952428/LUCENE-8611-fix-maven.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Build", "change_id": "LUCENE-8537", "change_description": ": ant test command fails under lucene/tools", "change_title": "ant test command fails under lucene/tools", "detail_type": "Bug", "detail_affect_versions": "8.0", "detail_fix_versions": "7.7,8.0", "detail_description": "The ant test command executed under lucene/tools folder fails because it does not have junit.classpath property. Since the module does not have any test folder we could override the -test and -check-totals targets. I ran into this issue when uploaded a patch where I removed an import from this module. This triggered a module-level build during precommit that failed with this error.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12944606/LUCENE-8537.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8669", "change_description": ": Fix LatLonShape WITHIN queries that fail with Multiple search Polygons\nthat share the dateline.", "change_title": "LatLonShape WITHIN queries fail with Multiple search Polygons that share the dateline", "detail_type": "Bug", "detail_affect_versions": "7.7,8.0", "detail_fix_versions": "None", "detail_description": "LatLonShape.newPolygonQuery does not support dateline crossing polygons. It is therefore up to the calling application / user to split dateline crossing polygons into a MultiPolygon query with two search polygons that share the dateline. This, however, does not produce expected results because EdgeTree.internalComponentRelateTriangle does not differentiate between a triangle that CROSSES or is WITHIN the target polygon. Therefore MultiPolygon WITHIN queries that share the dateline behave as an INTERSECT and will therefore produce incorrect results. Consider the following test, for example:  In the example above, a dateline spanning polygon is indexed as a MultiPolygon with two polygons that share the dateline. Similarly, a polygon that spans the dateline is provided as  two polygons that share the dateline in a WITHIN query. The indexed polygon should be returned as a match; but it does not.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956943/LUCENE-8669.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8603", "change_description": ": Fix the inversion of right ids for additional nouns in the Korean user dictionary.", "change_title": "There seems to be a typo in the user dictionary constant definition part", "detail_type": "Bug", "detail_affect_versions": "7.5", "detail_fix_versions": "7.7,8.0", "detail_description": "Hello, It seems that you found a typo while changing the dictionary version of nori to the latest version. I think it should be changed as shown below. Please review. Thank you.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8624", "change_description": ": int overflow in ByteBuffersDataOutput.size().", "change_title": "int overflow in ByteBuffersDataOutput.size()", "detail_type": "Bug", "detail_affect_versions": "7.5", "detail_fix_versions": "7.7", "detail_description": "Hi, When indexing large data sets with ByteBuffersDirectory, an exception like the below is thrown: Caused by: java.lang.IllegalArgumentException: cannot write negative vLong (got: -4294888321)  at org.apache.lucene.store.DataOutput.writeVLong(DataOutput.java:225)  at org.apache.lucene.codecs.lucene50.Lucene50SkipWriter.writeSkipData(Lucene50SkipWriter.java:182)  at org.apache.lucene.codecs.MultiLevelSkipListWriter.bufferSkip(MultiLevelSkipListWriter.java:143)  at org.apache.lucene.codecs.lucene50.Lucene50SkipWriter.bufferSkip(Lucene50SkipWriter.java:162)  at org.apache.lucene.codecs.lucene50.Lucene50PostingsWriter.startDoc(Lucene50PostingsWriter.java:228)  at org.apache.lucene.codecs.PushPostingsWriterBase.writeTerm(PushPostingsWriterBase.java:148)  at org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter$TermsWriter.write(BlockTreeTermsWriter.java:865)  at org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter.write(BlockTreeTermsWriter.java:344)  at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:105)  at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.merge(PerFieldPostingsFormat.java:169)  at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:244)  at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:139)  at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4453)  at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4075) The exception is caused by an integer overflow while calling getFilePointer() in Lucene50PostingsWriter, which eventually calls the size() method in ByteBuffersDataOutput. In my case, I had a blockCount = 65 and a blockSize() = 33554432 which overflows fullBlockSize. The fix:  Thanks", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8625", "change_description": ": int overflow in ByteBuffersDataInput.sliceBufferList.", "change_title": "int overflow in ByteBuffersDataInput.sliceBufferList", "detail_type": "Bug", "detail_affect_versions": "7.5", "detail_fix_versions": "7.7", "detail_description": "Hi, Once I fixed the bug here, https://issues.apache.org/jira/browse/LUCENE-8624, I encountered another Integer Overflow error in ByteBuffersDataInput: Exception in thread \"Lucene Merge Thread #1540\" Exception in thread \"main\" org.apache.lucene.index.MergePolicy$MergeException: java.lang.ArithmeticException: integer overflow  at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:705)  at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:685)  Caused by: java.lang.ArithmeticException: integer overflow  at java.lang.Math.toIntExact(Math.java:1011)  at org.apache.lucene.store.ByteBuffersDataInput.sliceBufferList(ByteBuffersDataInput.java:299)  at org.apache.lucene.store.ByteBuffersDataInput.slice(ByteBuffersDataInput.java:223)  at org.apache.lucene.store.ByteBuffersIndexInput.clone(ByteBuffersIndexInput.java:186)  at org.apache.lucene.store.ByteBuffersDirectory$FileEntry.openInput(ByteBuffersDirectory.java:254)  at org.apache.lucene.store.ByteBuffersDirectory.openInput(ByteBuffersDirectory.java:223)  at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:100)  at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:100)  at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:100)  at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:157)  at org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat.write(Lucene50CompoundFormat.java:89)  at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:5004)  at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4517)  at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4075)  at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:626)  at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:663) The exception is caused by a Math.toIntExact in sliceBufferList in ByteBuffersDataInput. Removing the Math.toIntExact works but I'm not sure if the logic will still be right since absEnd is used to calculate endOffset after a few lines:  Thanks,", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8639", "change_description": ": Newly created threadstates while flushing / refreshing can cause duplicated\nsequence IDs on IndexWriter.", "change_title": "SeqNo accounting in IW is broken if many threads start indexing while we flush.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "While this is rare in the wild we have a test failure that shows that our seqNo accounting is broken  when we carry over seqNo to a new delete queue. We had this test-failure: The reason seems to be that we don't prevent new threadstates from being created while we move over to a new delete queue.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8649", "change_description": ": LatLonShape's within and disjoint queries can return false positives with\nindexed multi-shapes.", "change_title": "LatLonShape: Within and disjoint queries don’t work with indexed multishapes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "Within and disjoint queries return wrong results (false positives) when querying for fields containing more than one shape. For example, a multipolygon will return true for a within query if some of the polygons are within and the other are disjoint. The same query will return true for disjoint. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955638/LUCENE-8649.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8654", "change_description": ": Polygon2D#relateTriangle returns the wrong answer if polygon is inside\nthe triangle.", "change_title": "Polygon2D#relateTriangle returns the wrong answer if polygon is inside the triangle", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "The method returns CELL_OUTSIDE_QUERY but the right answer should be CELL_CROSSES_QUERY.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955817/LUCENE-8654.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8650", "change_description": ": ConcatenatingTokenStream did not correctly clear its state in reset(), and\nwas not propagating final position increments from its child streams correctly.", "change_title": "ConcatenatingTokenStream does not end() nor reset() properly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7", "detail_description": "All (I think) TokenStream implementations set a \"final offset\" after calling super.end() in their end() methods. ConcatenatingTokenStream fails to do this. Because of this, it's final offset is not readable and DefaultIndexingChain in turn fails to set the lastStartOffset properly. This results in problems with indexing which can include unsearchable content or IllegalStateExceptions. ConcatenatingTokenStream also fails to reset() properly. Specifically, it does not set its currentSource and offsetIncrement back to 0. Because of this, copyField directives (in the schema) do not work and content becomes unsearchable. I've created a few patches that illustrate the problem and then provide a fix. The first patch enhances the TestConcatenatingTokensStream to check for finalOffset, which as you can see ends up being 0. I created the next patch separately because it includes extra classes used for the testing that Lucene may or may not want to merge in. This patch adds an integration test that loads some content into the 'text' field. The schema then copies it to 'content' using a copyField directive. The test searches in the content field for the loaded text and fails to find it even though the field does contain the content. Flip the debug flag to see a nicer printout of the response and what's in the index. Notice that the added class I alluded to is KeywordTokenStream .This class had to be added because of another (ultimately unrelated) problem: ConcatenatingTokenStream cannot concatenate Tokenziers. This is because Tokenizer violates the contract put forth by TokenStream.reset(). This separate problem warrants its own ticket, though. However, ultimately KeywordTokenStream may be useful to others and could be considered for adding to the repo. The third patch finally fixes ConcatenatingTokenStream by storing and setting a finalOffset as the last task in the end() method, and resetting currentSource, offsetIncrement and finalOffset when reset() is called.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955678/LUCENE-8650.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Bug fixes", "change_id": "LUCENE-8676", "change_description": ": The Korean tokenizer does not update the last position if the backtrace is caused\nby a big buffer (1024 chars).", "change_title": "TestKoreanTokenizer#testRandomHugeStrings failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "KoreanTokenizer#testRandomHugeString failed in CI with the following exception: I am able to reproduce locally with: After some investigation I found out that the position of the buffer is not updated when the maximum backtrace size is reached (1024).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12957171/LUCENE-8676.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "New Features", "change_id": "LUCENE-8026", "change_description": ": ExitableDirectoryReader may now time out queries that run on\npoints such as range queries or geo queries.", "change_title": "ExitableDirectoryReader does not instrument points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "This means it cannot interrupt range or geo queries.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "New Features", "change_id": "LUCENE-8508", "change_description": ": IndexWriter can now set the created version via\nIndexWriterConfig#setIndexCreatedVersionMajor. This is an expert feature.", "change_title": "Add the ability to set the version that created the index", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "It is sometimes useful to be able to set the creation version of an index. For instance in a sharded environment, it may be useful to make sure that all indices are created with the same major in order to be able to increase/reduce the number of shards later on using IndexWriter#addIndexes. This is an expert feature.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12940243/LUCENE-8508.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "New Features", "change_id": "LUCENE-8601", "change_description": ": Attributes set in the IndexableFieldType for each field during indexing will\nnow be recorded into the corresponding FieldInfo's attributes, accessible at search\ntime", "change_title": "Adding attributes to IndexFieldType", "detail_type": "Improvement", "detail_affect_versions": "7.5", "detail_fix_versions": "7.7,8.0", "detail_description": "Today, we can write a custom Field using custom IndexFieldType, but when the DefaultIndexingChain converts IndexFieldType to FieldInfo, only few key informations such as indexing options and doc value type are retained. The Codec gets the FieldInfo, but not the type details.     FieldInfo has support for 'attributes' and it would be great if we can add 'attributes' to IndexFieldType also and copy it to FieldInfo's 'attribute'.     This would allow someone to write a custom codec (extending docvalueformat for example) for only the 'special field' that he wants and delegate the rest of the fields to the default codec.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12953073/LUCENE-8601.05.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8463", "change_description": ": TopFieldCollector can now early-terminates queries when sorting by SortField.DOC.", "change_title": "Early-terminate queries sorted by SortField.DOC", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "Currently TopFieldCollector only early-terminates when the search sort is a prefix of the index sort, but it could also early-terminate when sorting by doc id.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8562", "change_description": ": Speed up merging segments of points with data dimensions by only sorting on the indexed\ndimensions.", "change_title": "Speed up merging segments of points with data dimensions", "detail_type": "Improvement", "detail_affect_versions": "7.7,8.0", "detail_fix_versions": "7.7,8.0", "detail_description": "Currently when merging segments of points with data dimensions, all dimensions are sorted and carried over down the tree even though only indexing dimensions are needed to build the BKD tree. This is needed so leaf node data can be compressed by common prefix. But when using MutablePointValues, this ordering is done at the leaf level so we can se a similar approach from data dimensions and delay the sorting at leaf level. This seems to speed up indexing time as well as reduce the storage needed for building the index.   ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12949498/LUCENE-8562.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8529", "change_description": ": TopSuggestDocsCollector will now use the completion key to tiebreak completion\nsuggestion with identical scores.", "change_title": "Use the completion key to tiebreak completion suggestion", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "Today the completion suggester uses the document id to tiebreak completion suggestion with same scores. It would improve the stability of the sort to use the surface form of suggestions as the first tiebreaker.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12943477/LUCENE-8529.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8575", "change_description": ": SegmentInfos#toString now includes attributes and diagnostics.", "change_title": "Improve toString() in SegmentInfo", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "I saw the following code in SegmentInfo class. Of course, we can.  So I wrote a code for that part. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12949850/LUCENE-8575.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8548", "change_description": ": The KoreanTokenizer no longer splits unknown words on combining diacritics and\ndetects script boundaries more accurately with Character#UnicodeScript#of.", "change_title": "Reevaluate scripts boundary break in Nori's tokenizer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "This was first reported in https://issues.apache.org/jira/browse/LUCENE-8526: ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12949740/LUCENE-8548.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8581", "change_description": ": Change LatLonShape encoding to use 4 bytes Per Dimension.", "change_title": "Change LatLonShape encoding to use 4 BYTES Per Dimension", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "LatLonShape tessellated triangles currently use a relatively naive encoding with the first four dimensions as the bounding box of the triangle and the last three dimensions as the vertices of the triangle. To encode the x,y vertices in the last three dimensions requires bytesPerDim to be set to 8, with 4 bytes for the x & y axis, respectively. We can reduce bytesPerDim to 4 by encoding the index(es) of the vertices shared by the bounding box along with the orientation of the triangle. This also opens the door for supporting CONTAINS queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12952140/LUCENE-8581.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8527", "change_description": ": Upgrade JFlex dependency to 1.7.0; in StandardTokenizer and UAX29URLEmailTokenizer,\nincrease supported Unicode version from 6.3 to 9.0, and support Unicode UTS#51 v11.0 Emoji tokenization.", "change_title": "Upgrade JFlex to 1.7.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "JFlex 1.7.0, supporting Unicode 9.0, was released recently: http://jflex.de/changelog.html#jflex-1.7.0.  We should upgrade.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12953536/LUCENE-8527.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Improvements", "change_id": "LUCENE-8640", "change_description": ": Date Range format validation", "change_title": "validate delimiters when parsing date ranges", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "DateRangePrefixTree.parseCalendar() should validate delimiters to rejects dates like 2000-11T13", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956539/LUCENE-8640.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Optimizations", "change_id": "LUCENE-8552", "change_description": ": FieldInfos.getMergedFieldInfos no longer does any merging if there is <= 1 segment.", "change_title": "optimize getMergedFieldInfos for one-segment FieldInfos", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.7", "detail_description": "FieldInfos.getMergedFieldInfos could trivially return the FieldInfos of the first and only LeafReader if there is only one LeafReader. Also... if there is more than one LeafReader, and if FieldInfos & FieldInfo implemented equals() & hashCode() (including a cached hashCode), maybe we could also call equals() iterating through the FieldInfos to see if we should bother adding it to the FieldInfos.Builder?  Admittedly this is speculative; may not be worth the bother.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Optimizations", "change_id": "LUCENE-8590", "change_description": ": BufferedUpdates now uses an optimized storage for buffering docvalues updates that\ncan safe up to 80% of the heap used compared to the previous implementation and uses non-object\nbased datastructures.", "change_title": "Optimize DocValues update datastructures", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "Today we are using a LinkedHashMap to buffer doc-values updates in     BufferedUpdates. This on the one hand uses an Object based datastructure     and on the other requires re-encoding the data into a more compact representation     once the BufferedUpdates are frozen. This change uses a more compact represenation     for the updates already in the BufferedUpdates in a parallel-array like datastructure     that can be reused in FrozenBufferedDeletes. It also adds an much simpler to use     API to consume the updates and allows for internal memory optimization for common     case updates.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Optimizations", "change_id": "LUCENE-8598", "change_description": ": Moved to the default accepted overhead ratio for packet ints in DocValuesFieldUpdats\nyields an up-to 4x performance improvement when applying doc values updates.", "change_title": "Improve field updates packed values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "DocValuesFieldUpdats are using compact settings for packet ints that causes     dramatic slowdowns when the updates are finished and sorted. Moving to the default     accepted overhead ratio yields up to 4x improvements in applying updates. This change     also improves the packing of numeric values since we know the value range in advance and     can choose a different packing scheme in such a case.     Overall this change yields a good performance improvement since 99% of the times of applying     DV field updates are spend in the sort method which essentially makes applying the updates     4x faster.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Optimizations", "change_id": "LUCENE-8599", "change_description": ": Use sparse bitset to store docs in SingleValueDocValuesFieldUpdates.", "change_title": "Use sparse bitset to store docs in SingleValueDocValuesFieldUpdates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "Using a sparse bitset in SingleValueDocValuesFieldUdpates allows storing     which documents have an update much more efficient and prevents the need     to sort the docs array altogether that showed to be a significant bottleneck     in LUCENE-8598. Using the spares bitset yields another 10x performance improvement     in applying updates versus the changes proposed in LUCENE-8598.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Optimizations", "change_id": "LUCENE-8600", "change_description": ": Doc-value updates get applied faster by sorting with quicksort,\nrather than an in-place mergesort, which needs to perform fewer swaps.", "change_title": "DocValuesFieldUpdates should use a better sort", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "This is a follow-up to LUCENE-8598: Simon identified that swaps are a bottleneck to applying doc-value updates, in particular due to the overhead of packed ints. It turns out that InPlaceMergeSorter does LOTS of swaps in order to perform in-place. Replacing with a more efficient sort should help.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12951341/LUCENE-8600.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Optimizations", "change_id": "LUCENE-8623", "change_description": ": Decrease I/O pressure when merging high dimensional points.", "change_title": "Decrease I/O pressure when merging high dimensional points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0,9.0", "detail_description": "Related with LUCENE-8619, after indexing 60 million shapes(~1.65 billion triangles) using LatLonShape, the index directory grew to a size of 265 GB when performing merging of different segments. After the processes were over the index size was 57 GB. As an example imagine we are merging several segments to a new segment of size 10GB (4 dimensions). The BKD tree merging logic will create the following files: 1) Level 0: 4 copies of the data, each one sorted by one dimensions : 40GB 2) Level 1: 6 copies of half of the data, left and right : 30GB 3) Level 2: 6 copies of one quarter of the data, left and right : 15 GB 4) Level 3: 6 more copies halving the previous level, left and right : 7.5 GB 5) Level 4: 6 more copies halving the previous level, left and right : 3.75 GB  and so on... So it requires around 100GB to merge that segment. In this issue is proposed to delay the creation of sorted copies to when they are needed. It reduces the total size required to half of what it is needed now.   ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12954126/LUCENE-8623.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Other", "change_id": "LUCENE-8573", "change_description": ": BKDWriter now uses FutureArrays#mismatch to compute shared prefixes.", "change_title": "BKDWriter should use FutureArrays#mismatch", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "In a number of places, BKDWriter tries to find the first mismatching byte between multiple arrays with a for loop. It could use the safer FutureArrays#mismatch instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Other", "change_id": "LUCENE-8605", "change_description": ": Separate bounding box spatial logic from query logic on LatLonShapeBoundingBoxQuery.", "change_title": "Separate BBOX logic in LatLonShapeBoundingBoxQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.7,8.0", "detail_description": "Currently LatLonShapeBoundingBoxQuery contains the logic to relate the query bounding box with the incoming triangles. This issue proposes the creation of a new class Rectangle2D that encapsulates the spatial logic of the bounding box", "patch_link": "https://issues.apache.org/jira/secure/attachment/12951504/LUCENE-8605.patch", "patch_content": "none"}
{"library_version": "7.7.0", "change_type": "Other", "change_id": "LUCENE-8609", "change_description": ": Deprecated IndexWriter#numDocs() and IndexWriter#maxDoc() in favor of IndexWriter#getDocStats()\nthat allows to get consistent numDocs and maxDoc stats that are not subject to concurrent changes.", "change_title": "Allow getting consistent docstats from IndexWriter", "detail_type": "Improvement", "detail_affect_versions": "7.7,8.0", "detail_fix_versions": "7.7,8.0", "detail_description": "Today we have #numDocs() and #maxDoc() on IndexWriter. This is enough     to get all stats for the current index but it's subject to concurrency     and might return numbers that are not consistent ie. some cases can     return maxDoc < numDocs which is undesirable. This change adds a getDocStats()     method to index writer to allow fetching consistent numbers for these stats.", "patch_link": "none", "patch_content": "none"}

{"library_version": "8.10.0", "change_type": "API Changes", "change_id": "LUCENE-9962", "change_description": ": DrillSideways allows sub-classes to provide \"drill down\" FacetsCollectors. They\nmay provide a null collector if they choose to bypass \"drill down\" facet collection.", "change_title": "DrillSideways users should be able to opt-out of \"drill down\" facet collecting", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "The DrillSideways search methods will always populate a FacetsCollector for the \"drill down\" dimensions in addition to the \"drill sideways\" dimensions. For most cases, this makes sense, but it would be nice if users had a way to opt-out of this collection. It's possible a user may not care to do any faceting on \"drill down\" dims, or may have custom needs for facet collecting on the \"drill downs.\" For the latter case, the user might want to provide a Collector/CollectorManager that does facet collecting with some custom logic (e.g., behind a MultiCollector/MultiCollectorManager), in which case the population of an additional FacetsCollector in DrillSideways is wasteful. The DrillSidewaysScorer already supports a null drillDownCollector gracefully, so this change should mostly just involve creating a protected method in DrillSideways for the purpose of creating a \"drill down\" FacetsCollector that users can override by providing null.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "API Changes", "change_id": "LUCENE-9902", "change_description": ": Change the getValue method from IntTaxonomyFacets to be protected instead of private.\nUsers can now access the count of an ordinal directly without constructing an extra FacetLabel.\nAlso use variable length arguments for the getOrdinal call in TaxonomyReader.", "change_title": "Update faceting API to use modern Java features", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I was using the public int getOrdinal(String dim, String[] path) API for a single path String and found myself creating an array with a single element. We can start using variable length args for this method. I also propose this change:  I wanted to know the specific count of an ordinal using using the getValue API from IntTaxonomyFacets but the method is private. It would be good if we could change it to protected so that users can know the value of an ordinal without looking up the FacetLabel and then checking its value.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "API Changes", "change_id": "LUCENE-10036", "change_description": ": Replaced the ScoreCachingWrappingScorer ctor with a static factory method that\nensures unnecessary wrapping doesn't occur.", "change_title": "Ensure ScoreCachingWrappingScorer doesn't unnecessarily wrap another ScoreCachingWrappingScorer", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.10", "detail_fix_versions": "9.0,8.10", "detail_description": "This is a trivial issue, but it's easy to mistakenly \"double wrap\" an instance of ScoreCachingWrappingScorer, which is a bit wasteful. The calling code currently needs to check the instance type of the Scorable they intend to wrap to avoid this. FieldComparator is actually the only calling code that does this check. It would be nice to add a factory method that encapsulates this check in ScoreCachingWrappingScorer so that calling code doesn't need to worry about it. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "API Changes", "change_id": "LUCENE-10027", "change_description": ": Add a new Directory reader open API from indexCommit and\na custom comparator for sorting leaf readers", "change_title": "Custom order for leaves in DirectoryReader opened from IndexCommit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "This is a left-over from LUCENE-9507 to provide a leaf sorter also for directory readers opened from IndexCommit. This part was missed in  LUCENE-9507.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "API Changes", "change_id": "LUCENE-7020", "change_description": ": TieredMergePolicy#setMaxMergeAtOnceExplicit is deprecated\nand the number of segments that get merged via explicit merges is unlimited\nby default.", "change_title": "TieredMergePolicy - cascade maxMergeAtOnce setting to maxMergeAtOnceExplicit", "detail_type": "Improvement", "detail_affect_versions": "5.4.1", "detail_fix_versions": "8.10", "detail_description": "SOLR-8621 covers improvements in configuring a merge policy in Solr. Discussions on that issue brought up the fact that if large values are configured for maxMergeAtOnce and segmentsPerTier, but maxMergeAtOnceExplicit is not changed, then doing a forceMerge is likely to not work as expected. When I first configured maxMergeAtOnce and segmentsPerTier to 35 in Solr, I saw an optimize (forceMerge) fully rewrite most of the index twice in order to achieve a single segment, because there were approximately 80 segments in the index before the optimize, and maxMergeAtOnceExplicit defaults to 30.  On advice given via the solr-user mailing list, I configured maxMergeAtOnceExplicit to 105 and have not had that problem since. I propose that setting maxMergeAtOnce should also set maxMergeAtOnceExplicit to three times the new value – unless the setMaxMergeAtOnceExplicit method has been invoked, indicating that the user wishes to set that value themselves.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12787222/LUCENE-7020.patch", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "New Features", "change_id": "LUCENE-10083", "change_description": ": Analyzer and stemmer for Telugu language", "change_title": "Telugu analyzer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "Lucene does not have an analyzer for Telugu language. I have created Telugu analyzer and testing it with SOLR for అమర్కోష్.భారత్ website. Search results are very encouraging. I will submit a pull request to get the code merged into main branch.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "New Features", "change_id": "LUCENE-10035", "change_description": ": The SimpleText codec now writes skip lists.", "change_title": "Simple text codec add  multi level skip list data", "detail_type": "Wish", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "Simple text codec add skip list data( include impact) to help understand index format，For debugging, curiosity, transparency only!! When term's docFreq greater than or equal to SimpleTextSkipWriter.BLOCK_SIZE (default value is 8), Simple text codec will write skip list, the .pst (simple text term dictionary file) file will looks like this compare with previous，we add skipList，level, skipDoc, skipDocFP, impacts, impact, freq, norm nodes, at the same, simple text codec can support advanceShallow when search time.  Because the MultiLevelSkipListWriter will write \"length\" and \"childPointer\" with VLong No!!! It can be advanceShallow when search, but why not speed up yet? Because the skip list data after docs(see the file described before), it must iterate all docs before read skip list data, so it never speed up search time. it has no \"skipOffset\" to direct read skip list data, but as mentioned before, it is For debugging, curiosity, transparency only!! If this is a problem, may be the next time, i can add \"skipOffset\", so we can read skip list data directly.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9944", "change_description": ": Allow DrillSideways users to provide their own CollectorManager without also requiring\nthem to provide an ExecutorService.", "change_title": "Implement alternative drill sideways faceting with provided CollectorManager", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "Today, if a user of DrillSideways wants to provide their own CollectorManager when invoking search, they get this alternate, \"concurrent\" implementation that creates N copies of the provided DrillDownQuery (where N is the number of drill-down dimensions) and runs them all concurrently. This is a very different implementation than the one a user would get if providing a Collector instead. Additionally, an ExecutorService must be provided when constructing a DrillSideways instance if the user wants to bring their own CollectorManager (otherwise, they'll get an unfriendly NPE when calling search). I propose adding an implementation to DrillSideways that will run the \"non-concurrent\" algorithm in the case that a user wants to provide their own CollectorManager but doesn't want to provide an ExecutorService (and doesn't want the concurrent algorithm).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9946", "change_description": ": Support for multi-value fields in LongRangeFacetCounts and\nDoubleRangeFacetCounts.", "change_title": "Support multi-value fields in range facet counting", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "The RangeFacetCounts implementations (LongRangeFacetCounts and DoubleRangeFacetCount) only work on single-valued fields today. In contrast, the more recently added LongValueFacetCounts implementation supports both single- and multi-valued fields (LUCENE-7927). I'd like to extend multi-value support to both of the LongRangeFacetCounts implementations as well. Looking through the implementations, I can't think of a good reason to not support this, but maybe I'm overlooking something?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9965", "change_description": ": Added QueryProfilerIndexSearcher and ProfilerCollector to support debugging\nquery execution strategy and timing.", "change_title": "Add tooling to introspect query execution time", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "Based on the discussion from this email thread we could add a set of classes to compile timings for different pieces of a query or multiple queries. This could be used to better debug changes in performance moving forward.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9981", "change_description": ": Operations.getCommonSuffix/Prefix(Automaton) is now much more\nefficient, from a worst case exponential down to quadratic cost in the\nnumber of states + transitions in the Automaton.  These methods no longer\nuse the costly determinize method, removing the risk of\nTooComplexToDeterminizeException", "change_title": "CompiledAutomaton.getCommonSuffix can be extraordinarily slow, even with default maxDeterminizedStates limit", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "We have a maxDeterminizedStates = 10000 limit designed to keep regexp-type queries from blowing up. But we have an adversary that will run for 268s on my laptop before hitting exception, first reported here: https://github.com/opensearch-project/OpenSearch/issues/687 When I run the test and jstack the threads, this what I see: This is really sad, as getCommonSuffixBytesRef() is only supposed to be an \"up-front\" optimization to make the actual subsequent terms-intensive part of the query faster. But it makes the whole query run for nearly 5 minutes before it does anything. So I definitely think we should improve getCommonSuffixBytesRef to be more \"best-effort\". For example, it can reduce the lower bound to 1000 and catch the exception like such: Another, maybe simpler option, is to just check that input state/transitions accounts don't exceed some low limit N. Basically this opto is geared at stuff like leading wildcard query of \"*foo\". By computing that the common suffix is \"foo\" we can spend less CPU in the terms dictionary because we can first do a memcmp before having to run data thru any finite state machine. It's really a microopt and we shouldn't be spending whole seconds of cpu on it, ever. But I still don't quite understand how the current limits are giving the behavior today, maybe there is a bigger issue and I don't want to shove something under the rug.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13026203/LUCENE-9981.patch", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9981", "change_description": ": Operations.determinize now throws TooComplexToDeterminizeException\nbased on too much \"effort\" spent determinizing rather than a precise state\ncount on the resulting returned automaton, to better handle adversarial\ncases like det(rev(regexp(\"(.*a){2000}\"))) that spend lots of effort but\nresult in smallish eventual returned automata.", "change_title": "CompiledAutomaton.getCommonSuffix can be extraordinarily slow, even with default maxDeterminizedStates limit", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "We have a maxDeterminizedStates = 10000 limit designed to keep regexp-type queries from blowing up. But we have an adversary that will run for 268s on my laptop before hitting exception, first reported here: https://github.com/opensearch-project/OpenSearch/issues/687 When I run the test and jstack the threads, this what I see: This is really sad, as getCommonSuffixBytesRef() is only supposed to be an \"up-front\" optimization to make the actual subsequent terms-intensive part of the query faster. But it makes the whole query run for nearly 5 minutes before it does anything. So I definitely think we should improve getCommonSuffixBytesRef to be more \"best-effort\". For example, it can reduce the lower bound to 1000 and catch the exception like such: Another, maybe simpler option, is to just check that input state/transitions accounts don't exceed some low limit N. Basically this opto is geared at stuff like leading wildcard query of \"*foo\". By computing that the common suffix is \"foo\" we can spend less CPU in the terms dictionary because we can first do a memcmp before having to run data thru any finite state machine. It's really a microopt and we shouldn't be spending whole seconds of cpu on it, ever. But I still don't quite understand how the current limits are giving the behavior today, maybe there is a bigger issue and I don't want to shove something under the rug.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13026203/LUCENE-9981.patch", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9983", "change_description": ": Stop sorting determinize powersets unnecessarily.", "change_title": "Stop sorting determinize powersets unnecessarily", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Spinoff from LUCENE-9981. Today, our Operations.determinize implementation builds powersets of all subsets of NFA states that \"belong\" in the same determinized state, using this algorithm. To hold each powerset, we use a malleable SortedIntSet and periodically freeze it to a FrozenIntSet, also sorted.  We pay a high price to keep these growing maps of int key, int value sorted by key, e.g. upgrading to a TreeMap once the map is large enough (> 30 entries). But I think sorting is entirely unnecessary here!  Really all we need is the ability to add/delete keys from the map, and hashCode / equals (by key only – ignoring value!), and to freeze the map (a small optimization that we could skip initially).  We only use these maps to lookup in the (growing) determinized automaton whether this powerset has already been seen. Maybe we could simply poach the IntIntScatterMap implementation from HPPC?  And then change its hashCode/{{equals }}to only use keys (not values). This change should be a big speedup for the kinds of (admittedly adversarial) regexps we saw on LUCENE-9981. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9177", "change_description": ": ICUNormalizer2CharFilter no longer requires normalization-inert\ncharacters as boundaries for incremental processing, vastly improving worst-case\nperformance.", "change_title": "ICUNormalizer2CharFilter worst case is very slow", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "ICUNormalizer2CharFilter is fast most of the times but we've had some report in Elasticsearch that some unrealistic data can slow down the process very significantly. For instance an input that consists of characters to normalize with no normalization-inert character in between can take up to several seconds to process few hundreds of kilo-bytes on my machine. While the input is not realistic, this worst case can slow down indexing considerably when dealing with uncleaned data. I attached a small test that reproduces the slow processing using a stream that contains a lot of repetition of the character `℃` and no normalization-inert character. I am not surprised that the processing is slower than usual but several seconds to process seems a lot. Adding normalization-inert character makes the processing a lot more faster so I wonder if we can improve the process to split the input more eagerly ? ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13027371/LUCENE-9177_LUCENE-8972.patch", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-10030", "change_description": ": Lazily evaluate score in DrillSidewaysScorer.doQueryFirstScoring", "change_title": "[DrillSidewaysScorer] redundant score() calculations in doQueryFirstScoring", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "Diff Motivation  1. Performance degradation: we have quite heavy custom implementation of score(). So when we started using DrillSideways, this call became top-1 in a profiler snapshot (top-3 with default scoring). We tried doUnionScoring and doDrillDownAdvanceScoring, but no luck:  doUnionScoring scores all baseQuery docIds  doDrillDownAdvanceScoring avoids some redundant docIds scorings, considering symmetric difference of top two iterator's docIds, but still scores some docIds, that will be filtered out by 3rd, 4th, ... dimension iterators  doQueryFirstScoring scores near-miss docIds  Best way is to score only true hits (where baseQuery and all N drill-down iterators match). So we suggest a small modification of doQueryFirstScoring.     2. Speaking of doQueryFirstScoring, it doesn't look like we need to calculate a score for near-miss hit, because it won't be used anywhere.  FacetsCollectorManager creates FacetsCollector with default constructor https://github.com/apache/lucene/blob/main/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollectorManager.java#L35  so FacetCollector has false for keepScores  https://github.com/apache/lucene/blob/main/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java#L119  and collectScore is not being used https://github.com/apache/lucene/blob/main/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java#L200", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9945", "change_description": ": Extend DrillSideways to support exposing FacetCollectors directly.", "change_title": "Extend DrillSideways to support exposing FacetCollectors directly", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "The DrillSideways logic currently encapsulates, 1) the creation of multiple FacetsCollector instances, and 2) the processing of those FacetsCollectors into a single Facets instance. While I suspect this works well for most common cases, and is simple to understand, it's difficult to extend to more advanced cases. I propose extending DrillSideways to support exposing the underlying FacetsCollector instances if the user needs them, in addition to maintaining the current functionality for all of the more common cases. Specifically, I'd like to add both the \"drill down\" FacetsCollector and map of dim -> FacetsCollector for \"drill sideways\" to the DrillSidewaysResult and ConcurrentDrillSidewaysResult classes. While it's true that a user can extend DrillSideways and override buildFacetsResult to keep track of these, it seems reasonable to provide this in DrillSideways itself so users don't need to sub-class for only this purpose. Here are two use-cases illustrating the desire for this:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-10043", "change_description": ": Decrease default for LRUQueryCache's skipCacheFactor to 10.\nThis prevents caching a query clause when it is much more expensive than\nrunning the top-level query.", "change_title": "Decrease default for LRUQueryCache#skipCacheFactor?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "In LUCENE-9002 we introduced logic to skip caching a clause if it would be too expensive compared to the usual query cost. Specifically, we avoid caching a clause if its cost is estimated to be a factor higher than the lead iterator's: Choosing good defaults is hard! We've seen some examples in Elasticsearch where caching a query clause causes a major slowdown, contributing to poor tail latencies. It made me think that the default 'skipCacheFactor' of 250 may be too high – interpreted simply, this means we'll cache a clause even if it is ~250 times more expensive than running the top-level query on its own. Would it make sense to decrease this to 10 or so? It seems okay to air on the side of less caching for individual clauses, especially since any parent 'BooleanQuery' is already eligible for caching? As a note, the interpretation \"~250 times more expensive than running the top-level query on its own\" isn't perfectly accurate. The true cost doesn't dependent on the number of matched documents, but also the cost of matching itself. Making it even more complex, some queries like 'IndexOrDocValuesQuery' have different matching strategies based on whether they're used as a lead iterator or verifier.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-5309", "change_description": ": Optimize facet counting for single-valued SSDV / StringValueFacetCounts.", "change_title": "when using SortedSetDV faceting, specialize the case when all docs are single-valued", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "Spinoff from LUCENE-5300 ...", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Improvements", "change_id": "LUCENE-9917", "change_description": ": The BEST_SPEED compression mode now trades more compression ratio\nin exchange of faster reads.", "change_title": "Reduce block size for BEST_SPEED", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "As benchmarks suggested major savings and minor slowdowns with larger block sizes, I had increased them on LUCENE-9486. However it looks like this slowdown is still problematic for some users, so I plan to go back to a smaller block size, something like 10*16kB to get closer to the amount of data we had to decompress per document when we had 16kB blocks without shared dictionaries.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Optimizations", "change_id": "LUCENE-9996", "change_description": ": Improved memory efficiency of IndexWriter's RAM buffer, in\nparticular in the case of many fields and many indexing threads.", "change_title": "Can we improve DWPT's initial memory footprint?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "Say you are indexing only keyword fields, that are both indexed and have doc values. The first document that gets added to a DWPT will increase memory usage by about 80kB per field. This is due mostly to: So if you have 10 actively indexing indices that have 100 fields each and 24 indexing threads, this gives a total of 10*100*24*80kB = 1.8GB. If you happened to give less than 1.8GB for your indexing buffers overall, Lucene will likely do very small flushes that have only a few documents, which in-turn will make indexing rather slow. Could we improve DWPT so that it more progressively reserves memory as more documents get added?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Optimizations", "change_id": "LUCENE-10022", "change_description": ": Rewrite empty DisjunctionMaxQuery to MatchNoDocsQuery.", "change_title": "Empty DisjunctionMaxQuery can rewrite to MatchNoDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "It's possible to create a DisjunctionMaxQuery with no clauses. This could be rewritten to MatchNoDocsQuery, matching the approach we take for BooleanQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13030246/LUCENE-10022.patch", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Optimizations", "change_id": "LUCENE-10031", "change_description": ": Slightly faster segment merging for sorted indices.", "change_title": "Speedup to SortedDocIDMerger when sorting on low-cardinality fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "I've been looking at profiles of indexing with index sorting enabled and saw non-negligible time spent in SortedDocIDMerger. This isn't completely surprising as this little class is called on every document whenever merging postings, doc values, stored fields, etc. I'm especially interested in cases when the sort key is on a low cardinality field, so the priority queue doesn't get reordered often. I've been playing with a change to SortedDocIdMerger that makes merging significantly faster in that case.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Optimizations", "change_id": "LUCENE-10014", "change_description": ": Lucene90DocValuesFormat was using too many bits per\nvalue when compressing via gcd, unnecessarily wasting index storage.", "change_title": "docvalue writeBlock gcd encode improve", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "Lucene90DocValuesConsumer.writeBlock calculate bitsPerValue  as: it can use gcd in this place as:", "patch_link": "https://issues.apache.org/jira/secure/attachment/13031357/LUCENE-10014.patch", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9988", "change_description": ": Fix DrillSideways correctness bug introduced in", "change_title": "Address DrillSideways bug discovered during randomized testing", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "There appears to be a correctness bug in DrillSideways likely introduced in LUCENE-9944. Need to track it down and fix.  Build: https://ci-builds.apache.org/job/Lucene/job/Lucene-Coverage-main/62/ 1 tests failed. FAILED:  org.apache.lucene.facet.TestDrillSideways.testRandom Error Message: java.lang.AssertionError Stack Trace: java.lang.AssertionError         at __randomizedtesting.SeedInfo.seed([ADCF6881460FEE2F:DF834D8EF76F585C]:0)         at org.junit.Assert.fail(Assert.java:87)         at org.junit.Assert.assertTrue(Assert.java:42)         at org.junit.Assert.assertTrue(Assert.java:53)         at org.apache.lucene.facet.TestDrillSideways.verifyEquals(TestDrillSideways.java:1580)         at org.apache.lucene.facet.TestDrillSideways.testRandom(TestDrillSideways.java:1159)         at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.base/java.lang.reflect.Method.invoke(Method.java:566)         at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1754)         at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:942)         at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:978)         at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:992)         at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)         at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)         at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)         at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)         at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)         at org.junit.rules.RunRules.evaluate(RunRules.java:20)         at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)         at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:370)         at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:819)         at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:470)         at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:951)         at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:836)         at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:887)         at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:898)         at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)         at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)         at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)         at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)         at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)         at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)         at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)         at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)         at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)         at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)         at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)         at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)         at org.junit.rules.RunRules.evaluate(RunRules.java:20)         at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)         at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:370)         at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:826)         at java.base/java.lang.Thread.run(Thread.java:834) Build Log: [...truncated 553 lines...] ERROR: The following test(s) have failed:   - org.apache.lucene.facet.TestDrillSideways.testRandom (:lucene:facet)     Test output: /home/jenkins/jenkins-slave/workspace/Lucene/Lucene-Coverage-main/lucene/facet/build/test-results/test/outputs/OUTPUT-org.apache.lucene.facet.TestDrillSideways.txt     Reproduce with: gradlew :lucene:facet:test --tests \"org.apache.lucene.facet.TestDrillSideways.testRandom\" -Ptests.jvms=4 -Ptests.haltonfailure=false -Ptests.jvmargs=-XX:TieredStopAtLevel=1 -Ptests.seed=ADCF6881460FEE2F -Ptests.multiplier=2 -Ptests.badapples=false -Ptests.file.encoding=UTF-8", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9944", "change_description": ": Fix DrillSideways correctness bug introduced in", "change_title": "Implement alternative drill sideways faceting with provided CollectorManager", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.10", "detail_description": "Today, if a user of DrillSideways wants to provide their own CollectorManager when invoking search, they get this alternate, \"concurrent\" implementation that creates N copies of the provided DrillDownQuery (where N is the number of drill-down dimensions) and runs them all concurrently. This is a very different implementation than the one a user would get if providing a Collector instead. Additionally, an ExecutorService must be provided when constructing a DrillSideways instance if the user wants to bring their own CollectorManager (otherwise, they'll get an unfriendly NPE when calling search). I propose adding an implementation to DrillSideways that will run the \"non-concurrent\" algorithm in the case that a user wants to provide their own CollectorManager but doesn't want to provide an ExecutorService (and doesn't want the concurrent algorithm).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9964", "change_description": ": Duplicate long values in a document field should only be counted once when using SortedNumericDocValuesFields", "change_title": "FacetResult.labelValues.value is not accurate for duplicate labels in a document", "detail_type": "Bug", "detail_affect_versions": "8.8.1", "detail_fix_versions": "9.0,8.10", "detail_description": "As part of a separate bug in FacetResult#value we discovered that FacetResult.labelValues.value is not accurate for duplicate labels in a document that uses SortedNumericDocValuesFields. In theory, each label should only be counted once from a document when returning the labelValues, but today, each duplicate label in a document is counted uniquely. A test case showing the current (inaccurate) output is here", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9999", "change_description": ": CombinedFieldQuery can fail with an exception when document\nis missing some fields.", "change_title": "CombinedFieldQuery can fail when document is missing fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "If some documents match but don't contain all fields, then CombinedFieldQuery can fail when attempting to compute norms. This is because MultiFieldNormValues assumes all fields in the document have norms. Originally surfaced in this Elasticsearch issue: https://github.com/elastic/elasticsearch/issues/74037.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10020", "change_description": ": DocComparator should not skip docs with the same docID on\nmultiple sorts with search after", "change_title": "DocComparator should not skip docs with the same docID on multiple sorts with search after", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "Because of the optimization introduced in LUCENE-9449, when searching with sort on  [_doc, other fields] with search after,  DocComparator can efficiently skip all docs before and including the provided [search after docID]. This is a desirable behaviour in a single index search. But in a distributed search, where multiple indices have docs with the same docID, and when searching on [_doc, other fields], the sort optimization should not skip documents with the same docIDs. Relates to LUCENE-9449", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10026", "change_description": ": Fix CombinedFieldQuery equals and hashCode, which ensures\nquery rewrites don't drop CombinedFieldQuery clauses.", "change_title": "CombinedFieldQuery has incorrect equals/ hashCode", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "The current equals and hashCode methods only compare query terms. This meant that queries on different fields, or with different field weights, are considered equal. During boolean query rewrites, duplicate clauses are removed. So because equals/ hashCode is incorrect, rewrites can accidentally drop CombinedFieldQuery clauses.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10039", "change_description": ": Correct CombinedFieldQuery scoring when there is a single\nfield.", "change_title": "With a single field, CombinedFieldQuery can score incorrectly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.10", "detail_description": "When there's only one field, CombinedFieldQuery will ignore its weight while scoring. This makes the scoring inconsistent, since the field weight is supposed to multiply its term frequency. This can also come up when searching over multiple fields, when some segment happens to contain only one field. The problem was caught by this test:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10046", "change_description": ": Counting bug fixed in StringValueFacetCounts.", "change_title": "StringValueFacetCounts can incorrectly count in \"sparse\" counting cases", "detail_type": "Bug", "detail_affect_versions": "9.0,8.10", "detail_fix_versions": "9.0,8.9", "detail_description": "StringValueFacetCounts incorrectly skips global ordinal mapping in one counting execution path, when the number of hits is very sparse relative to the cardinality of the field being counted.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9963", "change_description": ": FlattenGraphFilter is now more robust when handling\nincoming holes in the input token graph", "change_title": "Flatten graph filter has errors when there are holes at beginning or end of alternate paths", "detail_type": "Bug", "detail_affect_versions": "8.8", "detail_fix_versions": "None", "detail_description": "If asserts are enabled having gaps at the beginning or end of an alternate path can result in assertion errors ex:   Or    If asserts are not enabled these the same conditions will result in either IndexOutOfBounds Exceptions, or dropped tokens.   These issues can be recreated with the following unit tests I believe Lucene-8723 is a related issue as it looks like the last token in an alternate path is being deleted.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10060", "change_description": ": Ensure DrillSidewaysQuery instances never get cached.", "change_title": "Ensure DrillSidewaysQuery instances don't get cached", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "We need to make sure DSQ instances don't end up in the query cache. It's important that the DrillSidewaysScorer (bulk scorer implementation) actually runs during query evaluation in order to populate the \"sideways\" FacetsCollector instances with \"near miss\" docs. If it gets cached, this won't happen. There may also be an implication around acceptDocs getting honored as well. zacharymorn may be able to provide more details. UPDATE: The original issue I detailed above isn't actually an issue since DrillDownQuery doesn't implement equals, so the cache always misses and it always executes the BulkScorer ( DrillSidewaysScorer ). Tricky! There is a separate issue found by Zach (as mentioned above) related to \"acceptDocs\" though. See below conversation and link off to the separate PR conversation for more details.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10081", "change_description": ": KoreanTokenizer should check the max backtrace gap on whitespaces.", "change_title": "KoreanTokenizer should check the max backtrace gap on whitespaces", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "Today the KoreanTokenizer keeps track of the whitespaces that appear before a known term in order to apply a space penalty factor. These whitespaces are considered part of the next term so the backtrace gap limit is not applied.  As a result, the position buffer can grow up to the maximum number of consecutive whitespaces in the input. This is problematic since the buffer is reused on reset() so we should ensure that the max backtrace gap limit is applied on consecutive whitespaces consistently.", "patch_link": "none", "patch_content": "none"}

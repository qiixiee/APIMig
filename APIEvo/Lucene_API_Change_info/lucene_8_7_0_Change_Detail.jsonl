{"library_version": "8.7.0", "change_type": "API Changes", "change_id": "LUCENE-9437", "change_description": ": Lucene's facet module's DocValuesOrdinalsReader.decode method\nis now public, making it easier for applications to decode facet\nordinals into their corresponding labels", "change_title": "Make DocValuesOrdinalsReader.decode(BytesRef, IntsRef) method publicly accessible", "detail_type": "Improvement", "detail_affect_versions": "8.6", "detail_fix_versions": "8.7", "detail_description": "Visibility of DocValuesOrdinalsReader.decode(BytesRef, IntsRef) method is set to 'protected'. This prevents the method from being used outside this class in a setting where BinaryDocValues reader is instantiated outside the class and binary payload containing ordinals still needs to be decoded.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13008123/LUCENE-9437.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "API Changes", "change_id": "LUCENE-9515", "change_description": ": IndexingChain now accepts individual primitives rather than a\nDocumentsWriterPerThread instance in order to create a new DocConsumer.", "change_title": "Detach DWPT from DefaultIndexingChain", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.7", "detail_fix_versions": "None", "detail_description": "DefaultIndexingChain or rather its super class DocConsumer is still tightly coupled to DocumentsWriterPerThread. In oder to guarantee better code isolation and hopefully independent testing down the road we can expand the IndexingChain interface to take all the primitives needed rather than the complex DWPT class. It's unclear anyway how this API (DocConsumer and IndexingChain) should be implemented with all the classes it depends on being package private. I will open a follow-up to discuss if we can remove that abstraction.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "API Changes", "change_id": "LUCENE-9449", "change_description": ": Field comparators for numeric fields and _doc\nwere moved to their own package. TopFieldCollector sets\nTotalHits.relation to GREATER_THAN_OR_EQUAL_TO,\nas soon as the requested total hits threshold is reached, even though\nin some cases no skipping optimization is applied and all hits are collected.", "change_title": "Skip non-competitive documents when sort by _doc with search after", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "Enhance DocComparator to provide an iterator over competitive documents when search ing with \"after\" FieldDoc. This iterator can quickly position on the desired \"after\" document, and skip all documents or even segments that contain documents before \"after\" This is especially efficient when \"after\" is high.  Related to LUCENE-9280", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "New Features", "change_id": "LUCENE-9386", "change_description": ": RegExpQuery added case insensitive matching option.", "change_title": "RegExpQuery - add case insensitive matching option", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "In searches sometimes case sensitivity is important and sometimes not. However, users don't want to have to index two versions of their data (lowercased and original) in order to service both case sensitive and case insensitive queries. To get around this users have been commonly seen to take a user query e.g. `powershell.exe` and search for it with the regex `[Pp][Oo][Ww][Ee][Rr][Ss][Hh][Ee][Ll][Ll]\\.[Ee][Xx][Ee]`. The proposal is that we add an extra \"case insensitive\" option to the RegExp query flags to automatically do this sort of expansion when we create Automatons.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "New Features", "change_id": "LUCENE-8962", "change_description": ": Add IndexWriter merge-on-refresh feature to selectively merge\nsmall segments on getReader, subject to a configurable timeout, to improve\nsearch performance by reducing the number of small segments for searching.", "change_title": "Can we merge small segments during refresh, for faster searching?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6,8.7", "detail_description": "Two improvements were added: 8.6 has merge-on-commit (by Froh et. all), 8.7 has merge-on-refresh (by Simon).  See MergePolicy.findFullFlushMerges The original description follows: With near-real-time search we ask IndexWriter to write all in-memory segments to disk and open an IndexReader to search them, and this is typically a quick operation. However, when you use many threads for concurrent indexing, IndexWriter will accumulate write many small segments during refresh and this then adds search-time cost as searching must visit all of these tiny segments. The merge policy would normally quickly coalesce these small segments if given a little time ... so, could we somehow improve {{IndexWriter'}}s refresh to optionally kick off merge policy to merge segments below some threshold before opening the near-real-time reader?  It'd be a bit tricky because while we are waiting for merges, indexing may continue, and new segments may be flushed, but those new segments shouldn't be included in the point-in-time segments returned by refresh ... One could almost do this on top of Lucene today, with a custom merge policy, and some hackity logic to have the merge policy target small segments just written by refresh, but it's tricky to then open a near-real-time reader, excluding newly flushed but including newly merged segments since the refresh originally finished ... I'm not yet sure how best to solve this, so I wanted to open an issue for discussion!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12996016/failed-tests.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "New Features", "change_id": "LUCENE-9484", "change_description": ": Allow sorting an index after it was created. With SortingCodecReader, existing\nunsorted segments can be wrapped and merged into a fresh index using IndexWriter#addIndices\nAPI.", "change_title": "Allow index sorting to happen after the fact", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "I did look into sorting an index after it was created and found that with some smallish modifications we can actually allow that by piggibacking on SortingLeafReader and addIndices in a pretty straight-forward and simple way. With some smallish modifications / fixes to SortingLeafReader we can just merge and unsorted index into a sorted index using a fresh index writer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "New Features", "change_id": "LUCENE-9444", "change_description": ": Add utility class to retrieve facet labels from the\ntaxonomy index for a facet field so such fields do not also have to\nbe redundantly stored", "change_title": "Need an API to easily fetch facet labels for a field in a document", "detail_type": "Improvement", "detail_affect_versions": "8.6", "detail_fix_versions": "9.0,8.7", "detail_description": "A facet field may be included in the list of fields whose values are to be returned for each hit. In order to get the facet labels for each hit we need to  Ideally there should be a simple API - String[] getLabels(docId) that hides all the above details and gives us the string labels. This can be part of TaxonomyFacets but that's just one idea. I am opening this issue to get community feedback and suggestions. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13011648/LUCENE-9444.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-8574", "change_description": ": Add a new ExpressionValueSource which will enforce only one value per name\nper hit in dependencies, ExpressionFunctionValues will no longer\nrecompute already computed values", "change_title": "ExpressionFunctionValues should cache per-hit value", "detail_type": "Bug", "detail_affect_versions": "7.5,8.0", "detail_fix_versions": "9.0,8.7", "detail_description": "The original version of ExpressionFunctionValues had a simple per-hit cache, so that nested expressions that reference the same common variable would compute the value for that variable the first time it was referenced and then use that cached value for all subsequent invocations, within one hit.  I think it was accidentally removed in LUCENE-7609? This is quite important if you have non-trivial expressions that reference the same variable multiple times. E.g. if I have these expressions: Then evaluating x should only cause b's value to be computed once (for a given hit), but today it's computed twice.  The problem is combinatoric if b then references another variable multiple times, etc. I think to fix this we just need to restore the per-hit cache? ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12949422/LUCENE-8574.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9416", "change_description": ": Fix CheckIndex to print an invalid non-zero norm as\nunsigned long when detecting corruption.", "change_title": "Fix CheckIndex to print norms as unsigned integers", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.7", "detail_description": "In the discussion on \"CheckIndex complaining about -1 for norms value\" in the java-user list, it was identified that we should \"fix CheckIndex to print norms as unsigned integers\". I'd like to take a stab at this. I'm trying to understand the problem and from what I gather, while norms are `byte`s, the API exposes them as `long` values. While printing the error message, we want it to print a zero instead of -1?", "patch_link": "https://issues.apache.org/jira/secure/attachment/13007532/LUCENE-9416.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9440", "change_description": ": FieldInfo#checkConsistency called twice from Lucene50(60)FieldInfosFormat#read;\nRemoved the (redundant?) assert and do these checks for real.", "change_title": "FieldInfo#checkConsistency called twice from Lucene50(60)FieldInfosFormat#read", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.7", "detail_description": "Reviewing code I noticed that we do call infos[i].checkConsistency(); method twice: first time inside the FiledInfo's constructor and a second one just after we've created an object. org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java:150 FileInfo's constructor(notice the last line)  By this patch, I will remove the second call and leave only one in the constructor. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13008365/LUCENE-9440.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9446", "change_description": ": In BooleanQuery rewrite, always remove MatchAllDocsQuery filter clauses\nwhen possible.", "change_title": "Boolean rewrite could remove more MatchAllDocsQuery filters", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "It seems that BooleanQuery rewrites will remove a MatchAllDocsQuery 'filter' clause only if there is at least one 'must' clause. I think we could also remove the 'filter' clause if there are no 'must' clauses, but at least one other 'filter' clause. This could let queries like #field:value #: be rewritten to #field:value.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9501", "change_description": ": Improve coverage for Asserting* test classes: make sure to handle singleton doc\nvalues, and sometimes exercise Weight#scorer instead of Weight#bulkScorer for top-level\nqueries.", "change_title": "IndexSortSortedNumericDocValuesRangeQuery violates iterator invariant.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "In LUCENE-7714 we added a new query to sandbox called IndexSortSortedNumericDocValuesRangeQuery that optimizes range calculations when the field is sorted. The query has a bad bug: its DocIdSetIterator can return an old value for docID() even after advance has returned NO_MORE_DOCS. This violates the DocIdSetIterator contract and means that it's possible for DocIdSetIterator#advance to be called when it's already been exhausted (which can result in invalid reads). We would have expected this issue to be caught in tests, especially because classes like AssertingIndexSearcher check for these invariants. As part of this fix I'll look into improvements to the Asserting* wrapper framework.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9511", "change_description": ": Include StoredFieldsWriter in DWPT accounting to ensure that it's\nheap consumption is taken into account when IndexWriter stalls or should flush\nDWPTs.", "change_title": "Include StoredFieldsWriter in DWPT accounting", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "StoredFieldsWriter might consume some heap space memory that can have a significant impact on decisions made in the IW if writers should be stalled or DWPTs should be flushed if memory settings are small in IWC and flushes are frequent. We should add some accounting to the StoredFieldsWriter since it's part of the DWPT lifecycle and not just present during flush. Our nightly builds ran into some OOMs due to the large chunk size used in the CompressedStoredFieldsFormat. The reason are very frequent flushes due to small maxBufferedDocs which causes 300+ DWPTs to be blocked for flush causing ultimately an OOM exception.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9514", "change_description": ": Include TermVectorsWriter in DWPT accounting to ensure that it's\nheap consumption is taken into account when IndexWriter stalls or should flush\nDWPTs.", "change_title": "Include TermVectorsWriter in DWPT accounting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.7", "detail_description": "As a followup of LUCENE-9511 we should also include the TermVectorsWriter into the DWPT accounting since it might hold on to RAM in it's byte buffer recycling etc.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9523", "change_description": ": In query shapes over shape fields, skip points while traversing the\nBKD tree when the relationship with the document is already known.", "change_title": "Speedup query shapes for geometries that generate multiple points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "When indexing lines or polygons into a shape field, we generally index multiple points for the same document. When querying, we typically visited a subset of this points. In many cases when visiting points from the same document, we already know the relationship and therefore computing the relationship between that point and the query shape is not necessary. When using dense visitors(eg. visitors backed by a FixedBitSet), we can check if the relationship is already known and therefore skip that point. Finally for intersects relationship we normally use sparse visitors but I wonder in the case where the number of points >> number of docs, we should use a dense visitor so we can skip points from documents where we know it intersects.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9539", "change_description": ": Use more compact datastructures to represent sorted doc-values in memory when\nsorting a segment before flush and in SortingCodecReader.", "change_title": "Improve memory footprint of SortingCodecReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "SortingCodecReader is a very memory heavy since it needs to re-sort and load large parts of the index into memory. We can try to make it more efficient by using more compact internal data-structures, remove the caches it uses provided we define it's usage as a merge only reader wrapper. Ultimately we need to find a way to allow the reader or some other structure to minimize its heap memory. One way is to slice existing readers and merge them in multiple steps. There will be multiple steps towards a more useable version of this class.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Improvements", "change_id": "LUCENE-9458", "change_description": ": WordDelimiterGraphFilter should order tokens at the same position by endOffset to\nemit longer tokens first.  The same graph is produced.", "change_title": "WordDelimiterGraphFilter (and non-graph) should tie-break order using end offset", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "WordDelimiterGraphFilter and WordDelimiterFilter do not consult the end offset in their sub-token ordering.  In the event of a tie-break, I propose the longer token come first.  This usually happens already, but not always, and so this also feels like an inconsistency when you see it.  This issue can be thought of as a bug fix to LUCENE-9006 or an improvement; I have no strong feelings on the issue classification.  Before reading further, definitely read that issue. I see this is a problem when using CATENATE_ALL with either GENERATE_WORD_PARTS xor GENERATE_NUMBER_PARTS when the input ends with that part not being generated.  Consider the input: \"other-9\" and let's assume we want to catenate all, generate word parts, but nothing else (not numbers).  This should be tokenized in this order: \"other9\", \"other\" but today is emitted in reverse order. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Optimizations", "change_id": "LUCENE-9395", "change_description": ": ConstantValuesSource now shares a single DoubleValues\ninstance across all segments", "change_title": "ConstantValuesSource creates more than one DoubleValues unnecessarily", "detail_type": "Improvement", "detail_affect_versions": "8.5.2", "detail_fix_versions": "9.0,8.7", "detail_description": "At my day job, we use ConstantValuesSource to represent default values or a constant query-level feature by calling DoubleValuesSource.constant. I realized under the hood the ConstantValuesSource.getDoubleValues creates a new DoubleValues which simply return the specified value each time it is called. Unless I missed something, I don't see a risk of creating one DoubleValues as use it as the return value of all getDoubleValues() calls given that the constant DoubleValues doesn't maintain any state. We can also offer the user flexibilities of how to initialize it. 1) DoubleValuesSource.constant(double constant) – we can eagerly initialize an `DoubleValues` that returns the constant and make it the return value of all getDoubleValues() calls. 2) DoubleValuesSource.constant(DoubleSupplier doubleSupplier)  – For lazy evaluation if the constant takes some time to compute and user expects the returned DVS will not be used in all code path.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13005170/LUCENE-9395.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Optimizations", "change_id": "LUCENE-9447", "change_description": ",", "change_title": "Make BEST_COMPRESSION compress more aggressively?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "The Lucene86 codec supports setting a \"Mode\" for stored fields compression, that is either \"BEST_SPEED\", which translates to blocks of 16kB or 128 documents (whichever is hit first) compressed with LZ4, or \"BEST_COMPRESSION\", which translates to blocks of 60kB or 512 documents compressed with DEFLATE with default compression level (6). After looking at indices that spent most disk space on stored fields recently, I noticed that there was quite some room for improvement by increasing the block size even further: For this specific dataset, I had 1M documents that each had about 2kB of stored fields each and quite some redundancy. This makes me want to look into bumping this block size to maybe 256kB. It would be interesting to re-do the experiments we did on LUCENE-6100 to see how this affects the merging speed. That said I don't think it would be terrible if the merging time increased a bit given that we already offer the BEST_SPEED option for CPU-savvy users.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Optimizations", "change_id": "LUCENE-9486", "change_description": ",", "change_title": "Explore using preset dictionaries with LZ4 for stored fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "Follow-up of LUCENE-9447: using preset dictionaries with DEFLATE provided very significant gains. Adding support for preset dictionaries with LZ4 would be easy so let's give it a try?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Optimizations", "change_id": "LUCENE-9373", "change_description": ": FunctionMatchQuery now accepts a \"matchCost\" optimization hint.", "change_title": "Allow FunctionMatchQuery to customize matchCost of TwoPhaseIterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "FunctionMatchQuery internally has a TwoPhaseIterator using a constant matchCost.  If it were customizable by the query, the user could control this ordering.  I propose an optional matchCost via an overloaded constructor.  Ideally the DoubleValues abstraction would have a matchCost but it doesn't, and even if it did, the user might just want real control over this at a query construction/parse level. See similar LUCENE-9114", "patch_link": "https://issues.apache.org/jira/secure/attachment/13009753/LUCENE-9373.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Optimizations", "change_id": "LUCENE-9510", "change_description": ": Indexing with an index sort is now faster by not compressing\ntemporary representations of the data.", "change_title": "SortingStoredFieldsConsumer should use a format that has better random-access", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "We noticed some indexing rate regressions in Elasticsearch after upgrading to a new Lucene snapshot. This is due to the fact that SortingStoredFieldsConsumer is using the default codec to write stored fields on flush. Compression doesn't matter much for this case since these are temporary files that get removed on flush after the segment is sorted anyway so we could switch to a format that has faster random access.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Optimizations", "change_id": "LUCENE-9449", "change_description": ": Enhance DocComparator to provide an iterator over competitive\ndocuments when searching with \"after\". This iterator can quickly position\non the desired \"after\" document skipping all documents and segments before\n\"after\".", "change_title": "Skip non-competitive documents when sort by _doc with search after", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "Enhance DocComparator to provide an iterator over competitive documents when search ing with \"after\" FieldDoc. This iterator can quickly position on the desired \"after\" document, and skip all documents or even segments that contain documents before \"after\" This is especially efficient when \"after\" is high.  Related to LUCENE-9280", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9427", "change_description": ": Fix a regression where the unified highlighter didn't produce\nhighlights on fuzzy queries that correspond to exact matches.", "change_title": "Unified highlighter can fail to highlight fuzzy query", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "If a fuzzy query corresponds to an exact match (for example it has with maxEdits: 0), then the unified highlighter doesn't produce highlights for the matching terms. I think this is due to the fact that when visiting a fuzzy query, the exact terms are now consumed separately from automata. The unified highlighter doesn't account for the terms and misses highlighting them.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9467", "change_description": ": Fix NRTCachingDirectory to use Directory#fileLength to check if a file\nalready exists instead of opening an IndexInput on the file which might throw a AccessDeniedException\nin some Directory implementations.", "change_title": "TestOfflineSorter fails with combination of NRTCaching and ByteBuffersDirectory", "detail_type": "Bug", "detail_affect_versions": "8.7,8.6.1", "detail_fix_versions": "8.7", "detail_description": "TestOfflineSorter#testThreadSafety fails on 8.x since NRTCachingDirectory uses the delegates openInput method to slowly check if the file exists. This is not present anymore on main line. The problem is that ByteBuffersDirectory fires an AccessDeniedException when the file is not fully written yet.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9501", "change_description": ": Fix a bug in IndexSortSortedNumericDocValuesRangeQuery where it could violate the\nDocIdSetIterator contract.", "change_title": "IndexSortSortedNumericDocValuesRangeQuery violates iterator invariant.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "In LUCENE-7714 we added a new query to sandbox called IndexSortSortedNumericDocValuesRangeQuery that optimizes range calculations when the field is sorted. The query has a bad bug: its DocIdSetIterator can return an old value for docID() even after advance has returned NO_MORE_DOCS. This violates the DocIdSetIterator contract and means that it's possible for DocIdSetIterator#advance to be called when it's already been exhausted (which can result in invalid reads). We would have expected this issue to be caught in tests, especially because classes like AssertingIndexSearcher check for these invariants. As part of this fix I'll look into improvements to the Asserting* wrapper framework.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9401", "change_description": ": Include field in ComplexPhraseQuery's toString()", "change_title": "ComplexPhraseQuery's toString method always omits field name", "detail_type": "Bug", "detail_affect_versions": "8.5.2", "detail_fix_versions": "8.7", "detail_description": "The toString(String field) method in org.apache.lucene.queryparser.complexPhrase.ComplexPhraseQueryParser$ComplexPhraseQuery should only omit the field name if query's field name is not equal to field name that is passed as an argument. Instead, the query's field name is never included in the returned String.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13012243/LUCENE-9401.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9578", "change_description": ": Fix TermRangeQuery when there is no upper bound and the lower\nbound is the empty string excluded. This would previously match no strings at\nall while it should match all non-empty strings.", "change_title": "TermRangeQuery with empty string lower bound edge case", "detail_type": "Bug", "detail_affect_versions": "trunk,8.6.3", "detail_fix_versions": "8.7", "detail_description": "Currently a TermRangeQuery with the empty String (\"\") as lower bound and includeLower=false leads internally constructs an Automaton that doesn't match anything. This is unexpected expecially for open upper bounds where any string should be considered to be \"higher\" than the empty string. I think \"Automata#makeBinaryInterval\" should be changed so that for an empty string lower bound and an open upper bound, any String should match the query regardless or the includeLower flag.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9524", "change_description": ": Fix NPE in SpanWeight#explain when no scoring is required and\nSpanWeight has null Similarity.SimScorer.", "change_title": "NullPointerException in IndexSearcher.explain() when using ComplexPhraseQueryParser", "detail_type": "Bug", "detail_affect_versions": "8.6,8.6.2", "detail_fix_versions": "8.7", "detail_description": "I get NPE when I use IndexSearcher.explain(). Checked with Lucene 8.6.0 and 8.6.2. The query: (lorem AND NOT \"dolor lorem\") OR ipsum The text: dolor lorem ipsum Stack trace: Minimal example code:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Documentation", "change_id": "LUCENE-9424", "change_description": ": Add a performance warning to AttributeSource.captureState javadocs", "change_title": "Have a warning comment for AttributeSource.captureState", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.7", "detail_description": "AttributeSource.captureState is a powerful method that can be used to store and (later on) restore the current state, but it comes with a cost of copying all attributes in this source and sometimes can be a big cost if called multiple times. We could probably add a warning to indicate this cost, as this method is encapsulated quite well and sometimes people who use it won't be aware of the cost.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13008366/LUCENE-9424.patch", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-9539", "change_description": ": SortingCodecReader now doesn't cache doc values fields anymore. Previously, SortingCodecReader\nused to cache all doc values fields after they were loaded into memory. This reader should only be used\nto sort segments after the fact using IndexWriter#addIndices.", "change_title": "Improve memory footprint of SortingCodecReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "SortingCodecReader is a very memory heavy since it needs to re-sort and load large parts of the index into memory. We can try to make it more efficient by using more compact internal data-structures, remove the caches it uses provided we define it's usage as a merge only reader wrapper. Ultimately we need to find a way to allow the reader or some other structure to minimize its heap memory. One way is to slice existing readers and merge them in multiple steps. There will be multiple steps towards a more useable version of this class.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Other", "change_id": "LUCENE-9292", "change_description": ": Refactor BKD point configuration into its own class.", "change_title": "BKDWriter refactor: Group point configuration in its own class", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "In order to build a BKD tree, we need to provide  to the BKDWriter the point configuration we are indexing. That is the number of dimensions, the number of indexed dimension, the number of bytes per dimensions and the  max number of points per leaf. From this information, we actually derive some important parameters like the number of bytes per point  The idea of this change is to group all this information into its own class so we can share this information more easily with other objects.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Other", "change_id": "LUCENE-9470", "change_description": ": Make TestXYMultiPolygonShapeQueries more resilient for CONTAINS queries.", "change_title": "TestXYMultiPolygonShapeQueries reproducing failure", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "8.7", "detail_description": "I hit this while beasting tests against simonw's PR to add \"merge segments during getReader\": It reproduces for me on current (77a4d495cc553ec80001346376fd87d6b73a6059) mainline:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.7.0", "change_type": "Other", "change_id": "LUCENE-9512", "change_description": ": Move LockFactory stress test to be a unit/integration\ntest.", "change_title": "Add test-lock-factory to Gradle build", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.x,9.0", "detail_description": "When porting to Gradle, the following task was missed to be ported to Gradle: https://github.com/apache/lucene-solr/blob/92139985c3eeb80adb8af345e61a3090d30430a5/lucene/core/build.xml#L148-L234 This is somehow an integration test. It's not part of the test suite, as the code is part of main distribution and is a client-server implementation that has one coordinator to handle other JVMs to hammer a directories' lock factory. It may be included into the test suite, but as it spawns multiple JVMs (thats essential for it to work), I see this as a separate thing. I would like to implement that snippet of ANT code in Gradle and attach it to :lucene:core's test task. If we have an integration test framework at some point we can make a real integTest out of it, but for now a simple Groovy script is fine.", "patch_link": "none", "patch_content": "none"}

{"library_version": "6.4.0", "change_type": "API Changes", "change_id": "LUCENE-7533", "change_description": ": Classic query parser no longer allows autoGeneratePhraseQueries\nto be set to true when splitOnWhitespace is false (and vice-versa).", "change_title": "Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false", "detail_type": "Bug", "detail_affect_versions": "6.2,6.2.1,6.3", "detail_fix_versions": "6.4,7.0", "detail_description": "LUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis. From the javadocs for QueryParser.setAutoGeneratePhraseQueries(): phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text. When splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.  E.g. simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autoGeneratePhraseQueries=true, we should not be creating a phrase query here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12839472/LUCENE-7533-disallow-option-combo.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "API Changes", "change_id": "LUCENE-7607", "change_description": ": LeafFieldComparator.setScorer and SimpleFieldComparator.setScorer\nare declared as throwing IOException", "change_title": "LeafFieldComparator.setScorer() should throw IOException", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Spinoff of LUCENE-5325.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845045/LUCENE-7607.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "API Changes", "change_id": "LUCENE-7617", "change_description": ": Collector construction for two-pass grouping queries is\nabstracted into a new Grouper class, which can be passed as a constructor\nparameter to GroupingSearch.  The abstract base classes for the different\ngrouping Collectors are renamed to remove the Abstract* prefix.", "change_title": "Improve GroupingSearch API and extensibility", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "While looking at how to make grouping work with the new XValuesSource API in core, I thought I'd try and clean up GroupingSearch a bit.  We have three different ways of grouping at the moment: by doc block, using a single-pass collector; by field; and by ValueSource.  The latter two both use essentially the same two-pass mechanism, with different Collector implementations. I can see a number of possible improvements here:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12846168/LUCENE-7617.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "API Changes", "change_id": "LUCENE-7609", "change_description": ": The expressions module now uses the DoubleValuesSource API, and\nno longer depends on the queries module.  Expression#getValueSource() is\nreplaced with Expression#getDoubleValuesSource().", "change_title": "Refactor expressions module to use DoubleValuesSource", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "With DoubleValuesSource in core, we can refactor the expressions module to use these instead of ValueSource, and remove the dependency of expressions on the queries module in master.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845774/LUCENE-7609.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "API Changes", "change_id": "LUCENE-7610", "change_description": ": The facets module now uses the DoubleValuesSource API, and\nmethods that take ValueSource parameters are deprecated", "change_title": "Migrate facets module from ValueSource to Double/LongValuesSource", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Unfortunately this doesn't allow us to break the facets dependency on the queries module, because facets also uses TermsQuery - perhaps this should move to core as well?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845986/LUCENE-7610.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "API Changes", "change_id": "LUCENE-7611", "change_description": ": DocumentValueSourceDictionary now takes a LongValuesSource\nas a parameter, and the ValueSource equivalent is deprecated", "change_title": "Make suggester module use LongValuesSource", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "This allows us to remove the suggester module's dependency on the queries module.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12846014/LUCENE-7611.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-5867", "change_description": ": Added BooleanSimilarity.", "change_title": "Add BooleanSimilarity", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "This can be used when the user doesn't want tf/idf scoring for some reason. The idea is that the score is just query_time_boost * index_time_boost, no queryNorm/IDF/TF/lengthNorm...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12835519/LUCENE-5867.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-7466", "change_description": ": Added AxiomaticSimilarity.", "change_title": "add axiomatic similarity", "detail_type": "Improvement", "detail_affect_versions": "7.0", "detail_fix_versions": "6.4,7.0", "detail_description": "Add axiomatic similarity approaches to the similarity family. More details can be found at http://dl.acm.org/citation.cfm?id=1076116 and https://www.eecis.udel.edu/~hfang/pubs/sigir05-axiom.pdf There are in total six similarity models. All of them are based on BM25, Pivoted Document Length Normalization or Language Model with Dirichlet prior.  We think it is worthy to add the models as part of Lucene.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-7590", "change_description": ": Added DocValuesStatsCollector to compute statistics on DocValues\nfields.", "change_title": "Add DocValues statistics helpers", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "I think it can be useful to have DocValues statistics helpers, that can allow users to query for the min/max/avg etc. stats of a DV field. In this issue I'd like to cover numeric DV, but there's no reason not to add it to other DV types too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12843749/LUCENE-7590-sorted-set.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-7587", "change_description": ": The new FacetQuery and MultiFacetQuery helper classes\nmake it simpler to execute drill down when drill sideways counts are\nnot needed", "change_title": "New FacetQuery and MultiFacetQuery", "detail_type": "Improvement", "detail_affect_versions": "6.3.1,7.0", "detail_fix_versions": "6.4,7.0", "detail_description": "This patch introduces two convenient queries: FacetQuery and MultiFacetQuery. It can be useful to be able to filter a complex query on one or many facet value.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12843596/LUCENE-7587.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-6664", "change_description": ": A new SynonymGraphFilter outputs a correct graph\nstructure for multi-token synonyms, separating out a\nFlattenGraphFilter that is hardwired into the current\nSynonymFilter.  This finally makes it possible to implement\ncorrect multi-token synonyms at search time.  See", "change_title": "Replace SynonymFilter with SynonymGraphFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Spinoff from LUCENE-6582. I created a new SynonymGraphFilter (to replace the current buggy SynonymFilter), that produces correct graphs (does no \"graph flattening\" itself).  I think this makes it simpler. This means you must add the FlattenGraphFilter yourself, if you are applying synonyms during indexing. Index-time syn expansion is a necessarily \"lossy\" graph transformation when multi-token (input or output) synonyms are applied, because the index does not store posLength, so there will always be phrase queries that should match but do not, and then phrase queries that should not match but do. http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html goes into detail about this. However, with this new SynonymGraphFilter, if instead you do synonym expansion at query time (and don't do the flattening), and you use TermAutomatonQuery (future: somehow integrated into a query parser), or maybe just \"enumerate all paths and make union of PhraseQuery\", you should get 100% correct matches (not sure about \"proper\" scoring though...). This new syn filter still cannot consume an arbitrary graph.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12843988/LUCENE-6664.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html", "change_description": ": A new SynonymGraphFilter outputs a correct graph\nstructure for multi-token synonyms, separating out a\nFlattenGraphFilter that is hardwired into the current\nSynonymFilter.  This finally makes it possible to implement\ncorrect multi-token synonyms at search time.  See", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-5325", "change_description": ": Added LongValuesSource and DoubleValuesSource, intended as\ntype-safe replacements for ValueSource in the queries module.  These\nexpose per-segment LongValues or DoubleValues iterators.", "change_title": "Move ValueSource and FunctionValues under core/", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Spinoff from LUCENE-5298: ValueSource and FunctionValues are abstract APIs which exist under the queries/ module. That causes any module which wants to depend on these APIs (but not necessarily on any of their actual implementations!), to depend on the queries/ module. If we move these APIs under core/, we can eliminate these dependencies and add some mock impls for testing purposes. Quoting Robert from LUCENE-5298: we should eliminate the suggest/ dependencies on expressions and queries, the expressions/ on queries, the grouping/ dependency on queries, the spatial/ dependency on queries, its a mess. To add to that list, facet/ should not depend on queries too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845067/LUCENE-5325-6x-matchingbits.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-7603", "change_description": ": Graph token streams are now handled accurately by query\nparsers, by enumerating all paths and creating the corresponding\nquery/ies as sub-clauses", "change_title": "Support Graph Token Streams in QueryBuilder", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "With LUCENE-6664 we can use multi-term synonyms query time.  A \"graph token stream\" will be created which which is nothing more than using the position length attribute on stacked tokens to indicate how many positions a token should span.  Currently the position length attribute on tokens is ignored during query parsing.  This issue will add support for handling these graph token streams inside the QueryBuilder utility class used by query parsers.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-7588", "change_description": ": DrillSideways can now run queries concurrently, and\nsupports an IndexSearcher using an executor service to run each query\nconcurrently across all segments in the index", "change_title": "A parallel DrillSideways implementation", "detail_type": "Improvement", "detail_affect_versions": "6.3.1,7.0", "detail_fix_versions": "6.4,7.0", "detail_description": "Currently DrillSideways implementation is based on the single threaded IndexSearcher.search(Query query, Collector results). On large document set, the single threaded collection can be really slow. The ParallelDrillSideways implementation could: 1. Use the CollectionManager based method IndexSearcher.search(Query query, CollectorManager collectorManager)  to get the benefits of multithreading on index segments, 2. Compute each DrillSideway subquery on a single thread.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844916/LUCENE-7588.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "New features", "change_id": "LUCENE-7627", "change_description": ": Added .intersect methods to SortedDocValues and\nSortedSetDocValues to allow filtering their TermsEnums with a\nCompiledAutomaton", "change_title": "Improve TermsEnum automaton filtering APIs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "To filter a TermsEnum by a CompiledAutomaton, we currently have a number of different possibilities: It's easy to do the wrong thing here, and at the moment we only guard against incorrect usage via runtime checks (see eg LUCENE-7576, https://github.com/flaxsearch/marple/issues/24).  We should try and clean this up.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12847489/LUCENE-7627.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7547", "change_description": ": JapaneseTokenizerFactory was failing to close the\ndictionary file it opened", "change_title": "JapaneseTokenizerFactory opens dictionary file but never closes it again", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "JapaneseTokenizerFactory opens dictionary file in line 130 InputStream stream = loader.openResource(userDictionaryPath); but never closes it again. This leads to too many open files after after a couple of query executions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7562", "change_description": ": CompletionFieldsConsumer sometimes throws\nNullPointerException on ghost fields", "change_title": "CompletionFieldsConsumer throws NPE on ghost fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "If you index SuggestField for some field X, but later delete all documents with that field, it can cause a ghost situation where the field infos believes field X exists yet the postings do not. I believe this bug is the root cause of this ES issue: https://github.com/elastic/elasticsearch/issues/21500", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838953/LUCENE-7562.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7533", "change_description": ": Classic query parser: disallow autoGeneratePhraseQueries=true\nwhen splitOnWhitespace=false (and vice-versa).", "change_title": "Classic query parser: autoGeneratePhraseQueries=true doesn't work when splitOnWhitespace=false", "detail_type": "Bug", "detail_affect_versions": "6.2,6.2.1,6.3", "detail_fix_versions": "6.4,7.0", "detail_description": "LUCENE-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis. From the javadocs for QueryParser.setAutoGeneratePhraseQueries(): phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text. When splitOnWhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autoGeneratePhraseQueries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis.  E.g. simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autoGeneratePhraseQueries=true, we should not be creating a phrase query here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12839472/LUCENE-7533-disallow-option-combo.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7536", "change_description": ": ASCIIFoldingFilterFactory used to return an illegal multi-term\ncomponent when preserveOriginal was set to true.", "change_title": "ASCIIFoldingFilterFactory.getMultiTermComponent can emit two tokens", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "My understanding is that it is a requirement for multi-term analysis to only normalize tokens, and not eg. remove tokens (stop filter) or add tokens (by tokenizing or adding synonyms). Yet ASCIIFoldingFilterFactory.getMultiTermComponent will return a factory that emits synonyms if preserveOriginal is set to true on the original filter. This looks like a bug to me but I'm not entirely sure how to fix it. Should the multi-term analysis component do ascii folding or not if the original factory has preserveOriginal set to true?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838983/LUCENE-7536.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7576", "change_description": ": Fix Terms.intersect in the default codec to detect when\nthe incoming automaton is a special case and throw a clearer\nexception than NullPointerException", "change_title": "RegExp automaton causes NPE on Terms.intersect", "detail_type": "Bug", "detail_affect_versions": "6.2.1", "detail_fix_versions": "6.4,7.0", "detail_description": "Calling org.apache.lucene.index.Terms.intersect(automaton, null) causes an NPE: String index_path = <path to index>     String term = <a valid term name> Directory directory = FSDirectory.open(Paths.get(index_path));     IndexReader reader = DirectoryReader.open(directory);     Fields fields = MultiFields.getFields(reader);     Terms terms = fields.terms(args[1]);     CompiledAutomaton automaton = new CompiledAutomaton(       new RegExp(\"do_not_match_anything\").toAutomaton()); TermsEnum te = terms.intersect(automaton, null); throws: Exception in thread \"main\" java.lang.NullPointerException \tat org.apache.lucene.codecs.blocktree.IntersectTermsEnum.<init>(IntersectTermsEnum.java:127) \tat org.apache.lucene.codecs.blocktree.FieldReader.intersect(FieldReader.java:185) \tat org.apache.lucene.index.MultiTerms.intersect(MultiTerms.java:85)         ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12840670/LUCENE-7576.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6989", "change_description": ": Fix Exception handling in MMapDirectory's unmap hack\nsupport code to work with Java 9's new InaccessibleObjectException\nthat does not extend ReflectiveAccessException in Java 9.", "change_title": "Implement MMapDirectory unmapping for coming Java 9 changes", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.0,6.4", "detail_description": "Originally, the sun.misc.Cleaner interface was declared as \"critical API\" in JEP 260 Unfortunately the decission was changed in favor of a oficially supported java.lang.ref.Cleaner API. A side effect of this change is to move all existing sun.misc.Cleaner APIs into a non-exported package. This causes our forceful unmapping to no longer work, because we can get the cleaner instance via reflection, but trying to invoke it will throw one of the new Jigsaw RuntimeException because it is completely inaccessible. This will make our forceful unmapping fail. There are also no changes in Garbage collector, the problem still exists. For more information see this mailing list thread. This commit will likely be done, making our unmapping efforts no longer working. Alan Bateman is aware of this issue and will open a new issue at OpenJDK to allow forceful unmapping without using the now private sun.misc.Cleaner. The idea is to let the internal class sun.misc.Cleaner implement the Runable interface, so we can simply cast to runable and call the run() method to unmap. The code would then work. This will lead to minor changes in our unmapper in MMapDirectory: An instanceof check and casting if possible. I opened this issue to keep track and implement the changes as soon as possible, so people will have working unmapping when java 9 comes out. Current Lucene versions will no longer work with Java 9.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844298/LUCENE-6989-v3-testFixes.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7581", "change_description": ": Lucene now prevents updating a doc values field that is used\nin the index sort, since this would lead to corruption.", "change_title": "IndexWriter#updateDocValues can break index sorting", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "IndexWriter#updateDocValues can break index sorting if it is called on a field that is used in the index sorting specification.  TestIndexSorting has a test for this case: #testConcurrentDVUpdates  but only L1 merge are checked. Any LN merge would fail the test because the inner sort of the segment is not re-compute during/after DV updates.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12842567/LUCENE-7581.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7570", "change_description": ": IndexWriter may deadlock if a commit is running while\nthere are too many merges running and one of the merges hits a\ntragic exception", "change_title": "Tragic events during merges can lead to deadlock", "detail_type": "Bug", "detail_affect_versions": "5.5,7.0", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "When an IndexWriter#commit() is stalled due to too many pending merges, you can get a deadlock if the currently active merge thread hits a tragic event. We hit this bug with Lucene 5.5, but I looked at the code in the master branch and it looks like the deadlock still exists there as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12842472/LUCENE-7570.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7594", "change_description": ": Fixed point range queries on floating-point types to recommend\nusing helpers for exclusive bounds that are consistent with Double.compare.", "change_title": "Float/DoublePoint should not recommend using Math.nextUp/nextDown for exclusive ranges", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Float/Double points are supposed to be consistent with Double/Float.compare, so +0 is supposed to compare greater than -0. However Math.nextUp/nextDown is not consistent with Double/Float.compare and returns MIN_VALUE for nextUp(-0).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12843438/LUCENE-7594.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7606", "change_description": ": Normalization with CustomAnalyzer would only apply the last\ntoken filter.", "change_title": "CustomAnalyzer.normalize only applies the last token filter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "This had not been uncovered by testing since we only tested with one token filter at most.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844905/LUCENE-7606.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7612", "change_description": ": Removed an unused dependency from the suggester to the misc\nmodule.", "change_title": "Remove suggester dependency on misc", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "AnalyzingInfixSuggester uses IndexSorter, which was in the misc module when the dependency was added in LUCENE-5477.  IndexSorter is in core now, though, so this dependency can be removed. lucene/misc/src/java/org/apache/lucene/index/Sorter.java became lucene/core/src/java/org/apache/lucene/index/Sorter.java as part of LUCENE-6766", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845182/LUCENE-7612.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7532", "change_description": ": Add back lost codec file format documentation", "change_title": "Add Lucene 6.2 file format description in codecs/lucene62/package-info.java", "detail_type": "Bug", "detail_affect_versions": "6.2", "detail_fix_versions": "6.4", "detail_description": "Currently that description is missing at branch_6x so I'd like to restore it. User feedback: http://markmail.org/message/hxtxzue7qn6ne6vz", "patch_link": "https://issues.apache.org/jira/secure/attachment/12836289/LUCENE-7532.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-6824", "change_description": ": TermAutomatonQuery now rewrites to TermQuery,\nPhraseQuery or MultiPhraseQuery when the word automaton is simple", "change_title": "TermAutomatonQuery should rewrite to a simpler query when possible", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Spinoff from LUCENE-6664. I think TermAutomatonQuery would be easier to integrate into query parsers if you could simply use it always and it would rewrite to simpler / faster queries when possible. This way, when a query parser is confronted with a phrase query requested by the user, it can just make a TermAutomatonQuery and run that. But the non-explicit phrase query case is still tricky...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12837726/LUCENE-6824.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7431", "change_description": ": Allow a certain amount of overlap to be specified between the include\nand exclude arguments of SpanNotQuery via negative pre and/or post arguments.", "change_title": "Allow negative pre/post values in SpanNotQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "I need to be able to specify a certain range of allowed overlap between the include and exclude parameters of SpanNotQuery. Since this behaviour is the inverse of the behaviour implemented by the pre and post constructor arguments, I suggest that this be implemented with negative pre and post values. Patch incoming.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12826303/LUCENE-7431.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7544", "change_description": ": UnifiedHighlighter: add extension points for handling custom queries.", "change_title": "UnifiedHighlighter should allow extension for custom query types", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "In our use case, we have custom query types (both SpanQuery and non-SpanQuery) which are not provided by Lucene. UnifiedHighlighter needs extension points to handle some custom query types in order for highlighting to be accurate. This issue represents adding two extension point methods to support custom query types.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12837917/LUCENE-7544.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7538", "change_description": ": Asking IndexWriter to store a too-massive text field\nnow throws IllegalArgumentException instead of a cryptic exception\nthat closes your IndexWriter", "change_title": "Uploading large text file to a field causes \"this IndexWriter is closed\" error", "detail_type": "Bug", "detail_affect_versions": "5.5.1", "detail_fix_versions": "6.4,7.0", "detail_description": "We have seen \"this IndexWriter is closed\" error after we tried to upload a large text file to a single Solr text field. The field definition in the schema.xml is: After that, the IndexWriter remained closed and couldn't be recovered until we reloaded the Solr core.  The text file had size of 800MB, containing only numbers and English characters. Stack trace is shown below: We debugged and traced down the issue.  It was an integer overflow problem that was not properly handled.  The GrowableByteArrayDataOutput::writeString(String string) method is shown below: The 800MB text file stored in the string parameter of the method had a length of 800 million, the maxLen became negative integer as the result of the length times 3. The negative integer was then passed into ArrayUtil.grow(scratchBytes, maxLen): Assertion was disabled in production so the execution won't stop. The original array was returned from the method call without increasing the size, which caused an ArrayIndexOutOfBoundsException to be thrown.  The ArrayIndexOutOfBoundsException was wrapped into AbortingException, and later on caused the IndexWriter to be closed in IndexWriter class. The code should fail faster with a more-specific error for the integer overflow problem, and shouldn't cause the IndexWriter to be closed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12837226/LUCENE-7538.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7524", "change_description": ": Added more detailed explanation of how IDF is computed in\nClassicSimilarity and BM25Similarity.", "change_title": "More detailed explanation of idf", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "The explanations of idf give the docCount and docFreq, but they do not say how the idf is computed even though the formula is different eg. for ClassicSimilarity and BM25Similarity. Maybe it would help to put the formula in the explanations?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12835355/LUCENE-7524.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7564", "change_description": ": AnalyzingInfixSuggester should close its IndexWriter by default\nat the end of build().", "change_title": "AnalyzingInfixSuggester should close its IndexWriter by default at the end of build()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "From SOLR-6246, where AnalyzingInfixSuggester's write lock on its index is causing trouble when reloading a Solr core: gsingers wrote: One suggestion that might minimize the impact: close the writer after build varunthacker wrote: This is what I am thinking - Create a Lucene issue in which AnalyzingInfixSuggester#build closes the writer by default at the end. The add and update methods call ensureOpen and those who do frequent real time updates directly via lucene won't see any slowdowns. mikemccand - Would this approach have any major drawback from Lucene's perspective? Else I can go ahead an tackle this in a Lucene issue mikemccand wrote: Fixing AnalyzingInfixSuggester to close the writer at the end of build seems reasonable?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845106/LUCENE-7564-fix-random-NRT-failures.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7526", "change_description": ": Enhanced UnifiedHighlighter's passage relevancy for queries with\nwildcards and sometimes just terms. Added shouldPreferPassageRelevancyOverSpeed()\nwhich can be overridden to return false to eek out more speed in some cases.", "change_title": "Improvements to UnifiedHighlighter OffsetStrategies", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "This ticket improves several of the UnifiedHighlighter FieldOffsetStrategies by reducing reliance on creating or re-creating TokenStreams. The primary changes are as follows:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838921/LUCENE-7526.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7560", "change_description": ": QueryBuilder.createFieldQuery is no longer final,\ngiving custom query parsers subclassing QueryBuilder more freedom to\ncontrol how text is analyzed and converted into a query", "change_title": "Can we make QueryBuilder.createFieldQuery un-final?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "It's marked final, I assume because we want people who customize their query parsers to only override newXXXQuery instead. But for deeper query parser customization, like using exploring consuming a graph and creating a TermAutomatonQuery, or a union of PhraseQuery, etc., it is not possible today and one must fork the class.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12839558/LUCENE-7560.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7537", "change_description": ": Index time sorting now supports multi-valued sorts\nusing selectors (MIN, MAX, etc.)", "change_title": "Add multi valued field support to index sorting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Today index sorting can be done on single valued field through the NumericDocValues (for numerics) and SortedDocValues (for strings). I'd like to add the ability to sort on multi valued fields. Since index sorting does not accept custom comparator we could just take the minimum value of each document for an ascending sort and the maximum value for a descending sort. This way we could handle all cases instead of throwing an exception during a merge when we encounter a multi valued DVs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12839004/LUCENE-7537.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7575", "change_description": ": UnifiedHighlighter can now highlight fields with queries that don't\nnecessarily refer to that field (AKA requireFieldMatch==false). Disabled by default.\nSee UH get/setFieldMatcher.", "change_title": "UnifiedHighlighter: add requireFieldMatch=false support", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "The UnifiedHighlighter (like the PostingsHighlighter) only supports highlighting queries for the same fields that are being highlighted.  The original Highlighter and FVH support loosening this, AKA requireFieldMatch=false.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12841762/LUCENE-7575.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7592", "change_description": ": If the segments file is truncated, we now throw\nCorruptIndexException instead of the more confusing EOFException", "change_title": "EOFException while opening index should be rethrown as CorruptIndexException", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "When opening an index, if some files were previously truncated then this should throw the more general CorruptIndexException instead of the specific EOFException to indicate to a consumer that this is not a transient or internally recoverable state.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12843299/LUCENE-7592.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-6989", "change_description": ": Make MMapDirectory's unmap hack work with Java 9 EA (b150+):\nUnmapping uses new sun.misc.Unsafe#invokeCleaner(ByteBuffer).\nJava 9 now needs same permissions like Java 8;\nRuntimePermission(\"accessClassInPackage.jdk.internal.ref\")\nis no longer needed. Support for older Java 9 builds was removed.", "change_title": "Implement MMapDirectory unmapping for coming Java 9 changes", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.0,6.4", "detail_description": "Originally, the sun.misc.Cleaner interface was declared as \"critical API\" in JEP 260 Unfortunately the decission was changed in favor of a oficially supported java.lang.ref.Cleaner API. A side effect of this change is to move all existing sun.misc.Cleaner APIs into a non-exported package. This causes our forceful unmapping to no longer work, because we can get the cleaner instance via reflection, but trying to invoke it will throw one of the new Jigsaw RuntimeException because it is completely inaccessible. This will make our forceful unmapping fail. There are also no changes in Garbage collector, the problem still exists. For more information see this mailing list thread. This commit will likely be done, making our unmapping efforts no longer working. Alan Bateman is aware of this issue and will open a new issue at OpenJDK to allow forceful unmapping without using the now private sun.misc.Cleaner. The idea is to let the internal class sun.misc.Cleaner implement the Runable interface, so we can simply cast to runable and call the run() method to unmap. The code would then work. This will lead to minor changes in our unmapper in MMapDirectory: An instanceof check and casting if possible. I opened this issue to keep track and implement the changes as soon as possible, so people will have working unmapping when java 9 comes out. Current Lucene versions will no longer work with Java 9.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844298/LUCENE-6989-v3-testFixes.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7401", "change_description": ": Changed the way BKD trees pick the split dimension in order to\nensure all dimensions are indexed.", "change_title": "BKDWriter should ensure all dimensions are indexed", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "The current heuristic is to use the dimension that has the largest span, so if dimensions have a different distribution of values, we could theoretically (but maybe in practice too?) end up with one dimension that is not indexed at all and queries that are mostly selective on this dimension would need to scan lots of blocks.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844547/LUCENE-7401.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7614", "change_description": ": Complex Phrase Query parser ignores double quotes around single token\nprefix, wildcard, range queries", "change_title": "Allow single prefix \"phrase*\" in complexphrase queryparser", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "From\tOtmar Caduff <ocad...@gmail.com> Subject\tComplexPhraseQueryParser with wildcards Date\tTue, 20 Dec 2016 13:55:42 GMT Hi, I have an index with a single document with a field \"field\" and textual content \"johnny peters\" and I am using org.apache.lucene.queryparser.complexPhrase.ComplexPhraseQueryParser to parse the query:    field: (john* peter) When searching with this query, I am getting the document as expected. However with this query:    field: (\"john*\" \"peter\") I am getting the following exception: Exception in thread \"main\" java.lang.IllegalArgumentException: Unknown query type \"org.apache.lucene.search.PrefixQuery\" found in phrase query string \"john*\" at org.apache.lucene.queryparser.complexPhrase.ComplexPhraseQueryParser$ComplexPhraseQuery.rewrite(ComplexPhraseQueryParser.java:268)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12845852/LUCENE-7614.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Improvements", "change_id": "LUCENE-7620", "change_description": ": Added LengthGoalBreakIterator, a wrapper around another B.I. to skip breaks\nthat would create Passages that are too short.  Only for use with the UnifiedHighlighter\n(and probably PostingsHighlighter).", "change_title": "UnifiedHighlighter: add target character width BreakIterator wrapper", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "The original Highlighter includes a SimpleFragmenter that delineates fragments (aka Passages) by a character width.  The default is 100 characters. It would be great to support something similar for the UnifiedHighlighter.  It's useful in its own right and of course it helps users transition to the UH.  I'd like to do it as a wrapper to another BreakIterator â€“ perhaps a sentence one.  In this way you get back Passages that are a number of sentences so they will look nice instead of breaking mid-way through a sentence.  And you get some control by specifying a target number of characters.  This BreakIterator wouldn't be a general purpose java.text.BreakIterator since it would assume it's called in a manner exactly as the UnifiedHighlighter uses it.  It would probably be compatible with the PostingsHighlighter too. I don't propose doing this by default; besides, it's easy enough to pick your BreakIterator config.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12846161/LUCENE_7620_UH_LengthGoalBreakIterator.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Optimizations", "change_id": "LUCENE-7568", "change_description": ": Optimize merging when index sorting is used but the\nindex is already sorted", "change_title": "Optimize merge when index sorting is used but the index is already sorted", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "When the index sorting is defined a lot of optimizations are disabled during the merge. For instance the bulk merge of the compressing stored fields is disabled since documents are not merged sequentially. Though it can happen that index sorting is enabled but the index is already in sorted order (the sort field is not filled or filled with the same value for all documents). In such case we can detect that the sort is not needed and activate the merge optimization.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12840016/LUCENE-7568.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Optimizations", "change_id": "LUCENE-7563", "change_description": ": The BKD in-memory index for dimensional points now uses\na compressed format, using substantially less RAM in some cases", "change_title": "BKD index should compress unused leading bytes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Today the BKD (points) in-heap index always uses dimensionNumBytes per dimension, but if e.g. you are indexing LongPoint yet only use the bottom two bytes in a given segment, we shouldn't store all those leading 0s in the index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12841977/LUCENE-7563-prefixlen-unary.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Optimizations", "change_id": "LUCENE-7583", "change_description": ": BKD writing now buffers each leaf block in heap before\nwriting to disk, giving a small speedup in points-heavy use cases.", "change_title": "Can we improve OutputStreamIndexOutput's byte buffering when writing each BKD leaf block?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "When BKD writes its leaf blocks, it's essentially a lot of tiny writes (vint, int, short, etc.), and I've seen deep thread stacks through our IndexOutput impl (OutputStreamIndexOutput) when pulling hot threads while BKD is writing. So I tried a small change, to have BKDWriter do its own buffering, by first writing each leaf block into a RAMOutputStream, and then dumping that (in 1 KB byte[] chunks) to the actual IndexOutput. This gives a non-trivial reduction (~6%) in the total time for BKD writing + merging time on the 20M NYC taxis nightly benchmark (2 times each): Trunk, sparse: Patch, sparse: Trunk dense: Patch dense: The results seem to be consistent and reproducible.  I'm using Java 1.8.0_101 on a fast SSD on Ubuntu 16.04. It's sort of weird and annoying that this helps so much, because OutputStreamIndexOutput already uses java's BufferedOutputStream (default 8 KB buffer) to buffer writes. thetaphi suggested maybe hotspot is failing to inline/optimize the writeByte / the call stack just has too many layers. We could commit this patch (it's trivial) but it'd be nice to understand and fix why buffering writes is somehow costly so any other Lucene codec components that write lots of little things can be improved too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12842144/LUCENE-7583.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Optimizations", "change_id": "LUCENE-7572", "change_description": ": Doc values queries now cache their hash code.", "change_title": "Cache the hashcode of the doc values terms queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "DocValuesNumbersQuery and DocValuesTermsQuery can potentially wrap a large number of terms so it would help if we cached their hashcode.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12840262/LUCENE-7572.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Other", "change_id": "LUCENE-7546", "change_description": ": Fixed references to benchmark wikipedia data and the Jenkins line-docs file", "change_title": "Rename uses of people.apache.org to home.apache.org", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "The people.apache.org server was replaced by a different server home.apache.org officially last year, and it appears to have completed sometime this year.  DNS for both points to the same machine but we should reference home.apache.org now.  Unfortunately, some data was large enough that ASF Infra didn't automatically move it, leaving that up to the individuals to do.  I think any data that hasn't been moved by now might be gone. Here's a useful reference to this: EMPIREDB-234   The second part of that issue also informs us that RC artifacts don't belong on home.apache.org; there is https://dist.apache.org/repos/dist/dev/<project> for that.  6.3 was done the right way... yet I see references to using people.apache.org in the build for RCs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838410/LUCENE_7546.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Other", "change_id": "LUCENE-7534", "change_description": ": fix smokeTestRelease.py to run on Cygwin", "change_title": "smokeTestRelease.py on cygwin [Errno 2] No such file or directory:", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "$ python3 -u dev-tools/scripts/smokeTestRelease.py https://dist.apache.org/repos/dist/dev/lucene/lucene-solr-6.3.0-RC2-rev1fe1a54db32b8c27bfae81887cd4d75242090613/ Revision: 1fe1a54db32b8c27bfae81887cd4d75242090613 Java 1.8 JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_102 Traceback (most recent call last):   File \"dev-tools/scripts/smokeTestRelease.py\", line 1440, in <module>     main()   File \"dev-tools/scripts/smokeTestRelease.py\", line 1377, in main     c = parse_config()   File \"dev-tools/scripts/smokeTestRelease.py\", line 1239, in parse_config     c.java = make_java_config(parser, c.test_java8)   File \"dev-tools/scripts/smokeTestRelease.py\", line 1193, in make_java_config     run_java8 = _make_runner(java8_home, '1.8')   File \"dev-tools/scripts/smokeTestRelease.py\", line 1179, in _make_runner     java_home = subprocess.check_output('cygpath -u \"%s\"' % java_home).read().decode('utf-8').strip()   File \"/usr/lib/python3.4/subprocess.py\", line 607, in check_output     with Popen(*popenargs, stdout=PIPE, **kwargs) as process:   File \"/usr/lib/python3.4/subprocess.py\", line 859, in _init_     restore_signals, start_new_session)   File \"/usr/lib/python3.4/subprocess.py\", line 1457, in _execute_child     raise child_exception_type(errno_num, err_msg) FileNotFoundError: [Errno 2] No such file or directory: 'cygpath -u \"C:Program Files\\\\Javajdk1.8.0_102\"' giving the doc path and args should be either supplied as array of terms or supplied as shell=True", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838610/LUCENE-7534.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Other", "change_id": "LUCENE-7559", "change_description": ": UnifiedHighlighter: Make Passage and OffsetsEnum more exposed to allow\npassage creation to be customized.", "change_title": "UnifiedHighlighter: Make Passage public for extensibility", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.4", "detail_description": "(UnifiedHighlighter) Passage.java has a variety of package-level fields and methods that are accessed by FieldHighlighter.highlightOffsetsEnums (the kernel of the UH).  It should be possible for people to extend this method to tweak the process of producing passages.  That method is extensible but Passage.java is too closed.  Even if one were to write their own equivalent to Passage.java, it would be impossible to then use PassageScorer or PassageFormatter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12838707/LUCENE-7559.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Other", "change_id": "LUCENE-7599", "change_description": ": Simplify TestRandomChains using Java's built-in Predicate and\nFunction interfaces.", "change_title": "replace TestRandomChains.Predicate with java.util.function.Predicate", "detail_type": "Improvement", "detail_affect_versions": "6.3", "detail_fix_versions": "6.4,7.0", "detail_description": "TestRandomChains has its own Predicate interface which can be replaced with java.util.function.Predicate", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844127/LUCENE-7599.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Other", "change_id": "LUCENE-7595", "change_description": ": Improve RAMUsageTester in test-framework to estimate memory usage of\nruntime classes and work with Java 9 EA (b148+). Disable static field heap usage\nchecker in LuceneTestCase.", "change_title": "RAMUsageTester in test-framework and static field checker no longer works with Java 9", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "Lucene/Solr tests have a special rule that records memory usage in static fields before and after test, so we can detect memory leaks. This check dives into JDK classes (like java.lang.String to detect their size). As Java 9 build 148 completely forbids setAccessible on any runtime class, we have to change or disable this check: rcmuir had some ideas for the 2nd point: In addition we also have RAMUsageTester, that has similar problems and is used to compare estimations of Lucene's calculations of Codec/IndexWriter/IndexReader memory usage with reality. We should simply disable those tests.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844712/LUCENE-7595.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Build", "change_id": "LUCENE-7387", "change_description": ": fix defaultCodec in build.xml to account for the line ending", "change_title": "Something wrong with how \"File Formats\" link is generated in docs/index.html - can cause precommit to fail on some systems", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.4,7.0", "detail_description": "I'm not sure what's going on, but here's what I've figured out while poking at things with Ishan to try and figure out why ant precommit fails for him on a clean checkout of master... ...note there is a newline in the href after lucene62 ...note that he has a URL escaped 'NO-BREAK SPACE' (U+00A0) character in href attribute. Raising the following questions...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12818999/LUCENE-7387.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Build", "change_id": "LUCENE-7543", "change_description": ": Make changes-to-html target an offline operation, by moving the\nLucene and Solr DOAP RDF files into the Git source repository under\ndev-tools/doap/ and then pulling release dates from those files, rather than\nfrom JIRA.", "change_title": "Make changes-to-html target an offline operation", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,5.6,6.0.2,6.1.1,6.2.2,6.3.1,6.4,6.4.1,6.5,7.0", "detail_description": "Currently changes-to-html pulls release dates from JIRA, and so fails when JIRA is inaccessible (e.g. from behind a firewall). SOLR-9711 advocates adding a build sysprop to ignore JIRA connection failures, but I'd rather make the operation always offline. In an offline discussion, hossman advocated moving Lucene's and Solr's doap.rdf files, which contain all of the release dates that the changes-to-html now pulls from JIRA, from the CMS Subversion repository (downloadable from the website at http://lucene.apache.org/core/doap.rdf and http://lucene.apache.org/solr/doap.rdf) to the Lucene/Solr git repository. If we did that, then the process could be entirely offline if release dates were taken from the local doap.rdf files instead of downloaded from JIRA.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12839932/LUCENE-7543-drop-XML-Simple.patch", "patch_content": "none"}
{"library_version": "6.4.0", "change_type": "Build", "change_id": "LUCENE-7596", "change_description": ": Update Groovy to version 2.4.8 to allow building with Java 9\nbuild 148+. Also update JGit version for working-copy checks.", "change_title": "Update Groovy to 2.4.8 in build system", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.4,7.0", "detail_description": "The current version of Groovy used by several Ant components is incompatible with Java 9 build 148+. We need to update to 2.4.8 once it is released: http://mail.openjdk.java.net/pipermail/jigsaw-dev/2016-December/010474.html", "patch_link": "none", "patch_content": "none"}

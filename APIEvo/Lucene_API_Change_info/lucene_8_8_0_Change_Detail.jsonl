{"library_version": "8.8.0", "change_type": "New Features", "change_id": "LUCENE-9552", "change_description": ": New LatLonPoint query that accepts an array of LatLonGeometries.", "change_title": "Add a LatLonPoint query that accepts an array of LatLonGeometries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "LatLonPoint currently support three types of queries, against a bound box, an array of polygons or by distance (circle). Therefore if you currently want to combine a query with two of those geometries, a user will need to combine the query with a boolean OR. It would be a nice addition to have a query that accepts an array of LatLonGeometries and generates that boolean OR query in a single query.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "New Features", "change_id": "LUCENE-9641", "change_description": ": LatLonPoint query support for spatial relationships.", "change_title": "Support for spatial relationships in LatLonPoint", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "Currently LatLonPoint can only generate queries that compute the documents that has at least one point intersecting a provided geometry.  LatLonShape on the other hand supports different spatial relationships, namely CONTAINS, WITHIN, DISJOINT and INTERSECTS. It would be good if LatLonPoint can work as well with different spatial relationships. It should be possible to modify ShapeQuery to work in both Shape and Points and therefore add the capability to compute spatial relationships for point as well. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "New Features", "change_id": "LUCENE-9553", "change_description": ": New XYPoint query that accepts an array of XYGeometries.", "change_title": "Add a XYPoint query that accepts an array of XYGeometries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "Similar to LUCENE-9553, it would be nice to have a query accepts an array of LatLonGeometries.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "New Features", "change_id": "LUCENE-9594", "change_description": ": FeatureField supports newLinearQuery that for scoring uses raw indexed\nvalues of features without any transformation.", "change_title": "Linear function for FeatureField", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "Currently FeatureField supports only 3 functions: log, saturation and sigmoid. It is useful for certain cases to have a linear function.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "New Features", "change_id": "LUCENE-9378", "change_description": ": Doc values now allow configuring how to trade compression for\nretrieval speed.", "change_title": "Configurable compression for BinaryDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "Lucene 8.5.1 includes a change to always compress BinaryDocValues. This caused (~30%) reduction in our red-line QPS (throughput). We think users should be given some way to opt-in for this compression feature instead of always being enabled which can have a substantial query time cost as we saw during our upgrade. mikemccand suggested one possible approach by introducing a mode in Lucene80DocValuesFormat (COMPRESSED and UNCOMPRESSED) and allowing users to create a custom Codec subclassing the default Codec and pick the format they want. Idea is similar to Lucene50StoredFieldsFormat which has two modes, Mode.BEST_SPEED and Mode.BEST_COMPRESSION. Here's related issues for adding benchmark covering BINARY doc values query-time performance - https://github.com/mikemccand/luceneutil/issues/61", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "New Features", "change_id": "LUCENE-9413", "change_description": ": Add CJKWidthCharFilter and its factory", "change_title": "Add a char filter corresponding to CJKWidthFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.8", "detail_description": "In association with issues in Elasticsearch (https://github.com/elastic/elasticsearch/issues/58384 and https://github.com/elastic/elasticsearch/issues/58385), it might be useful for Japanese default analyzer. Although I don't think it's a bug to not normalize FULL and HALF width characters before tokenization, the behaviour sometimes confuses beginners or users who have limited knowledge about Japanese analysis (and Unicode). If we have a FULL and HALF width character normalization filter in analyzers-common, we can include it into JapaneseAnalyzer (currently, JapaneseAnalyzer contains CJKWidthFilter but it is applied after tokenization so some of FULL width numbers or latin alphabets are separated by the tokenizer).", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Improvements", "change_id": "LUCENE-9455", "change_description": ": ExitableTermsEnum should sample timeout and interruption\ncheck before calling next().", "change_title": "ExitableTermsEnum (in ExitableDirectoryReader) should sample next()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "ExitableTermsEnum calls \"checkAndThrow\" on every call to next().  This is too expensive; it should sample.  I observed ElasticSearch uses the same approach; I think Lucene would benefit from this: https://github.com/elastic/elasticsearch/blob/4af4eb99e18fdaadac879b1223e986227dd2ee71/server/src/main/java/org/elasticsearch/search/internal/ExitableDirectoryReader.java#L151 CC jimczi", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Improvements", "change_id": "LUCENE-9023", "change_description": ": GlobalOrdinalsWithScore should not compute occurrences when the\nprovided min is 1.", "change_title": "GlobalOrdinalsWithScore should not compute occurrences when the provided min is 1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.8", "detail_description": "This is a continuation of https://issues.apache.org/jira/browse/LUCENE-9022 Today the GlobalOrdinalsWithScore collector and query checks the number of matching docs per parent if the provided min is greater than 0. However we should also not compute the occurrences of children when min is equals to 1 since this is the minimum requirement for a document to match.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Improvements", "change_id": "LUCENE-9675", "change_description": ": Binary doc values fields now expose their configured compression mode\nin the attributes of the field info.", "change_title": "Expose the compression mode of the binary doc values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.8", "detail_description": "LUCENE-9378 introduced a way to configure the compression mode of the binary doc values. This issue is a proposal to expose this information in the attributes of each binary field. That would expose this information to external readers on a per-field basis.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13018951/LUCENE-9675.patch", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Optimizations", "change_id": "LUCENE-9536", "change_description": ": Reduced memory usage for OrdinalMap when a segment has all\nvalues.", "change_title": "Optimize OrdinalMap when one segment contains all distinct values?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "For doc values that are not too high cardinality, it seems common to have some large segments that contain all distinct values (plus many small segments who are missing some values). In this case, we could check if the first segment ords map perfectly to global ords and if so store `globalOrdDeltas` and `firstSegments` as `LongValues.ZEROES`. This could save a small amount of space. I don’t think it would help a huge amount, especially since the optimization might only kick in with small/ medium cardinalities, which don’t create huge `OrdinalMap` instances anyways? But it is simple and seemed worth mentioning.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Optimizations", "change_id": "LUCENE-9021", "change_description": ": QueryParser: re-use the LookaheadSuccess exception.", "change_title": "QueryParser should avoid creating an LookaheadSuccess(Error) object with every instance", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "This is basically the same as https://issues.apache.org/jira/browse/SOLR-11242 , but for Lucene QueryParser", "patch_link": "https://issues.apache.org/jira/secure/attachment/12983713/LUCENE-9021.patch", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Optimizations", "change_id": "LUCENE-9636", "change_description": ": Faster decoding of postings for some numbers of bits per value.", "change_title": "Exact and operation to get a SIMD optimize", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.8", "detail_description": "In decode6(), decode7(), decode14(), decode15(), decode24() longs always `&` a same mask and do some shift. By printing assemble language, i find that JIT did not optimize them with SIMD instructions. But when we extract all `&` operations and do them first, JIT will use SIMD optimize on them.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Optimizations", "change_id": "LUCENE-9346", "change_description": ": WANDScorer now supports queries that have a\n`minimumNumberShouldMatch` configured.", "change_title": "WANDScorer should support minimumNumberShouldMatch", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "Currently we deoptimize when a minimumNumberShouldMatch is provided and fall back to a scorer that doesn't dynamically prune hits based on scores. Given how WANDScorer and MinShouldMatchSumScorer are similar I wonder if we could remove MinShouldSumScorer once WANDScorer supports minimumNumberShould match. Then any improvements we bring to WANDScorer like two-phase support (LUCENE-8806) would automatically cover more queries.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9508", "change_description": ": DocumentsWriter was only stalling threads for 1 second allowing\ndocuments to be indexed even the DocumentsWriter wasn't able to keep up flushing.\nUnless IW can't make progress due to an ill behaving DWPT this issue was barely\nnoticeable.", "change_title": "DocumentsWriter doesn't check for BlockedFlushes in stall mode``", "detail_type": "Bug", "detail_affect_versions": "8.5.1", "detail_fix_versions": "9.0,8.8", "detail_description": "Hi, I was investigating an issue where the memory usage by a single Lucene IndexWriter went up to ~23GB. Lucene has a concept of stalling in case the memory used by each index breaches the 2 X ramBuffer limit (10% of JVM heap, this case ~3GB). So ideally memory usage should not go above that limit. I looked into the heap dump and found that the fullFlush thread when enters markForFullFlush method, it tries to take lock on the ThreadStates of all the DWPT thread sequentially. If lock on one of the ThreadState is blocked then it will block indefinitely. This is what happened in my case, where one of the DWPT thread was stuck in indexing process. Due to this fullFlush thread was unable to populate the flush queue even though the stall mode was detected. This caused the new indexing request which came on indexing thread to continue after sleeping for a second, and continue with indexing. In *preUpdate()* method it looks for the stalled case and see if there is any pending flushes (based on flush queue), if not then sleep and continue. Question:  1) Should *preUpdate* look into the blocked flushes information as well instead of just flush queue ? 2) Should the fullFlush thread wait indefinitely for the lock on ThreadStates ? Since single blocking writing thread can block the full flush here.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9581", "change_description": ": Japanese tokenizer should discard the compound token instead of disabling the decomposition\nof long tokens when discardCompoundToken is activated.", "change_title": "Clarify discardCompoundToken behavior in the JapaneseTokenizer", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.8", "detail_description": "At first sight, the discardCompoundToken option added in LUCENE-9123 seems redundant with the NORMAL mode of the Japanese tokenizer. When set to true, the current behavior is to disable the decomposition for compounds, that's exactly what the NORMAL mode does. So I wonder if the right semantic of the option would be to keep only the decomposition of the compound or if it's really needed. If the goal is to make the output compatible with a graph token filter, the current workaround to set the mode to NORMAL should be enough. That's consistent with the mode that should be used to preserve positions in the index since we don't handle position length on the indexing side. Am I missing something regarding the new option ? Is there a compelling case where it differs from the NORMAL mode ?", "patch_link": "https://issues.apache.org/jira/secure/attachment/13014947/LUCENE-9581.patch", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9595", "change_description": ": Make Component2D#withinPoint implementations consistent with ShapeQuery logic.", "change_title": "Component2D#withinPoint logic is inconsistent with ShapeQuery logic", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "The logic of ShapeQuery for contains assumes that if a branch of the BKD tree is inside of the  shape query, the all documents in that branch are excluded from the result. On the other hand, Component2D#withinPoint implementation, eg. Polygon2D,  ignores points even when the point is inside the query. That might lead to inconsistencies in edges cases with geometry collections. The proposal here is to keep the logic of the shapeQuery and therefore contains logic will only return true if the query shape is inside a geometry and it does not intersects with any other geometry belonging to the same document.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9606", "change_description": ": Wrap boolean queries generated by shape fields with a Constant score query.", "change_title": "Wrap boolean queries generated by shape fields with a Constant score query", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "When querying a shape field with a Geometry collection and a CONTAINS spatial relationship, the query is rewritten as a boolean query. We should wrap the resulting query with a ConstantScoreQuery. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9635", "change_description": ": BM25FQuery - Mask encoded norm long value in array lookup.", "change_title": "BM25FQuery - MultiNormsLeafSimScorer needs to mask long value for long documents", "detail_type": "Bug", "detail_affect_versions": "8.6", "detail_fix_versions": "8.8", "detail_description": "Through some experimentation with the BM25FQuery on long documents, I've discovered that there is a bug that doesn't mask the encoded norm's long value during scoring. For long documents (or long fields) this may cause ArrayIndexOutOfBoundsExceptions. The line where I suspect the bug is being exposed is here https://github.com/apache/lucene-solr/blob/master/lucene/sandbox/src/java/org/apache/lucene/sandbox/search/MultiNormsLeafSimScorer.java#L131 Here is a similar use in BM25Similarity with the masking https://github.com/apache/lucene-solr/blob/c413656b627160d49eb9e9f1f84ec4945db80f0e/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java#L233 My experimentation shows that to expose this bug, there must be a match for a token in more than one field (which is what BM25FQuery is for). In addition one of the fields must be >= 32792 tokens long. I've provided tests in the pull request to demonstrate this. Created a PR here: https://github.com/apache/lucene-solr/pull/2138", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9617", "change_description": ": Fix per-field memory leak in IndexWriter.deleteAll(). Reset next available internal\nfield number to 0 on FieldInfos.clear(), to avoid wasting FieldInfo references.", "change_title": "FieldNumbers.clear() should reset lowestUnassignedFieldNumber", "detail_type": "Bug", "detail_affect_versions": "8.7", "detail_fix_versions": "8.8", "detail_description": "A call to IndexWriter.deleteAll() should completely reset the state of the index. Part of that is a call to globalFieldNumbersMap.clear(), which purges all knowledge of fields by clearing name -> number and number -> name maps. However, it does not reset lowestUnassignedFieldNumber. If we have loop that adds some documents, calls deleteAll(), adds documents, etc. lowestUnassignedFieldNumber keeps counting up. Since FieldInfos allocates an array for number -> FieldInfo, this array will get larger and larger, effectively leaking memory. We can fix this by resetting lowestUnassignedFieldNumber to -1 in FieldNumbers.clear(). I'll write a unit test and attach a patch.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9642", "change_description": ": When encoding triangles in ShapeField, make sure generated triangles are CCW by rotating\ntriangle points before checking triangle orientation.", "change_title": "TestXYShapeEncoding.testRandomPolygonEncoding failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "This test fails for the following seed:  ./gradlew test --tests TestXYShapeEncoding.testRandomPolygonEncoding -Dtests.seed=CB4BC47404510050 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=ug -Dtests.timezone=America/Matamoros -Dtests.asserts=true -Dtests.file.encoding=UTF-8  When encoding.a triangle we perform two actions: 1) Make sure that the orientation of the triangle is CCW 2) Place the point with minimum X in the first position so we might rotate the points in the triangle. In this case the triangle is degenerated so when the triangle is rotated,  it changes the orientation, tripping an assertion when decoding it. I think the fix for this is to change the order of the actions above.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9661", "change_description": ": Fix deadlock in TermsEnum.EMPTY that occurs when trying to initialize TermsEnum and BaseTermsEnum\nat the same time", "change_title": "Another classloader deadlock?", "detail_type": "Bug", "detail_affect_versions": "8.0,9.0", "detail_fix_versions": "8.x,9.0,8.8", "detail_description": "The java processes spawned by our Lucene nightly benchmarks sometimes randomly hang, apparently while loading classes across threads, under contention. I've opened this luceneutil issue with some details, but uschindler suggested I open an issue here too since he has been seeing this in CI builds too. It is rare, maybe once a week in the nightly benchmarks (which spawn many java processes with many threads across 128 CPU cores).  It is clearly a deadlock – when it strikes, the process hangs forever until I notice and kill -9 it.  I posted a coupled jstacks in the issue above. rcmuir suggested using classcycle to maybe statically dig into possible deadlocks ... I have not tried that yet.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13018433/deadlock_test.patch", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Other", "change_id": "SOLR-14995", "change_description": ": Update Jetty to 9.4.34", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.8.0", "change_type": "Other", "change_id": "LUCENE-9637", "change_description": ": Removes some unused code and replaces the Point implementation on ShapeField/ShapeQuery\nrandom tests.", "change_title": "Clean up ShapeField /ShapeQuery random test", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "8.8", "detail_description": "There it seems to be a few unused code on those tests and in addition we can replace the Point implementation on those test with the Point implementation in the geo package.", "patch_link": "none", "patch_content": "none"}

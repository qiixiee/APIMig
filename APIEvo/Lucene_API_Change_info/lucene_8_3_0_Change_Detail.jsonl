{"library_version": "8.3.0", "change_type": "API Changes", "change_id": "LUCENE-8909", "change_description": ": IndexWriter#getFieldNames() method is used to get fields present in index. After", "change_title": "Deprecate getFieldNames from IndexWriter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "From SOLR-12368 Would be nice to be able to remove IndexWriter.getFieldNames as well, which was added in LUCENE-7659 only for this workaround. Once Solr task resolved, deprecate IndexWriter#getFieldNames from 8x and remove it from master", "patch_link": "https://issues.apache.org/jira/secure/attachment/12975078/LUCENE-8909.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "API Changes", "change_id": "LUCENE-8316", "change_description": ": IndexWriter#getFieldNames() method is used to get fields present in index. After", "change_title": "Allow DV updates for not existing fields", "detail_type": "Improvement", "detail_affect_versions": "7.4,8.0", "detail_fix_versions": "7.4,8.0", "detail_description": "Today we prevent DV updates for non-existing fields except     of the soft deletes case. Yet, this can cause inconsitent field numbers     etc. since we don't go through the global field number map etc. This     change removes the limitation of updating DVs in docs even if the field     doesn't exists. This also has the benefit that the error messages if     the field type doesn't match is consistent with what DWPT throws.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12923696/LUCENE-8316.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "API Changes", "change_id": "LUCENE-8755", "change_description": ": SpatialPrefixTreeFactory now consumes the \"version\" parsed with Lucene's Version class.  The quad\nand packed quad prefix trees are sensitive to this.  It's recommended to pass the version like you\nshould do likewise for analysis components for tokenized text, or else changes to the encoding in future versions\nmay be incompatible with older indexes.", "change_title": "QuadPrefixTree robustness: can throw exception while indexing a point at high precision", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "When trying to index a below document with apache solr 7.5.0 i am getting java.lang.IndexOutOfBoundsException, this data is causing the whole full import to be failed. I have also defined my schema for your reference  Data: [ ]  Configuration in managed-schema.xml  <fieldType name=\"oslocation\" class=\"solr.SpatialRecursivePrefixTreeFieldType\" geo=\"false\" maxDistErr=\"0.000009\" worldBounds=\"ENVELOPE(0,700000,1300000,0)\" distErrPct=\"0.15\"/> <field name=\"root\" type=\"string\" docValues=\"false\" indexed=\"true\" stored=\"false\"/>  <field name=\"text\" type=\"text_en\" multiValued=\"true\" indexed=\"true\" stored=\"false\"/>  <field name=\"version\" type=\"long\" indexed=\"true\" stored=\"false\"/>  <field name=\"administrative_area\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"auth_name\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"id\" type=\"string\" multiValued=\"false\" indexed=\"true\" required=\"true\" stored=\"true\"/>  <field name=\"locality_name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"location\" type=\"oslocation\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"logical_status\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"lpi_logical_status\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"organisation\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"organisation_name\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_end_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_end_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_start_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_start_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_text\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"postcode_locator\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_end_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_end_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_start_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_start_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_text\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description_str\" type=\"text_gp_custom\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description_full\" type=\"text_gp_custom\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>   <field name=\"street_record_type\" type=\"int\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"town_name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"uprn\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"x_coordinate\" type=\"double\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"y_coordinate\" type=\"double\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>", "patch_link": "https://issues.apache.org/jira/secure/attachment/12966228/LUCENE-8755.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "API Changes", "change_id": "LUCENE-8956", "change_description": ": QueryRescorer now only sorts the first topN hits instead of all\ninitial hits.", "change_title": "QueryRescorer sort optimization", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "This patch addresses a TODO in QueryRescorer: We should not sort the full array of the results returned from rescoring, but rather only topN, when topN is less than total hits.  Made this optimization with some suggestions from jpountz and jimczi, this is my first lucene patch submission.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12979575/LUCENE-8956.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "API Changes", "change_id": "LUCENE-8921", "change_description": ": IndexSearcher.termStatistics() no longer takes a TermStates; it takes the docFreq and totalTermFreq.\nAnd don't call if docFreq <= 0.  The previous implementation survives as deprecated and final.  It's removed in 9.0.", "change_title": "IndexSearcher.termStatistics should not require TermStates but docFreq and totalTermFreq", "detail_type": "Improvement", "detail_affect_versions": "8.1", "detail_fix_versions": "8.3", "detail_description": "IndexSearcher.termStatistics(Term term, TermStates context) is the way to create a TermStatistics. It requires a TermStates param although it only cares about the docFreq and totalTermFreq.  For customizations that what to create TermStatistics based on docFreq and totalTermFreq, but that do not have available TermStates, this method forces to create a TermStates instance (which is not very lightweight) only to pass two ints. termStatistics could be modified to the following signature: termStatistics(Term term, int docFreq, int totalTermFreq) Since it would change the API, it could be done in master for next major release.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "API Changes", "change_id": "LUCENE-8990", "change_description": ": PointValues#estimateDocCount(visitor) estimates the number of documents that would be matched by\nthe given IntersectVisitor. THe method is used to compute the cost() of ScorerSuppliers instead of\nPointValues#estimatePointCount(visitor).", "change_title": "IndexOrDocValuesQuery can take a bad decision for range queries if field has many values per document", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "Heuristics of IndexOrDocValuesQuery are somewhat inconsistent for range queries . The leadCost that is provided is based on number of documents, meanwhile the cost() of a range query is based on the number of points that potentially match the query. Therefore it might happen that a BKD tree has millions of points but this points correspond to just a few documents. Therefore we can take the decision of executing the query using docValues and in fact we are almost scanning all the points. Maybe the cost() function for range queries need to take into account the average number of points per document in the tree and adjust the value accordingly. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8936", "change_description": ": Add SpanishMinimalStemFilter", "change_title": "Add SpanishMinimalStemFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "SpanishMinimalStemmerFilter is less aggressive stemmer than SpanishLightStemmerFilter Ex: input tokens -> output tokens  1. camiseta niños -> camiseta and nino  2. camisas -> camisa camisetas and camisas are t-shirts and shirts respectively.  Stemming both of the tokens to camis will match both tokens and returns both t-shirts and shirts for query camisas(shirts). SpanishMinimalStemmerFilter will help handling these cases. And importantly It will preserve gender context with tokens. Ex:  niños ,niñas chicos and chicas are stemmed to nino, nina, chico and chica", "patch_link": "https://issues.apache.org/jira/secure/attachment/12976054/LUCENE-8936.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8764", "change_description": "", "change_title": "Add \"export all terms\" feature to Luke", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "This is a migrated issue from previous Luke project in GitHub: https://github.com/DmitryKey/luke/issues/3 (There are users' requests so I moved this from GitHub to Jira) You can browse terms in arbitrary field via Luke GUI, but in some cases \"exporting all terms (and optionally docids) to a file\" feature would be useful for further inspection. It might be similar to Solr's terms component. As for the user interface, \"Export terms\" button should be located in Overview tab and/or Documents tab. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12976612/LUCENE-8764.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8945", "change_description": "", "change_title": "Allow to change the output file delimiter on Luke \"export terms\" feature", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "This is a follow-up issue for LUCENE-8764. Current delimiter is fixed to \",\" (comma), but terms also can include comma and they are not escaped. It would be better if the delimiter can be changed/selected to a tab or whitespace when exporting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12980603/LUCENE-8945-final.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8747", "change_description": ": Composite Matches from multiple subqueries now allow access to\ntheir submatches, and a new NamedMatches API allows marking of subqueries\nand a simple way to find which subqueries have matched on a given document", "change_title": "Allow access to submatches from Matches instances", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "A Matches object currently allows access to all matching terms from a query, but the structure of the matching query is flattened out, so if you want to find which subqueries have matched you need to iterate over all matches, collecting queries as you go.  It should be easier to get this information from the parent Matches object.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12976820/LUCENE-8747.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8769", "change_description": ": Introduce Range Query For Multiple Connected Ranges", "change_title": "Range Query Type With Logically Connected Ranges", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "Today, we visit BKD tree for each range specified for PointRangeQuery. It would be good to have a range query type which can take multiple ranges logically ANDed or ORed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12971664/LUCENE-8769.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8960", "change_description": ": Introduce LatLonDocValuesPointInPolygonQuery for LatLonDocValuesField", "change_title": "Add LatLonDocValuesPointInPolygonQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "Currently LatLonDocValuesField contain queries for bounding box and circle. This issue adds a polygon query as well.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "New Features", "change_id": "LUCENE-8753", "change_description": ": New UniformSplitPostingsFormat (name \"UniformSplit\") primarily benefiting in simplicity and\nextensibility.  New STUniformSplitPostingsFormat (name \"SharedTermsUniformSplit\") that shares a single internal\nterm dictionary across fields.", "change_title": "New PostingFormat - UniformSplit", "detail_type": "Improvement", "detail_affect_versions": "8.0", "detail_fix_versions": "8.3", "detail_description": "This is a proposal to add a new PostingsFormat called \"UniformSplit\" with 4 objectives: (the pdf attached explains visually the technique in more details)  The principle is to split the list of terms into blocks and use a FST to access the block, but not as a prefix trie, rather with a seek-floor pattern. For the selection of the blocks, there is a target average block size (number of terms), with an allowed delta variation (10%) to compare the terms and select the one with the minimal distinguishing prefix.  There are also several optimizations inside the block to make it more compact and speed up the loading/scanning. The performance obtained is interesting with the luceneutil benchmark, comparing UniformSplit with BlockTree. Find it in the first comment and also attached for better formatting. Although the precise percentages vary between runs, three main points: Compared to BlockTree, FST size is reduced by 15%, and segment writing time is reduced by 20%. So this PostingsFormat scales to lots of docs, as BlockTree. This initial version passes all Lucene tests. Use “ant test -Dtests.codec=UniformSplitTesting” to test with this PostingsFormat. Subjectively, we think we have fulfilled our goal of code simplicity. And we have already exercised this PostingsFormat extensibility to create a different flavor for our own use-case. Contributors: Juan Camilo Rodriguez Duran, Bruno Roustant, David Smiley", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8874", "change_description": ": Show SPI names instead of class names in Luke Analysis tab.", "change_title": "Show SPI names only instead of class names in Luke Analysis tab", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Each analyzer component factory now has an explicitly documented \"NAME\" static field (LUCENE-8778) to promote the proper use of (symbolic) SPI names.  It would be better to get rid of all concrete factory class names from the Analysis tab UI, but instead show SPI names. From implementation perspective, reflection tricks are needed to obtain the NAME field value without knowing the concrete classes (frameworks like Luke Analysis tab often don't know them). APIs to obtain the SPI name from a factory object might be needed for convenience and unified access to the names.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12973272/LUCENE-8874.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8894", "change_description": ": Add APIs to find SPI names for Tokenizer/CharFilter/TokenFilter factory classes.", "change_title": "Add APIs to tokenizer/charfilter/tokenfilter factories to get their SPI names from concrete classes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.2", "detail_description": "Currently, reflection tricks are needed to obtain SPI name (this is now stored in static NAME fields in each factory class) from a concrete factory class. While it is easy to implement that logic, it would be much better to provide unified APIs to get SPI name from a factory class. In other words, the APIs would provide \"inverse\" operation of lookupClass(String) method.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8914", "change_description": ": move the logic for discarding inner modes in FloatPointNearestNeighbor to the IntersectVisitor\nso we take advantage of the change introduced in", "change_title": "Small improvement in FloatPointNearestNeighbor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "Currently the logic to visit inner nodes of the BKD tree in FloatPointNearestNeighbor is in the custom tree traversing logic instead of in the IntersectVisitor. This approach is missing the improvement added on LUCENE-7862 which my experiments shows that for a high number of dimensions can give a performance improvements of around 10%. This change proposes to move the logic for discarding inner modes to the IntersectVisitor.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-7862", "change_description": ": move the logic for discarding inner modes in FloatPointNearestNeighbor to the IntersectVisitor\nso we take advantage of the change introduced in", "change_title": "Should BKD cells store their min/max packed values?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "The index of the BKD tree already allows to know lower and upper bounds of values in a given dimension. However the actual range of values might be more narrow than what the index tells us, especially if splitting on one dimension reduces the range of values in at least one other dimension. For instance this tends to be the case with range fields: since we enforce that lower bounds are less than upper bounds, splitting on one dimension will also affect the range of values in the other dimension. So I'm wondering whether we should store the actual range of values for each dimension in leaf blocks, this will hopefully allow to figure out that either none or all values match in a block without having to check them all.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938777/LUCENE-7862.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8955", "change_description": ": move the logic for discarding inner modes in LatLonPoint NearestNeighbor to the IntersectVisitor\nso we take advantage of the change introduced in", "change_title": "Move compare logic to IntersectVisitor in NearestNeighbor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "Similar to LUCENE-8914, move compare logic to the IntersectVisitor so we can take advantage of the improvement added on LUCENE-7862. I ran the geoBenchmark for nearest 10 locally and the change provides an improvement of around 30%.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-7862", "change_description": ": move the logic for discarding inner modes in LatLonPoint NearestNeighbor to the IntersectVisitor\nso we take advantage of the change introduced in", "change_title": "Should BKD cells store their min/max packed values?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "The index of the BKD tree already allows to know lower and upper bounds of values in a given dimension. However the actual range of values might be more narrow than what the index tells us, especially if splitting on one dimension reduces the range of values in at least one other dimension. For instance this tends to be the case with range fields: since we enforce that lower bounds are less than upper bounds, splitting on one dimension will also affect the range of values in the other dimension. So I'm wondering whether we should store the actual range of values for each dimension in leaf blocks, this will hopefully allow to figure out that either none or all values match in a block without having to check them all.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938777/LUCENE-7862.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8918", "change_description": ": PhraseQuery throws exceptions at construction time if it is passed\nnull arguments.", "change_title": "Catch null terms in PhraseQuery early", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "It's currently possible to build a PhraseQuery with a single null term, which will go undetected until query time, when a NullPointerException gets thrown from deep within the postings format code.  We should catch this at build time instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8916", "change_description": ": GraphTokenStreamFiniteStrings preserves all Token attributes\nthrough its finite strings TokenStreams", "change_title": "GraphTokenStreamFiniteStrings.FiniteStringsTokenStream does not play well with subsequent TokenFilters", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "GraphTokenStreamFiniteStrings provides a view over multiple paths through a Token graph, which is useful when building queries over multiple length synonyms.  This view is exposed as an iterator over simple TokenStreams.  However, these TokenStreams do not work correctly when further wrapped in token filters, because they do not use a CharTermAttribute. For an example of issues this can cause, see https://github.com/elastic/elasticsearch/issues/43976, where elasticsearch uses a special shingle field to speed up phrase searches.  Queries are converted to shingles if they have multiple terms. However, if the query resolves into a graph due to synonyms, then this conversion breaks because the FixedShingleFilter is given a token stream built by GTSFS; terms are set using BytesTermAttribute, but then read using CharTermAttribute, and as these have different backing implementations, FSF ends up emitting null tokens.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8933", "change_description": ": Check kuromoji user dictionary beforehand to avoid unexpected runtime exceptions. (Tomoko Uchida", "change_title": "JapaneseTokenizer creates Token objects with corrupt offsets", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "An Elasticsearch user reported the following stack trace when parsing synonyms. It looks like the only reason why this might occur is if the offset of a org.apache.lucene.analysis.ja.Token is not within the expected range. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8906", "change_description": ": Expose Lucene50PostingsFormat.IntBlockTermState as public so that other postings formats can re-use it.", "change_title": "Lucene50PostingsReader.postings() casts BlockTermState param to private IntBlockTermState", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "Lucene50PostingsReader is the public API that offers the postings() method to read the postings. Any PostingFormat can use it (as well as Lucene50PostingsWriter) to read/write postings. But the postings() method asks for a (public) BlockTermState param which is internally cast to the private IntBlockTermState. This BlockTermState is provided by Lucene50PostingsReader.newTermState(). public PostingsEnum postings(FieldInfo fieldInfo, BlockTermState termState, PostingsEnum reuse, int flags) This actually makes impossible to a custom PostingFormat customizing the Block file structure to use this postings() method by providing their (Int)BlockTermState, because they cannot access the FP fields of the IntBlockTermState returned by PostingsReaderBase.newTermState(). Proposed change:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8942", "change_description": ": Remove redundant parameters and improve visibility strictness in\nLRUQueryCache", "change_title": "Tighten Up LRUQueryCache's Methods", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "SOLR-13663", "change_description": ": Introduce <SpanPositionRange> into XML Query Parser", "change_title": "XML Query Parser to Support SpanPositionRangeQuery", "detail_type": "Improvement", "detail_affect_versions": "8.2", "detail_fix_versions": "8.3", "detail_description": "Currently the XML Query Parser support a vast array of span queries, including the SpanFirstQuery, but it doesn't support the generic SpanPositionRangeQuery. < SpanPositionRange start=\"2\" end=\"3\">  <SpanTerm fieldName=\"title\">prejudice</SpanTerm>  </ SpanPositionRange>  Scope of this issue is to introduce the related builder and allow the possibility to build such queries. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12977588/SOLR-13663.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8952", "change_description": ": Use a sort key instead of true distance in NearestNeighbor", "change_title": "Use a sort key instead of true distance in NearestNeighbors.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "The NearestNeighbors class contains a TODO to switch to SloppyMath.haversinSortKey when comparing candidate nearest neighbors. This change is not high priority, but could be a nice way to get more familiar with the kNN search implementation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8620", "change_description": ": Tessellator labels the edges of the generated triangles whether they belong to\nthe original polygon. This information is added to the triangle encoding.", "change_title": "Add CONTAINS support for LatLonShape", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.4", "detail_description": "Currently the only spatial operation that cannot be performed using LatLonShape is CONTAINS. This issue will add such capability by tracking if an edge of a generated triangle from the Tessellator is an edge of the polygon.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12955253/LUCENE-8620.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8964", "change_description": ": Fix geojson shape parsing on string arrays in properties", "change_title": "Allow GeoJSON parser to properly skip string arrays", "detail_type": "Improvement", "detail_affect_versions": "trunk", "detail_fix_versions": "8.3", "detail_description": "The Geo JSON parser throws an exception when trying to parse an array of strings, which is somewhat common in some free geojson services like https://whosonfirst.org An example file can be seen at https://data.whosonfirst.org/101/748/479/101748479.geojson This fixes the parser to also parse a string array.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12979465/lucene-parse-geojson-arrays-0.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8976", "change_description": ": Use exact distance between point and bounding rectangle in FloatPointNearestNeighbor.", "change_title": "Use exact distance between point and bounding rectangle in FloatPointNearestNeighbor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "Calculating minimum distance between a point and a bounding rectangle can be computed quite efficiently. This allows the FloatPointNearestNeighbor algorithm to discard inner nodes based on that calculation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8966", "change_description": ": The Korean analyzer now splits tokens on boundaries between digits and alphabetic characters.", "change_title": "KoreanTokenizer should split unknown words on digits", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "Since https://issues.apache.org/jira/browse/LUCENE-8548 the Korean tokenizer groups characters of unknown words if they belong to the same script or an inherited one. This is ok for inputs like Мoscow (with a Cyrillic М and the rest in Latin) but this rule doesn't work well on digits since they are considered common with other scripts. For instance the input \"44사이즈\" is kept as is even though \"사이즈\" is part of the dictionary. We should restore the original behavior and splits any unknown words if a digit is followed by another type. This issue was first discovered in https://github.com/elastic/elasticsearch/issues/46365", "patch_link": "https://issues.apache.org/jira/secure/attachment/12979562/LUCENE-8966.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Improvements", "change_id": "LUCENE-8984", "change_description": ": MoreLikeThis MLT is biased for uncommon fields", "change_title": "MoreLikeThis MLT is biased for uncommon fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "MLT always uses the total doc count and not the count of docs with the specific field  To quote Maria Mestre from the discussion on the mailing list - 29/01/19  The issue I have is that when retrieving the key scored terms (interestingTerms), the code uses the total number of documents in the index, not the total number of documents with populated “description” field. This is where it’s done in the code: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_lucene-2Dsolr_blob_master_lucene_queries_src_java_org_apache_lucene_queries_mlt_MoreLikeThis.java-23L651&d=DwIFaQ&c=RoP1YumCXCgaWHvlZYR8PZh8Bv7qIrMUB65eapI_JnE&r=XIYHWqjoenB2nuyYPl8m6c5xBIOD8PZJ4CWx0j6tQjA&m=gYOyL1Msgk2dpzigOsIvXq3CiFF0T7ApMLBVVDKW2dQ&s=v4mgEvgP3HWtMZcL3FTiKeY2nBOPJpTypmCpCBwPkQs&e= The effect of this choice is that the “idf” does not vary much, given that numDocs >> number of documents with “description”, so the key terms end up being just the terms with the highest term frequencies. It is inconsistent because the MLT-search then uses these extracted key terms and scores all documents using an idf which is computed only on the subset of documents with “description”. So one part of the MLT uses a different numDocs than another part. This sounds like an odd choice, and not expected at all, and I wonder if I’m missing something.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8922", "change_description": ": DisjunctionMaxQuery more efficiently leverages impacts to skip\nnon-competitive hits.", "change_title": "Speed up retrieval of top hits of DisjunctionMaxQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "There a simple optimization that we are not doing in the case that tieBreakMultiplier is 0: we could propagate the min competitive score to sub clauses as-is. Even in the general case, we currently compute the block boundary of the DisjunctionMaxQuery as the minimum of the block boundaries of its sub clauses. This generates blocks that have very low score upper bounds but unfortunately they are also very small, which means that we might sometimes not make progress quickly enough.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8935", "change_description": ": BooleanQuery with no scoring clause can now early terminate the query when", "change_title": "BooleanQuery with no scoring clauses cannot skip documents when running TOP_SCORES mode", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "Today a boolean query that is composed of filtering clauses only (more than one) cannot skip documents when the search is executed with the TOP_SCORES mode. However since all documents have a score of 0 it should be possible to early terminate the query as soon as we collected enough top hits. Wrapping the resulting boolean scorer in a constant score scorer should allow early termination in this case and would speed up the retrieval of top hits case considerably if the total hit count is not requested.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12975961/LUCENE-8935.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8941", "change_description": ": Matches on wildcard queries will defer building their full\ndisjunction until a MatchesIterator is pulled", "change_title": "Build wildcard matches more lazily", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "When retrieving a Matches object from a multi-term query, such as an AutomatonQuery or TermInSetQuery, we currently find all matching term iterators up-front, to return a disjunction over all of them.  This can be inefficient if we're only interested in finding out if anything matched, and are iterating over a different field to retrieve offsets. We can improve this by returning immediately when the first matching term is found, and only collecting other matching terms when we start iterating.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12976671/LUCENE-8941.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8755", "change_description": ": spatial-extras quad and packed quad prefix trees now index points faster.", "change_title": "QuadPrefixTree robustness: can throw exception while indexing a point at high precision", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "When trying to index a below document with apache solr 7.5.0 i am getting java.lang.IndexOutOfBoundsException, this data is causing the whole full import to be failed. I have also defined my schema for your reference  Data: [ ]  Configuration in managed-schema.xml  <fieldType name=\"oslocation\" class=\"solr.SpatialRecursivePrefixTreeFieldType\" geo=\"false\" maxDistErr=\"0.000009\" worldBounds=\"ENVELOPE(0,700000,1300000,0)\" distErrPct=\"0.15\"/> <field name=\"root\" type=\"string\" docValues=\"false\" indexed=\"true\" stored=\"false\"/>  <field name=\"text\" type=\"text_en\" multiValued=\"true\" indexed=\"true\" stored=\"false\"/>  <field name=\"version\" type=\"long\" indexed=\"true\" stored=\"false\"/>  <field name=\"administrative_area\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"auth_name\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"id\" type=\"string\" multiValued=\"false\" indexed=\"true\" required=\"true\" stored=\"true\"/>  <field name=\"locality_name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"location\" type=\"oslocation\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"logical_status\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"lpi_logical_status\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"organisation\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"organisation_name\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_end_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_end_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_start_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_start_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_text\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"postcode_locator\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_end_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_end_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_start_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_start_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_text\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description_str\" type=\"text_gp_custom\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description_full\" type=\"text_gp_custom\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>   <field name=\"street_record_type\" type=\"int\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"town_name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"uprn\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"x_coordinate\" type=\"double\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"y_coordinate\" type=\"double\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>", "patch_link": "https://issues.apache.org/jira/secure/attachment/12966228/LUCENE-8755.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8860", "change_description": ": add additional leaf node level optimizations in LatLonShapeBoundingBoxQuery.", "change_title": "LatLonShapeBoundingBoxQuery could make more decisions on inner nodes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "Currently LatLonShapeBoundingBoxQuery with the INTERSECTS relation only returns CELL_INSIDE_QUERY if the query contains ALL minimum bounding rectangles of the indexed triangles. I think we could return CELL_INSIDE_QUERY if the box contains either of the edges of all MBRs of indexed triangles since triangles are guaranteed to touch all edges of their MBR by definition. In some cases this would help save decoding triangles and running costly point-in-triangle computations.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8968", "change_description": ": Improve performance of WITHIN and DISJOINT queries for Shape queries by\ndoing just one pass whenever possible.", "change_title": "Improve performance of WITHIN and DISJOINT queries for Shape queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "We are currently walking the tree twice for INTERSECTS and WITHIN queries in ShapeQuery when we can do it in just one pass. Still we need most of the times to visit all documents to remove false positives due to multi-shapes except in the case where all documents up to maxDoc are on the tree. This issue refactors that class and tries to improve the strategy for such cases.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8939", "change_description": ": Introduce shared count based early termination across multiple slices", "change_title": "Shared Hit Count Early Termination", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "When collecting hits across sorted segments, it should be possible to terminate early across all slices when enough hits have been collected globally i.e. hit count > numHits AND hit count < totalHitsThreshold", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Optimizations", "change_id": "LUCENE-8980", "change_description": ": Blocktree's seekExact now short-circuits false if the term isn't in the min-max range of the segment.\nLarge perf gain for ID/time like data when populated sequentially.", "change_title": "Optimise SegmentTermsEnum.seekExact performance", "detail_type": "Improvement", "detail_affect_versions": "8.2", "detail_fix_versions": "8.3", "detail_description": "Description In Elasticsearch, which is based on Lucene, each document has an indexed _id field that uniquely identifies it. When Elasticsearch use the _id field to find a document from Lucene, Lucene have to check all the segments of the index. When the values of the _id field are very sequentially, the performance is optimizable. Solution Since Lucene stores min/maxTerm metrics for each segment and field, we can use those metrics to optimise performance of Lucene look up API. When calling SegmentTermsEnum.seekExact() to lookup an term in an index, we can check whether the term fall in the range of minTerm and maxTerm, so that we can skip some useless segments as soon as possible.     This improvement is beneficial to ES read/write API and Lucene look up API.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8755", "change_description": ": spatial-extras quad and packed quad prefix trees could throw a\nNullPointerException for certain cell edge coordinates", "change_title": "QuadPrefixTree robustness: can throw exception while indexing a point at high precision", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "When trying to index a below document with apache solr 7.5.0 i am getting java.lang.IndexOutOfBoundsException, this data is causing the whole full import to be failed. I have also defined my schema for your reference  Data: [ ]  Configuration in managed-schema.xml  <fieldType name=\"oslocation\" class=\"solr.SpatialRecursivePrefixTreeFieldType\" geo=\"false\" maxDistErr=\"0.000009\" worldBounds=\"ENVELOPE(0,700000,1300000,0)\" distErrPct=\"0.15\"/> <field name=\"root\" type=\"string\" docValues=\"false\" indexed=\"true\" stored=\"false\"/>  <field name=\"text\" type=\"text_en\" multiValued=\"true\" indexed=\"true\" stored=\"false\"/>  <field name=\"version\" type=\"long\" indexed=\"true\" stored=\"false\"/>  <field name=\"administrative_area\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"auth_name\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"id\" type=\"string\" multiValued=\"false\" indexed=\"true\" required=\"true\" stored=\"true\"/>  <field name=\"locality_name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"location\" type=\"oslocation\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"logical_status\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"lpi_logical_status\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"organisation\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"organisation_name\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_end_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_end_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_start_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_start_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"pao_text\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"postcode_locator\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_end_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_end_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_start_number\" type=\"int\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_start_suffix\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"sao_text\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description\" type=\"text_en\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description_str\" type=\"text_gp_custom\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"street_description_full\" type=\"text_gp_custom\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>   <field name=\"street_record_type\" type=\"int\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>  <field name=\"town_name\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"uprn\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"x_coordinate\" type=\"double\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>  <field name=\"y_coordinate\" type=\"double\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>", "patch_link": "https://issues.apache.org/jira/secure/attachment/12966228/LUCENE-8755.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9005", "change_description": ": BooleanQuery.visit() would pull subVisitors from its parent visitor, rather\nthan from a visitor for its own specific query.  This could cause problems when BQ was\nnested under another BQ. Instead, we now pull a MUST subvisitor, pass it to any MUST\nsubclauses, and then pull SHOULD, MUST_NOT and FILTER visitors from it rather than from\nthe parent.", "change_title": "BooleanQuery.visit() incorrectly pulls subvisitors from its parent", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "BooleanQuery.visit() calls getSubVisitor once for each of its clause sets; however, this sub visitor is called on the passed-in visitor, which means that sub clauses get attached to its parent, rather than a visitor for that particular BQ. To illustrate, consider the following nested BooleanQuery: (\"a b\" (+c +d %e f)); we have a top-level disjunction query containing one phrase query (essentially a conjunction), and one boolean query containing both MUST, FILTER and SHOULD clauses.  When visiting, the top level query will pull a SHOULD subvisitor, and pass both queries into it.  The phrase query will pull a MUST subvisitor and all its two terms.  The nested boolean will pull a MUST, and FILTER and a SHOULD; but these are all attached to the parent SHOULD visitor - in particular, the MUST and FILTER clauses will end up being attached to this SHOULD visitor, and be mis-interpreted as a disjunction. To fix this, BQ should first pull a MUST visitor and visit its MUST clauses using this visitor; SHOULD, FILTER and MUST_NOT clauses should then be pulled from this top-level MUST visitor.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12982946/LUCENE-9005.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8778", "change_description": "", "change_title": "Define analyzer SPI names as static final fields and document the names in Javadocs", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "Each built-in analysis component (factory of tokenizer / char filter / token filter)  has a SPI name but currently this is not  documented anywhere. The goals of this issue: and, (Just for quick reference) we now have:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12972574/LUCENE-8778-koreanNumber.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8911", "change_description": "", "change_title": "Backport LUCENE-8778 (improved analysis SPI name handling) to 8.x", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "In LUCENE-8907 I reverted LUCENE-8778 from the 8x branch. Can we backport it to 8x branch again, with transparent backwards compatibility (by emulating the factory loading method of Lucene 8.1)? I am not so sure about it would be better or not to backport the changes, however, maybe it is good for Solr to have SOLR-13593 without waiting for release 9.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12975346/LUCENE-8911-fix-mock.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8957", "change_description": "", "change_title": "Update examples in CustomAnalyzer Javadocs", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "CustomAnalyzer Javadocs need to be updated:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12978753/LUCENE-8957.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8758", "change_description": ": QuadPrefixTree: removed levelS and levelN fields which weren't used.", "change_title": "Class Field levelN is not populated correctly in QuadPrefixTree", "detail_type": "Improvement", "detail_affect_versions": "4.0,5.0,6.0,7.0,8.0", "detail_fix_versions": "8.3", "detail_description": "QuadPrefixTree in Lucene prepopulates these arrays: levelW = new double[maxLevels]; levelH = new double[maxLevels]; levelS = new int[maxLevels]; levelN = new int[maxLevels]; Like this for (int i = 1; i < levelW.length; i++) { {{ levelW[i] = levelW[i - 1] / 2.0;}} {{ levelH[i] = levelH[i - 1] / 2.0;}} {{ levelS[i] = levelS[i - 1] * 2;}} {{ levelN[i] = levelN[i - 1] * 4;}} } The field levelN[] overflows after level 14 = 1073741824 where maxLevels is limited to MAX_LEVELS_POSSIBLE = 50; The field levelN appears not to be used anywhere. Likewise, the field {{levelS[] }} is only used in the printInfo method. I would propose either to remove both levelN[],levelS[] or to change the datatype levelN = new long[maxLevels];", "patch_link": "https://issues.apache.org/jira/secure/attachment/12978887/LUCENE-8758.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8975", "change_description": ": Code Cleanup: Use entryset for map iteration wherever possible.", "change_title": "Code Cleanup: Use entryset for map iteration wherever possible.", "detail_type": "Improvement", "detail_affect_versions": "8.2", "detail_fix_versions": "8.3", "detail_description": "Simple, non-important code cleanup. Again, to clarify, please don't bother yourself with this ticket on company time, on personal time you could be working on something that makes you money or improves the product for your feature personally.  This entire ticket is an afterthough. A look back at the code base that most people don't have the time for.  ================  While true that using `entrySet()` is really only an improvement for traversing a TreeMap(at least that's how it was in JDK8), it's a good practice in general to use it over keySet(), if you then use that keyset to do an extra lookup to get the value as well as the key.  So that's what this ticket is.  All changes were done automatically via Intellij's built-in code analysis.  Putting this on LUCENE because code both in lucene and solr was changed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8993", "change_description": ",", "change_title": "Change Maven POM repository URLs to https", "detail_type": "Task", "detail_affect_versions": "7.7.2,8.2,8.1.1", "detail_fix_versions": "9.0,8.3", "detail_description": "After fixing LUCENE-8807 I figured out today, that Lucene's build system uses HTTPS URLs everywhere. But the POMs deployed to Maven central still use http (I assumed that those are inherited from the ANT build). This will fix it for later versions by changing the POM templates. Hopefully this will not happen in Gradle! markrmiller@gmail.com: Can you make sure that the new Gradle build uses HTTPS for all hard configured repositories (like Cloudera)?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12981727/LUCENE-8993.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8807", "change_description": ",", "change_title": "Change all download URLs in build files to HTTPS", "detail_type": "Task", "detail_affect_versions": "8.1", "detail_fix_versions": "7.7.2,9.0,8.2,8.1.1", "detail_description": "At least for Lucene this is not a security issue, because we have checksums for all downloaded JAR dependencies: [...] Projects like Lucene do checksum whitelists of all their build dependencies, and you may wish to consider that as a protection against threats beyond just MITM [...] This patch fixes the URLs for most files referenced in *build.xml and *ivy*.xml to HTTPS. There are a few data files in benchmark which use HTTP only, but that's uncritical and I added a TODO. Some were broken already. I removed the \"uk.maven.org\" workarounds for Maven, as this does not work with HTTPS. By keeping those inside, we break the whole chain of trust, as any non-working HTTPS would fallback to the insecure uk.maven.org Maven mirror. As the great chinese firewall is changing all the time, we should just wait for somebody complaining.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969291/LUCENE-8807.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8998", "change_description": ": Fix OverviewImplTest.testIsOptimized reproducible failure.", "change_title": "OverviewImplTest.testIsOptimized reproducible failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "The following seed reproduces reliably for me on master... (NOTE: the ERROR StatusLogger messages include the one about the AccessControlException occur even with other seeds when the test passes) This method fails on that seed regardless of wether tests.method is used, or if all methods in that class are run", "patch_link": "https://issues.apache.org/jira/secure/attachment/12982269/LUCENE-8998.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8999", "change_description": ": LuceneTestCase.expectThrows now propogates assert/assumption failures up to the test\nw/o wrapping in a new assertion failure unless the caller has explicitly expected them", "change_title": "expectThrows doesn't play nicely with \"assume\" failures", "detail_type": "Test", "detail_affect_versions": "8.2", "detail_fix_versions": "9.0,8.3", "detail_description": "Up to Lucene 8.2, if expectThrows (or one of it's variants) was given a Runnable that  contained an assert/assume call which failed somehwere down it's stack, it would catch these and re-wrap them in a new assertion failure. (unless it matched the \"expected\" exception type).  This would mean that tests which should have been SKIPed due to a bad assumption about the local ENV would instead FAIL. This issue tracks a change such that expectThrow now directly re-throws any instances of AssertionError or AssumptionViolatedException unless they are instances of the expected exception type specified by the user. Original jira summary below... Once upon a time, TestRunWithRestrictedPermissions use to have test methods that looked like this... LUCENE-8938 changed this code to look like this... But a nuance of the existing code that isn't captured in the new code is that runWithRestrictedPermissions(...) explicitly uses assumeTrue(..., System.getSecurityManager() != null) to ensure that if a security manager is not in use, the test should be SKIPed and not considered a pass or a fail. The key issue being that assumeTrue(...) (and other 'assume' related methods like it) throws an AssumptionViolatedException when the condition isn't met, expecting this to propagate up to the Test Runner. With the old code this worked as expected - the AssumptionViolatedException would abort execution before the fail(...) but not be caught by the catch and bubble up all the way to the test runner so the test would be recorded as a SKIP. With the new code, expectThrows() is catching the AssumptionViolatedException and since it doesn't match the expected SecurityException.class is generating a test failure instead... While there might be easy fixes that could be made explicitly to TestRunWithRestrictedPermissions to deal with this particular problem, it seems like perhaps we should consider changes to better deal with this type of problem that might exist elsewhere or occur in the future?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12982179/LUCENE-8999.patch", "patch_content": "none"}
{"library_version": "8.3.0", "change_type": "Other", "change_id": "LUCENE-8062", "change_description": ": GlobalOrdinalsWithScoreQuery is no longer eligible for query caching.", "change_title": "Never cache GlobalOrdinalQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "GlobalOrdinalsQuery holds a possibly large bitset of global ordinals that can pollute the query cache because the size of the query is not accounted in the memory usage of the cache. Moreover two instances of this query must share the same top reader context to be considered equal so they are not the ideal candidate for segment level caching.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898951/LUCENE-8062.patch", "patch_content": "none"}

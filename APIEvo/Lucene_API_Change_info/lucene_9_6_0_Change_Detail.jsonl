{"library_version": "9.6.0", "change_type": "API Changes", "change_id": "GITHUB#12116", "change_description": ": Introduce IndexableField#storedValue() to expose the value that\nshould be stored to IndexingChain without needing to guess the field's type.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently stored fields have to look at binaryValue(), stringValue() and numericValue() to guess the type of the value and then store it. This has a few issues: This commit introduces IndexableField#storedValue() , which is used only for stored fields. This addresses the above issues. IndexingChain passes the storedValue() directly to the codec, so it's impossible for a stored fields format to mistakenly use binaryValue()/stringValue()/numericValue() instead of storedValue().", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "API Changes", "change_id": "GITHUB#12129", "change_description": ": Move DocValuesTermsQuery from sandbox to SortedDocValuesField#newSlowSetQuery\nand SortedSetDocValuesField#newSlowSetQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "API Changes", "change_id": "GITHUB#12173", "change_description": ": TermInSetQuery#getTermData has been deprecated. This exposes internal implementation details that we\nmay want to change in the future, and users shouldn't rely on the encoding directly.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "TermInSetQuery#getTermData is effectively leaking our internal encoding of the query terms, which will prevent us from changing this in the future if we want to. Nothing in our code relies on this right now, and I think we ought to remove it. If users really need to get the terms, they can keep track of them (they provide them in the first place) in whatever encoding they like. If there's really a need to go beyond this, we could expose some sort of Iterator<BytesRef> method. But let's stop exposing the PrefixCodedTerms instance directly. Note : I will remove the method completely on main but mark deprecated on 9x as this PR demonstrates.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "API Changes", "change_id": "GITHUB#11746", "change_description": ": Deprecate LongValueFacetCounts#getTopChildrenSortByCount.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a companion (backport) PR to #11744 that marks functionality deprecated instead of outright removing it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "New Features", "change_id": "GITHUB#12054", "change_description": ": Introduce a new KeywordField for simple and efficient\nfiltering, sorting and faceting.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "KeywordField is a combination of StringField and SortedSetDocValuesField , similarly to how LongField is a combination of LongPoint and SortedNumericDocValuesField . This makes it easier for users to create fields that can be used for filtering, sorting and faceting.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "New Features", "change_id": "GITHUB#12188", "change_description": ": Add support for Java 20 foreign memory API. If exactly Java 19\nor 20 is used, MMapDirectory will mmap Lucene indexes in chunks of 16 GiB\n(instead of 1 GiB) and indexes closed while queries are running can no longer\ncrash the JVM. To disable this feature, pass the following sysprop on Java command line:\n\"-Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\"", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Improvements", "change_id": "GITHUB#12055", "change_description": ": MultiTermQuery#CONSTANT_SCORE_BLENDED_REWRITE rewrite method introduced and used as the new default\nfor multi-term queries with a FILTER rewrite (PrefixQuery, WildcardQuery, TermRangeQuery). This introduces better\nskipping support for common use-cases.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently multi-term queries with a filter rewrite internally rewrite to a disjunction if 16 terms or less match the query. Otherwise postings lists of matching terms are collected into a DocIdSetBuilder . This change replaces the latter with a mixed approach where a disjunction is created between the 16 terms that have the highest document frequency and an iterator produced from the DocIdSetBuilder that collects all other terms. On fields that have a zipfian distribution, it's quite likely that no high-frequency terms make it to the DocIdSetBuilder . This provides two main benefits: The slowdown is unfortunate, but my gut feeling is that this change still has more pros than cons.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Improvements", "change_id": "GITHUB#12156", "change_description": ": TermInSetQuery now extends MultiTermQuery instead of providing its own custom implementation (which\nwas essentially a clone of MultiTermQuery#CONSTANT_SCORE_REWRITE). It uses the new CONSTANT_SCORE_BLENDED_REWRITE\nby default, but can be overridden through the constructor.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "TermInSetQuery 's implementation is more-or-less an exactly copy of MultiTermQuery + MultiTermQueryConstantScoreWrapper . This PR removes the custom TermInSetQuery implementation in favor of extending MultiTermQuery with MultiTermQueryConstantScoreWrapper as the default rewrite behavior. One nice benefit of this (beyond code cleanup) is that different rewrite methods can be provided for different behavior. Specifically, we can leverage DocValuesRewriteMethod to completely replace SortedSetDocValuesSetQuery (I would propose doing that in a follow up PR once we're happy with this change). A couple notes about the change: When this has been discussed in the past, there's been an open question around performance since the term intersection happens a bit differently ( seekCeil vs. seekExact ). I ran some benchmarks using a one-off tool (similar to #12151 ) and found no noticeable regressions/issues. Here's the output of my tool (which you can check out here: TiSBench.java.txt )", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Improvements", "change_id": "GITHUB#12175", "change_description": ": Remove SortedSetDocValuesSetQuery in favor of TermInSetQuery with DocValuesRewriteMethod.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Now that TermInSetQuery extends MultiTermQuery ( #12156 ), we can leverage other RewriteMethod s to change the query execution behavior. Because of this, we can use DocValuesRewriteMethod in place of SortedSetDocValuesSetQuery , which lets us remove some mostly redundant code and simplify.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Improvements", "change_id": "GITHUB#12126", "change_description": ": Refactor part of IndexFileDeleter and ReplicaFileDeleter into a public common utility class\nFileDeleter.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "See #11885 I extracted mainly the ref counting part into the new FileDeleter so that both IndexFileDeleter and ReplicaFileDeleter will use it as a component. It does not provide any level of thread-safety since the IndexFileDeleter originally was implemented in a lock-free way, so the user of this new FileDeleter is responsible for the synchronization if necessary. I haven't written any specific test for this since I feel like the existing tests (mainly TestIndexFileDeleter ) should already provide a good coverage", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#11900", "change_description": ": BloomFilteringPostingsFormat now uses multiple hash functions\nin order to achieve the same false positive probability with less memory.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "BloomFilteringPostingsFormat currently relies on a bloom filter with one hash function (k=1). For a target false positive probability of 10%, 1 is never the optimal value for k. Using the best value for k would either:achieve a much better fpp with the same bitset size, orachieve the same fpp with a reduced size (half of what is currently used.From tests:targeting a better false positive probability doesn't bring significant enough better performance for the increased size;Targeting a smaller size by degrading the false positive probability comes with a significant performance hit.As consequence, a target false positive probability of about 10% seems to be a good trade-off. I slightly raised this value (to 0.1023f) so the size of newly allocated bloom filters is always half the size of what they used to be. The effective false positive probability varies from significantly better in most cases to slightly worse in rare cases. This graph compares both size and effective false positive probability of the current and proposed implementations. Overall performance remains comparable (slightly but not significantly better); the reduced size and the improved false positive probability compensate for the cost of having additional hashes. You can find in branch bloomPerfBench the class BloomBench I used to check for performance.In addition, the implementation of the bitset is based on a long array, so picking up a size lower than 64 bits is pointless.API change:HashFunction.hash(BytesRef) returns a long: more accuracy with a 64bits hash useful to derivate additional hashes from the original one.The proposed implementation remains compatible with existing/persisted bloom filters.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12118", "change_description": ": Optimize FeatureQuery to TermQuery & weight when scoring is not required.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "While FeatureQuery is a powerful tool in the scoring case, there are scenarios when caching should be allowed and scoring disabled. A particular case is when the FeatureQuery is used in conjunction with learned-sparse retrieval. It is useful to iterate and calculate the entire matching doc set when combined with various other queries. related to: #11799", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12128", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "LongHashSet is used for the set of numbers, but it has some issues: Practically we should take advantage of the fact numbers come in sorted order for multivalued fields: just like range queries do. So we use min/max to our advantage, including termination of docvalues iteration. Actually it is generally a win to just check min/max even in the single-valued case: these constant time comparisons are cheap and can avoid hashing, etc. In the worst-case, if all of your query Sets contain both the minimum and maximum possible values, then it won't help, but it doesn't hurt either.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12133", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Instead return LongStream for toString() and testing (and possible other use-cases) This is a followup of @rmuir 's PR #12128 and trashes even more code.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12050", "change_description": ": Reuse HNSW graph for intialization during merge", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Related to #11354 (performance metrics can be found here). I also started a draft PR in #11719 , but decided to refactor into a new PR. This PR adds the functionality to initialize a merged segment's HNSW graph from the largest HNSW graph from the segments being merged. The graph selected must not contain any dead documents. If no suitable intiailizer graph is found, it will fall back to creating the graph from scratch. To support this functionality, a couple of changes to current graph construction process needed to be made. OnHeapHnswGraph had to support out of order insertion. This is because the mapped ordinals of the nodes in the graph used for initialization are not necessarily the first X ordinals in the new graph. I also removed the implicit addition of the first node into the graph. Implicitly adding the first node created a lot of complexity for initialization. In #11719 , I got it to work without changing this but thought it was cleaner to switch to require the first node to be added explicitly. In addition to this, graphs produced by merging two segments are no longer necessarily going to be equivalent to indexing one segment directly. This is caused by both differences in assigned random values as well as insertion order dictating which neighbors are selected for which nodes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12155", "change_description": ": Speed up DocValuesRewriteMethod by making use of sortedness.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This brings over the recent \"max ord\" optimization in SortedSetDocValuesSetQuery ( #12129 ) to DocValuesRewriteMethod .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12139", "change_description": ": Faster indexing of string fields.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Indexing simple keywords through a TokenStream abstraction introduces a bit of overhead due to attribute management. Not much, but indexing keywords boils down to adding to a hash map and appending to a postings list, which is quite cheap too so even some low overhead can significantly impact indexing speed. The way that this change works is by making IndexingChain check binaryValue() when a field is indexed but tokenStream() returns null . Then KeywordField only has to return null in tokenStream() to take advantage of this optimization. I hesitated doing the same with StringField but wondered if this could be breaking to some users who might pull a TokenStream themselves.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12179", "change_description": ": Better PostingsEnum reuse in MultiTermQueryConstantScoreBlendedWrapper.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We can be a bit better about PostingsEnum reuse when backfilling collected terms (i.e., reuse the same postings if we consume them into the bitset).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12198", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "lucene-util's IndexGeoNames benchmark is heavily contended when running with many indexing threads, 20 in my case. The main offender is DocumentsWriterFlushControl#doAfterDocument , which runs after every index operation to update doc and RAM accounting. This change reduces contention by only updating RAM accounting if the amount of RAM consumption that has not been committed yet by a single DWPT is at least 0.1% of the total RAM buffer size or 16kB. This effectively batches updates to RAM accounting, similarly to what happens when using IndexWriter#addDocuments to index multiple documents at once. Since updates to RAM accounting may be batched, FlushPolicy can no longer distinguish between inserts, updates and deletes, so all 3 methods got merged into a single one. With this change, IndexGeoNames goes from ~22s to ~19s and the main offender for contention is now DocumentsWriterPerThreadPool#getAndLock .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12199", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Obtaining a DWPT and putting it back into the pool is subject to contention. This change reduces contention by using 8 sub pools that are tried sequentially. When applied on top of #12198 , this reduces the time to index geonames with 20 threads from ~19s to ~16-17s.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Optimizations", "change_id": "GITHUB#12241", "change_description": ": Add ordering of files in compound files.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today there is no specific ordering of how files are written to a compound file. The current order is determined by iterating over the set of file names in SegmentInfo, which is unspecific. This PR proposes to change to an order based on file size. Colocating data from files that are smaller (typically metadata files like terms index, field info etc...) but accessed often can help when parts of these files are held in cache. In our particular case, the motivation is coming from reading larger compound files from a remote object store through a caching layer that keeps chunks of the file in pages. Keeping small files together can help improve the efficiency of the cache because data that is read often (like metadata) is kept together.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12158", "change_description": ": KeywordField#newSetQuery should clone input BytesRef[] to avoid modifying provided array.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "SortedSetDocValuesSetQuery#new sorts the provided terms BytesRef[] in place, so we should really clone the values provided to KeywordField#newSetQuery before initializing the query (we do this properly in the other locations where we invoke the ctor).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12196", "change_description": ": Fix MultiFieldQueryParser to handle both query boost and phrase slop at the same time.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This change fixes #12195 . In Lucene 5.4.0 this commit introduced some changes for immutability of queries. setBoost() function was replaced with new BoostQuery(), but BoostQuery is not handled in setSlop function. This pull request adds the handling of BoostQuery in setSlop() function", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12202", "change_description": ": Fix MultiFieldQueryParser to apply boosts to regexp, wildcard, prefix, range, fuzzy queries.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This fixes #8966 . When using boost along with any of fuzzy, wildcard, regexp, range or prefix queries, the boost is not applied.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12178", "change_description": ": Add explanations for TermAutomatonQuery", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Hi! This is my first time posting a GitHub issue for Apache Lucene. Please let me know if you need anything further.  lucene/lucene/sandbox/src/java/org/apache/lucene/sandbox/search/TermAutomatonQuery.java Lines 443 to 447\n      in 569533b  We should return relevant explain output here instead of returning null for TermAutomatonQuery . This would be helpful for users running document explain on synonym queries leveraging TermAutomatonQuery . Otherwise, certain queries that can nest other queries could throw a NullPointerException if their weight's explain() calls TermAutomatonQuery#explain() . For example, if a BooleanQuery nests a TermAutomatonQuery , then BooleanWeight throws an NPE here when running explain() :  lucene/lucene/core/src/java/org/apache/lucene/search/BooleanWeight.java Line 75\n      in 569533b  No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12214", "change_description": ": Fix ordered intervals query to avoid skipping some of the results over interleaved terms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR aims to address issue #12213 Ordered intervals over interleaved terms. A more detailed explanation of the issue and the reasoning behind the fix can be found in the report link above.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12212", "change_description": ": Bug fix for a DrillSideways issue where matching hits could occasionally be missed.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR aims to address issue #12211 : Searches made via DrillSideways may miss documents that should match the query. A more detailed explanation of the issue and the reasoning behind the fix can be found in the report linked above, but it basically boils down to the fact that the score method in DrillSidewaysScorer results in more than one consecutive call to the matches method exposed by the TwoPhaseIterator instance without re-positioning the iterator first. This in turn, makes the matching of documents erratic, and lead to more or less subtle issues in searches from the end user's view point, where documents that should match there query sometime don't appear in the result list (but may still do if other factors such as the inner type of query resulting from parsing, caching, order in which docs where indexed, etc...) I propose solving the issue by initializing the position of the scorer by call nextDoc on baseApproximation instead of baseIterator , which should produced the expected result regardless of the type of iterator. In fact, looking back through the history of this code, it feels to me that calling nextDoc on baseIterator is a left over from before two phase iterator where introduced, and should have been changed then.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12220", "change_description": ": Hunspell: disallow hidden title-case entries from compound middle/end", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "if we only have custom-case uART and capitalized UART, we shouldn't accept StandUart as a compound (although we keep hidden \"Uart\" dictionary entries for internal purposes)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Build", "change_id": "GITHUB#12131", "change_description": ": Generate gradle.properties from gradlew, if absent", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In Apache Solr we improved the local settings generation to be done directly in gradlew startup (similar to gradle downloader). This has several positive effects: See apache/solr#1320 and https://issues.apache.org/jira/browse/SOLR-16641 for details.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Build", "change_id": "GITHUB#12188", "change_description": ": Building the lucene-core MR-JAR file is now possible without installing\nadditionally required Java versions (Java 19, Java 20,...). For compilation, a special\nJAR file with Panama-foreign API signatures of each supported Java version was added to\nsource tree. Those can be regenerated an demand with \"gradlew :lucene:core:regenerate\".", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is an altenative version to PR #12042 . The problem with the previous PR (and also the Java 19 version) is mainly the following issues: The approach here is a new idea that came to me when I studied, how the --release flag is implemented in javac. It works like that: Java has basically a copy of all class files of previous Java versions in some specially designed JAR file. When you request --release 11 from Java compiler, it will overlay the compile classpath wit this JAR file that contains all classes (only signatures). Of course this will spend a lot of space, so there are 2 optimizations: it removes all bytecode from those JAR files, so all class files in it have empty method bodies. In addition, it will only add modified class files. After each relaese of Java they build this special signatures JAR file and store it in the JDK folder. FYI, preview API are excluded there, this is why we can't compile. The PR here implements the following: This PR allows anybody to compile the MR-JAR classes without the JDK installed. We can also merge this PR now, as the JDK 20 release is already in RC phase, so the API is finalized. The APIJAR files don't get outdated anymore. We can also release Lucene now with JDK 20 support! Downsides: If anybody has an idea, speak up. If we agree, this would be the easiest to support MR-JARs of MMapDirectory in future (also with Java 21 where it is not yet fully sure that the API gets to its final stage! ....unfortunately.... damn! The same approach could be used for vector APIs in future. Please add a comment about if you agree to do it like that!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Build", "change_id": "GITHUB#12215", "change_description": ": Upgrade forbiddenapis to version 3.5.  This tones down some verbose warnings\nprinted while checking Java 19 and Java 20 sourcesets for the MR-JAR.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This tones down some verbose warnings printed while checking Java 19 and Java 20 sourcesets for the MR-JAR.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Documentation", "change_id": "GITHUB#10633", "change_description": ": Update javadocs in TestBackwardsCompatibility to use gradle and not ant.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In PR-1733 we noticed that the comments for generating old indexes in org.apache.lucene.index.TestBackwardsCompatibility use ant instead of gradle. However, since support for ant has been removed we should update the docs to reflect the new command. New tests in the same PR-1733 suggest that the change in the comments is simple: Replace ant test by gradlew test . (with no change in the JVM args) I've verified that this works for the tests in PR-1733 Migrated from LUCENE-9593 by Gautam Worah ( @gautamworah96 ) The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Other", "change_id": "GITHUB#11868", "change_description": ": Add a FilterIndexInput and FilterIndexOutput class to more easily and safely create delegate\nIndexInput and IndexOutput classes", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We have several subclasses of IndexOutput that have delegates, most recently one was added in this PR: #11796 . Adding a FilterIndexOutput , similar to FilterDirectory , to make sure all these delegators get tested properly would be a good idea. (suggested by @mikemccand here: https://github.com/apache/lucene/pull/11796/files#r1000886175 ). The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.6.0", "change_type": "Other", "change_id": "GITHUB#12239", "change_description": ": Hunspell: reduced suggestion set dependency on the hash table order", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When adding words to a dictionary, suggestions for other words shouldn't change unless they're directly related to the added words. But before, GeneratingSuggester selected 100 best first matches from the hash table, whose order can change significantly after adding any unrelated word. That resulted in unexpected suggestion changes on seemingly unrelated dictionary edits.", "patch_link": "none", "patch_content": "none"}

{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5084", "change_description": ": Added new Elias-Fano encoder, decoder and DocIdSet\nimplementations.", "change_title": "EliasFanoDocIdSet", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "DocIdSet in Elias-Fano encoding", "patch_link": "https://issues.apache.org/jira/secure/attachment/12590919/LUCENE-5084.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5081", "change_description": ": Added WAH8DocIdSet, an in-memory doc id set implementation based\non word-aligned hybrid encoding.", "change_title": "WAH8DocIdSet", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Our filters use bit sets a lot to store document IDs. However, it is likely that most of them are sparse hence easily compressible. Having efficient compressed sets would allow for caching more data.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12591684/LUCENE-5081.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5098", "change_description": ": New broadword utility methods in oal.util.BroadWord.", "change_title": "Broadword bit selection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12592025/LUCENE-5098.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5030", "change_description": ": FuzzySuggester now supports optional unicodeAware\n(default is false).  If true then edits are measured in Unicode code\npoints instead of UTF8 bytes.", "change_title": "FuzzySuggester has to operate FSTs of Unicode-letters, not UTF-8, to work correctly for 1-byte (like English) and multi-byte (non-Latin) letters", "detail_type": "Bug", "detail_affect_versions": "4.3", "detail_fix_versions": "4.5,6.0", "detail_description": "There is a limitation in the current FuzzySuggester implementation: it computes edits in UTF-8 space instead of Unicode character (code point) space. This should be fixable: we'd need to fix TokenStreamToAutomaton to work in Unicode character space, then fix FuzzySuggester to do the same steps that FuzzyQuery does: do the LevN expansion in Unicode character space, then convert that automaton to UTF-8, then intersect with the suggest FST. See the discussion here: http://lucene.472066.n3.nabble.com/minFuzzyLength-in-FuzzySuggester-behaves-differently-for-English-and-Russian-td4067018.html#none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12589721/nonlatin_fuzzySuggester_combo2.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5118", "change_description": ": SpatialStrategy.makeDistanceValueSource() now has an optional\nmultiplier for scaling degrees to another unit.", "change_title": "spatial strategy- add multiplier to makeDistanceValueSource()", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "SpatialStrategy has this abstract method: I'd like to add another argument double multiplier that is internally multiplied to the result per document.  It's a convenience over having the user wrap this with another ValueSource, and it'd be faster too.  Typical usage would be to add a degrees-to-kilometers multiplier. The current method could be marked deprecated with a default implementation that invokes the new one with a 1.0 multiplier.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12592715/LUCENE-5118__multiplier_to_spatial_makeDistanceValueSource.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5091", "change_description": ": SpanNotQuery can now be configured with pre and post slop to act\nas a hypothetical SpanNotNearQuery.", "change_title": "Modify SpanNotQuery to act as SpanNotNearQuery too", "detail_type": "Improvement", "detail_affect_versions": "4.3.1", "detail_fix_versions": "4.5,6.0", "detail_description": "With very small modifications, SpanNotQuery can act as a SpanNotNearQuery. To find \"a\" but not if \"b\" appears 3 tokens before or 4 tokens after \"a\": new SpanNotQuery(\"a\", \"b\", 3, 4) Original constructor still exists and calls SpanNotQuery(\"a\", \"b\", 0, 0). Patch with tests on way.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593963/LUCENE-5091.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-4985", "change_description": ": FacetsAccumulator.create() is now able to create a\nMultiFacetsAccumulator over a mixed set of facet requests. MultiFacetsAccumulator\nallows wrapping multiple FacetsAccumulators, allowing to easily mix\nexisting and custom ones. TaxonomyFacetsAccumulator supports any\nFacetRequest which implements createFacetsAggregator and was indexed\nusing the taxonomy index.", "change_title": "Make it easier to mix different kinds of FacetRequests", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Spinoff from LUCENE-4980, where we added a strange class called RangeFacetsAccumulatorWrapper, which takes an incoming FSP, splits out the FacetRequests into range and non-range, delegates to two accumulators for each set, and then zips the results back together in order. Somehow we should generalize this class and make it work with SortedSetDocValuesAccumulator as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594677/LUCENE-4985.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5153", "change_description": ": AnalyzerWrapper.wrapReader allows wrapping the Reader given to\ninputReader.", "change_title": "Allow wrapping Reader from AnalyzerWrapper", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "It can be useful to allow AnalyzerWrapper extensions to wrap the Reader given to initReader, e.g. with a CharFilter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12595035/LUCENE-5153.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5155", "change_description": ": FacetRequest.getValueOf and .getFacetArraysSource replaced by\nFacetsAggregator.createOrdinalValueResolver. This gives better options for\nresolving an ordinal's value by FacetAggregators.", "change_title": "Add OrdinalValueResolver in favor of FacetRequest.getValueOf", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "FacetRequest.getValueOf is responsible for resolving an ordinal's value. It is given FacetArrays, and typically does something like arrays.getIntArray()[ord] – for every ordinal! The purpose of this method is to allow special requests, e.g. average, to do some post processing on the values, that couldn't be done during aggregation. I feel that getValueOf is in the wrong place – the calls to getInt/FloatArray are really redundant. Also, if an aggregator maintains some statistics by which it needs to \"correct\" the aggregated values, it's not trivial to pass it from the aggregator to the request. Therefore I would like to make the following changes: This allows an OVR to initialize the right array instance(s) in the ctor, and return the value of the requested ordinal, without doing arrays.getArray() calls. Will post a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12595273/LUCENE-5155.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5165", "change_description": ": Add SuggestStopFilter, to be used with analyzing\nsuggesters, so that a stop word at the very end of the lookup query,\nand without any trailing token characters, will be preserved.  This\nenables query \"a\" to suggest apple; see", "change_title": "Add SuggestStopFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "This is like StopFilter, except if the token is the very last token and there were no non-token characters after it, it keeps the token. This is useful with analyzing suggesters (AnalyzingSuggester, AnalyzingInfixSuggester, FuzzySuggester), where you often want to remove stop words, but not if it's the last word and the user hasn't finished typing it. E.g. \"fast a\" might complete to \"fast amoeba\", but if you simply use StopFilter then the a is removed. Really our analysis APIs aren't quite designed to handle a \"partial\" tokens that suggesters need to work with.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597299/LUCENE-5165.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "http://blog.mikemccandless.com/2013/08/suggeststopfilter-carefully-removes.html", "change_description": ": Add SuggestStopFilter, to be used with analyzing\nsuggesters, so that a stop word at the very end of the lookup query,\nand without any trailing token characters, will be preserved.  This\nenables query \"a\" to suggest apple; see", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5178", "change_description": ": Added support for missing values to DocValues fields.\nAtomicReader.getDocsWithField returns a Bits of documents with a value,\nand FieldCache.getDocsWithField forwards to that for DocValues fields. Things like\nSortField.setMissingValue, FunctionValues.exists, and FieldValueFilter now\nwork with DocValues fields.", "change_title": "doc values should expose missing values (or allow configurable defaults)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "DocValues should somehow allow a configurable default per-field. Possible implementations include setting it on the field in the document or registration of an IndexWriter callback. If we don't make the default configurable, then another option is to have DocValues fields keep track of whether a value was indexed for that document or not.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598849/LUCENE-5178_reintegrate.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5124", "change_description": ": Lucene 4.5 has a new Lucene45Codec with Lucene45DocValues,\nsupporting missing values and with most datastructures residing off-heap.\nAdded \"Memory\" docvalues format that works entirely in heap, and \"Disk\"\nloads no datastructures into RAM. Both of these also support missing values.\nAdded DiskNormsFormat (in case you want norms entirely on disk).", "change_title": "fix+document+rename DiskDV to Lucene45", "detail_type": "New Feature", "detail_affect_versions": "4.5", "detail_fix_versions": "4.5,6.0", "detail_description": "The idea is that the default implementation should not hold everything in memory, we can have a \"Memory\" impl for that. I think stuff being all in heap memory is just a relic of FieldCache. In my benchmarking diskdv works well, and its much easier to manage (keep a smaller heap, leave it to the OS, no OOMs etc from merging large FSTs, ...) If someone wants to optimize by forcing everything in memory, they can then use the usual approach (e.g. just use FileSwitchDirectory, or pick \"Memory\" for even more efficient stuff). Ill keep the issue here for a bit. If we decide to do this, ill work up file format docs and so on. We should also fix a few things that are not great about it (LUCENE-5122) before making it the default.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-2750", "change_description": ": Added PForDeltaDocIdSet, an in-memory doc id set implementation\nbased on the PFOR encoding.", "change_title": "add Kamikaze 3.0.1 into Lucene", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Kamikaze 3.0.1 is the updated version of Kamikaze 2.0.0. It can achieve significantly better performance then Kamikaze 2.0.0 in terms of both compressed size and decompression speed. The main difference between the two versions is Kamikaze 3.0.x uses the much more efficient implementation of the PForDelta compression algorithm. My goal is to integrate the highly efficient PForDelta implementation into Lucene Codec.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597131/LUCENE-2750.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5186", "change_description": ": Added CachingWrapperFilter.getFilter in order to be able to get\nthe wrapped filter.", "change_title": "Add CachingWrapperFilter.getFilter()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "There are a couple of use cases I can think of where being able to get the underlying filter out of CachingWrapperFilter would be useful: 1. You might want to introspect the filter to figure out what's in it (the use case we hit.) 2. You might want to serialise the filter since Lucene no longer supports that itself. We currently work around this by subclassing, keeping another copy of the underlying filter reference and implementing a trivial getter, which is an easy workaround, but the trap is that a junior developer could unknowingly create a CachingWrapperFilter without knowing that the BetterCachingWrapperFilter exists, introducing a filter which cannot be introspected.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12599572/LUCENE-5186.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "New features", "change_id": "LUCENE-5197", "change_description": ": Added SegmentReader.ramBytesUsed to return approximate heap RAM\nused by index datastructures.", "change_title": "Add a method to SegmentReader to get the current index heap memory size", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "It would be useful to at least estimate the index heap size being used by Lucene. Ideally a method exposing this information at the SegmentReader level.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601847/LUCENE-5197.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5116", "change_description": ": IndexWriter.addIndexes(IndexReader...) should drop empty (or all\ndeleted) segments.", "change_title": "IW.addIndexes doesn't prune all deleted segments", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "at the least, this can easily create segments with maxDoc == 0. It seems buggy: elsewhere we prune these segments out, so its expected to have a commit point with no segments rather than a segment with 0 documents...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593180/LUCENE-5116.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5132", "change_description": ": Spatial RecursivePrefixTree Contains predicate will throw an NPE\nwhen there's no indexed data and maybe in other circumstances too.", "change_title": "Spatial RPT CONTAINS predicate will fail with NPE when there's no indexed data", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "If there is no indexed spatial data, the SpatialRecursivePrefixTree strategy will fail with an NPE if the Contains predicate is used.  I'm not certain but it might fail even when there is data, when termsEnum reaches the end. (bug found via randomized testing & continuous integration)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594090/LUCENE-5132.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5146", "change_description": ": AnalyzingSuggester sort comparator read part of the input key as the\nweight that caused the sorter to never sort by weight first since the weight is only\nconsidered if the input is equal causing the malformed weight to be identical as well.", "change_title": "AnalyzingSuggester sort order doesn't respect the actual weight", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "Uwe would say: \"sorry but your code is wrong\". We don't actually read the weight value in AnalyzingComparator which can cause really odd suggestions since we read parts of the input as the weight. Non of our tests catches that so I will go ahead and add some tests for it as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594927/LUCENE-5146.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5151", "change_description": ": Associations FacetsAggregators could enter an infinite loop when\nsome result documents were missing category associations.", "change_title": "Associations aggregators enter an infinite loop if some documents have no category associations", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Stupid error, they do this: Since they don't advance 'doc', the hang on that if forever. I'll post a fix soon.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594949/LUCENE-5151.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5152", "change_description": ": Fix MemoryPostingsFormat to not modify borrowed BytesRef from FSTEnum\nseek/lookup which can cause sideeffects if done on a cached FST root arc.", "change_title": "Lucene FST is not immutable", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "a spinnoff from LUCENE-5120 where the analyzing suggester modified a returned output from and FST (BytesRef) which caused sideffects in later execution. I added an assertion into the FST that checks if a cached root arc is modified and in-fact this happens for instance in our MemoryPostingsFormat and I bet we find more places. We need to think about how to make this less trappy since it can cause bugs that are super hard to find.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597308/LUCENE-5152.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5160", "change_description": ": Handle the case where reading from a file or FileChannel returns -1,\nwhich could happen in rare cases where something happens to the file between the\ntime we start the read loop (where we check the length) and when we actually do\nthe read.", "change_title": "NIOFSDirectory, SimpleFSDirectory (others?) don't properly handle valid file and FileChannel read conditions", "detail_type": "Bug", "detail_affect_versions": "4.4,6.0", "detail_fix_versions": "4.5,6.0", "detail_description": "Around line 190 of NIOFSDirectory, the loop to read in bytes doesn't properly handle the -1 condition that can be returned from FileChannel.read().  If it returns -1, then it will move the file pointer back and you will enter an infinite loop.  SimpleFSDirectory displays the same characteristics, although I have only seen the issue on NIOFSDirectory. The code in question from NIOFSDirectory:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12596926/LUCENE-5160.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5166", "change_description": ": PostingsHighlighter would throw IOOBE if a term spanned the maxLength\nboundary, made it into the top-N and went to the formatter.", "change_title": "PostingsHighlighter fails with IndexOutOfBoundsException", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "Given a document with a match at a startIndex < PostingsHighlighter.maxlength and an endIndex > PostingsHighlighter.maxLength, DefaultPassageFormatter will throw an IndexOutOfBoundsException when DefaultPassageFormatter.append() is invoked.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598187/LUCENE-5166-revisited.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4583", "change_description": ": Indexing core no longer enforces a limit on maximum\nlength binary doc values fields, but individual codecs (including\nthe default one) have their own limits", "change_title": "StraightBytesDocValuesField fails if bytes > 32k", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1,6.0", "detail_fix_versions": "4.5,6.0", "detail_description": "I didn't observe any limitations on the size of a bytes based DocValues field value in the docs.  It appears that the limit is 32k, although I didn't get any friendly error telling me that was the limit.  32k is kind of small IMO; I suspect this limit is unintended and as such is a bug.    The following test fails:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597704/LUCENE-4583.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-3849", "change_description": ": TokenStreams now set the position increment in end(),\nso we can handle trailing holes.  If you have a custom TokenStream\nimplementing end() then be sure it calls super.end().", "change_title": "position increments should be implemented by TokenStream.end()", "detail_type": "Bug", "detail_affect_versions": "3.6,4.0-ALPHA", "detail_fix_versions": "4.5,6.0", "detail_description": "if you have pages of a book as multivalued fields, with the default position increment gap of analyzer.java (0), phrase queries won't work across pages if one ends with stopword(s). This is because the 'trailing holes' are not taken into account in end(). So I think in TokenStream.end(), subclasses of FilteringTokenFilter (e.g. stopfilter) should do: One problem is that these filters need to 'add' to the posinc, but currently nothing clears the attributes for end() [they are dirty, except offset which is set by the tokenizer]. Also the indexer should be changed to pull posIncAtt from end().", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598788/LUCENE-3849.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5192", "change_description": ": IndexWriter could allow adding same field name with different\nDocValueTypes under some circumstances.", "change_title": "FieldInfos.Builder failed to catch adding field with different DV type under some circumstances", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "I found it while working on LUCENE-5189. I'll attach a patch with a simple testcase which reproduces the problem and a fix.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12600629/LUCENE-5192.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5191", "change_description": ": SimpleHTMLEncoder in Highlighter module broke Unicode\noutside BMP because it encoded UTF-16 chars instead of codepoints.\nThe escaping of codepoints > 127 was removed (not needed for valid HTML)\nand missing escaping for ' and / was added.", "change_title": "SimpleHTMLEncoder in Highlighter module breaks Unicode outside BMP", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "The highlighter provides a function to escape HTML, which does to much. To create valid HTML only \", <, >, & must be escaped, everything else can kept unescaped. The escaper unfortunately does also additionally escape everything > 127, which is unneeded if your web site has the correct encoding. It also produces huge amounts of HTML entities if used with eastern languages. This would not be a bugf if the escaping would be correct, but it isn't, it escapes like that: result.append(\"\\&#\").append((int)ch).append(\";\"); So it escapes not (as HTML needs) the unicode codepoint, instead it escapes the UTF-16 char, which is incorrect, e.g. for our all-time favourite Deseret: U+10400 (deseret capital letter long i) would be escaped as &#55297;&#56320; and not as &#66560;. So we should remove the stupid encoding of chars > 127 which is simply useless See also: https://github.com/elasticsearch/elasticsearch/issues/3587", "patch_link": "https://issues.apache.org/jira/secure/attachment/12600655/LUCENE-5191.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5201", "change_description": ": Fixed compression bug in LZ4.compressHC when the input is highly\ncompressible and the start offset of the array to compress is > 0.", "change_title": "Compression issue on highly compressible inputs with LZ4.compressHC", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "LZ4.compressHC sometimes fails at compressing highly compressible inputs when the start offset is > 0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601421/LUCENE-5201.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5221", "change_description": ": SimilarityBase did not write norms the same way as DefaultSimilarity\nif discountOverlaps == false and index-time boosts are present for the field.", "change_title": "SimilarityBase.computeNorm is inconsistent with TFIDFSimilarity", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "SimilarityBase.computeNorm Javadoc indicates that the doc length should be encoded in the same way as TFIDFSimilarity. However, when discountOverlaps is false, what gets encoded is SmallFloat.floatToByte315((boost / (float) Math.sqrt(docLen / boost))); rather than  SmallFloat.floatToByte315((boost / (float) Math.sqrt(length))); due to the extra / state.getBoost() term in SimilarityBase.computeNorm: final float numTerms;   if (discountOverlaps)     numTerms = state.getLength() - state.getNumOverlap();   else     numTerms = state.getLength() / state.getBoost();   return encodeNormValue(state.getBoost(), numTerms);", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603844/LUCENE-5221.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5223", "change_description": ": Fixed IndexUpgrader command line parsing: -verbose is not required\nand -dir-impl option now works correctly.", "change_title": "IndexUpgrader (4.4.0) fails when -verbose is not set, or when any value of -dir-impl is specified", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "Here it fails because -verbose is not set: $ java -cp ./lucene-core-4.4-SNAPSHOT.jar org.apache.lucene.index.IndexUpgrader ./INDEX Exception in thread \"main\" java.lang.IllegalArgumentException: printStream must not be null \tat org.apache.lucene.index.IndexWriterConfig.setInfoStream(IndexWriterConfig.java:514) \tat org.apache.lucene.index.IndexUpgrader.<init>(IndexUpgrader.java:126) \tat org.apache.lucene.index.IndexUpgrader.main(IndexUpgrader.java:109) Here it works with -verbose set: $ java -cp ./lucene-core-4.4-SNAPSHOT.jar org.apache.lucene.index.IndexUpgrader -verbose ./INDEX IFD 0 [Mon Sep 16 18:25:53 PDT 2013; main]: init: current segments file is \"segments_5\"; deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@42698403 ... IW 0 [Mon Sep 16 18:25:53 PDT 2013; main]: at close: _2(4.4):C4", "patch_link": "https://issues.apache.org/jira/secure/attachment/12603876/LUCENE-5223.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5245", "change_description": ": Fix MultiTermQuery's constant score rewrites to always\nreturn a ConstantScoreQuery to make scoring consistent. Previously it\nreturned an empty unwrapped BooleanQuery, if no terms were available,\nwhich has a different query norm.", "change_title": "ConstantScoreAutoRewrite rewrites prefix queryies that don't match anything before query weight is calculated", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "ConstantScoreAutoRewrite rewrites prefix queryies that don't match anything before query weight is calculated.  This dramatically changes the resulting score which is bad when comparing scores across different Lucene indexes/shards/whatever.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12605118/LUCENE-5245.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5218", "change_description": ": In some cases, trying to retrieve or merge a 0-length\nbinary doc value would hit an ArrayIndexOutOfBoundsException.", "change_title": "background merge hit exception && Caused by: java.lang.ArrayIndexOutOfBoundsException", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "forceMerge(80) ============================== Caused by: java.io.IOException: background merge hit exception: _3h(4.4):c79921/2994 _3vs(4.4):c38658 _eq(4.4):c38586 _h1(4.4):c37370 _16k(4.4):c36591 _j4(4.4):c34316 _dx(4.4):c30550 _3m6(4.4):c30058 _dl(4.4):c28440 _d8(4.4):c19599 _dy(4.4):c1500/75 _h2(4.4):c1500 into _3vt [maxNumSegments=80] \tat org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1714) \tat org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1650) \tat com.xxx.yyy.engine.lucene.LuceneEngine.flushAndReopen(LuceneEngine.java:1295) \t... 4 more Caused by: java.lang.ArrayIndexOutOfBoundsException: 2 \tat org.apache.lucene.util.PagedBytes$Reader.fillSlice(PagedBytes.java:92) \tat org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer$6.get(Lucene42DocValuesProducer.java:267) \tat org.apache.lucene.codecs.DocValuesConsumer$2$1.setNext(DocValuesConsumer.java:239) \tat org.apache.lucene.codecs.DocValuesConsumer$2$1.hasNext(DocValuesConsumer.java:201) \tat org.apache.lucene.codecs.lucene42.Lucene42DocValuesConsumer.addBinaryField(Lucene42DocValuesConsumer.java:218) \tat org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addBinaryField(PerFieldDocValuesFormat.java:110) \tat org.apache.lucene.codecs.DocValuesConsumer.mergeBinaryField(DocValuesConsumer.java:186) \tat org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:171) \tat org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:108) \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3772) \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3376) \tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405) \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482) ===============", "patch_link": "https://issues.apache.org/jira/secure/attachment/12605084/LUCENE-5218.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5094", "change_description": ": Add ramBytesUsed() to MultiDocValues.OrdinalMap.", "change_title": "add a ramBytesUsed to OrdinalMap", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "I think this would be useful. it could e.g. be exposed via SortedSetDocValuesReaderState and so on.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12591089/LUCENE-5094.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5114", "change_description": ": Remove unused boolean useCache parameter from\nTermsEnum.seekCeil and .seekExact", "change_title": "remove boolean useCache param from TermsEnum.seekCeil/Exact", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Long ago terms dict had a cache, but it was problematic and we removed it, but the API still has a relic boolean useCache ... I think we should drop it from the API as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12592564/LUCENE-5114.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5128", "change_description": ": IndexSearcher.searchAfter throws IllegalArgumentException if\nsearchAfter exceeds the number of documents in the reader.", "change_title": "Calling IndexSearcher.searchAfter beyond the number of stored documents causes ArrayIndexOutOfBoundsException", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.5,6.0", "detail_description": "ArrayIndexOutOfBoundsException makes it harder to reason about the cause. Is there a better way to notify programmers of the cause?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593591/LUCENE-5128.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5129", "change_description": ": CategoryAssociationsContainer no longer supports null\nassociation values for categories. If you want to index categories without\nassociations, you should add them using FacetFields.", "change_title": "CategoryAssociationsContainer should not allow null associations", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Currently you can set a null CategoryAssociation to a CategoryPath, but alas, it's just dropped by AssociationFacetFields. The history behind this break is that in the past, category associations were indexed twice, with and without the association value. In the move to 4.2 I guess with all the transitions the code went through, this behavior was nuked (as it doesn't make sense in general to index these categories twice), but AssociationFacetFields has a comment that these categories are added anyway – this is of course wrong. Instead of supporting null category associations, which is not so trivial in AssociationFacetFields, I think that we should just prevent it (throw a hard exception). If a user wants to add a category such as Category/Computer Science=0.74 (i.e. with weight 0.74) and be able to aggregate Category/ with and without associations, he should either write a special FacetsAggregator which ignores the associated value, or add the category twice, once using FacetFields (counting) and second time with AssociationFacetFields. I'll post a patch soon.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593925/LUCENE-5129.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-4876", "change_description": ": IndexWriter no longer clones the given IndexWriterConfig. If you\nneed to use the same config more than once, e.g. when sharing between multiple\nwriters, make sure to clone it before passing to each writer.", "change_title": "IndexWriterConfig.clone should clone the MergeScheduler", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.5,6.0", "detail_description": "ConcurrentMergeScheduler has a List<MergeThread> member to track the running merging threads, so IndexWriterConfig.clone should clone the merge scheduler so that both IndexWriterConfig instances are independant.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594391/LUCENE-4876.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5144", "change_description": ": StandardFacetsAccumulator renamed to OldFacetsAccumulator, and all\nassociated classes were moved under o.a.l.facet.old. The intention to remove it\none day, when the features it covers (complements, partitiona, sampling) will be\nmigrated to the new FacetsAggregator and FacetsAccumulator API. Also,\nFacetRequest.createAggregator was replaced by OldFacetsAccumulator.createAggregator.", "change_title": "Nuke FacetRequest.createAggregator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Aggregator was replaced by FacetsAggregator. FacetRequest has createAggregator() which by default throws an UOE. It was left there until we migrate the aggregators to FacetsAggregator – now all of our requests support FacetsAggregator. Aggregator is used only by StandardFacetsAccumulator, which too needs to vanish, at some point. But it currently it's the only one which handles sampling, complements aggregation and partitions. What I'd like to do is remove FacetRequest.createAggregator and in StandardFacetsAccumulator support only CountFacetRequest and SumScoreFacetRequest, which are the only ones that make sense for sampling and partitions. SumScore does not even support complements (which only work for counting). I'll also rename StandardFA to OldStandardFA. The plan is to eventually implement a SamplingAccumulator, PartitionsAccumulator/Aggregator and ComplementsAggregator, removing that class entirely. Until then ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594706/LUCENE-5144.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5149", "change_description": ": CommonTermsQuery now allows to set the minimum number of terms that\nshould match for its high and low frequent sub-queries. Previously this was only\nsupported on the low frequent terms query.", "change_title": "CommonTermsQuery should allow minNrShouldMatch for high & low freq terms", "detail_type": "Improvement", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "Currently CommonTermsQuery only allows a minShouldMatch for the low frequent query. Yet, we should also allow this for the high frequent part to have better control over scoring. here is an ES issue that is related to this: https://github.com/elasticsearch/elasticsearch/issues/3188", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594704/LUCENE-5149.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5156", "change_description": ": CompressingTermVectors TermsEnum no longer supports ord().", "change_title": "CompressingTermVectors termsEnum should probably not support seek-by-ord", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Just like term vectors before it, it has a O seek-by-term. But this one also advertises a seek-by-ord, only this is also O. This could cause e.g. checkindex to be very slow, because if termsenum supports ord it does a bunch of seeking tests. (Another solution would be to leave it, and add a boolean so checkindex never does seeking tests for term vectors, only real fields). However, I think its also kinda a trap, in my opinion if seek-by-ord is supported anywhere, you kinda expect it to be faster than linear time...?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12596270/LUCENE-5156.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5161", "change_description": ",", "change_title": "review FSDirectory chunking defaults and test the chunking", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Today there is a loop in SimpleFS/NIOFS: I bet if you look at the clover report its untested, because its fixed at 100MB for 32-bit users and 2GB for 64-bit users (are these defaults even good?!). Also if you call the setter on a 64-bit machine to change the size, it just totally ignores it. We should remove that, the setter should always work. And we should set it to small values in tests so this loop is actually executed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597279/LUCENE-5161.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5164", "change_description": ",", "change_title": "Remove the OOM catching in SimpleFSDirectory and NIOFSDirectory", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Followup from LUCENE-5161: In former times we added the OOM cactching in NIOFSDir and SimpleFSDir because nobody understand why the OOM could happen on FileChannel.read() or SimpleFSDir.read(). By reading the Java code its easy to understand (it allocates direct buffers with same size as the requested length to read). As we have chunking now reduce to a few kilobytes it cannot happen anymore that we get spurious OOMs. In fact we might hide a real OOM! So we should remove it. I am also not sure if we should make chunk size configureable in FSDirectory at all! It makes no sense to me (it was in fact only added for people that hit the OOM to fine-tune). In my opinion we should remove the setter in trunk and keep it deprecated in 4.x. The buf size is then in trunk equal to the defaults from LUCENE-5161.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597340/LUCENE-5164-4x.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5170", "change_description": ": Analyzer.ReuseStrategy instances are now stateless and can\nbe reused in other Analyzer instances, which was not possible before.\nLucene ships now with stateless singletons for per field and global reuse.\nLegacy code can still instantiate the deprecated implementation classes,\nbut new code should use the constants. Implementors of custom strategies\nhave to take care of new method signatures. AnalyzerWrapper can now be\nconfigured to use a custom strategy, too, ideally the one from the wrapped\nAnalyzer. Analyzer adds a getter to retrieve the strategy for this use-case.", "change_title": "Add getter for reuse strategy to Analyzer, make AnalyzerWrapper's reuse strategy configureable, fix strategy to be stateless", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "If you write an Analyzer that wraps another one (but without using AnalyzerWrapper) you may need use the same reuse strategy in your wrapper. This is not possible as there is no way to get the reuse startegy (private field and no getter). An example is ES's NamedAnalyzer, see my comment: https://github.com/elasticsearch/elasticsearch/commit/b9a2fbd8741aa1b9beffb7d2922fc9b4525397e4#src/main/java/org/elasticsearch/index/analysis/NamedAnalyzer.java This would add a getter, just a 3-liner.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597956/LUCENE-5170.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5173", "change_description": ": Lucene never writes segments with 0 documents anymore.", "change_title": "Add checkindex piece of LUCENE-5116", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "LUCENE-5116 fixes addIndexes(Reader) to never write a 0-document segment (in the case you merge in empty or all-deleted stuff). I considered it just an inconsistency, but it could cause confusing exceptions to real users too if there was a \"regression\" here. (see solr users list:Split Shard Error - maxValue must be non-negative).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597819/LUCENE-5173.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5178", "change_description": ": SortedDocValues always returns -1 ord when a document is missing\na value for the field. Previously it only did this if the SortedDocValues\nwas produced by uninversion on the FieldCache.", "change_title": "doc values should expose missing values (or allow configurable defaults)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "DocValues should somehow allow a configurable default per-field. Possible implementations include setting it on the field in the document or registration of an IndexWriter callback. If we don't make the default configurable, then another option is to have DocValues fields keep track of whether a value was indexed for that document or not.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598849/LUCENE-5178_reintegrate.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "API Changes", "change_id": "LUCENE-5183", "change_description": ": remove BinaryDocValues.MISSING. In order to determine a document\nis missing a field, use getDocsWithField instead.", "change_title": "BinaryDocValues inconsistencies", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Some current inconsistencies:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12600370/LUCENE-5183.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5178", "change_description": ": DocValues codec consumer APIs (iterables) return null values\nwhen the document has no value for the field.", "change_title": "doc values should expose missing values (or allow configurable defaults)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "DocValues should somehow allow a configurable default per-field. Possible implementations include setting it on the field in the document or registration of an IndexWriter callback. If we don't make the default configurable, then another option is to have DocValues fields keep track of whether a value was indexed for that document or not.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598849/LUCENE-5178_reintegrate.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5200", "change_description": ": The HighFreqTerms command-line tool returns the true top-N\nby totalTermFreq when using the -t option, it uses the term statistics (faster)\nand now always shows totalTermFreq in the output.", "change_title": "HighFreqTerms has confusing behavior with -t option", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Problem #1: Its tricky what happens with -t: if you ask for the top-100 terms, it requests the top-100 terms (by docFreq), then resorts the top-N by totalTermFreq. So its not really the top 100 most frequently occurring terms. Problem #2:  Using the -t option can be confusing and slow: the reported docFreq includes deletions, but totalTermFreq does not (it actually walks postings lists if there is even one deletion). I think this is a relic from 3.x days when lucene did not support this statistic. I think we should just always output both TermsEnum.docFreq() and TermsEnum.totalTermFreq(), and -t just determines the comparator of the PQ.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601418/LUCENE-5200.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5088", "change_description": ": Added TermFilter to filter docs by a specific term.", "change_title": "Add term filter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "I think it makes sense add a term filter:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12592568/LUCENE-5088.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5119", "change_description": ": DiskDV keeps the document-to-ordinal mapping on disk for\nSortedDocValues.", "change_title": "DiskDV SortedDocValues shouldnt hold doc-to-ord in heap memory", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "These are accessed sequentially when e.g. faceting, and can be a fairly large amount of data (based on # of docs and # of unique terms). I think this was done so that conceptually \"random\" access to a specific docid would be faster than eg. stored fields, but I think we should instead target the DV datastructures towards real use cases (faceting,sorting,grouping,...)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593005/LUCENE-5119.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5145", "change_description": ": New AppendingPackedLongBuffer, a new variant of the former\nAppendingLongBuffer which assumes values are 0-based.", "change_title": "Added AppendingPackedLongBuffer & extended AbstractAppendingLongBuffer family (customizable compression ratio + bulk retrieval)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Made acceptableOverheadRatio configurable  Added bulk get to AbstractAppendingLongBuffer classes, for faster retrieval. Introduced a new variant, AppendingPackedLongBuffer which solely relies on PackedInts as a back-end. This new class is useful where people have non-negative numbers with a fairly uniform distribution over a fixed (limited) range. Ex. facets ordinals. To distinguish it from AppendingPackedLongBuffer, delta based AppendingLongBuffer was renamed to AppendingDeltaPackedLongBuffer Fixed an Issue with NullReader where it didn't respect it's valueCount in bulk gets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594906/LUCENE-5145.v2.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5145", "change_description": ": All Appending*Buffer now support bulk get.", "change_title": "Added AppendingPackedLongBuffer & extended AbstractAppendingLongBuffer family (customizable compression ratio + bulk retrieval)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Made acceptableOverheadRatio configurable  Added bulk get to AbstractAppendingLongBuffer classes, for faster retrieval. Introduced a new variant, AppendingPackedLongBuffer which solely relies on PackedInts as a back-end. This new class is useful where people have non-negative numbers with a fairly uniform distribution over a fixed (limited) range. Ex. facets ordinals. To distinguish it from AppendingPackedLongBuffer, delta based AppendingLongBuffer was renamed to AppendingDeltaPackedLongBuffer Fixed an Issue with NullReader where it didn't respect it's valueCount in bulk gets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594906/LUCENE-5145.v2.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5140", "change_description": ": Fixed a performance regression of span queries caused by", "change_title": "Slowdown of the span queries caused by LUCENE-4946", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "romseygeek noticed that span queries have been slower since LUCENE-4946 got committed. http://people.apache.org/~mikemccand/lucenebench/SpanNear.html", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594412/LUCENE-5140.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-4946", "change_description": ": Fixed a performance regression of span queries caused by", "change_title": "Refactor SorterTemplate", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.4", "detail_description": "When working on TimSort (LUCENE-4839), I was a little frustrated of not being able to add galloping support because it would have required to add new primitive operations in addition to compare and swap. I started working on a prototype that uses inheritance to allow some sorting algorithms to rely on additional primitive operations. You can have a look at https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts (but beware it is a prototype and still misses proper documentation and good tests). I think it would offer several advantages: If you are interested in comparing these implementations with Arrays.sort, there is a Benchmark class in src/examples. What do you think?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12581687/LUCENE-4946.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5150", "change_description": ": Make WAH8DocIdSet able to inverse its encoding in order to\ncompress dense sets efficiently as well.", "change_title": "WAH8DocIdSet: dense sets compression", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "In LUCENE-5101, Paul Elschot mentioned that it would be interesting to be able to encode the inverse set to also compress very dense sets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594757/LUCENE-5150.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5159", "change_description": ": Prefix-code the sorted/sortedset value dictionaries in DiskDV.", "change_title": "compressed diskdv sorted/sortedset termdictionaries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Sorted/SortedSet give you ordinal(s) per document, but them separately have a \"term dictionary\" of all the values. You can do a few operations on these: The current implementation for diskdv was the simplest thing that can possibly work: under the hood it just makes a binary DV for these (treating ordinals as document ids). When the terms are fixed length, you can address a term directly with multiplication. When they are variable length though, we have to store a packed ints structure in RAM. This variable length case is overkill and chews up a lot of RAM if you have many unique values. It also chews up a lot of disk since all the values are just concatenated (no sharing).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597207/LUCENE-5159.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5170", "change_description": ": Fixed several wrapper analyzers to inherit the reuse strategy\nof the wrapped Analyzer.", "change_title": "Add getter for reuse strategy to Analyzer, make AnalyzerWrapper's reuse strategy configureable, fix strategy to be stateless", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "If you write an Analyzer that wraps another one (but without using AnalyzerWrapper) you may need use the same reuse strategy in your wrapper. This is not possible as there is no way to get the reuse startegy (private field and no getter). An example is ES's NamedAnalyzer, see my comment: https://github.com/elasticsearch/elasticsearch/commit/b9a2fbd8741aa1b9beffb7d2922fc9b4525397e4#src/main/java/org/elasticsearch/index/analysis/NamedAnalyzer.java This would add a getter, just a 3-liner.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597956/LUCENE-5170.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5006", "change_description": ": Simplified DocumentsWriter and DocumentsWriterPerThread\nsynchronization and concurrent interaction with IndexWriter. DWPT is now\nonly setup once and has no reset logic. All segment publishing and state\ntransition from DWPT into IndexWriter is now done via an Event-Queue\nprocessed from within the IndexWriter in order to prevent suituations\nwhere DWPT or DW calling int IW causing deadlocks.", "change_title": "Simplify / understand IndexWriter/DocumentsWriter synchronization", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The concurrency in IW/DW/BD is terrifying: there are many locks involved, not just intrinsic locks but IW also has fullFlushLock, commitLock, and there are no clear rules about lock order to avoid deadlocks like LUCENE-5002. We have to somehow simplify this, and define the allowed concurrent behavior eg when an app calls deleteAll while other threads are indexing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598004/LUCENE-5006.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5182", "change_description": ": Terminate phrase searches early if max phrase window is\nexceeded in FastVectorHighlighter to prevent very long running phrase\nextraction if phrase terms are high frequent.", "change_title": "FVH can end in very very long running recursion on phrase highlight", "detail_type": "Bug", "detail_affect_versions": "4.4,6.0", "detail_fix_versions": "4.5,6.0", "detail_description": "due to the nature of FVH extract logic a simple phrase query can put a FHV into a super long running recursion. I had documents taking literally days to return form the extract phrases logic. I have a test that reproduces the problem and a possible fix. The reason for this is that the FVH never tries to early terminate if a phrase is already way beyond the slop coming from the phrase query. If there is a document with lot of occurrences or two or more terms in the phrase this literally tries to match all possible combinations of the terms in the doc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598954/LUCENE-5182.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5188", "change_description": ": CompressingStoredFieldsFormat now slices chunks containing big\ndocuments into fixed-size blocks so that requesting a single field does not\nnecessarily force to decompress the whole chunk.", "change_title": "Make CompressingStoredFieldsFormat more friendly to StoredFieldVisitors", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "The way CompressingStoredFieldsFormat works is that it first decompresses data and then consults the StoredFieldVisitor. This is a bit wasteful in case documents are big and only the first field of a document is of interest so maybe we could decompress and consult the StoredFieldVicitor in a more streaming fashion.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601240/LUCENE-5188.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Optimizations", "change_id": "LUCENE-5101", "change_description": ": CachingWrapper makes it easier to plug-in a custom cacheable\nDocIdSet implementation and uses WAH8DocIdSet by default, which should be\nmore memory efficient than FixedBitSet on average as well as faster on small\nsets.", "change_title": "make it easier to plugin different bitset implementations to CachingWrapperFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Currently this is possible, but its not so friendly: Is there any value to having all this other logic in the protected API? It seems like something thats not useful for a subclass... Maybe this stuff can become final, and \"INTERESTING PART\" calls a simpler method, something like:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601673/LUCENE-5101.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Documentation", "change_id": "LUCENE-4894", "change_description": ": remove facet userguide as it was outdated. Partially absorbed into\npackage's documentation and classes javadocs.", "change_title": "Facet User Guide for lucene 4.2 has deleted classes/methods and needs more explanations.", "detail_type": "Bug", "detail_affect_versions": "4.2", "detail_fix_versions": "4.5,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12593244/LUCENE-4894.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Documentation", "change_id": "LUCENE-5206", "change_description": ": Clarify FuzzyQuery's unexpected behavior on short\nterms.", "change_title": "FuzzyQuery: matching terms must be longer than maxEdits", "detail_type": "Bug", "detail_affect_versions": "4.5", "detail_fix_versions": "4.5,6.0", "detail_description": "FuzzyQuery's maxEdit value must be larger than the length of both terms for there to be a match.  Based on a response from the java-user list, it looks like I wasn't the only one surprised by this.  Let's document this design choice more clearly in the documentation or modify the behavior. Apologies if I missed the documentation of this.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5141", "change_description": ": CheckIndex.fixIndex(Status,Codec) is now\nCheckIndex.fixIndex(Status). If you used to pass a codec to this method, just\nremove it from the arguments.", "change_title": "CheckIndex.fixIndex doesn't need a Codec", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "CheckIndex.fixIndex takes a codec as an argument although it doesn't need one.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12594418/LUCENE-5141.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5089", "change_description": ",", "change_title": "Update morfologik (polish stemmer) to 1.6.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in backwards compatibility policy", "change_id": "SOLR-5126", "change_description": ",", "change_title": "Update Carrot2 clustering to version 3.8.0, update Morfologik to version 1.7.1", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5170", "change_description": ": Changed method signatures of Analyzer.ReuseStrategy to take\nAnalyzer. Closeable interface was removed because the class was changed to\nbe stateless.", "change_title": "Add getter for reuse strategy to Analyzer, make AnalyzerWrapper's reuse strategy configureable, fix strategy to be stateless", "detail_type": "Bug", "detail_affect_versions": "4.4", "detail_fix_versions": "4.5,6.0", "detail_description": "If you write an Analyzer that wraps another one (but without using AnalyzerWrapper) you may need use the same reuse strategy in your wrapper. This is not possible as there is no way to get the reuse startegy (private field and no getter). An example is ES's NamedAnalyzer, see my comment: https://github.com/elasticsearch/elasticsearch/commit/b9a2fbd8741aa1b9beffb7d2922fc9b4525397e4#src/main/java/org/elasticsearch/index/analysis/NamedAnalyzer.java This would add a getter, just a 3-liner.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12597956/LUCENE-5170.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5187", "change_description": ": SlowCompositeReaderWrapper constructor is now private,\nSlowCompositeReaderWrapper.wrap should be used instead.", "change_title": "Make SlowCompositeReaderWrapper constructor private", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "I found a couple of places in the code base that duplicate the logic of SlowCompositeReaderWrapper.wrap. I think SlowCompositeReaderWrapper.wrap (vs. new SlowCompositeReaderWrapper) is what users need so we should probably make SlowCompositeReaderWrapper constructor private to enforce usage of wrap.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12599601/LUCENE-5187.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-5101", "change_description": ": CachingWrapperFilter doesn't always return FixedBitSet instances\nanymore. Users of the join module can use\noal.search.join.FixedBitSetCachingWrapperFilter instead.", "change_title": "make it easier to plugin different bitset implementations to CachingWrapperFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "Currently this is possible, but its not so friendly: Is there any value to having all this other logic in the protected API? It seems like something thats not useful for a subclass... Maybe this stuff can become final, and \"INTERESTING PART\" calls a simpler method, something like:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12601673/LUCENE-5101.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Build", "change_id": "SOLR-5159", "change_description": ": Manifest includes non-parsed maven variables.", "change_title": "Manifest includes non-parsed maven variables", "detail_type": "Bug", "detail_affect_versions": "4.4,4.5,6.0", "detail_fix_versions": "4.5,6.0", "detail_description": "When building Lucene/Solr with Apache Maven 3, all MANIFEST.MF files included into JAR artifacts contain non-parsed POM variables: namely, there are entries like Specification-Version: 5.0.0.${now.version} In the end, Solr displays these values on admin page in \"Versions\" section. This is caused by unresolved bug in maven-bundle-plugin (FELIX-3392).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12598227/SOLR-5159.patch", "patch_content": "none"}
{"library_version": "4.5.0", "change_type": "Build", "change_id": "LUCENE-5193", "change_description": ": Add jar-src as top-level target to generate all Lucene and Solr\n*-src.jar.", "change_title": "Add jar-src to build.xml", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.5,6.0", "detail_description": "I think it's useful if we have a top-level jar-src which generates source jars for all modules. One can basically do that by iterating through the directories and calling 'ant jar-src' already, so this is just a convenient way to do it. Will attach a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12600658/LUCENE-5193.patch", "patch_content": "none"}

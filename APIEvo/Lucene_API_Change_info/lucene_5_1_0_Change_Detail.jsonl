{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6066", "change_description": ": Added DiversifiedTopDocsCollector to misc for collecting no more\nthan a given number of results under a choice of key. Introduces new remove\nmethod to core's PriorityQueue.", "change_title": "Collector that manages diversity in search results", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "This issue provides a new collector for situations where a client doesn't want more than N matches for any given key (e.g. no more than 5 products from any one retailer in a marketplace). In these circumstances a document that was previously thought of as competitive during collection has to be removed from the final PQ and replaced with another doc (eg a retailer who already has 5 matches in the PQ receives a 6th match which is better than his previous ones). This requires a new remove method on the existing PriorityQueue class.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697507/LUCENE-6066.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6191", "change_description": ": New spatial 2D heatmap faceting for PrefixTreeStrategy.", "change_title": "Spatial 2D faceting (heatmaps)", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "Lucene spatial's PrefixTree (grid) based strategies index data in a way highly amenable to faceting on grids cells to compute a so-called heatmap. The underlying code in this patch uses the PrefixTreeFacetCounter utility class which was recently refactored out of faceting for NumberRangePrefixTree LUCENE-5735.  At a low level, the terms (== grid cells) are navigated per-segment, forward only with TermsEnum.seek, so it's pretty quick and furthermore requires no extra caches & no docvalues.  Ideally you should use QuadPrefixTree (or Flex once it comes out) to maximize the number grid levels which in turn maximizes the fidelity of choices when you ask for a grid covering a region.  Conveniently, the provided capability returns the data in a 2-D grid of counts, so the caller needn't know a thing about how the data is encoded in the prefix tree.  Well almost... at this point they need to provide a grid level, but I'll soon provide a means of deriving the grid level based on a min/max cell count. I recommend QuadPrefixTree with geo=false so that you can provide a square world-bounds (360x360 degrees), which means square grid cells which are more desirable to display than rectangular cells.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697219/LUCENE-6191__Spatial_heatmap.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6227", "change_description": ": Added BooleanClause.Occur.FILTER to filter documents without\nparticipating in scoring (on the contrary to MUST).", "change_title": "Add BooleanClause.Occur.FILTER", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Now that we have weight-level control of whether scoring is needed or not, we could add a new clause type to BooleanQuery. It would behave like MUST exept that it would not participate in scoring. Why do we need it given that we already have FilteredQuery? The idea is that by having a single query that performs conjunctions, we could potentially take better decisions. It's not ready to replace FilteredQuery yet as FilteredQuery has handling of random-access filters that BooleanQuery doesn't, but it's a first step towards that direction and eventually FilteredQuery would just rewrite to a BooleanQuery. I've been calling this new clause type FILTER so far, but feel free to propose a better name.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697811/LUCENE-6227.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6294", "change_description": ": Added oal.search.CollectorManager to allow for parallelization\nof the document collection process on IndexSearcher.", "change_title": "Generalize how IndexSearcher parallelizes collection execution", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "IndexSearcher takes an ExecutorService that can be used to parallelize collection execution. This is useful if you want to trade throughput for latency. However, this executor service will only be used if you search for top docs. In that case, we will create one collector per slide and call TopDocs.merge in the end. If you use search(Query, Collector), the executor service will never be used. But there are other collectors that could work the same way as top docs collectors, eg. TotalHitCountCollector. And maybe also some of our users' collectors. So maybe IndexSearcher could expose a generic way to take advantage of the executor service?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700749/LUCENE-6294.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6303", "change_description": ": Added filter caching baked into IndexSearcher, disabled by\ndefault.", "change_title": "CachingWrapperFilter -> CachingWrapperQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "As part of the filter -> query migration, we should migrate the caching wrappers (including the filter cache). I think the behaviour should be to delegate to the wrapped query when scores are needed and cache otherwise like CachingWrapperFilter does today. Also the cache should ignore query boosts so that field:value^2 and field:value^3 are considered equal if scores are not needed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12707935/LUCENE-6303-disable_auto-caching.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6304", "change_description": ": Added a new MatchNoDocsQuery that matches no documents.", "change_title": "Add MatchNoDocsQuery that matches no documents", "detail_type": "Improvement", "detail_affect_versions": "5.0", "detail_fix_versions": "5.1,6.0", "detail_description": "As a followup to LUCENE-6298, it would be nice to have an explicit MatchNoDocsQuery to indicate that no documents should be matched. This would hopefully be a better indicator than a BooleanQuery with no clauses or (even worse) null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12702239/LUCENE-6304.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6341", "change_description": ": Add a -fast option to CheckIndex.", "change_title": "add CheckIndex -fast option", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "CheckIndex is great for testing and when tracking down lucene bugs. But in cases where users just want to verify their index files are OK, it is very slow and expensive. I think we should add a -fast option, that only opens the reader and calls checkIntegrity(). This means all files are the correct files (identifiers match) and have the correct CRC32 checksums. For our 10M doc wikipedia index, this is the difference between a 2 second check and a 2 minute check.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12702973/LUCENE-6341.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6355", "change_description": ": IndexWriter's infoStream now also logs time to write FieldInfos\nduring merge", "change_title": "Add verbose IndexWriter logging for writing field infos", "detail_type": "Improvement", "detail_affect_versions": "5.0", "detail_fix_versions": "5.1,6.0", "detail_description": "SegmentMerger should also write the amount of time it takes to write the field infos during a merge. This makes it much easier to determine the contributing times for the total merge time.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12703770/LUCENE-6355.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "New Features", "change_id": "LUCENE-6339", "change_description": ": Added Near-real time Document Suggester via custom postings format", "change_title": "[suggest] Near real time Document Suggester", "detail_type": "New Feature", "detail_affect_versions": "5.0", "detail_fix_versions": "5.1,6.0", "detail_description": "The idea is to index documents with one or more SuggestField(s) and be able to suggest documents with a SuggestField value that matches a given key. A SuggestField can be assigned a numeric weight to be used to score the suggestion at query time. Document suggestion can be done on an indexed SuggestField. The document suggester can filter out deleted documents in near real-time. The suggester can filter out documents based on a Filter (note: may change to a non-scoring query?) at query time. A custom postings format (CompletionPostingsFormat) is used to index SuggestField(s) and perform document suggestions. Index analyzer set through IndexWriterConfig Query analyzer set through SuggestIndexSearcher. Hits are collected in descending order of the suggestion's weight CompletionAnalyzer can be used instead to wrap another analyzer to tune suggest field only parameters.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12707866/LUCENE-6339.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6368", "change_description": ": FST.save can truncate output (BufferedOutputStream may be closed\nafter the underlying stream).", "change_title": "FST.save can truncate output (BufferedOutputStream may be closed after the underlying stream)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,5.3,6.0", "detail_description": "Are used in save (Path) method of FST class, BufferedOutputStream has not been closed. When create a dictionary, there is a possibility that the buffered data is not written out.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12706387/LUCENE-6368.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6249", "change_description": ": StandardQueryParser doesn't support pure negative clauses.", "change_title": "StandardQueryParser doesn't support pure negative clauses", "detail_type": "Bug", "detail_affect_versions": "4.10.3,5.0", "detail_fix_versions": "4.10.5,5.1,6.0", "detail_description": "At first I thought it's by design but Uwe says it's a bug. SQP emits this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699136/LUCENE-6249.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6190", "change_description": ": Spatial pointsOnly flag on PrefixTreeStrategy shouldn't switch all predicates to\nIntersects.", "change_title": "spatial pointsOnly flag shouldn't force predicate to Intersects", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "In the process of testing the pointsOnly flag, I realized RPT's optimization to force the predicate to Intersects from Within|Contains isn't sound.  In the case of Within, this is only valid if there is one point per document but not multiple (since all points on a doc need to intersect the query shape), and for Contains it was simply wrong. Note that the strategy has no multi-valued hint or some-such.  If it did, then if !multiValued && pointsOnly, then Within could be changed to Intersects.  Regardless, swapping the predicate can be done at a higher level (Solr/ES).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693145/LUCENE-6190__pointsOnly_RPT_predicate_bug.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6242", "change_description": ": Ram usage estimation was incorrect for SparseFixedBitSet when\nobject alignment was different from 8.", "change_title": "SparseFixedBitDocIdSet.ramBytesUsed() reports wrong size if alignment of JVM is not 8", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "There seems to be a bug in SparseFixedBitDocIdSet's ramBytesUsed. To me this is a bit crazy implemented, so I have not yet found the issue. To me it looks like some of the summing up breaks if the alignment of the JVM is not 8: To reproduce this failure, run inside core (with 64 bits JVM): The default works: I think we should randomly also specify the ObjectAlignmentInBytes for test runs on Policeman Jenkins. Any Power of 2 is fine.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698428/LUCENE-6242.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6293", "change_description": ": Fixed TimSorter bug.", "change_title": "TimSort bug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Robert pointed me to http://envisage-project.eu/proving-android-java-and-python-sorting-algorithm-is-broken-and-how-to-fix-it/ yesterday which explains how most implementations of TimSort are broken. We should check our TimSorter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700744/LUCENE-6293.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6001", "change_description": ": DrillSideways hits NullPointerException for certain\nBooleanQuery searches.", "change_title": "DrillSideways throws NullPointerException for some searches", "detail_type": "Bug", "detail_affect_versions": "4.10.1", "detail_fix_versions": "4.10.4,5.1,6.0", "detail_description": "For some DrillSideways searches I get NullPointerException. I have tracked the problem to DrillSidewaysScorer class, on line 126 in DrillSidewaysScorer.java: long baseQueryCost = baseScorer.cost(); On some of my index segments, this call throws NullPoinerException.  \"baseScorer\" is instance of ReqExclScorer.  In ReqExclScorer.java: public long cost() throws NullPointerException because reqScorer is null.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701148/LUCENE-6001.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6311", "change_description": ": Fix NIOFSDirectory and SimpleFSDirectory so that the\ntoString method of IndexInputs confess when they are from a compound\nfile.", "change_title": "IndexInput.toString should always confess when it's inside a compound file", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "MMapDir and RAMDir seem do this properly but at least SimpleFSDir and NIOFSDir do not state when a given name was opened from within a compound file.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701609/LUCENE-6311.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6381", "change_description": ": Add defensive wait time limit in\nDocumentsWriterStallControl to prevent hangs during indexing if we\nmiss a .notify/All somewhere", "change_title": "DocumentsWriterStallControl's .wait() should have a time limit", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "This build was hung: http://build-us-00.elastic.co/job/es_core_15_centos/230/testReport/junit/org.elasticsearch.index.engine/InternalEngineTests/testDeletesAloneCanTriggerRefresh/ Only one thread was stalled in DocumentsWriterStallControl, which means we have a bug somewhere, because that thread should have un-stalled once the other (too many) threads finished flushing their segments. I think we should make a simple defensive change here: instead of wait(), which waits forever for a .notify/All() to wake it up, we should wait for up to a time limit.  This way when any concurrency bug like this strikes, we won't hang forever. I cannot reproduce that particular hang... what's unique about that test is it uses a positively minuscule (1 KB) IW buffer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708376/LUCENE-6381.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6386", "change_description": ": Correct IndexWriter.forceMerge documentation to state\nthat up to 3X (X = current index size) spare disk space may be needed\nto complete forceMerge(1).", "change_title": "TestIndexWriterForceMerge still unreliable in NIGHTLY", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Discovered by ryan beasting (trunk): ant test -Dtestcase=TestIndexWriterForceMerge -Dtests.method=testForceMergeTempSpaceUsage -Dtests.seed=DC9ADB74850A581B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=sr__#Latn -Dtests.timezone=Indian/Chagos -Dtests.asserts=true -Dtests.file.encoding=US-ASCII", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708847/LUCENE-6386.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6395", "change_description": ": Seeking by term ordinal was failing to set the term's\nbytes in MemoryIndex", "change_title": "Seeking by term ord is broken in MemoryIndex", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "MemoryIndex fails to set the term's bytes when you seek by ord. I hit this in LUCENE-5860 ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12723323/LUCENE-6395.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6183", "change_description": ",", "change_title": "Avoid re-compression on stored fields merge", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "We removed this optimization before, it didnt really work right because it required things to be \"aligned\". But I think we can do it simpler and safer. This recompression is a big cpu hog in merge, and limits our options compression-wise (especially ones like LZ4-HC that are only slower at write-time).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12692565/LUCENE-6183.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-5647", "change_description": ",", "change_title": "fix current term vectors bulk merge", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "See LUCENE-5646 for the motivation. Long term it might be nice to add algorithm #2 to term vectors if its possible and not too complex. But for now, I think we should avoid such rare optimizations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12692909/LUCENE-5647.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6184", "change_description": ": Make BooleanScorer only score windows that contain\nmatches.", "change_title": "BooleanScorer should better deal with sparse clauses", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The way that BooleanScorer works looks like this: This is not efficient for very sparse clauses (doc freq much lower than maxDoc/2048) since we keep on scoring windows of documents that do not match anything. BooleanScorer2 currently performs better in those cases.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693065/LUCENE-6184.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6161", "change_description": ": Speed up resolving of deleted terms to docIDs by doing\na combined merge sort between deleted terms and segment terms\ninstead of a separate merge sort for each segment.  In delete-heavy\nuse cases this can be a sizable speedup.", "change_title": "Applying deletes is sometimes dog slow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "I hit this while testing various use cases for LUCENE-6119 (adding auto-throttle to ConcurrentMergeScheduler). When I tested \"always call updateDocument\" (each add buffers a delete term), with many indexing threads, opening an NRT reader once per second (forcing all deleted terms to be applied), I see that BufferedUpdatesStream.applyDeletes sometimes seems to take a loooong time, e.g.: What this means is even though I want an NRT reader every second, often I don't get one for up to ~7 or more seconds. This is on an SSD, machine has 48 GB RAM, heap size is only 2 GB.  12 indexing threads. As hideously complex as this code is, I think there are some inefficiencies, but fixing them could be hard / make code even hairier ... Also, this code is mega-locked: holds IW's lock, holds BD's lock.  It blocks things like merges kicking off or finishing... E.g., we pull the MergedIterator many times on the same set of sub-iterators.  Maybe we can create the sorted terms up front and reuse that? Maybe we should go \"term stride\" (one term visits all N segments) not \"segment stride\" (visit each segment, iterating all deleted terms for it).  Just iterating the terms to be deleted takes a sizable part of the time, and we now do that once for every segment in the index. Also, the \"isUnique\" bit in LUCENE-6005 should help here, since if we know the field is unique, we can stop seekExact once we found a segment that has the deleted term, we can maybe pass false for removeDuplicates to MergedIterator...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693596/LUCENE-6161.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6201", "change_description": ": BooleanScorer can now deal with values of minShouldMatch that\nare greater than one and is used when queries produce dense result sets.", "change_title": "MinShouldMatchSumScorer should advance less and score lazily", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "MinShouldMatchSumScorer currently computes the score eagerly, even on documents that do not eventually match if it cannot find minShouldMatch matches on the same document.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695327/LUCENE-6201.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6218", "change_description": ": Don't decode frequencies or match all positions when scoring\nis not needed.", "change_title": "don't decode freqs or enumerate all positions, when scores are not needed", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Today if you don't call score() some things are faster, we won't invoke similarity or read the norm for the document or other things. On the other hand, its sad in this case that we are decompressing twice as many packed integers as we need (freqs can be skipped over, and our postings lists supports that) and walking all positions in phrase matching to determine the number of times the phrase matched (1 is enough, then we can stop). When scoring is not needed, things can be optimized in other cases too (e.g. thats the whole concept of filters).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12696605/LUCENE-6218.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6233", "change_description": "Speed up CheckIndex when the index has term vectors", "change_title": "CheckIndex is dog slow when checking term vectors", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "I'm working on a test that creates a biggish index and I noticed the CheckIndex takes a surprisingly long time to check term vectors. I profiled it and uncovered that we spend a lot of time (not sure this explains all of it) in Terms.getMin/getMax.  Since CompressingTermVectorsReader doesn't impl these methods efficiently (which is fine), we fallback to super's impl, which does a digit-by-digit binary search using seekCeil. But for TVs this sometimes results in a linear scan. I think CheckIndex should not check Terms.getMin/Max for TVs?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697881/LUCENE-6223.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6198", "change_description": ": Added the TwoPhaseIterator API, exposed on scorers which\nis for now only used on phrase queries and conjunctions in order to check\npositions lazily if the phrase query is in a conjunction with other queries.", "change_title": "two phase intersection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Currently some scorers have to do a lot of per-document work to determine if a document is a match. The simplest example is a phrase scorer, but there are others (spans, sloppy phrase, geospatial, etc). Imagine a conjunction with two MUST clauses, one that is a term that matches all odd documents, another that is a phrase matching all even documents. Today this conjunction will be very expensive, because the zig-zag intersection is reading a ton of useless positions. The same problem happens with filteredQuery and anything else that acts like a conjunction.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12707197/LUCENE-6198_move_approximation_to_constructor.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6244", "change_description": ",", "change_title": "Approximations on disjunctions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Like we just did on exact phrases and conjunctions, we should also support approximations on disjunctions in order to apply \"matches()\" lazily.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699064/LUCENE-6244.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6251", "change_description": ",", "change_title": "Two-phase support on ConstantScorer, ReqOptScorer, ReqExclScorer, BoostedScorer", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Some of our scorers are mostly delegators and two-phase support should be reasonably easy to implement.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699165/LUCENE-6251.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6198", "change_description": ",", "change_title": "two phase intersection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Currently some scorers have to do a lot of per-document work to determine if a document is a match. The simplest example is a phrase scorer, but there are others (spans, sloppy phrase, geospatial, etc). Imagine a conjunction with two MUST clauses, one that is a term that matches all odd documents, another that is a phrase matching all even documents. Today this conjunction will be very expensive, because the zig-zag intersection is reading a ton of useless positions. The same problem happens with filteredQuery and anything else that acts like a conjunction.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12707197/LUCENE-6198_move_approximation_to_constructor.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6241", "change_description": ": FSDirectory.listAll() doesnt filter out subdirectories anymore,\nfor faster performance. Subdirectories don't matter to Lucene. If you need to\nfilter out non-index files with some custom usage, you may want to look at\nthe IndexFileNames class.", "change_title": "don't filter subdirectories in listAll()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The issue is, today this means listAll() is always slow, sometimes MUCH slower, because it must do the fstat()-equivalent of each file to check if its a directory to filter it out. When i benchmarked this on a fast filesystem, doing all these filesystem metadata calls only makes listAll() 2.6x slower, but on a non-ssd, slower i/o, it can be more than 60x slower. Lucene doesn't make subdirectories, so hiding these for abuse cases just makes real use cases slower. To add insult to injury, most code (e.g. all of lucene except for where RAMDir copies from an FSDir) does not actually care if extraneous files are directories or not. Finally it sucks the name is listAll() when it is doing anything but that. I really hate to add a method here to deal with this abusive stuff, but I'd rather add isDirectory(String) for the rare code that wants to filter out, than just let stuff always be slow.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698706/LUCENE-6241-alternative.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6262", "change_description": ": ConstantScoreQuery does not wrap the inner weight anymore when\nscores are not required.", "change_title": "No need to wrap with ConstantWeight when needsScores is false", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Today ConstantScoreQuery always wraps the inner weight into a ConstantWeight, but this is only necessary if scores are needed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699671/LUCENE-6262.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6263", "change_description": ": MultiCollector automatically caches scores when several\ncollectors need them.", "change_title": "Automatically wrap with ScoreCachingWrapperScorer when several collectors need scores", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Now that we know which collectors need scores, we could take advantage of this information in order to automatically wrap with ScoreCachingWrapperScorer when it would help.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699683/LUCENE-6263.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6275", "change_description": ": SloppyPhraseScorer now uses the same logic as ConjunctionScorer\nin order to advance doc IDs, which takes advantage of the cost() API.", "change_title": "SloppyPhraseScorer should use ConjunctionDISI", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Currently, this guy has his own little built-in algorithm, which doesn't seem optimal to me. It might be better if it reused ConjunctionDISI like ExactPhraseScorer does.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700216/LUCENE-6275.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6290", "change_description": ": QueryWrapperFilter propagates approximations and FilteredQuery\nrewrites to a BooleanQuery when the filter is a QueryWrapperFilter in order\nto leverage approximations.", "change_title": "Make the filter -> query migration less performance trappy", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The filter->query migration might be a bit trappy in terms of performance for our users. For instance a FilteredQuery over a TermQuery and a DocValuesRangeFilter should be migrated to a BooleanQuery with a MUST clause for the TermQuery and a FILTER clause for the DocValuesRangeQuery. Performance will be similar since in both case we would use the query to drive the iteration and the filter would only be used to check documents that match the query (we would NOT try to advance the filter iterator). However, if you only go half-way through the migration and end up with a FilteredQuery over a TermQuery and a QueryWrapperFilter(DocValuesRangeQuery) then performance will be terrible because this QueryWrapperFilter does not support random-access (which is the way such filters were not too slow before) and hides the approximation support from the DocValuesRangeQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700545/LUCENE-6290.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6318", "change_description": ": Reduce RAM usage of FieldInfos when there are many fields.", "change_title": "re-use immutable maps across fieldinfos when they are the same", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "From mike's patch, now that these are no longer mutable (LUCENE-6317), this should be easier to do and we know it won't cause trouble.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701765/LUCENE-6318.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-6320", "change_description": ": Speed up CheckIndex.", "change_title": "speed up checkindex", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "This is fairly slow today, very ram intensive, and has some buggy stuff (e.g. postingsenum reuse bugs). We can do better...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701874/LUCENE-6320.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Optimizations", "change_id": "LUCENE-4942", "change_description": ": Optimized the encoding of PrefixTreeStrategy indexes for\nnon-point data: 33% smaller index, 68% faster indexing, and 44% faster\nsearching. YMMV", "change_title": "Indexed non-point shapes index excessive terms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "Indexed non-point shapes are comprised of a set of terms that represent grid cells.  Cells completely within the shape or cells on the intersecting edge that are at the maximum detail depth being indexed for the shape are denoted as \"leaf\" cells.  Such cells have a trailing '+' at the end.  Such tokens are actually indexed twice, one with the leaf byte and one without. The TermQuery based PrefixTree Strategy doesn't consider the notion of 'leaf' cells and so the tokens with '+' are completely redundant. The Recursive [algorithm] based PrefixTree Strategy better supports correct search of indexed non-point shapes than TermQuery does and the distinction is relevant.  However, the foundational search algorithms used by this strategy (Intersects & Contains; the other 2 are based on these) could each be upgraded to deal with this correctly.  Not trivial but very doable. In the end, spatial non-point indexes can probably be trimmed my ~40% by doing this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12703681/LUCENE-4942_non-point_excessive_terms.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6204", "change_description": ",", "change_title": "remove CompoundFileFormat.files()", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Just like the rest of the codec api, we shouldn't require codec to supply this. Instead IndexWriter can use TrackingDirectoryWrapper, like it does otherwise, to see what files were created when handling exceptional cases.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695025/LUCENE-6204.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6208", "change_description": ",", "change_title": "CompoundFileFormat.write() has unnecessary parameter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Today it's: write(Directory dir, SegmentInfo si, Collection<String> files, IOContext context) But 'files' is not needed. its always si.files() because thats all this format is responsible for (e.g. its not an api for /bin/cat).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695080/LUCENE-6208.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6217", "change_description": ": Add IndexWriter.isOpen and getTragicException.", "change_title": "IndexWriter should make it clear when tragedy strikes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "If you hit an exception at a \"bad time\" e.g. when writing files for a newly flushed segment, IndexWriter declares it a tragedy and secretly closes itself as a side effect of the exception. Subsequent operations will throw an ACE with the exception that caused the tragedy as its cause. This requires messy code, if you want to know when this happened to you, since the first exception doesn't make it clear that it was \"tragic\". I think we should make it easier to know when this happens? Maybe we could instead throw a new exception (IWClosedByTragedy or something), or maybe we add a getter (.getTragicException) to IW?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12696513/LUCENE-6217.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6218", "change_description": ",", "change_title": "don't decode freqs or enumerate all positions, when scores are not needed", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Today if you don't call score() some things are faster, we won't invoke similarity or read the norm for the document or other things. On the other hand, its sad in this case that we are decompressing twice as many packed integers as we need (freqs can be skipped over, and our postings lists supports that) and walking all positions in phrase matching to determine the number of times the phrase matched (1 is enough, then we can stop). When scoring is not needed, things can be optimized in other cases too (e.g. thats the whole concept of filters).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12696605/LUCENE-6218.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6220", "change_description": ",", "change_title": "Move needsScores from Weight.scorer to Query.createWeight", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Whether scores are needed is currently a Scorer-level property while it should actually be a Weight thing I think?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697030/LUCENE-6220.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-4524", "change_description": ",", "change_title": "Merge DocsEnum and DocsAndPositionsEnum into PostingsEnum", "detail_type": "Improvement", "detail_affect_versions": "4.0", "detail_fix_versions": "5.1,6.0", "detail_description": "spinnoff from http://www.gossamer-threads.com/lists/lucene/java-dev/172261", "patch_link": "https://issues.apache.org/jira/secure/attachment/12695507/LUCENE-4524.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6246", "change_description": ",", "change_title": "Fix DocsEnum -> PostingsEnum transition", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The current back compat introduced in LUCENE-4524, does not really help the users calling e.g. LeafReader.termDocsEnum() or LeafReader.termPositionsEnum(), because the former's return value changes to PostingsEnum, its superclass, and the latter got removed. It also does not help users using TermsEnum.docs() or TermsEnum.docsAndPositions() which got removed and just replaced with postings(). DocsEnum is different, but not deprecated, instead only used by some codecs as a convenience class. DocsAndPositionsEnum is removed. I think we can do this a little better. First, we need to fix trunk to work the way we want it to look. I think we should have LeafReader.postings() and TermsEnum.postings(), and everything should use PostingsEnum. This is simplest. But in 5.x, I think we should have DocsEnum and DocsAndPositionsEnum which are deprecated, to help guide the user. The \"sugar\" methods on LeafReader that exist in 5.0 (termDocsEnum(), termPositionsEnum()) should be deprecated (with message to use postings()) and final, and can just wrap PostingsEnum. There is no reuse and flags here so this is very simple. On TermsEnum its more complicated, but i dont think impossible. We should add back deprecated and final termDocsEnum() and termPositionsEnum() (with message to use postings()) and these deprecated ones can have an instanceof check, unwrapping back to PostingsEnum before they invoke postings behind the scenes. For the 2 remaining ones on TermsEnum that take flags, thats the most tricky. I actually think we shouldn't change the existing constant values when we dont have to. And I don't think the names FLAG_FREQS are special, i'd rather these just be constants like FREQS. I looked thru JDK constants (http://docs.oracle.com/javase/7/docs/api/constant-values.html) and only one class uses this FLAG_xxx prefix. So I think we should have PostingsEnum.FREQS etc with new values, not conflicting with the old FLAG_FREQS etc values (which we can add back, deprecated, to DocsEnum and DocsAndPositionsEnum). We can even add a check to the deprecated methods that only valid values are passed. This just means we have contained back compat, only for deprecated and final sugar methods in LeafReader and TermsEnum, and the 2 deprecated classes. I think we can live with that and it would save users pain.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700293/LUCENE-6246-flags-cleanup.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6256", "change_description": ",", "change_title": "PostingsEnum impls should respect documented behavior when no positions exist", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "PostingsEnum.nextPositions says that if no positions exist, NO_MORE_POSITIONS will be returned on the first call (actually it refers to DocsEnum.NO_MORE_DOCS, which should be changed since DocsEnum doesn't exist on trunk).  At least one impl (BlockDocsEnum) does assert false inside nextPosition().  This means if you have assertions turned on (e.g. in a test) you get an assertion here, when the behavior should return NO_MORE_POSITIONS.  I'm still going through all the implementations, but I also see MultiTermHighlighting which returns Integer.MAX_VALUE.  I think we should clean up all these implementations which have no positions (including maybe the fake scorers that are copied around in a lot of places?) to return NO_MORE_POSITIONS.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699615/LUCENE-6256.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6271", "change_description": ",", "change_title": "PostingsEnum should have consistent flags behavior", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "When asking for flags like OFFSETS or PAYLOADS with DocsAndPositionsEnum, the behavior was to always return an enum, even if offsets or payloads were not indexed.  They would just not be available from the enum if they were not present.  This behavior was carried over to PostingsEnum, which is good. However, the new POSITIONS flag has different behavior.  If positions are not available, null is returned, instead of a PostingsEnum that just gives access to freqs.  This behavior is confusing, as it means you have to special case asking for positions (only ask if you know they were indexed) which sort of defeats the purpose of the unified PostingsEnum. We should make POSITIONS have the same behavior as other flags. The trickiest part will be maintaining backcompat for DocsAndPositionsEnum in 5.x, but I think it can be done.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708715/LUCENE-6271.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6222", "change_description": ": Removed TermFilter, use a QueryWrapperFilter(TermQuery)\ninstead. This will be as efficient now that queries can opt out from\nscoring.", "change_title": "Remove TermFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "It used to be better than a QueryWrapperFilter(TermQuery) by not decoding freqs but it is not the case anymore since LUCENE-6218", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698885/LUCENE-6222-bw.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6269", "change_description": ": Removed BooleanFilter, use a QueryWrapperFilter(BooleanQuery)\ninstead.", "change_title": "Remove BooleanFilter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Like TermFilter, we should remove this filter and recommend on using BooleanQuery instead. One reason why this is a bit more tricky than TermFilter is that BooleanFilter creates doc id sets that support random-access while a BooleanQuery would not provide random-access support.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699913/LUCENE-6269.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6270", "change_description": ": Replaced TermsFilter with TermsQuery, use a\nQueryWrapperFilter(TermsQuery) instead.", "change_title": "Replace TermsFilter with TermsQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "We should replace TermsFilter with a TermsQuery like we started doing for other filters.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699911/LUCENE-6270.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6223", "change_description": ": Move BooleanQuery.BooleanWeight to BooleanWeight.", "change_title": "Move BooleanWeight to its own file", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "BooleanQuery is currently largish (~ 700LOC), but more than half of that is the inner BooleanWeight. Splitting this out to its own file might make things easier to work with.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12697031/LUCENE-6223.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-1518", "change_description": ": Make Filter extend Query and return 0 as score.", "change_title": "Merge Query and Filter classes", "detail_type": "Improvement", "detail_affect_versions": "2.4", "detail_fix_versions": "5.1,6.0", "detail_description": "This issue presents a patch, that merges Queries and Filters in a way, that the new Filter class extends Query. This would make it possible, to use every filter as a query. The new abstract filter class would contain all methods of ConstantScoreQuery, deprecate ConstantScoreQuery. If somebody implements the Filter's getDocIdSet()/bits() methods he has nothing more to do, he could just use the filter as a normal query. I do not want to completely convert Filters to ConstantScoreQueries. The idea is to combine Queries and Filters in such a way, that every Filter can automatically be used at all places where a Query can be used (e.g. also alone a search query without any other constraint). For that, the abstract Query methods must be implemented and return a \"default\" weight for Filters which is the current ConstantScore Logic. If the filter is used as a real filter (where the API wants a Filter), the getDocIdSet part could be directly used, the weight is useless (as it is currently, too). The constant score default implementation is only used when the Filter is used as a Query (e.g. as direct parameter to Searcher.search()). For the special case of BooleanQueries combining Filters and Queries the idea is, to optimize the BooleanQuery logic in such a way, that it detects if a BooleanClause is a Filter (using instanceof) and then directly uses the Filter API and not take the burden of the ConstantScoreQuery (see LUCENE-1345). Here some ideas how to implement Searcher.search() with Query and Filter: For the user this has the main advantage: That he can construct his query using a simplified API without thinking about Filters oder Queries, you can just combine clauses together. The scorer/weight logic then identifies the cases to use the filter or the query weight API. Just like the query optimizer of a RDB.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698113/LUCENE-1518.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6245", "change_description": ": Force Filter subclasses to implement toString API from Query.", "change_title": "ConstantScoreQuery etc have crazy toString()'s", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "For backwards compatibility reasons, LUCENE-1518 gave Filter a default \"crap\" toString(String) impl of getClass().getSimpleName(). I don't think we should do this. It causes problems e.g. when filters are wrapped in ConstantScoreQuery or other places, because toString(String) does the wrong thing. Instead i think that impl should be removed (leaving it abstract), and Query.toString() should be final for a hard break: having buggy toString's is worse than requiring a change in custom filters. It impacts all users rather than just expert ones. Also by doing this, all the current toString bugs in the codebase show up as compile errors.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698938/LUCENE-6245.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6268", "change_description": ": Replace FieldValueFilter and DocValuesRangeFilter with equivalent\nqueries that support approximations.", "change_title": "Replace doc values filters with queries having approximations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "We should use approximations in order to deal with queries/filters that have slow iterators such as doc-values based queries/filters.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699875/LUCENE-6268.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6289", "change_description": ": Replace DocValuesRangeFilter with DocValuesRangeQuery which\nsupports approximations.", "change_title": "Replace DocValuesTermsFilter with a new DocValuesTermsQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Similarly to what we did with other filters. Additionally, DocValuesTermsQuery could support approximations.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700524/LUCENE-6289.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6266", "change_description": ": Remove unnecessary Directory params from SegmentInfo.toString,\nSegmentInfos.files/toString, and SegmentCommitInfo.toString.", "change_title": "Remove unnecessary Directory parameters from SIS/SIPC/SI", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "A few methods take Directory where it makes no sense: The incoming Directory parameter for files() is bogus, if assertions are enabled, it fails if any underlying segment's 'dir' differs. But if assertions are not enabled, it just silently drops them. If we want to add safety around this kind of thing, this is not the way. For toString(), it just makes the API hard to use for everyone. This change means commits have a working Object.toString() method.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699860/LUCENE-6266.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6272", "change_description": ": Scorer extends DocSetIdIterator rather than DocsEnum", "change_title": "Scorer should not extend PostingsEnum", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "Scorer currently has to implement a whole bunch of methods that are never called.  The only method that Scorer uses in addition to the methods on DocIdSetIterator is freq(), and as currently implemented this means different things on different Scorers: PhraseScorer returns how many phrases it has found on a document In addition, freq() is never actually called on TermScorer, and it's only used in explain() on the phrase scorers. We should make Scorer extend DocIdSetIterator instead.  In place of freq(), Scorer would have a coord() method that by default returns 1, and for boolean scorers returns how many subscorers are matching.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699953/LUCENE-6272.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6281", "change_description": ": Removed support for slow collations from lucene/sandbox. Better\nperformance would be achieved through CollationKeyAnalyzer or\nICUCollationKeyAnalyzer.", "change_title": "Remove slow collation support", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The support for slow collations is documented as deprecated and \"will be removed in 5.0\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700192/LUCENE-6281.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6286", "change_description": ": Removed IndexSearcher methods that take a Filter object.\nA BooleanQuery with a filter clause must be used instead.", "change_title": "Remove Filter from IndexSearcher APIs", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "IndexSearcher has lots of methods that take a filter, and merge it with the query using a FilteredQuery when it is non null. I would like to remove these methods in favour of methods that only take a query and leave the responsibility to build a FilteredQuery or a BooleanQuery with FILTER clause to the user.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700502/LUCENE-6286.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6300", "change_description": ": PrefixFilter, TermRangeFilter and NumericRangeFilter have been\nremoved. Use PrefixQuery, TermRangeQuery and NumericRangeQuery instead.", "change_title": "Remove multi-term filters", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "We have TermRangeFilter, NumericRangeFilter, ... that we should remove in favour of their equivalent queries (TermRangeQuery, NumericRangeQuery, ...).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701057/LUCENE-6300.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6303", "change_description": ": Replaced FilterCache with QueryCache and CachingWrapperFilter\nwith CachingWrapperQuery.", "change_title": "CachingWrapperFilter -> CachingWrapperQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "As part of the filter -> query migration, we should migrate the caching wrappers (including the filter cache). I think the behaviour should be to delegate to the wrapped query when scores are needed and cache otherwise like CachingWrapperFilter does today. Also the cache should ignore query boosts so that field:value^2 and field:value^3 are considered equal if scores are not needed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12707935/LUCENE-6303-disable_auto-caching.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6317", "change_description": ": Deprecate DataOutput.writeStringSet and writeStringStringMap.\nUse writeSetOfStrings/Maps instead.", "change_title": "Fix readStringStringMap api", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Currently this api is not very efficient, and always returns a mutable map. Can we change it to allow immutability? its sad we don't return Collections.emptyMap so its the same instance for all empty cases.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701732/LUCENE-6317.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6307", "change_description": ": Rename SegmentInfo.getDocCount -> .maxDoc,\nSegmentInfos.totalDocCount -> .totalMaxDoc, MergeInfo.totalDocCount", "change_title": "Rename SegmentInfo.docCount -> .maxDoc", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "We already have maxDoc and numDocs, I think it's crazy we have a 3rd one docCount. We should just rename to maxDoc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701569/LUCENE-6307.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "API Changes", "change_id": "LUCENE-6367", "change_description": ": PrefixQuery now subclasses AutomatonQuery, removing the\nspecialized PrefixTermsEnum.", "change_title": "Can PrefixQuery subclass AutomatonQuery?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "Spinoff/blocker for LUCENE-5879. It seems like PrefixQuery should \"simply\" be an AutomatonQuery rather than specializing its own TermsEnum ... with maybe some performance improvements to ByteRunAutomaton.run to short-circuit once it's in a \"sink state\", AutomatonTermsEnum could be just as fast as PrefixTermsEnum. If we can do this it will make LUCENE-5879 simpler.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12706778/LUCENE-6367.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Other", "change_id": "LUCENE-6248", "change_description": ": Remove unused odd constants from StandardSyntaxParser.jj", "change_title": "Remove unused odd constants from StandardSyntaxParser.jj", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "This block of constants struct me as odd  looked like a bug to me at first: but it turns out they're not used at all anymore  there is a Conjunction block that is all commented out.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699086/LUCENE-6248.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Other", "change_id": "LUCENE-6193", "change_description": ": Collapse identical catch branches in try-catch statements.", "change_title": "Collapse identical catch branches in try-catch statements", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "We are on Java 7+ so we can reduce verbosity by collapsing identical catch statements into one. We did the same for solr in SOLR-7014.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12693741/LUCENE-6193.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Other", "change_id": "LUCENE-6239", "change_description": ": Removed RAMUsageEstimator's sun.misc.Unsafe calls.", "change_title": "Remove RAMUsageEstimator Unsafe calls", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,5.3,6.0", "detail_description": "This is unnecessary risk. We should remove this stuff, it is not needed here. We are a search engine, not a ram calculator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698424/LUCENE-6239.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Other", "change_id": "LUCENE-6292", "change_description": ": Seed StringHelper better.", "change_title": "seed StringHelper better", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The current code is good, it avoids SecureRandom/blocking when we don't need real security (just used for safety checks). On the other hand it has some downsides: I think we should use /dev/urandom when its available, its just practical and exactly what we need. If its not available (e.g. windows) we can use the current logic. If sysprops arent available we can just use another hashcode instead and lucene can still be used.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700606/LUCENE-6292.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Other", "change_id": "LUCENE-6333", "change_description": ": Refactored queries to delegate their equals and hashcode\nimpls to the super class.", "change_title": "Clean up overridden .equals and .hashCode methods in Query subclasses", "detail_type": "Improvement", "detail_affect_versions": "5.0", "detail_fix_versions": "5.1,6.0", "detail_description": "As a followup to LUCENE-6304, all classes that subclass Query and override the equals and hashCode methods should call super.equals/hashCode and, when possible, not override the methods at all. For example, TermQuery.hashCode overrides the Query.hashCode, but will be exactly the same code once LUCENE-6304 is merged.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12704459/LUCENE-6333-2.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Other", "change_id": "LUCENE-6343", "change_description": ": DefaultSimilarity javadocs had the wrong float value to\ndemonstrate precision of encoded norms", "change_title": "Missing character in DefaultSimilarity's javadoc", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "The part which describes precision loss of norm values is missing a character; the encoded input value 0.89 in the example will actually be decoded to 0.875, not 0.75.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12703050/LUCENE-6343.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6255", "change_description": ": PhraseQuery now ignores leading holes and requires that\npositions are positive and added in order.", "change_title": "PhraseQuery inconsistencies", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.1", "detail_description": "PhraseQuery behaves quite inconsistently when the position of the first term is greater than 0. Here is an example: The reason is that when you add a term with position P on a PhraseQuery, ExactPhraseScorer ignores all positions for this term which are less than P. But this is inconsistent: So I think we have two options:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12699631/LUCENE-6255.patch", "patch_content": "none"}
{"library_version": "5.1.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6298", "change_description": ": SimpleQueryParser returns an empty query rather than\nnull, if e.g. the terms were all stopwords.", "change_title": "empty SimpleQueryParser query should return empty BooleanQuery", "detail_type": "Bug", "detail_affect_versions": "5.0", "detail_fix_versions": "5.1,6.0", "detail_description": "In order to be consistent with QueryParser, SimpleQueryParser should return an empty BooleanQuery instead of null when the analyzed query state is null (if the query text is entirely removed during analysis, for instance). Long term it would also be nice to be able to return a MatchNoDocsQuery (or something like that) instead of using null as a stand-in value for this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12700877/LUCENE-6298.patch", "patch_content": "none"}

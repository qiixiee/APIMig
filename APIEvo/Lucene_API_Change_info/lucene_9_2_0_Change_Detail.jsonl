{"library_version": "9.2.0", "change_type": "API Changes", "change_id": "LUCENE-10325", "change_description": ": Facets API extended to support getTopFacets.", "change_title": "Add getTopDims functionality to Facets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "The current getAllDims functionality is really the only way for users to determine the \"top\" dimensions in a faceting field (i.e., get the top dims by count along with their top-n children), but it has the unfortunate side-effect of resolving all child paths for every dim, even if the user doesn't intend to use those dims. For example, if a match set contains docs relating to 100 different dims (and various values under each), but the user only wants the top 10 dims with their top 5 children, they can call getAllDims(5) then just grab the first 10 results, but a lot of wasted work has been done for the other 90 dims. It would be nice to implement something like getTopDims(int numDims, int numChildren) that would only do the work necessary to resolve numDims dims instead of all dims.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "API Changes", "change_id": "LUCENE-10482", "change_description": ": Allow users to create their own DirectoryTaxonomyReaders with empty taxoArrays instead of letting the\ntaxoEpoch decide. Add a test case that demonstrates the inconsistencies caused when you reuse taxoArrays on older\ncheckpoints.", "change_title": "Allow users to create their own DirectoryTaxonomyReaders with empty taxoArrays instead of letting the taxoEpoch decide", "detail_type": "Improvement", "detail_affect_versions": "9.1", "detail_fix_versions": "None", "detail_description": "I was experimenting with the taxonomy index and DirectoryTaxonomyReaders in my day job where we were trying to replace the index underneath a reader asynchronously and then call the doOpenIfChanged call on it. It turns out that the taxonomy index uses its own index based counter (the taxonomyIndexEpoch) to determine if the index was opened in write mode after the last time it was written and if not, it directly tries to reuse the previous taxoArrays it had created. This logic fails in a scenario where both the old and new index were opened just once but the index itself is completely different in both the cases. In such a case, it would be good to give the user the flexibility to inform the DTR to recreate its taxoArrays, ordinalCache and categoryCache (not refreshing these arrays causes it to fail in various ways). Luckily, such a constructor already exists! But it is private today! The idea here is to allow subclasses of DTR to use this constructor. Curious to see what other folks think about this idea.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "API Changes", "change_id": "LUCENE-10558", "change_description": ": Add new constructors to Kuromoji and Nori dictionary classes to support classpath /\nmodule system usage. It is now possible to use JDK's Class/ClassLoader/Module#getResource(...) apis\nand pass their returned URL to dictionary constructors to load resources from Classpath or Module\nresources.", "change_title": "Add URL constructors for classpath/module usage as complement to Path ctors in Kuromoji and Nori", "detail_type": "Improvement", "detail_affect_versions": "9.1", "detail_fix_versions": "10.0(main),9.2", "detail_description": "When we refactored the constructors for  these resource objects used by the kuromoji JapaneseTokenizer,  we (inadvertently, I expect) changed the behavior for consumers that were supplying these resources on the classpath. In that case, we silently replaced the custom resources with the Lucene built-in ones.  I think we cannot support the old API because of Java Module system restrictions, but we didn't provide any usable replacement or notice either.  This issue is for exposing the new (private) constructors that accept streams, and adding a notice to Migration.md to point users at them, since they can be used with resources streams loaded from the classpath by the caller.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "New Features", "change_id": "LUCENE-10312", "change_description": ": Add PersianStemmer based on the Arabic stemmer.", "change_title": "Add PersianStemmer", "detail_type": "Wish", "detail_affect_versions": "9.0", "detail_fix_versions": "10.0(main),9.2", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "New Features", "change_id": "LUCENE-10539", "change_description": ": Return a stream of completions from FSTCompletion.", "change_title": "Return a stream of completions from FSTCompletion", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "FSTLookup currently has a \"num\" parameter which limits the number of completions from the underlying automaton. But this has severe disadvantages if you need to collect completions that need to fulfill a secondary condition (for example, collect only verbs or terms that contain a certain infix). Then you can't determine the 'num' parameter easily because the number of filtered completions is unknown. I also think implementation-wise it's also much nicer to provide a stream that iterates over completions rather than a fixed-size list. This allows for much more elegant code (stream.filter, stream.limit). The provided patch adds a single Stream<Completion> lookup(key) method and modifies the existing lookup methods to use it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "New Features", "change_id": "LUCENE-10385", "change_description": ": Implement Weight#count on IndexSortSortedNumericDocValuesRangeQuery\nto speed up computing the number of hits when possible.", "change_title": "Implement Weight#count on IndexSortSortedNumericDocValuesRangeQuery.", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "This query can count matches by computing the first and last matching doc IDs using binary search.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "New Features", "change_id": "LUCENE-10422", "change_description": ": Monitor Improvements: `Monitor` can use a custom `Directory`\nimplementation. `Monitor` can be created with a readonly `QueryIndex` in order to\nhave readonly `Monitor` instances.", "change_title": "Monitor instantiation configurabilty improvements", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "I'm working on a project where I use very heavily Lucene Monitor package, but I miss  some simple things in how Monitor manages it's Directory, IndexWriter and IndexReader, what I want to do is extend MonitorConfiguration to make possible mainly these two things: * use a custom Directory implementation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "New Features", "change_id": "LUCENE-10456", "change_description": ": Implement rewrite and Weight#count for MultiRangeQuery\nby merging overlapping ranges .", "change_title": "Implement Weight#count for MultiRangeQuery", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "for one dimension MultiRangeQuery, we can firstly   merge overlapping ranges to have some unconnected ranges, then the doc count of this multiRangeQuery is the sum of each doc count of these unconnected range. For each unconnected range, we can take advantage of PointRangeQuery count() method.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "New Features", "change_id": "LUCENE-10444", "change_description": ": Support alternate aggregation functions in association facets.", "change_title": "Support alternate aggregation functions in association facets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "We currently only support sum aggregations in the various association facet implementations. I'd be really interested in extending the association facet implementations to support other aggregations, starting with max and min (in addition to sum). I've been sketching up a prototype of this and I think I have a reasonable way to introduce this idea. Will get a PR out for feedback soon.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Improvements", "change_id": "LUCENE-10229", "change_description": ": return -1 for unknown offsets in ExtendedIntervalsSource. Modify highlighting to\nwork properly with or without offsets.", "change_title": "Match offsets should be consistent for fields with positions and fields with offsets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "This is a follow-up of LUCENE-10223 in which it was discovered that fields with offsets don't highlight some more complex interval queries properly.  Alan says: It's because it returns the position of the inner match, but the offsets of the outer.  And so if you're re-analyzing and retrieving offsets by looking at the positions, you get the 'right' thing.  It's not obvious to me what the correct response is here, but thinking about it the current behaviour is kind of the worst of both worlds, and perhaps we should change it so that you get offsets of the inner match as standard, and then the outer match is returned as part of the sub matches. Intervals are nicely separated into \"basic intervals\" and \"filters\" which restrict some other source of intervals, here is the original documentation: https://github.com/apache/lucene/blob/main/lucene/queries/src/java/org/apache/lucene/queries/intervals/package-info.java#L29-L50 My experience from an extended period of using interval queries in a frontend where they're highlighted is that filters are restrictions that should not be highlighted - it's the source intervals that people care about. Filters are what you remove or where you give proper context to source intervals. The test code contributed in LUCENE-10223 contains numerous query-highlight examples (on fields with positions) where this intuition is demonstrated on all kinds of interval functions: https://github.com/apache/lucene/blob/main/lucene/highlighter/src/test/org/apache/lucene/search/matchhighlight/TestMatchHighlighter.java#L335-L542 This issue is about making the internals work consistently for fields with positions and fields with offsets.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Improvements", "change_id": "LUCENE-10494", "change_description": ": Implement method to bulk add all collection elements to a PriorityQueue.", "change_title": "Implement method to bulk add all collection elements to a PriorityQueue", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "PR at https://github.com/apache/lucene/pull/770/files", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Improvements", "change_id": "LUCENE-10484", "change_description": ": Add support for concurrent random sampling by calling\nRandomSamplingFacetsCollector#createManager.", "change_title": "Add support for concurrent facets random sampling", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "While FacetsCollectorManager exists to allows users to concurrently do facets collection through FacetsCollector, RandomSamplingFacetsCollector does not have a corresponding collector manager that easily allows users to concurrently do random sampling. The needed collector manager would be very similar to FacetsCollectorManager, yet it would need to expose a specialized reduced RandomSamplingFacetsCollector, and the reduction should call getOriginalMatchingDocs instead of getMatchingDocs, which modifies the internal totalHits when called. This relates to LUCENE-10002 and would allow to use a collector manager instead of a collector when doing random sampling, in the effort of reducing usages of IndexSearcher#search(Query, Collector).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Improvements", "change_id": "LUCENE-10467", "change_description": ": Throws IllegalArgumentException for Facets#getAllDims and Facets#getTopChildren\nif topN <= 0.", "change_title": "Throws IllegalArgumentException for getAllDims and getTopChildren if topN <= 0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Currently, there are different behaviors from subclass that implements  and overrides getAllDims and getTopChildren when passing in an invalid TopN parameter (topN <= 0). Some overridden implementations throw a NullPointerException, some throw an IllegalArgumentException, and others throw no exception. It would provide a better user experience by consistently throwing an IllegalArgumentException when requesting topN <= 0 for these two functionalities across all implementations.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Improvements", "change_id": "LUCENE-9848", "change_description": ": Correctly sort HNSW graph neighbors when applying diversity criterion", "change_title": "Correctly sort HNSW graph neighbors when applying diversity criterion", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "When indexing new documents in an HNSW graph, we first find its nearest maxConn neighbors (using HNSW search), and then link the new document to this neighbors in the graph. These neighbors are filtered using a diversity test. The neighbors are added one by one, from most similar to least. Each new neighbor is checked against all prior (better) neighbors, and if it is more similar to that neighbor than it is to the target document, it is rejected as insufficiently diverse. When we applied this diversity criterion (rather than simply picking the k nearest neighbors), we saw substantial improvements in recall / latency ROC curves across several data sets, and it is part of the reference implementation, too (where we got it). I believe the impact on indexing performance was relatively small; this is a good thing to do, even though it is n^2 at its heart, the n remains reasonable due to being bounded by the maximum graph fanout parameter, maxConn. Something funny happens when we reach the maximum fanout though. While a new document is being linked to its new neighbors, the neighbors are reciprocally linked to the new document, until their maximum fanout is reached. At that point, the diversity criterion is reapplied to select the neighbors to keep. Basically every neighbor is re-checked against every earlier (better) neighbor to verify the diversity criterion.  This is needed because we haven't really maintained the diversity property while adding these reciprocal links – the initial neighbors are checked for diversity, which often leads to fewer than maxConn of them being added. Then the new documents get linked in without checking, until maxConn is reached, and then diversity is checked again. This is kind of weird, but seems to work. But the really strange thing is that when we reject non-diverse documents (in HnswGraphBuilder.diversityUpdate), the neighbors are no longer sorted in nearness order. I did some rough checks to see if better graphs would result from re-sorting (so that when there are non-diverse neighbors, we always prefer to drop the worse-scoring one), but it didn't seem to matter all that much. But how can that be? At any rate this code is funky and hard to understand, and it would probably benefit from a second look to see if we can either improve indexing performance or improve search performance (by producing better graphs during indexing).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Improvements", "change_id": "LUCENE-10527", "change_description": ": Use 2*maxConn for the last layer in HNSW", "change_title": "Use bigger maxConn for last layer in HNSW", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Recently I was rereading the HNSW paper (https://arxiv.org/pdf/1603.09320.pdf) and noticed that they suggest using a different maxConn for the upper layers vs. the bottom one (which contains the full neighborhood graph). Specifically, they suggest using maxConn=M for upper layers and maxConn=2*M for the bottom. This differs from what we do, which is to use maxConn=M for all layers. I tried updating our logic using a hacky patch, and noticed an improvement in latency for higher recall values (which is consistent with the paper's observation): Results on glove-100-angular Parameters: M=32, efConstruction=100 As we'd expect, indexing becomes a bit slower: When we benchmarked Lucene HNSW against hnswlib in LUCENE-9937, we noticed a big difference in recall for the same settings of M and efConstruction. (Even adding graph layers in LUCENE-10054 didn't really affect recall.) With this change, the recall is now very similar: Results on glove-100-angular Parameters: M=32, efConstruction=100 I think it'd be nice update to maxConn so that we faithfully implement the paper's algorithm. This is probably least surprising for users, and I don't see a strong reason to take a different approach from the paper? Let me know what you think!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10555", "change_description": ": avoid NumericLeafComparator#iteratorCost repeated initialization\nwhen NumericLeafComparator#setScorer is called.", "change_title": "avoid repeated NumericLeafComparator setScorer calls", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "ElasticSearch use CancellableBulkScorer to fast cancel long time query execution by dividing one segment docs to many small split docs. For every split docs, collector.setScorer(scorer) is called, then  NumericLeafComparator#setScorer is called. As a result, for one segment, NumericLeafComparator#setScorer is called many times. Every time NumericLeafComparator#setScorer is called, the NumericLeafComparator#iteratorCost is reset to the Scorer.cost and increase many unnecessary  pointValues#intersect calls to get  competitive docs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10452", "change_description": ": Hunspell: call checkCanceled less frequently to reduce the overhead", "change_title": "Hunspell: call checkCanceled less frequently to reduce the overhead", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main),9.2", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10451", "change_description": ": Hunspell: don't perform potentially expensive spellchecking after timeout", "change_title": "Hunspell: don't perform potentially expensive spellchecking after timeout", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main),9.2", "detail_description": "Currently, to return partial result after timeout, additional processing with case-adjustment and `spell` calls is performed, which can take time and also result in superfluous `checkCanceled` invocations.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10418", "change_description": ": More `Query#rewrite` optimizations for the non-scoring case.", "change_title": "Improve Query rewriting for non-scoring clauses", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Query rewriting is occasionally important for performance, e.g. it may allow using an optimized bulk scorer instead of the default bulk scorer like in the example from LUCENE-10412. One case when we could simplify queries is in the non-scoring case. All layers of query wrappers that only affect scoring like BoostQuery and ConstantScore query can be removed, which might help identify new opportunities for rewriting. For instance, we have several rewrite rules that optimize for MatchAllDocsQuery and would fail to recognize it if it is behind a ConstantScoreQuery or a BoostQuery. Boolean queries can also simplify themselves in the non-scoring case, by changing MUST clauses to FILTER clauses, or removing fully optional SHOULD clauses.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10436", "change_description": ": Deprecate DocValuesFieldExistsQuery, NormsFieldExistsQuery and KnnVectorFieldExistsQuery\nwith FieldExistsQuery.", "change_title": "Combine DocValuesFieldExistsQuery, NormsFieldExistsQuery and KnnVectorFieldExistsQuery into a single FieldExistsQuery?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Now that we require consistency across data structures, we could merge DocValuesFieldExistsQuery, NormsFieldExistsQuery and KnnVectorFieldExistsQuery together into a FieldExistsQuery that would require that the field indexes either norms, doc values or vectors?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10481", "change_description": ": FacetsCollector will not request scores if it does not use them.", "change_title": "FacetsCollector does not need scores when not keeping them", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.11.2,9.2", "detail_description": "FacetsCollector currently always specifies ScoreMode.COMPLETE, we could get better performance by not requesting scores when we don't need them.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10503", "change_description": ": Potential speedup for pure disjunctions whose clauses produce\nscores that are very close to each other.", "change_title": "Preserve more significant bits of scores in WANDScorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "WANDScorer operates on longs to avoid accuracy issues with floating-point numbers. The current process loses more accuracy bits than it could, and making it better could help skip in a few more situations.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10315", "change_description": ": Use SIMD instructions to decode BKD doc IDs.", "change_title": "Speed up BKD leaf block ids codec by a 512 ints ForUtil", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Elasticsearch (which based on lucene) can automatically infers types for users with its dynamic mapping feature. When users index some low cardinality fields, such as gender / age / status... they often use some numbers to represent the values, while ES will infer these fields as long, and ES uses BKD as the index of long fields. When the data volume grows, building the result set of low-cardinality fields will make the CPU usage and load very high. This is a flame graph we obtained from the production environment: addall.svg It can be seen that almost all CPU is used in addAll. When we reindex long to keyword, the cluster load and search latency are greatly reduced ( We spent weeks of time to reindex all indices... ). I know that ES recommended to use keyword for term/terms query and long for range query in the document, but there are always some users who didn't realize this and keep their habit of using sql database, or dynamic mapping automatically selects the type for them. All in all, users won't realize that there would be such a big difference in performance between long and keyword fields in low cardinality fields. So from my point of view it will make sense if we can make BKD works better for the low/medium cardinality fields. As far as i can see, for low cardinality fields, there are two advantages of keyword over long: 1. ForUtil used in keyword postings is much more efficient than BKD's delta VInt, because its batch reading (readLongs) and SIMD decode. 2. When the query term count is less than 16, TermsInSetQuery can lazily materialize of its result set, and when another small result clause intersects with this low cardinality condition, the low cardinality field can avoid reading all docIds into memory. This ISSUE is targeting to solve the first point. The basic idea is trying to use a 512 ints ForUtil for BKD ids codec. I benchmarked this optimization by mocking some random LongPoint and querying them with PointInSetQuery. Benchmark Result The indices size is reduced for low cardinality fields and flat for high cardinality fields.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8836", "change_description": ": Speed up calls to TermsEnum#lookupOrd on doc values terms enums\nand sequences of increasing ords.", "change_title": "Optimize DocValues TermsDict to continue scanning from the last position when possible", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Lucene80DocValuesProducer.TermsDict is used to lookup for either a term or a term ordinal. Currently it does not have the optimization the FSTEnum has: to be able to continue a sequential scan from where the last lookup was in the IndexInput. For sparse lookups (when searching only a few terms or ordinal) it is not an issue. But for multiple lookups in a row this optimization could save re-scanning all the terms from the block start (since they are delat encoded). This patch proposes the optimization. To estimate the gain, we ran 3 Lucene tests while counting the seeks and the term reads in the IndexInput, with and without the optimization: TestLucene70DocValuesFormat - the optimization saves 24% seeks and 15% term reads. TestDocValuesQueries - the optimization adds 0.7% seeks and 0.003% term reads. TestDocValuesRewriteMethod.testRegexps - the optimization saves 71% seeks and 82% term reads. In some cases, when scanning many terms in lexicographical order, the optimization saves a lot. In some case, when only looking for some sparse terms, the optimization does not bring improvement, but does not penalize neither. It seems to be worth to always have it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10536", "change_description": ": Doc values terms dictionaries now use the first (uncompressed)\nterm of each block as a dictionary when compressing suffixes of the other 63\nterms of the block.", "change_title": "Doc values terms dicts should use the first term of each block as a dictionary", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Doc values terms dictionaries split data into blocks of 64 terms, where the first term is written uncompressed (which is useful for binary searches), and the 63 other terms are encoded by taking the difference with the previous term and compressing all suffixes together with LZ4. With this format, the suffix of the second term is also unlikely to benefit from any compression, since it doesn't have data to search for duplicate bytes into besides itself. A minor improvement we could make would consist of using the first term as a dictionary for suffixes of terms 2..64.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10411", "change_description": ": Add nearest neighbors vectors support to ExitableDirectoryReader.", "change_title": "Add NN vectors support to ExitableDirectoryReader", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This is currently unsupported.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10542", "change_description": ": FieldSource exists implementations can avoid value retrieval", "change_title": "FieldSource exists implementations can avoid value retrieval", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "While looking at LUCENE-10534, found that *FieldSource exists implementation after LUCENE-7407 can avoid value lookup when just checking for exists. Flamegraphs - x axis = time spent as a percentage of time being profiled, y axis = stack trace bottom being first call top being last call Looking only at the left most getValueForDoc highlight only (and it helps to make it bigger or download the original)  LongFieldSource#exists spends MOST of its time doing a LongFieldSource#getValueForDoc. LongFieldSource#getValueForDoc spends its time doing two things primarily: This makes sense based on looking at the code (copied below to make it easier to see at once)  https://github.com/apache/lucene/blob/main/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java#L72 LongFieldSource#exists - doesn't care about the actual longValue. Just that there was a value found when iterating through the doc values. https://github.com/apache/lucene/blob/main/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java#L95 So putting this all together for exists calling getValueForDoc, we spent ~50% of the time trying to get the long value when we don't need it in exists. We can save that 50% of time making exists not care about the actual value and just return if doc == curDocID basically. This 50% extra is exaggerated in MaxFloatFunction (and other places) since exists() is being called a bunch. Eventually the value will be needed from longVal(), but if we call exists() say 3 times for every longVal(), we are spending a lot of time computing the value when we only need to check for existence. I found the same pattern in DoubleFieldSource, EnumFieldSource, FloatFieldSource, IntFieldSource, LongFieldSource. I put together a change showing what this would look like: Simple JMH performance tests comparing the original FloatFieldSource to the new ones from PR #847. Source: https://github.com/risdenk/lucene-jmh", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10534", "change_description": ": MinFloatFunction / MaxFloatFunction exists check can be slow", "change_title": "MinFloatFunction / MaxFloatFunction calls exists twice", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "MinFloatFunction (https://github.com/apache/lucene/blob/main/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java) and MaxFloatFunction (https://github.com/apache/lucene/blob/main/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java) both check if values exist twice. This change prevents the duplicate exists check. Tested with JMH here: https://github.com/risdenk/lucene-jmh", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10496", "change_description": ": Queries sorted by field now better handle the degenerate case\nwhen the search order and the index order are in opposite directions.", "change_title": "avoid unnecessary attempts to evaluate skipping doc if index sort and search sort are in opposite direction", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "users ofter write doc with indexSorting in one direction(asc or desc) , but need to search top docs both in two direction (asc and desc) if index sort and search sort are in opposite direction, NumericLeafComparator needn't to check if can skip non-competitive doc inside one segments, because the rest docs are all competitive. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10502", "change_description": ": Use IndexedDISI to store docIds and DirectMonotonicWriter/Reader to handle\nordToDoc in HNSW vectors", "change_title": "Use IndexedDISI to store docIds and DirectMonotonicWriter/Reader to handle ordToDoc", "detail_type": "Improvement", "detail_affect_versions": "9.1", "detail_fix_versions": "9.2", "detail_description": "Since at search phase, vector's all docs of all fields will be fully loaded into memory, could we use IndexedDISI to store docIds and DirectMonotonicWriter/Reader to handle ordToDoc mapping?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Optimizations", "change_id": "LUCENE-10488", "change_description": ": Facets#getTopDims optimized for taxonomy faceting and\nConcurrentSortedSetDocValuesFacetCounts.", "change_title": "Optimize Facets#getTopDims across Facets implementations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "LUCENE-10325 added a new getTopDims API, allowing users to specify the number of \"top\" dimensions they want. The default implementation just delegates to getAllDims and returns the number of top dims requested, but some Facets sub-classes can do this more optimally. LUCENE-10325 demonstrated this in SortedSetDocValueFacetCounts, but we can take it further. There's at least some opportunity to do better in:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10477", "change_description": ": Highlighter: WeightedSpanTermExtractor.extractWeightedSpanTerms to Query#rewrite\nmultiple times if necessary.", "change_title": "SpanBoostQuery.rewrite was incomplete for boost==1 factor", "detail_type": "Bug", "detail_affect_versions": "8.11.1", "detail_fix_versions": "10.0(main),8.11.2,9.2", "detail_description": "(This bug report concerns pre-9.0 code only but it's so subtle that it warrants sharing I think and maybe fixing if there was to be a 8.11.2 release in future.) Some existing code e.g. https://github.com/apache/lucene-solr/blob/releases/lucene-solr/8.11.1/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanNearBuilder.java#L54 adds a SpanBoostQuery even if there is no boost or the boost factor is 1.0 i.e. technically wrapping is unnecessary. Query rewriting should counteract this somewhat except it might not e.g. note at https://github.com/apache/lucene-solr/blob/releases/lucene-solr/8.11.1/lucene/core/src/java/org/apache/lucene/search/spans/SpanBoostQuery.java#L81-L83 how the rewrite is a no-op i.e. this.query.rewrite is not called! This can then manifest in strange ways e.g. during highlighting: This stacktrace is not from 8.11.1 code but the general logic is that at line 293 rewrite was called (except it didn't a full rewrite because of SpanBoostQuery wrapping around the SpanNearQuery) and so then at line 295 the IllegalArgumentException(\"Rewrite first!\") arises: https://github.com/apache/lucene-solr/blob/releases/lucene-solr/8.11.1/lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java#L101", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10491", "change_description": ": A correctness bug in the way scores are provided within TaxonomyFacetSumValueSource\nwas fixed.", "change_title": "TaxonomyFacetSumValueSource incorrectly provides scores to doc values", "detail_type": "Bug", "detail_affect_versions": "10.0(main),9.2", "detail_fix_versions": "9.2", "detail_description": "TaxonomyFacetSumValueSource has a bug in the way it provides scores to the user-provided doc values. On this line it should be index = doc, not index++. Thanks to mikemccand for finding this over in #718! I've reproduced with a test and will post the test and a fix shortly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10466", "change_description": ": Ensure IndexSortSortedNumericDocValuesRangeQuery handles sort field\ntypes besides LONG", "change_title": "IndexSortSortedNumericDocValuesRangeQuery unconditionally assumes the usage of the LONG-encoded SortField", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.2", "detail_description": "We have run into this issue while migrating to OpenSearch and making changes to accommodate https://issues.apache.org/jira/browse/LUCENE-10087. It turned out that IndexSortSortedNumericDocValuesRangeQuery unconditionally assumes the usage of the LONG-encoded SortField, as could be seen inside static ValueComparator loadComparator method  Using the numeric range query (in case of sorted index) with anything but LONG ends up with class cast exception: Simple test case to reproduce (for TestIndexSortSortedNumericDocValuesRangeQuery):  The expectation is that IndexSortSortedNumericDocValuesRangeQuery should not fail with class cast but correctly convert the numeric values.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10292", "change_description": ": Suggest: Fix AnalyzingInfixSuggester / BlendedInfixSuggester to correctly return\nexisting lookup() results during concurrent build().  Fix other FST based suggesters so that\ngetCount() returned results consistent with lookup() during concurrent build().", "change_title": "AnalyzingInfixSuggester thread safety: lookup() fails during (re)build()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main),9.2", "detail_description": "I'm filing this based on anecdotal information from a Solr user w/o experiencing it first hand (and I don't have a test case to demonstrate it) but based on a reading of the code the underlying problem seems self evident... With all other Lookup implementations I've examined, it is possible to call lookup() regardless of whether another thread is concurrently calling build() – in all cases I've seen, it is even possible to call lookup() even if build() has never been called: the result is just an \"empty\" List<LookupResult> Typically this is works because the build() method uses temporary datastructures until it's \"build logic\" is complete, at which point it atomically replaces the datastructures used by the lookup() method.   In the case of AnalyzingInfixSuggester however, the build() method starts by closing & null'ing out the protected SearcherManager searcherMgr (which it only populates again once it's completed building up it's index) and then the lookup method starts with... ... meaning it is unsafe to call AnalyzingInfixSuggester.lookup() in any situation where another thread may be calling AnalyzingInfixSuggester.build()", "patch_link": "https://issues.apache.org/jira/secure/attachment/13042137/LUCENE-10292-3.patch", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10508", "change_description": ": Fixes some edge cases where GeoArea were built in a way that vertical planes\ncould not evaluate their sign, either because the planes where the same or the center between those\nplanes was lying in one of the planes.", "change_title": "GeoArea failure with degenerated latitude", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "I hit a failure when trying to build a GeoArea using the GeoAreaFactory. The issue seems to happen when you have an almost degenerated minLatitude and maxLatitude and you are close to the poles. Then you might hit the following exception\" The situation is easy to reproduce with the following test:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10495", "change_description": ": Fix return statement of siblingsLoaded() in TaxonomyFacets.", "change_title": "Fix return statement of siblingsLoaded() in TaxonomyFacets", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Found a bug in TaxonomyFacets when trying to use the siblingsLoaded function. siblingsLoaded() should return siblings != null and it returns children != null currently.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10533", "change_description": ": SpellChecker.formGrams is missing bounds check", "change_title": "SpellChecker.formGrams is missing bounds check", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "If using Solr IndexBasedSpellChecker and spellcheck.q is empty the following exception occurs (found in SOLR-16169). There is an argument that the caller should not be invalid, but a simple bounds check would prevent this in Lucene.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10529", "change_description": ": Properly handle when TestTaxonomyFacetAssociations test case randomly indexes\nno documents instead of throwing an NPE.", "change_title": "TestTaxonomyFacetAssociations may have floating point issues", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main),9.2", "detail_description": "Hit this in a jenkins CI build while testing something else:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10470", "change_description": ": Check if polygon has been successfully tessellated before we fail (we are failing some valid\ntessellations) and allow filtering edges that fold on top of the previous one.", "change_title": "Unable to Tessellate polygon", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.2", "detail_description": "I have a polygon that causes Tessellator.tessellate to throw an \"Unable to Tessellate shape\" error. I tried several versions of Lucene, and the issue does not happen with Lucene 8.2.0, but seems to happen with all Lucene versions >=8.3.0, including the latest main branch. I created a branch that reproduces the issue: https://github.com/apache/lucene/compare/main...yixunx:yx/reproduce-tessellator-error?expand=1 This is the polygon rendered on geojson.io:  Is this a bug in the Tesselator logic, or is there anything wrong with this polygon that maybe wasn't caught by Lucene 8.2.0?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10530", "change_description": ": Avoid floating point precision test case bug in TestTaxonomyFacetAssociations.", "change_title": "TestTaxonomyFacetAssociations test failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "10.0(main),9.2", "detail_description": "TestTaxonomyFacetAssociations.testFloatAssociationRandom seems to have some flakiness, it fails on the following random seed. This is because of a mismatch in (SUM) aggregated multi-valued, float_random facet field. We accept an error delta of 1 in this aggregation, but for the failing random seed, the delta is 1.3. Maybe we should change this delta to 1.5? My hunch is that it is some floating point approximation error. I'm unable to repro it without the randomization seed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10518", "change_description": ": Relax field consistency check for old indices", "change_title": "FieldInfos consistency check can refuse to open Lucene 8 index", "detail_type": "Bug", "detail_affect_versions": "8.10.1", "detail_fix_versions": "9.2,9.1.1", "detail_description": "A field-infos consistency check introduced in Lucene 9 (LUCENE-9334) can refuse to open a Lucene 8 index. Lucene 8 can create a partial FieldInfo if hitting a non-aborting exception (for example term is too long) during processing fields of a document. We don't have this problem in Lucene 9 as we process fields in two phases with the first phase processing only FieldInfos. The issue can be reproduced with this snippet. I would like to propose to: /cc mayya and jpountz", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10558", "change_description": ": Restore behaviour of deprecated Kuromoji and Nori dictionary constructors for\ncustom dictionary support. Please also use new URL-based constructors for classpath/module\nsystem ressources.", "change_title": "Add URL constructors for classpath/module usage as complement to Path ctors in Kuromoji and Nori", "detail_type": "Improvement", "detail_affect_versions": "9.1", "detail_fix_versions": "10.0(main),9.2", "detail_description": "When we refactored the constructors for  these resource objects used by the kuromoji JapaneseTokenizer,  we (inadvertently, I expect) changed the behavior for consumers that were supplying these resources on the classpath. In that case, we silently replaced the custom resources with the Lucene built-in ones.  I think we cannot support the old API because of Java Module system restrictions, but we didn't provide any usable replacement or notice either.  This issue is for exposing the new (private) constructors that accept streams, and adding a notice to Migration.md to point users at them, since they can be used with resources streams loaded from the classpath by the caller.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10552", "change_description": ": KnnVectorQuery has incorrect equals/ hashCode.", "change_title": "KnnVectorQuery has incorrect equals/ hashCode", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "filter query could impact document-filtering properties, so maybe it should be a condition in Query#equals and Query#hashCode ?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10564", "change_description": ": Make sure SparseFixedBitSet#or updates ramBytesUsed.", "change_title": "SparseFixedBitSet#or doesn't update memory accounting", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.11.2,9.2", "detail_description": "While debugging why a cache was using way more memory than expected, one of my colleagues noticed that SparseFixedBitSet#or doesn't update ramBytesUsed. Here's a unit test that demonstrates this: It also looks like we don't have any tests for SparseFixedBitSet memory accounting (unless I've missed them!) It'd be nice to add more coverage there too.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Build", "change_id": "GITHUB#768", "change_description": ": Upgrade forbiddenapis to version 3.3.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This just updates version of forbiddenapis to 3.3, released today. All build passes although new signatures were added.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Build", "change_id": "GITHUB#890", "change_description": ": Detect CI builds on Github or Jenkins and enable errorprone.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This detects CI builds (Github and Jenkins) using existence of some environment variables. There is a new ext property project.ext.isCIBuild . The ext property is used to enable errorprone on thse builds. You can still enable just errorprone on local builds with: -Perrorprone.enabled=true We can use the new project property to enable other expensive checks by default for CI builds.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Build", "change_id": "LUCENE-10532", "change_description": ": Remove LuceneTestCase.Slow annotation. All tests can be fast.", "change_title": "Remove @Slow annotation", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "This annotation is useless, people have gotten so lazy about using it, that now there are proposals to mark tests that are not actually slow, with the @Slow annotation. Let's remove the annotation. I can't imagine a situation where we mark a test @Slow and i don't veto it. we can keep tests clean.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Other", "change_id": "LUCENE-10526", "change_description": ": Test-framework: Add FilterFileSystemProvider.wrapPath(Path) method for mock filesystems\nto override if they need to extend the Path implementation.", "change_title": "add single method to mockfile to wrap a Path", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Currently, mockfilesystems wrap a path with \"new FilterPath\". but this \"wrapping\" logic is scattered everywhere in the code (and tests!). And it is hardcoded at filterpath (subclassing is not possible). This makes it impossible for a mock filesystem to extend FilterPath with some custom logic (example: check for special windows reserved characters). I don't think code/tests should be calling \"new FilterPath\" everywhere, this is also just messy. Instead they should ask the mockfilesystem's provider to wrap the path: provider.wrapPath(path, filesystem). This way, WindowsFS can then override wrapPath() with a subclass that looks for special characters. This issue is just for the API refactoring/cleanup. Additional Windows-simulation can happen on the parent issue.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Other", "change_id": "LUCENE-10525", "change_description": ": Test-framework: Add detection of illegal windows filenames to WindowsFS.", "change_title": "Improve WindowsFS emulation to catch directory names with : in them (which is not allowed)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "In PR (https://github.com/apache/lucene/pull/762) we missed the case where a tempDir name was using `:` in the dir name. This test was passing in Linux, MacOS environments but ended up failing in Windows build systems. We ended up pushing a fix to not use `:` in the names. Open to other ideas as well!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Other", "change_id": "LUCENE-10541", "change_description": ": Test-framework: limit the default length of MockTokenizer tokens to 255.", "change_title": "What to do about massive terms in our Wikipedia EN LineFileDocs?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "Spinoff from this fun build failure that dweiss root caused: https://lucene.markmail.org/thread/pculfuazll4oebra Thank you and sorry dweiss!! This test failure happened because the test case randomly indexed a chunk of the nightly (many GBs) LineFileDocs Wikipedia file that had a massive (> IW's ~32 KB limit) term, and IW threw an IllegalArgumentException failing the test. It's crazy that it took so long for Lucene's randomized tests to discover this too-massive term in Lucene's nightly benchmarks.  It's like searching for Nessie, or SETI. We need to prevent such false failures, somehow, and there are multiple options: fix this test to not use LineFileDocs, remove all \"massive\" terms from all tests (nightly and git) LineFileDocs, fix MockTokenizer to trim such ridiculous terms (I think this is the best option?), ...", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.2.0", "change_type": "Other", "change_id": "GITHUB#854", "change_description": ": Allow to link to GitHub pull request from CHANGES.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "changes2html already supports links to Github PRs ( https://issues.apache.org/jira/browse/LUCENE-5383 ), but the link is obsoleted so a small modification is needed to make it work again. With this change, developers can directly link to their pull requests from CHANGES.txt, skipping creating Jira issues. I'll also show an example in this PR. When the change made (2014) Github was still a \"third party\" to Apache and the integration had been on experimental status; the situation was changed - now GitHub is the first-class code hosting service for ASF projects. I think the requirement for opening a Jira issue can be relaxed. Here is the screenshot of generated CHANGES.html by changesToHtml task. The \"GITHUB#854\" hyperlink correctly points to #854 (this pr).", "patch_link": "none", "patch_content": "none"}

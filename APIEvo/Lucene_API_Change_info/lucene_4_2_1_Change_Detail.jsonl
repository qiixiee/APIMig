{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4713", "change_description": ": The SPI components used to load custom codecs or analysis\ncomponents were fixed to also scan the Lucene ClassLoader in addition\nto the context ClassLoader, so Lucene is always able to find its own\ncodecs. The special case of a null context ClassLoader is now also\nsupported.", "change_title": "SPI: Allow fallback to default ClassLoader if Thread#getContextClassLoader fails", "detail_type": "Improvement", "detail_affect_versions": "4.0,4.1,4.2", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "NOTE: This issue has been renamed from: \"Replace calls to Thread#getContextClassLoader with the ClassLoader of the current class\" because the revised patch provides a clean fallback path. I am not sure whether it is a design decision or if we can indeed consider this a bug: In core and analysis-common some classes provide on-demand class loading using SPI. In NamedSPILoader, SPIClassIterator, ClasspathResourceLoader and AnalysisSPILoader there are constructors that use the Thread's context ClassLoader by default whenever no particular other ClassLoader was specified. Unfortunately this does not work as expected when the Thread's ClassLoader can't see the required classes that are instantiated downstream with the help of Class.forName (e.g., Codecs, Analyzers, etc.). That's what happened to us here. We currently experiment with running Lucene 2.9 and 4.x in one JVM, both being separated by custom ClassLoaders, each seeing only the corresponding Lucene version and the upstream classpath. While NamedSPILoader and company get successfully loaded by our custom ClassLoader, their instantiation fails because our Thread's Context-ClassLoader cannot find the additionally required classes. We could probably work-around this by using Thread#setContextClassLoader at construction time (and quickly reverting back afterwards), but I have the impression this might just hide the actual problem and cause further trouble when lazy-loading classes later on, and potentially from another Thread. Removing the call to Thread#getContextClassLoader would also align with the behavior of AttributeSource.DEFAULT_ATTRIBUTE_FACTORY, which in fact uses Attribute#getClass().getClassLoader() instead. A simple patch is attached. All tests pass.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566155/LuceneContextClassLoader.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4819", "change_description": ": seekExact(BytesRef, boolean) did not work correctly with\nSorted[Set]DocValuesTermsEnum.", "change_title": "move Sorted[Set]DocValuesTermsEnum to codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "Currently a user can instantiate a SortedDocValuesTermsEnum(SortedDocValues). This is a generic termsenum, implementing all operations by lookupOrd(). I think instead this should be the default implementation, and we should have e.g. SortedDocValues.termsEnum() that returns it (codec can implement something fancier). For example the default codec implements lookupOrd as an FST binary search, which means next() on this termsenum is much slower than it needs to be for the places where this enum is actually used (segment merging, OrdinalMap used for faceting in SOLR-4490 and LUCENE-4795) So instead, it can override this method and use an FSTEnum, and these operations are significantly faster (3x faster for me with a simple benchmark with 10M terms).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573237/LUCENE-4819.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4826", "change_description": ": PostingsHighlighter was not returning the top N best\nscoring passages.", "change_title": "PostingsHighlighter doesn't keep the top N best scoring passages", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "The comparator we pass to the PQ is just backwards ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573399/LUCENE-4826.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4854", "change_description": ": Fix DocTermOrds.getOrdTermsEnum() to not return negative\nord on initial next().", "change_title": "DocTermsOrd getOrdTermsEnum() buggy, lookupTerm/termsEnum is slow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "Investigating a test failure in grouping/ I found the current dv api needs help for DocTermsOrds (this facet+grouping collector uses seekExact(BytesRef) on the termsenum):", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574319/LUCENE-4854.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4836", "change_description": ": Fix SimpleRateLimiter#pause to return the actual time spent\nsleeping instead of the wakeup timestamp in nano seconds.", "change_title": "SimpleRateLimiter#pause returns target time stamp instead of sleep time.", "detail_type": "Bug", "detail_affect_versions": "4.1,4.2", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "SimpleRateLimiter#pause is supposed to return the time it actually spend sleeping but it returns the actual time in nanos it is supposed to sleep until. This cause some problems in ES due to long overflows.... here is the original issue reported by a user: https://github.com/elasticsearch/elasticsearch/issues/2785", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573962/LUCENE-4836.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4828", "change_description": ": BooleanQuery no longer extracts terms from its MUST_NOT\nclauses.", "change_title": "BooleanQuery.extractTerms should not recurse into MUST_NOT clauses", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573430/LUCENE-4828.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "SOLR-4589", "change_description": ": Fixed CPU spikes and poor performance in lazy field loading\nof multivalued fields.", "change_title": "4.x + enableLazyFieldLoading + large multivalued fields + varying fl = pathological CPU load & response time", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1,4.2", "detail_fix_versions": "4.2.1,4.3,6.0", "detail_description": "Following up on a user report of exterme CPU usage in 4.1, I've discovered that the following combination of factors can result in extreme CPU usage and excessively HTTP response times... I haven't dug into the route cause yet, but the essential observations is: if lazyloading is used in 4.x, then once a document has been fetched with an initial fl list X, subsequent requests for that document using a differnet fl list Y can be many orders of magnitute slower (while pegging the CPU) â€“ even if those same requests using fl Y uncached (or w/o lazy laoding) would be extremely fast.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574476/SOLR-4589.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4870", "change_description": ": Fix bug where an entire index might be deleted by the IndexWriter\ndue to false detection if an index exists in the directory when\nOpenMode.CREATE_OR_APPEND is used. This might also affect application that set\nthe open mode manually using DirectoryReader#indexExists.", "change_title": "Lucene deletes entire index if and exception is thrown due do TooManyOpenFiles and OpenMode.CREATE_OR_APPEND", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1,4.2,3.6.2", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "The Lucene IndexWriter might delete an entire index if it hits a FileNotFoundException triggered by TooManyOpenFiles during IndexWriter creation. We try to figure out if the index exists already if the OpenMode.CREATE_OR_APPEND is set (which is default). Yet, the logic in DirectoryReader#indexExists(Directory) will just return false if we are not able to open the segment file. This will cause the IW to assume there is no index and it will try to create a new index there trashing all existing commit points treating this as a OpenMode.CREATE.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575032/LUCENE-4870.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Bug Fixes", "change_id": "LUCENE-4878", "change_description": ": Override getRegexpQuery in MultiFieldQueryParser to prefent\nNullPointerException when regular expression syntax is used with\nMultiFieldQueryParser.", "change_title": "Regular expression syntax with MultiFieldQueryParser causes assert/NPE", "detail_type": "Bug", "detail_affect_versions": "4.1,4.2", "detail_fix_versions": "4.2.1,6.0", "detail_description": "Using regex syntax causes MultiFieldQueryParser.parse() to throw an AssertionError (if asserts are on) or causes subsequent searches using the returned Query instance to throw NullPointerException (if asserts are off). Simon Willnauer's comment on the java-user alias: \"This is in-fact a bug in the MultiFieldQueryParser [...] MultifieldQueryParser should override getRegexpQuery but it doesn't\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12575287/LUCENE-4878.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Optimizations", "change_id": "LUCENE-4819", "change_description": ": Added Sorted[Set]DocValues.termsEnum(), and optimized the\ndefault codec for improved enumeration performance.", "change_title": "move Sorted[Set]DocValuesTermsEnum to codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "Currently a user can instantiate a SortedDocValuesTermsEnum(SortedDocValues). This is a generic termsenum, implementing all operations by lookupOrd(). I think instead this should be the default implementation, and we should have e.g. SortedDocValues.termsEnum() that returns it (codec can implement something fancier). For example the default codec implements lookupOrd as an FST binary search, which means next() on this termsenum is much slower than it needs to be for the places where this enum is actually used (segment merging, OrdinalMap used for faceting in SOLR-4490 and LUCENE-4795) So instead, it can override this method and use an FSTEnum, and these operations are significantly faster (3x faster for me with a simple benchmark with 10M terms).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12573237/LUCENE-4819.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Optimizations", "change_id": "LUCENE-4854", "change_description": ": Speed up TermsEnum of FieldCache.getDocTermOrds.", "change_title": "DocTermsOrd getOrdTermsEnum() buggy, lookupTerm/termsEnum is slow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.3,4.2.1,6.0", "detail_description": "Investigating a test failure in grouping/ I found the current dv api needs help for DocTermsOrds (this facet+grouping collector uses seekExact(BytesRef) on the termsenum):", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574319/LUCENE-4854.patch", "patch_content": "none"}
{"library_version": "4.2.1", "change_type": "Optimizations", "change_id": "LUCENE-4857", "change_description": ": Don't unnecessarily copy stem override map in\nStemmerOverrideFilter.", "change_title": "StemmerOverrideFilter should not copy the stem override dictionary in it's ctor.", "detail_type": "Improvement", "detail_affect_versions": "4.0,4.1,4.2", "detail_fix_versions": "4.2.1,6.0", "detail_description": "Currently the dictionary is cloned each time the token filter is created which is a serious bottleneck if you use this filter with large dictionaries and can also lead to OOMs if lots of those filters sit in ThreadLocals and new threads are added etc. I think cloning the map should be done in the analyzer (which all of our analyzers do btw. but this is the only TF that does that) no need to really copy that map.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574546/LUCENE-4857.patch", "patch_content": "none"}

{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8467", "change_description": ": RAMDirectory, RAMFile, RAMInputStream, RAMOutputStream are deprecated", "change_title": "RAMDirectory, RAMFile, RAMInputStream, RAMOutputStream are deprecated", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Splitting LUCENE-8438 into smaller, easier patches as discussed there.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937289/LUCENE-8467.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8356", "change_description": ": StandardFilter is deprecated", "change_title": "Remove StandardFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "StandardFilter does literally nothing, and is included all over the place, presumably for historical reasons.  We should just nuke it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12927946/LUCENE-8356-solr.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8373", "change_description": ": ENGLISH_STOP_WORD_SET on StandardAnalyzer is deprecated.  Instead\nuse EnglishAnalyzer.ENGLISH_STOP_WORD_SET.  The default constructor for\nStopAnalyzer is also deprecated, and a stop word set should be explicitly\npassed to the constructor.", "change_title": "Move ENGLISH_STOP_WORD_SET from StandardAnalyzer to EnglishAnalyzer", "detail_type": "New Feature", "detail_affect_versions": "8.0", "detail_fix_versions": "7.5,8.0", "detail_description": "Follow-up of LUCENE-7444.  English stopwords should be on the EnglishAnalyzer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929724/LUCENE-8373-master.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8378", "change_description": ": Add DocIdSetIterator.range static method to return an iterator\nmatching a range of docids", "change_title": "Add DocIdSetIterator.range method", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "We already have DocIdSetIterator.all and DocIdSetIterator.empty but I'd like to also add a range method to match a specified range of docids. E.g. this can be useful if you sort your index by a key, and then create a custom query to match documents by values for that key, or by range (LUCENE-7714).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12930194/LUCENE-8378.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8379", "change_description": ": Add experimental TermQuery.getTermStates method", "change_title": "Add TermQuery.getTermContext", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "The TermQuery class can optionally hold a TermContext which hold pre-computed, per leaf data to make seeking (to postings for that term) faster.  I'd like to add a getter for it, to help with diagnostics, just knowing whether a given TermQuery holds a term context or not.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929995/LUCENE-8379.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8407", "change_description": ": Add experimental SpanTermQuery.getTermStates method", "change_title": "Add SpanTermQuery.getTermStates()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Adding a getTermStates() to the TermStates in a SpanTermQuery would be useful, just like we have similarly for a TermQuery – LUCENE-8379.  It would be useful for LUCENE-6513 to avoid a needless inner ScoreTerm class when a SpanTermQuery would suffice.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8388", "change_description": ": PostingsEnum#attributes() has been deprecated", "change_title": "Deprecate and remove PostingsEnum#attributes()", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "This method isn't used anywhere in the codebase, and seems to be entirely useless.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12930535/LUCENE-8388-7x.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8390", "change_description": ": MatchesIteratorSupplier replaced by IOSupplier", "change_title": "Replace MatchesIteratorSupplier with IOSupplier", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Matches objects are constructed using a deferred supplier pattern.  This is currently done using a specialised MatchesIteratorSupplier interface, but this can be deprecated/removed and replaced  with the generic IOSupplier in the utils package", "patch_link": "https://issues.apache.org/jira/secure/attachment/12930812/LUCENE-8390.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8397", "change_description": ": Add DirectoryTaxonomyWriter.getCache", "change_title": "Add DirectoryTaxonomyWriter.getCache", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "DirectoryTaxonomyWriter uses a cache to hold recently mapped labels / ordinals.  You can provide an impl when you create the class, or it will use a default impl.  I'd like to add a getter, DirectoryTaxonomyWriter.getCache to retrieve the cache it's using; this is helpful for getting diagnostics (how many cached labels, how much RAM used, etc.).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931465/LUCENE-8397.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8387", "change_description": ": Add experimental IndexSearcher.getSlices API to see which slices\nIndexSearcher is searching concurrently when it's created with an ExecutorService", "change_title": "Add IndexSearcher.getSlices", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "When you pass an executor to IndexSearcher, it creates a LeafSlice[] slices, by default once slice per leaf, but a subclass can override.  It's helpful to later be able to get those slices e.g. if you want to do your own concurrent per-slice processing. This patch will just add a getter to IndexSearcher, and make the LeafSlice.leaves member public.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12930652/LUCENE-8387.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8263", "change_description": ": TieredMergePolicy's reclaimDeletesWeight has been replaced with a\nnew deletesPctAllowed setting to control how aggressively deletes should be\nreclaimed.", "change_title": "Add indexPctDeletedTarget as a parameter to TieredMergePolicy to control more aggressive merging", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "Spinoff of LUCENE-7976 to keep the two issues separate. The current TMP allows up to 50% deleted docs, which can be wasteful on large indexes. This parameter will do more aggressive merging of segments with deleted documents when the total percentage of deleted docs in the entire index exceeds it. Setting this to 50% should approximate current behavior. Setting it to 20% caused the first cut at this to increase I/O roughly 10%. Setting it to 10% caused about a 50% increase in I/O. I was conflating the two issues, so I'll change 7976 and comment out the bits that reference this new parameter. After it's checked in we can bring this back. That should be less work than reconstructing this later. Among the questions to be answered: 1> what should the default be? I propose 20% as it results in significantly less space wasted and helps control heap usage for a modest increase in I/O. 2> what should the floor be? I propose 10% with strong documentation warnings about not setting it below 20%. 3> should there be two parameters? I think this was discussed somewhat in 7976. The first cut at  this used this number for two purposes: 3a> the total percentage of deleted docs index-wide to trip this trigger 3b> the percentage of an individual segment that had to be deleted if the segment was over maxSegmentSize/2 bytes in order to be eligible for merging. Empirically, using the same percentage for both caused the merging to hover around the value specified for this parameter. My proposal for <3> would be to have the parameter do double-duty. Assuming my preliminary results hold, you specify this parameter at, say, 20% and once the index hits that % deleted docs it hovers right around there, even if you've forceMerged earlier down to 1 segment. This seems in line with what I'd expect and adding another parameter seems excessively complicated to no good purpose. We could always add something like that later if we wanted.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931321/LUCENE-8263.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-7314", "change_description": ": Graduate LatLonPoint and query classes to core", "change_title": "Graduate LatLonPoint to core", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Maybe we should graduate these fields (and related queries) to core for Lucene 6.1?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929205/LUCENE-7314.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8428", "change_description": ": The way that oal.util.PriorityQueue creates sentinel objects has\nbeen changed from a protected method to a java.util.function.Supplier as a\nconstructor argument.", "change_title": "Allow configurable sentinels in PriorityQueue", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "This is a follow-up to SOLR-12587: Lucene's PriorityQueue API makes it impossible to have a configurable sentinel object since the parent constructor is called before a sub class has the opportunity to set anything that helps create those sentinels.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933155/LUCENE-8428.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8437", "change_description": ": CheckIndex.Status.cantOpenSegments and missingSegmentVersion\nhave been removed as they were not computed correctly.", "change_title": "CheckIndex should not duplicate SegmentInfos serialization", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "CheckIndex tries to read the segments file to provide more information about the problem, but it gets almost everything wrong: it reads an int which based on comments and error messages is either the format or the version but this is actually the magic number of the codec header. It looks like this code is about10 years old and hasn't been updated when we added checksums to index files. SegmentInfos does a better job at detecting issues nowadays, so these manual checks can be removed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933744/LUCENE-8437.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8286", "change_description": ": The UnifiedHighlighter has a new HighlightFlag.WEIGHT_MATCHES flag that\nwill tell this highlighter to use the new MatchesIterator API as the underlying\napproach to navigate matching hits for a query.  This mode will highlight more\naccurately than any other highlighter, and can mark up phrases as one span instead of\nword-by-word.  The UH's public internal APIs changed a bit in the process.", "change_title": "UnifiedHighlighter should support the new Weight.matches API for better match accuracy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "The new Weight.matches() API should allow the UnifiedHighlighter to more accurately highlight some BooleanQuery patterns correctly – see LUCENE-7903. In addition, this API should make the job of highlighting easier, reducing the LOC and related complexities, especially the UH's PhraseHelper.  Note: reducing/removing PhraseHelper is not a near-term goal since Weight.matches is experimental and incomplete, and perhaps we'll discover some gaps in flexibility/functionality. This issue should introduce a new UnifiedHighlighter.HighlightFlag enum option for this method of highlighting.   Perhaps call it WEIGHT_MATCHES?  Longer term it could go away and it'll be implied if you specify enum values for PHRASES & MULTI_TERM_QUERY?", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8471", "change_description": ": IndexWriter.getFlushingBytes() returns how many bytes are currently\nbeing flushed to disk.", "change_title": "Expose the number of bytes currently being flushed in IndexWriter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "This is already available via the DocumentWriter and flush control.  Making it public on IndexWriter would allow for better memory accounting when using IndexWriter#flushNextBuffer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937734/LUCENE-8471.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "API Changes", "change_id": "LUCENE-8422", "change_description": ": Static helper functions for Matches and MatchesIterator implementations\nhave been moved from Matches to MatchesUtils", "change_title": "Add Matches iteration to interval queries", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Follow up to LUCENE-8404, we can now add Matches iteration to interval queries in the sandbox.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938141/LUCENE-8422.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8445", "change_description": ": Tighten condition when two planes are identical to prevent constructing\nbogus tiles when building GeoPolygons.", "change_title": "RandomGeoPolygonTest.testCompareBigPolygons() failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6.6,7.5,8.0", "detail_description": "Failure from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/22590/, reproduces for me on Java8:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12934406/LUCENE-8445.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8444", "change_description": ": Prevent building functionally identical plane bounds when constructing\nDualCrossingEdgeIterator .", "change_title": "Geo3D Test Failure: Test Point is Contained by shape but outside the XYZBounds", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.6.6,7.5,8.0", "detail_description": "Reproduces for me on branch_7x.  /cc daddywri  ivera", "patch_link": "https://issues.apache.org/jira/secure/attachment/12934448/LUCENE-8444.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8380", "change_description": ": UTF8TaxonomyWriterCache inconsistency.", "change_title": "UTF8TaxonomyWriterCache inconsistency", "detail_type": "Bug", "detail_affect_versions": "7.1", "detail_fix_versions": "7.5", "detail_description": "I’m facing a problem with taxonomy writer cache inconsistency. At some point in time UTF8TaxonomyWriterCache starts to return wrong ord for some facet labels. As result wrong ord are written in doc facet fields, and wrong counts are returned (undercount) during search. This bug is manifested on different servers with different index contents (we have several separate indexes with unique data).   Unfortunately I can’t reproduce this behaviour in tests.   I've dumped \"broken\" UTF8TaxonomyWriterCache instance and created app to load it and to compare with real taxonomy. Dumps and app are in attachment. To run demo extract archives content and exec: As you can see, labels [frametype, 7] and [modification_id, 682] have same ord in cache.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12930086/LUCENE-8380.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8164", "change_description": ": IndexWriter silently accepts broken payload. This has been fixed\nvia", "change_title": "IndexWriter silently accepts broken payload", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "IndexWriter seems to be missing bounds checks for payloads completely. If you pass a \"broken\" payload (e.g. BytesRef's offset + length is out of bounds), it will silently index it as if nothing went wrong. What actually happens? Doesn't matter, we should be getting an exception.  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12909701/LUCENE-8164_test.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8165", "change_description": ": IndexWriter silently accepts broken payload. This has been fixed\nvia", "change_title": "ban Arrays.copyOfRange with forbidden APIs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "This method is no good, because instead of throwing AIOOBE for bad bounds, it will silently fill with zeros (essentially silent corruption). Unfortunately it is used in quite a few places so replacing it with e.g. arrayCopy may uncover some interesting surprises. See LUCENE-8164 for motivation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12909710/LUCENE-8165_start.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8370", "change_description": ": Reproducing\nTestLucene{54,70}DocValuesFormat.testSortedSetVariableLengthBigVsStoredFields()\nfailures", "change_title": "Reproducing TestLucene{54,70}DocValuesFormat.testSortedSetVariableLengthBigVsStoredFields() failures", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "Policeman Jenkins found a reproducing seed for a TestLucene70DocValuesFormat.testSortedSetVariableLengthBigVsStoredFields() failure https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/22320/; git bisect blames commit 2519025 on LUCENE-7976:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929655/LUCENE-8370.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8376", "change_description": ",", "change_title": "TestRandomChains.testRandomChains() failure", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "From https://builds.apache.org/job/Lucene-Solr-NightlyTests-7.x/253/:", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8371", "change_description": ",", "change_title": "TestRandomChains.testRandomChainsWithLargeStrings() failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Reproducing seed for TestRandomChains.testRandomChainsWithLargeStrings() failure from https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/2196/:", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8395", "change_description": ": WordDelimiterGraphFilter would incorrectly insert a hole into a\nTokenStream if a token consisting entirely of delimiter characters was\nencountered, but preserve_original was set.", "change_title": "WordDelimiterGraphFilter can incorrectly add holes to a TokenStream", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "If a token consists entirely of delimiter characters, then WordDelimiterGraphFilter will remove the token and insert a hole into the TokenStream.  However, it does this even if preserve_original is set, which results in an extra gap being added to the stream.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931307/LUCENE-8395.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8398", "change_description": ": TieredMergePolicy.getMaxMergedSegmentMB has rounding error", "change_title": "TieredMergePolicy.getMaxMergedSegmentMB has rounding error", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "This is largely a test artifact since it's unlikely to show up for realistically sized segments, but the fix is simple and safe. This code first does long division then promotes to double for the last calculation. The error can be reproduced with:  -Dtests.seed=EF80BCABAD74A7CF", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931723/LUCENE-8398.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8429", "change_description": ": DaciukMihovAutomatonBuilder is no longer prone to stack\noverflows by enforcing a maximum term length.", "change_title": "DaciukMihovAutomatonBuilder needs protection against stack overflows", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "The maximum level of recursion of this class is the maximum term length, which is not low enough to ensure it never fails with a stack overflow.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933199/LUCENE-8429.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8441", "change_description": ": IndexWriter now checks doc value type for index sort fields\nand fails the document if they are not compatible.", "change_title": "Wrong index sort field type throws unexpected NullPointerException", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "I came across this scary exception if you pass the wrong SortField.Type for a field; I'll attach patch w/ small test case:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933874/LUCENE-8441.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8458", "change_description": ": Adjust initialization condition of PendingSoftDeletes and ensures\nit is initialized before accepting deletes", "change_title": "Carry-over hard-deletes after merge may not adjust soft-delete count", "detail_type": "Bug", "detail_affect_versions": "7.5,8.0", "detail_fix_versions": "7.5,8.0", "detail_description": "Attached is a test that can trip PendingDeletetes assertion around 5%. The assertion is violated because we do not reduce soft-deletes count accordingly when carrying over hard-deletes after a merge in IndexWriter#carryOverHardDeletes. If the newly merged segment has soft-deleted documents, its PendingDeletes requires a segment reader to \"transfer\" soft-deletes count to hard-deletes accordingly. testSoftDeleteWhileMergeSurvives (introduced in LUCENE-8293) always passes as a segment warmer used in that test forces ReadersAndUpdates to open a new segment reader. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12936478/LUCENE-8144.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8466", "change_description": ": IndexWriter.deleteDocs(Query... query) incorrectly applies deletes on flush\nif the index is sorted.", "change_title": "FrozenBufferedUpdates#apply*Deletes is incorrect when index sorting is enabled", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "This was reported by Vish Ramachandran at https://markmail.org/message/w27h7n2isb5eogos. When deleting by term or query, we record the term/query that is deleted and the current max doc id. Deletes are later applied on flush by FrozenBufferedUpdates#apply*Deletes. Unfortunately, this doesn't work when index sorting is enabled since documents are renumbered between the time that the current max doc id is computed and the time that deletes are applied.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937281/LUCENE-8466.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8502", "change_description": ": Allow access to delegate in FilterCodecReader. FilterCodecReader didn't\nallow access to it's delegate like other filter readers. This adds a new #getDelegate method\nto access the wrapped reader.", "change_title": "Allow access to delegate in FilterCodecReader", "detail_type": "Bug", "detail_affect_versions": "7.5,8.0", "detail_fix_versions": "7.5,8.0", "detail_description": "FilterCodecReader doesn't allow access to it's delegate like other     filter readers. This adds a new getDelegate method to access the     wrapped reader.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-7976", "change_description": ": TieredMergePolicy now respects maxSegmentSizeMB by default when executing\nfindForcedMerges and findForcedDeletesMerges", "change_title": "Make TieredMergePolicy respect maxSegmentSizeMB and allow singleton merges of very large segments", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "We're seeing situations \"in the wild\" where there are very large indexes (on disk) handled quite easily in a single Lucene index. This is particularly true as features like docValues move data into MMapDirectory space. The current TMP algorithm allows on the order of 50% deleted documents as per a dev list conversation with Mike McCandless (and his blog here:  https://www.elastic.co/blog/lucenes-handling-of-deleted-documents). Especially in the current era of very large indexes in aggregate, (think many TB) solutions like \"you need to distribute your collection over more shards\" become very costly. Additionally, the tempting \"optimize\" button exacerbates the issue since once you form, say, a 100G segment (by optimizing/forceMerging) it is not eligible for merging until 97.5G of the docs in it are deleted (current default 5G max segment size). The proposal here would be to add a new parameter to TMP, something like <maxAllowedPctDeletedInBigSegments> (no, that's not serious name, suggestions welcome) which would default to 100 (or the same behavior we have now). So if I set this parameter to, say, 20%, and the max segment size stays at 5G, the following would happen when segments were selected for merging: > any segment with > 20% deleted documents would be merged or rewritten NO MATTER HOW LARGE. There are two cases, >> the segment has < 5G \"live\" docs. In that case it would be merged with smaller segments to bring the resulting segment up to 5G. If no smaller segments exist, it would just be rewritten >> The segment has > 5G \"live\" docs (the result of a forceMerge or optimize). It would be rewritten into a single segment removing all deleted docs no matter how big it is to start. The 100G example above would be rewritten to an 80G segment for instance. Of course this would lead to potentially much more I/O which is why the default would be the same behavior we see now. As it stands now, though, there's no way to recover from an optimize/forceMerge except to re-index from scratch. We routinely see 200G-300G Lucene indexes at this point \"in the wild\" with 10s of  shards replicated 3 or more times. And that doesn't even include having these over HDFS. Alternatives welcome! Something like the above seems minimally invasive. A new merge policy is certainly an alternative.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12928030/LUCENE-7976.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8263", "change_description": ": TieredMergePolicy now reclaims deleted documents more\naggressively by default ensuring that no more than ~1/3 of the index size is\nused by deleted documents.", "change_title": "Add indexPctDeletedTarget as a parameter to TieredMergePolicy to control more aggressive merging", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "Spinoff of LUCENE-7976 to keep the two issues separate. The current TMP allows up to 50% deleted docs, which can be wasteful on large indexes. This parameter will do more aggressive merging of segments with deleted documents when the total percentage of deleted docs in the entire index exceeds it. Setting this to 50% should approximate current behavior. Setting it to 20% caused the first cut at this to increase I/O roughly 10%. Setting it to 10% caused about a 50% increase in I/O. I was conflating the two issues, so I'll change 7976 and comment out the bits that reference this new parameter. After it's checked in we can bring this back. That should be less work than reconstructing this later. Among the questions to be answered: 1> what should the default be? I propose 20% as it results in significantly less space wasted and helps control heap usage for a modest increase in I/O. 2> what should the floor be? I propose 10% with strong documentation warnings about not setting it below 20%. 3> should there be two parameters? I think this was discussed somewhat in 7976. The first cut at  this used this number for two purposes: 3a> the total percentage of deleted docs index-wide to trip this trigger 3b> the percentage of an individual segment that had to be deleted if the segment was over maxSegmentSize/2 bytes in order to be eligible for merging. Empirically, using the same percentage for both caused the merging to hover around the value specified for this parameter. My proposal for <3> would be to have the parameter do double-duty. Assuming my preliminary results hold, you specify this parameter at, say, 20% and once the index hits that % deleted docs it hovers right around there, even if you've forceMerged earlier down to 1 segment. This seems in line with what I'd expect and adding another parameter seems excessively complicated to no good purpose. We could always add something like that later if we wanted.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931321/LUCENE-8263.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-8503", "change_description": ": Call #getDelegate instead of direct member access during unwrap.\nFilter*Reader instances access the member or the delegate directly instead of\ncalling getDelegate(). In order to track access of the delegate these methods\nshould call #getDelegate()", "change_title": "Simplify unwrapping Filter*Reader", "detail_type": "Improvement", "detail_affect_versions": "7.5,8.0", "detail_fix_versions": "7.5,8.0", "detail_description": "Today we have 3 different kinds of FilterIndexReader. While FilterDirecotryReader     and FilterLeafReader are simple to distinguish, FilterCodecReader make decision harder     since now we need instanceof checks to deside which unwrap method we should call. This     adds a simple interface that allows to build generic unwrap methods to access the delegat     of each of the filtering readers.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8468", "change_description": ": A ByteBuffer based Directory implementation.", "change_title": "A ByteBuffer based Directory implementation (and associated classes)", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "A factored-out sub-patch with ByteBufferDirectory and associated index inputs, outputs, etc. and tests. No refactorings or cleanups to any other classes (these will go in to master after 8.0 branch is cut).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937428/LUCENE-8468.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8447", "change_description": ": Add DISJOINT and WITHIN support to LatLonShape queries.", "change_title": "Add DISJOINT and WITHIN support to LatLonShape queries", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This feature will add support to LatLonShapeBoundingBoxQuery and LatLonShapePolygonQuery for searching all indexed LatLonShape types that are  WITHIN, or DISJOINT to, the target query shape. INTERSECTS remains unchanged.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12934824/LUCENE-8447.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8440", "change_description": ": Add support for indexing and searching Line and Point shapes using LatLonShape encoding", "change_title": "Add support for indexing and searching Line and Point shapes using LatLonShape encoding", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This feature adds support to LatLonShape for indexing Line and latitude, longitude Point types using the 6 dimension Triangle encoding in LatLonShape. Indexed points and lines will be searchable using LatLonShapeBoundingBoxQuery and the new LatLonShapePolygonQuery in LUCENE-8435.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933950/LUCENE-8440.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8435", "change_description": ": Add new LatLonShapePolygonQuery for querying indexed LatLonShape fields by arbitrary polygons", "change_title": "Add new LatLonShapePolygonQuery", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This feature will provide the ability to query indexed LatLonShape fields with an arbitrary polygon. Initial implementation will support INTERSECT queries only and future enhancements will add other relations (e.g., CONTAINS, WITHIN)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12933939/LUCENE-8435.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8367", "change_description": ": Make per-dimension drill down optional for each facet dimension", "change_title": "Make per-dimension drill down optional for each facet dimension", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "Today, when you index a FacetField with path foo/bar, we index two drill down terms onto the document: foo and foo/bar. But I suspect some users (like me!) don't need to drilldown just on foo (effectively \"find all documents that have any value for this facet dimension\"), so I added an option to FacetsConfig to let you specify per-dimension whether you need to drill down (defaults to true, matching current behavior). I also added hashCode and equals to the LongRange and DoubleRange classes in facets module, and improved CheckIndex a bit to print the total %deletions across the index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12929011/LUCENE-8367.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8396", "change_description": ": Add Points Based Shape Indexing and Search that decomposes shapes\ninto a triangular mesh and indexes individual triangles as a 6 dimension point", "change_title": "Add Points Based Shape Indexing", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I've been tinkering with this for a while and would like to solicit some feedback. I'd like to introduce a new shape field based on the BKD/Points codec to bring much of the Points based performance improvements to the shape indexing and search usecase. Much like the existing shape indexing in spatial-extras the shape will be decomposed into smaller parts, but instead of decomposing into quad cells (which have the drawback of precision accuracy and sheer volume of terms) I'd like to explore decomposing the shapes into a triangular mesh; similar to gaming and computer graphics. Not only does this approach reduce the number of terms, but it has the added benefit of better accuracy (precision is based on the index encoding technique instead of the spatial resolution of the quad cell). For better clarity, consider the following illustrations (of a polygon in a 1 degree x 1 degree spatial area).  The first is using the quad tree technique applied in the existing inverted index. The second is using a triangular mesh decomposition as used by popular OpenGL and javascript rendering systems (such as those used by mapbox).  Decomposing this shape using a quad tree results in 1,105,889 quad terms at 3 meter spatial resolution.  Decomposing using a triangular mesh results in 8 triangles at the same resolution as encodeLat/Lon. The decomposed triangles can then be encoded as a 6 dimensional POINT and queries are implemented using the computed relations against these triangles (similar to how its done with the inverted index today).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12931617/LUCENE-8396.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8345", "change_description": ", GitHub PR #392: Remove instantiation of redundant wrapper classes for primitives;\nadd wrapper class constructors to forbiddenapis.", "change_title": "Add wrapper class constructors to forbiddenapis", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "Wrapper classes for the Java primitives (Boolean, Byte, Short, Character, Integer, Long, Float, Double) have constructors which will always create new objects. These constructors are officially deprecated as of Java 9 and it is recommended to use the public static methods since these can reuse the same underlying objects. In 99% of cases we should be doing this, so these constructors should be added to forbiddenapis and code corrected to use autoboxing or call the static methods (.valueOf, .parse*) explicitly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8415", "change_description": ": Clean up Directory contracts and JavaDoc comments.", "change_title": "Clean up Directory contracts (write-once, no reads-before-write-completed)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Created a PR here for early review. https://github.com/apache/lucene-solr/pull/424 I changed: Currently a number of Directory implementations fail the testReadFileOpenForWrites test that I added, so I'll keep working on that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12932689/LUCENE-8415.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8414", "change_description": ": Make segmentInfos private in IndexWriter", "change_title": "CI fails TestIndexWriter#testSoftUpdateDocuments", "detail_type": "Bug", "detail_affect_versions": "7.5,8.0", "detail_fix_versions": "7.5,8.0", "detail_description": "Elastic CI found the following issue. I can reproduce this by mucking an unlucky schedule (see unlucky-schedule.patch).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12932767/LUCENE-8414.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8446", "change_description": ": The UnifiedHighlighter's DefaultPassageFormatter now treats overlapping matches in\nthe passage as merged (as if one larger match).", "change_title": "UnifiedHighlighter DefaultPassageFormatter should merge overlapping offsets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "The UnifiedHighlighter's DefaultPassageFormatter (mostly unchanged from the old PostingsHighlighter) will format overlapping matches by closing a tag and immediately opening a tag.  I think this is a bit ugly structurally and it ought to continue the tag is if the matches were merged.  This is extremely rare in practice today since a match is always a word, and thus we'd only see this behavior if multiple words at the same position of different offsets are highlighted.  The advent of matches representing phrases will increase the probability of this, and indeed was discovered while working on LUCENE-8286.  Additionally, and related, OffsetsEnums should internally be ordered by the end offset if the start offset is the same.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12934329/LUCENE-8446.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8460", "change_description": ": Better argument validation in StoredField.", "change_title": "Better argument validation in StoredField", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "I have found some invalid Javadocs in StoredField Class.  (and I think Field Class has some problems too )  1) Line 45 method It is misleading because there is no explanation for type.  If you follow that super class, you can see the following code(Field class). Field class has the exception handling(IllegalArgumentException) for null IndexableFieldType object.  For that reason, I changed the Javadoc to:  2) Line 59 method It is misleading because there is no explanation for bytes.  If you follow that super class, you can see the following code(Field class). Field class has the exception handling(IllegalArgumentException) for null BytesRef object.  For that reason, I changed the Javadoc to:  3) Line 71 method It is misleading because there is no explanation for byte array.  If you follow that super class, you can see the following code(Field class). When declaring a new BytesRef, an Illegal exception will be thrown if value is null. For that reason, I changed the Javadoc to:  4) Line 85 method For the same reason as \"3)\", I changed the Javadoc to:  5) Line 97 method For the same reason as \"2)\", I changed the Javadoc to:  6) Line 119 method It is misleading because there is no explanation for type.  If you follow that super class, you can see the following code(Field class). Field class has the exception handling(NPE) for null IndexableFieldType object.  (if type is null, NPE can be occured when run type.stored())  For that reason, I changed the Javadoc to:  7) Wrong Javadocs in Field Class. I saw the wrong \"NullPointerException\" throws in Javadoc. Line 176, 194 and 210 methods have a NPE throws in Javadoc. Line 176 and 194 methods call Line 210 method.  However, this method can not cause the NullPointerException.  If type is null, it is just the following code. In fact, I think the null check is missing.  But it's just my idea, so I can not decide whether to remove Javadoc's throws or insert a null check code. If it is decided, I will work on the related issue separately.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937463/LUCENE-8460.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8432", "change_description": ": TopFieldComparator stops comparing documents if the index is\nsorted, even if hits still need to be visited to compute the hit count.", "change_title": "Stop calling comparator even if early termination is not possible", "detail_type": "Improvement", "detail_affect_versions": "7.3", "detail_fix_versions": "7.5,8.0", "detail_description": "TopFieldCollector continues calling comparator.compareBottom even if result is known in advance due to document order when trackMaxScore or trackTotalHits is set. Comparator call is not very cheap because it can involve DV read from disk and all calls can be avoided after first non competitive segment document is reached. There is a patch and luceneutil report on wikimedium10m sorted by DayOfYear: Unfortunately, luceneutil shows regression on non index sort match sorting (HighTermMonthSort). I can't reproduce the regression on any real case, but I'm afraid my benchmarks isn't quite accurate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12936994/LUCENE-8432.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-8422", "change_description": ": IntervalQuery now returns useful Matches", "change_title": "Add Matches iteration to interval queries", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "Follow up to LUCENE-8404, we can now add Matches iteration to interval queries in the sandbox.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938141/LUCENE-8422.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Improvements", "change_id": "LUCENE-7862", "change_description": ": Store the real bounds of the leaf cells in the BKD index when the\n number of dimensions is bigger than 1. It improves performance when there is\n correlation between the dimensions, for example ranges.", "change_title": "Should BKD cells store their min/max packed values?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "The index of the BKD tree already allows to know lower and upper bounds of values in a given dimension. However the actual range of values might be more narrow than what the index tells us, especially if splitting on one dimension reduces the range of values in at least one other dimension. For instance this tends to be the case with range fields: since we enforce that lower bounds are less than upper bounds, splitting on one dimension will also affect the range of values in the other dimension. So I'm wondering whether we should store the actual range of values for each dimension in leaf blocks, this will hopefully allow to figure out that either none or all values match in a block without having to check them all.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938777/LUCENE-7862.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Build", "change_id": "LUCENE-5143", "change_description": ": Stop publishing KEYS file with each version, use topmost lucene/KEYS file only.\nThe buildAndPushRelease.py script validates that RM's PGP key is in the KEYS file.\nRemove unused 'copy-to-stage' and '-dist-keys' targets from ant build.", "change_title": "rm or formalize dealing with \"general\" KEYS files in our dist dir", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "At some point in the past, we started creating a snapshots of KEYS (taken from the auto-generated data from id.apache.org) in the release dir of each release... http://www.apache.org/dist/lucene/solr/4.4.0/KEYS http://www.apache.org/dist/lucene/java/4.4.0/KEYS http://archive.apache.org/dist/lucene/java/4.3.0/KEYS http://archive.apache.org/dist/lucene/solr/4.3.0/KEYS etc... But we also still have some \"general\" KEYS files... https://www.apache.org/dist/lucene/KEYS https://www.apache.org/dist/lucene/java/KEYS https://www.apache.org/dist/lucene/solr/KEYS ...which (as i discovered when i went to add my key to them today) are stale and don't seem to be getting updated. I vaguely remember someone (rmuir?) explaining to me at one point the reason we started creating a fresh copy of KEYS in each release dir, but i no longer remember what they said, and i can't find any mention of a reason in any of the release docs, or in any sort of comment in buildAndPushRelease.py we probably do one of the following:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12925754/LUCENE-5143.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8485", "change_description": ": Update randomizedtesting to version 2.6.4.", "change_title": "Update randomizedtesting to version 2.6.4", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "https://github.com/randomizedtesting/randomizedtesting/releases/tag/release%2F2.6.4 A few minor bug fixes that showed up, junit4.tempDir is no longer required (set by the framework for the forked JVM).", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8366", "change_description": ": Upgrade to ICU 62.1. Emoji handling now uses Unicode 11's\nExtended_Pictographic property.", "change_title": "upgrade to icu 62.1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "trunk,7.5", "detail_description": "This gives unicode 11 support. Also emoji tokenization is simpler and it gives a way to have better tokenization for emoji from the future.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12928560/LUCENE-8366.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8408", "change_description": ": original Highlighter:  Remove obsolete static AttributeFactory instance\nin TokenStreamFromTermVector.", "change_title": "Code cleanup - TokenStreamFromTermVector - ATTRIBUTE_FACTORY", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5", "detail_description": "At the top of TokenStreamFromTermVector: This is the default if super() was called with no-args from the constructor, so I believe this can go away.  CC dsmiley", "patch_link": "https://issues.apache.org/jira/secure/attachment/12932017/LUCENE-8408.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8420", "change_description": ": Upgrade OpenNLP to 1.9.0 so OpenNLP tool can read the new model format which 1.8.x\ncannot read. 1.9.0 can read the old format.", "change_title": "Upgrade OpenNLP to 1.9.0", "detail_type": "Task", "detail_affect_versions": "7.4", "detail_fix_versions": "7.5,8.0", "detail_description": "OpenNLP 1.9.0 generates new format model file which 1.8.x cannot read. 1.9.0 can read the previous format for back-compat.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12932533/LUCENE-8420.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8453", "change_description": ": Add documentation to analysis factories of Korean (Nori) analyzer\nmodule.", "change_title": "Add example settings to Korean analyzer components' javadocs", "detail_type": "Improvement", "detail_affect_versions": "7.4", "detail_fix_versions": "7.5,8.0", "detail_description": "Korean analyzer (nori) javadoc needs example schema settings. I'll create a patch.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8455", "change_description": ": Upgrade ECJ compiler to 4.6.1 in lucene/common-build.xml", "change_title": "Upgrade ECJ compiler to 4.6.1 in lucene/common-build.xml", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "While working on SOLR-12625 I tried to use a map initialization construct like this that works programmatically but fails precommit. Robert pointed out that we're using ECJ 4.4.1 compiler and suggested trying 4.6.1, which does the trick. I wanted to put the upgrade as a separate Jira for traceability so here it is. static final Map<TEST_TYPE, String> solrClassMap = Collections.unmodifiableMap(Stream.of(       new SimpleEntry<>(TEST_TYPE.TINT, \"solr.TrieIntField\"),       new SimpleEntry<>(TEST_TYPE.BOOL, \"solr.BoolField\"))  .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue))); ", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8456", "change_description": ": Upgrade Apache Commons Compress to v1.18", "change_title": "Upgrade Apache Commons Compress to v1.18", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "CVE-2018-11771: Apache Commons Compress 1.7 to 1.17 denial of service vulnerability Announcement: https://lists.apache.org/thread.html/3f01b7315c83156875741faa56263adaf104233c6b7028092896a62c@%3Cdev.commons.apache.org%3E", "patch_link": "https://issues.apache.org/jira/secure/attachment/12935890/LUCENE-8456.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-765", "change_description": ": Improved org.apache.lucene.index javadocs.", "change_title": "Index package level javadocs needs content", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "The org.apache.lucene.index package level javadocs are sorely lacking.  They should be updated to give a summary of the important classes, how indexing works, etc.  Maybe give an overview of how the different writers coordinate.  Links to file formats, information on the posting algorithm, etc. would be helpful. See the search package javadocs as a sample of the kind of info that could go here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12937681/LUCENE-765.patch", "patch_content": "none"}
{"library_version": "7.5.0", "change_type": "Other", "change_id": "LUCENE-8476", "change_description": ": Remove redundant nullity check and switch to optimized List.sort in the\nKorean's user dictionary.", "change_title": "Optimizations in UserDictionary (KoreanAnalyzer)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.5,8.0", "detail_description": "■ Bug fix 1) BufferedReader's close method is not called.  (Wrong check) If you look at the code above, there is no close() method for the \"br\" variable.  As I know, BufferedReader can cause a memory leak if the close method is not called.  So I changed the code below. I solved this problem with \"try-with-resources\" method available since Java 7.  ■ Optimizations 1) Change from Collections.sort to List.sort (UserDictionary constructor) List.sort in Java 8 is known to be faster than existing Collections.sort. (http://ankitsambyal.blogspot.com/2014/03/difference-between-listsort-and.html) So I changed the code below.  2) Remove unnecessary null check (UserDictionary constructor) Looking at this part of the code, A null check for lastToken is unnecessary.  Because the equals method of the String class internally performs a null check.  So I changed the code as below.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12938267/LUCENE-8476.patch", "patch_content": "none"}

{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-9322", "change_description": ",", "change_title": "Discussing a unified vectors format API", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Two different approximate nearest neighbor approaches are currently being developed, one based on HNSW (LUCENE-9004) and another based on coarse quantization (LUCENE-9136). Each prototype proposes to add a new format to handle vectors. In LUCENE-9136 we discussed the possibility of a unified API that could support both approaches. The two ANN strategies give different trade-offs in terms of speed, memory, and complexity, and it’s likely that we’ll want to support both. Vector search is also an active research area, and it would be great to be able to prototype and incorporate new approaches without introducing more formats. To me it seems like a good time to begin discussing a unified API. The prototype for coarse quantization (https://github.com/apache/lucene-solr/pull/1314) could be ready to commit soon (this depends on everyone's feedback of course). The approach is simple and shows solid search performance, as seen here. I think this API discussion is an important step in moving that implementation forward. The goals of the API would be", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-9855", "change_description": ",", "change_title": "Reconsider names for ANN related format and APIs", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "There is some discussion about the codec name for ann search. https://lists.apache.org/thread.html/r3a6fa29810a1e85779de72562169e72d927d5a5dd2f9ea97705b8b2e%40%3Cdev.lucene.apache.org%3E Main points here are 1) use plural form for consistency, and 2) use more specific name for ann search (second point could be optional). A few alternatives were proposed:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-9004", "change_description": ",", "change_title": "Approximate nearest vector search", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "\"Semantic\" search based on machine-learned vector \"embeddings\" representing terms, queries and documents is becoming a must-have feature for a modern search engine. SOLR-12890 is exploring various approaches to this, including providing vector-based scoring functions. This is a spinoff issue from that. The idea here is to explore approximate nearest-neighbor search. Researchers have found an approach based on navigating a graph that partially encodes the nearest neighbor relation at multiple scales can provide accuracy > 95% (as compared to exact nearest neighbor calculations) at a reasonable cost. This issue will explore implementing HNSW (hierarchical navigable small-world) graphs for the purpose of approximate nearest vector search (often referred to as KNN or k-nearest-neighbor search). At a high level the way this algorithm works is this. First assume you have a graph that has a partial encoding of the nearest neighbor relation, with some short and some long-distance links. If this graph is built in the right way (has the hierarchical navigable small world property), then you can efficiently traverse it to find nearest neighbors (approximately) in log N time where N is the number of nodes in the graph. I believe this idea was pioneered in  [1]. The great insight in that paper is that if you use the graph search algorithm to find the K nearest neighbors of a new document while indexing, and then link those neighbors (undirectedly, ie both ways) to the new document, then the graph that emerges will have the desired properties. The implementation I propose for Lucene is as follows. We need two new data structures to encode the vectors and the graph. We can encode vectors using a light wrapper around BinaryDocValues (we also want to encode the vector dimension and have efficient conversion from bytes to floats). For the graph we can use SortedNumericDocValues where the values we encode are the docids of the related documents. Encoding the interdocument relations using docids directly will make it relatively fast to traverse the graph since we won't need to lookup through an id-field indirection. This choice limits us to building a graph-per-segment since it would be impractical to maintain a global graph for the whole index in the face of segment merges. However graph-per-segment is a very natural at search time - we can traverse each segments' graph independently and merge results as we do today for term-based search. At index time, however, merging graphs is somewhat challenging. While indexing we build a graph incrementally, performing searches to construct links among neighbors. When merging segments we must construct a new graph containing elements of all the merged segments. Ideally we would somehow preserve the work done when building the initial graphs, but at least as a start I'd propose we construct a new graph from scratch when merging. The process is going to be  limited, at least initially, to graphs that can fit in RAM since we require random access to the entire graph while constructing it: In order to add links bidirectionally we must continually update existing documents. I think we want to express this API to users as a single joint KnnGraphField abstraction that joins together the vectors and the graph as a single joint field type. Mostly it just looks like a vector-valued field, but has this graph attached to it. I'll push a branch with my POC and would love to hear comments. It has many nocommits, basic design is not really set, there is no Query implementation and no integration iwth IndexSearcher, but it does work by some measure using a standalone test class. I've tested with uniform random vectors and on my laptop indexed 10K documents in around 10 seconds and searched them at 95% recall (compared with exact nearest-neighbor baseline) at around 250 QPS. I haven't made any attempt to use multithreaded search for this, but it is amenable to per-segment concurrency. [1] https://www.semanticscholar.org/paper/Efficient-and-robust-approximate-nearest-neighbor-Malkov-Yashunin/699a2e3b653c69aff5cf7a9923793b974f8ca164  UPDATES:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-10040", "change_description": ",", "change_title": "Handle deletions in nearest vector search", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Currently nearest vector search doesn't account for deleted documents. Even if a document is not in LeafReader#getLiveDocs, it could still be returned from LeafReader#searchNearestVectors. This seems like it'd be surprising + difficult for users, since other search APIs account for deleted docs. We've discussed extending the search logic to take a parameter like Bits liveDocs. This issue discusses options around adding support. One approach is to just filter out deleted docs after running the KNN search. This behavior seems hard to work with as a user: fewer than k docs might come back from your KNN search! Alternatively, LeafReader#searchNearestVectors could always return the k nearest undeleted docs. To implement this, HNSW could omit deleted docs while assembling its candidate list. It would traverse further into the graph, visiting more nodes to ensure it gathers the required candidates. (Note deleted docs would still be visited/ traversed). The hnswlib library contains an implementation like this, where you can mark documents as deleted and they're skipped during search. This approach seems reasonable to me, but there are some challenges: Background links:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-9659", "change_description": ": SpanPayloadCheckQuery now supports inequalities.", "change_title": "Support inequality operations in payload check queries", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This is a ticket broken out from https://issues.apache.org/jira/browse/SOLR-14787 The patch will extend the SpanPayloadCheck query to support inequality checks to see if the term and payload should match.  Currently, this query operator only supports equals as the payload check.  This ticket introduces gt,gte,lt,lte and eq operations to support testing if a payload is greater than/less than a specified reference payload value.  One such use case is to have a label on a document with a confidence level stored as a payload.  This patch will support searching for the term where a confidence level is above a given threshold. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-9589", "change_description": ": Swedish Minimal Stemmer", "change_title": "Swedish Minimal Stemmer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Swedish has a SwedishLightStemmer but lacks a Minimal stemmer that would only stem singular/plural.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-9313", "change_description": ": Add SerbianAnalyzer based on the snowball stemmer.", "change_title": "Analyzer for Serbian language based on Snowball stemmer", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "My colleague and myself developed snowball stemmer for Serbian language recently: https://github.com/snowballstem/snowball/blob/master/algorithms/serbian.sbl  https://snowballstem.org/algorithms/serbian/stemmer.html We have a Serbian Lucene analyzer developed on top of that stemmer, and we would like to make a contribution to Lucene.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13010212/LUCENE-9313.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-10095", "change_description": ": Add NepaliAnalyzer based on the snowball stemmer.", "change_title": "Nepali Analyzer", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "The snowball2 upgrade in our main branch added a Nepali Stemmer. Let's \"shrink-wrap\" this into an Analyzer: add stopwords, normalization, tests, etc.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-10096", "change_description": ": Add TamilAnalyzer based on the snowball stemmer.", "change_title": "Tamil Analyzer", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Similar to LUCENE-10095, let's \"shrink-wrap\" the new snowball stemmer into a proper Analyzer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "New Features", "change_id": "LUCENE-10102", "change_description": ": Add JapaneseCompletionFilter for Input Method-aware auto-completion", "change_title": "Add JapaneseCompletionFilter for Input Method-aware auto-completion", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Basic background information As you know, Japanese texts are written in Kanji (ideogram), Katakana, Hiragana (phonetic symbols), and their combination. Therefore it is desirable for intelligent auto-completion systems to treat various representations; one common practice we use is - translate all inputs to \"romanized form\" (https://en.wikipedia.org/wiki/Romanization_of_Japanese) then reduce the problem to simple Latin-alphabet string matching.  For example: if a word \"桜\" (surface form) is given, we first convert it to \"サクラ\" (reading form) then further translate it to \"sakura\" (romanized form) so that we can suggest an auto-complete keyword \"桜\" for an incomplete query \"さ\" or \"サ\" or  \"sa\".  The difficulties  A simplistic approach to implementing such romanization-based auto-completion is to use JapaneseReadingFormFilter (this has \"useRomaji\" option). Unfortunately, this off-the-shelf method doesn't work due not to its fault - but complex combinations of multiple romanization systems and IMEs (https://en.wikipedia.org/wiki/Input_method). It is a little difficult for me to explain their detailed specifications in English, but let me provide some examples. 1) Multiple romanization systems  There are three major romanization systems - modified Hepburn-shiki, Kunrei-shiki (Nihon-shiki) and Wāpuro shiki. JapaneseReadingFormFilter supports only modified Hepburn-shiki, so it isn't sufficient to cover all possible romanized forms.  e.g.; \"新橋\" can be translated into eight romanized forms (in theory) - \"sinbasi\", \"shinbasi\", \"sinnbasi\", \"shinnbasi\", \"sinbashi\", \"shinbashi\", \"sinnbashi\", and \"shinnbashi\". 2) interaction with Input Method  When querying, mid-IME composition strings will be sent to the search systems, and auto-complete systems should handle them (or, it may just ignore such inputs, but it hurts users' experience).   e.g.; \"会ｓｙ\" can be an input to an auto-completion system. If we have a method to translate it to \"kaisy\", we can suggest \"会社\" (kaisya).  Solution  I implemented a token filter (and added an analyzer for ease of use) that handles those two challenges. With this filter, we can utilize AnalysingSuggester for fast automaton-based auto-completion for Japanese.  (Though I acknowledged it contains some peculiar logic, I suppose those are required complexities for a tool that deals with the intricacy of natural language systems...)  Note", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "System Requirements", "change_id": "LUCENE-8738", "change_description": ": Move to Java 11 as minimum Java version.", "change_title": "Bump minimum Java version requirement to 11", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "See vote thread for reference: https://markmail.org/message/q6ubdycqscpl43aq.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12966575/LUCENE-8738-javadoc-locale-en-US.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8638", "change_description": ": Remove many deprecated methods and classes including FST.lookupByOutput(),\nLegacyBM25Similarity and Jaspell suggester.", "change_title": "Remove deprecated code in main", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "There are a number of deprecations in master that should be removed. This issue is to keep track of deprecations as a whole, some individual deprecations may require their own issues.  Work on this issue should be pushed to the `master-deprecations` branch on gitbox", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8982", "change_description": ": Separate out native code to another module to allow cpp\nbuild with gradle. This also changes the name of the native \"posix-support\"\nlibrary to LuceneNativeIO.", "change_title": "Make NativeUnixDirectory pure java now that direct IO is possible", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "NativeUnixDirectory is a Directory implementation that uses direct IO to write newly merged segments.  Direct IO bypasses the kernel's buffer cache and write cache, making merge writes \"invisible\" to the kernel, though the reads for merging the N segments are still going through the kernel. But today, NativeUnixDirectory uses a small JNI wrapper to access the O_DIRECT flag to open ... since JDK9 we can now pass that flag in pure java code, so we should now fix NativeUnixDirectory to not use JNI anymore. We should also run some more realistic benchmarks seeing if this option really helps nodes that are doing concurrent indexing (merging) and searching.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9562", "change_description": ": All binary analysis packages (and corresponding\nMaven artifacts) with names containing '-analyzers-' have been renamed\nto '-analysis-'.", "change_title": "Unify 'analysis' package with produced artifact names", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Lucene has 'analysis' module but its sub-modules produce 'lucene-analyzers-*' artifacts. This inconsistency is currently handled by setting artifact names manually: but I keep wondering if we should just make it one or the other - either rename 'analysis' to 'analyzers' or produce 'lucene-analysis-' artifacts. My personal opinion is to produce 'lucene-analysis-' packages because this keeps repository structure the same (backports will be easier) and we're targeting a major release anyway so people can adjust dependency names when upgrading. This change would be also consistent with package naming inside those modules.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8474", "change_description": ": RAMDirectory and associated deprecated classes have been\nremoved.", "change_title": "Remove deprecated RAMDirectory", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Placeholder for the remainder of the original patch, removing all 8.x-deprecated RAMDirectory classes and replacing their use cases with ByteBuffersDirectory. This will have to wait until branch 8.x is officially cut. Local branch with RAMDirectory related classes removed (pending 8.x branch, will be updated then). https://github.com/dweiss/lucene-solr/tree/LUCENE-8438", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-3041", "change_description": ": The deprecated Weight#extractTerms() method has been\nremoved", "change_title": "Support Query Visting / Walking", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "8.1", "detail_description": "Out of the discussion in LUCENE-2868, it could be useful to add a generic Query Visitor / Walker that could be used for more advanced rewriting, optimizations or anything that requires state to be stored as each Query is visited. We could keep the interface very simple: and then use a reflection based visitor like Earwin suggested, which would allow implementators to provide visit methods for just Querys that they are interested in.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962584/LUCENE-3041-8x.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8805", "change_description": ": StoredFieldVisitor#stringField now takes a String rather than a\nbyte[] that stores the UTF-8 bytes of the stored string.", "change_title": "Parameter changes for stringField() in StoredFieldVisitor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "I wrote this patch after seeing the comments left by mikemccand when SortingStoredFieldsConsumer class was first created. I changed two things. 1) change binaryField() parameters from byte[] to BytesRef.  2) change stringField() parameters from byte[] to String. I also changed the related contents while doing the work.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969280/LUCENE-8805.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8811", "change_description": ": BooleanQuery#setMaxClauseCount() and #getMaxClauseCount() have\nmoved to IndexSearcher. The checks are now implemented using a QueryVisitor\nand apply to all queries, rather than only booleans.", "change_title": "Add maximum clause count check to IndexSearcher rather than BooleanQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Currently we only check whether boolean queries have too many clauses. However there are other ways that queries may have too many clauses, for instance if you have boolean queries that have themselves inner boolean queries. Could we use the new Query visitor API to move this check from BooleanQuery to IndexSearcher in order to make this check more consistent across queries? See for instance LUCENE-8810 where a rewrite rule caused the maximum clause count to be hit even though the total number of leaf queries remained the same.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12971571/LUCENE-8811.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8909", "change_description": ": The deprecated IndexWriter#getFieldNames() method has been removed.", "change_title": "Deprecate getFieldNames from IndexWriter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.3", "detail_description": "From SOLR-12368 Would be nice to be able to remove IndexWriter.getFieldNames as well, which was added in LUCENE-7659 only for this workaround. Once Solr task resolved, deprecate IndexWriter#getFieldNames from 8x and remove it from master", "patch_link": "https://issues.apache.org/jira/secure/attachment/12975078/LUCENE-8909.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8948", "change_description": ": Change \"name\" argument in ICU factories to \"form\". Here, \"form\" is\nnamed after \"Unicode Normalization Form\".", "change_title": "Change \"name\" argument in ICU factories to \"form\"", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "o.a.l.a.icu.ICUNormalizer2CharFilterFactory and o.a.l.a.icu.ICUNormalizer2FilterFactory have \"name\" arguments to specify Unicode Normalization Form. The \"name\" is vague and it causes problem with SOLR-13593. \"form\" would be suitable here instead of \"name\".", "patch_link": "https://issues.apache.org/jira/secure/attachment/12977236/LUCENE-8948.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8933", "change_description": ": Validate JapaneseTokenizer user dictionary entry.", "change_title": "JapaneseTokenizer creates Token objects with corrupt offsets", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.3", "detail_description": "An Elasticsearch user reported the following stack trace when parsing synonyms. It looks like the only reason why this might occur is if the offset of a org.apache.lucene.analysis.ja.Token is not within the expected range. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8905", "change_description": ": Better defence against malformed arguments in TopDocsCollector", "change_title": "TopDocsCollector Should Have Better Error Handling For Illegal Arguments", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "While writing some tests, I realised that TopDocsCollector does not behave well when illegal arguments are passed in (for eg, requesting more hits than the number of hits collected). Instead, we return a TopDocs instance with 0 hits.  This can be problematic when queries are being formed by applications. This can hide bugs where malformed queries return no hits and that is surfaced upstream to client applications.  I found a TODO at the relevant code space, so I believe it is time to fix the problem and throw an IllegalArgumentsException.  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9089", "change_description": ": FST Builder renamed FSTCompiler with fluent-style Builder.", "change_title": "FST.Builder with fluent-style constructor", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "A first step in a try to make the FST code easier to read and evolve. This step is just about the FST Builder constructor. By making it fluent, the many calls to it are simplified and it becomes easy to spot the intent and special param tuning. No functional change.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13001452/fix-fst-package-summary.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9212", "change_description": ": Deprecated Intervals.multiterm() methods that take a bare Automaton\nhave been removed", "change_title": "Intervals.multiterm() should take a CompiledAutomaton", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.5", "detail_description": "LUCENE-9028 added a `multiterm` factory method for intervals that accepts an arbitrary Automaton, and converts it internally into a CompiledAutomaton.  This isn't necessarily correct behaviour, however, because Automatons can be defined in both binary and unicode space, and there's no way of telling which it is when it comes to compiling them.  In particular, for automatons produced by FuzzyTermsEnum, we need to convert them to unicode before compilation. The `multiterm` factory should just take `CompiledAutomaton` directly, and we should deprecate the methods that take `Automaton` and remove in master.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9264", "change_description": ": SimpleFSDirectory has been removed in favor of NIOFSDirectory.", "change_title": "Remove SimpleFSDirectory in favor of NIOFsDirectory", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "SimpleFSDirectory looks to duplicate what's already offered by NIOFsDirectory. The only difference is that SimpleFSDirectory is using non-positional reads on the FileChannel (i.e., reads that are stateful, changing the current position), and SimpleFSDirectory therefore has to externally synchronize access to the read method. On Windows, positional reads are not supported, which is why FileChannel is already internally using synchronization to guarantee only access by one thread at a time for positional reads (see read(ByteBuffer dst, long position) in FileChannelImpl, and FileDispatcher.needsPositionLock, which returns true on Windows) and the JDK implementation for Windows is emulating positional reads by using non-positional ones, see http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/windows/native/sun/nio/ch/FileDispatcherImpl.c#l139. This means that on Windows, there should be no difference between NIOFsDirectory and SimpleFSDirectory in terms of performance (it should be equally poor as both implementations only allow one thread at a time to read). On Linux/Mac, NIOFsDirectory is superior to SimpleFSDirectory, however, as positional reads (pread) can be done concurrently. My proposal is to remove SimpleFSDirectory and replace its uses with NIOFsDirectory, given how similar these two directory implementations are (SimpleFSDirectory isn't really simpler).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9281", "change_description": ": Use java.util.ServiceLoader to load codec components and analysis\nfactories to be compatible with Java Module System. This allows to load factories\nwithout META-INF/service from a Java module exposing the factory in the module\ndescriptor. This breaks backwards compatibility as custom analysis factories\nmust now also implement the default constructor (see MIGRATE.md).", "change_title": "Retire SPIClassIterator from master because Java 9+ uses different mechanism to load services when module system is used", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "We currently have our own implementation of the service loader standard (SPI) fo several reasons: (1) In some older JDKs the order of classpath was not respected and this lead to wrong order of codecs implementing the same SPI name. This caused tests to sometimes use wrong class (we had this in Lucene 4 where we had a test-only read/write Lucene3 codec that was listed before the read-only one). That's no longer an issue, the order of loading does not matter. In addition, Java now does everything correct. (2) In Analysis, we require SPI classes to have a constructor taking args (a Map of params in our case). We also extract the NAME from a static field. Standard service loader does not support this, it tries to instantiate the class with default ctor. With Java 9+, the ServiceLoader now has a stream() method that allows to filter and preprocess classes: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html#stream() This allows us to use the new interface and just get the loaded class (which may come from module-info.class or a conventional SPI file): https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.Provider.html#type() This change allows us to convert Lucene to modules listing all SPIs in the module-info.java.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9307", "change_description": ": BufferedIndexInput#setBufferSize has been removed.", "change_title": "Remove the ability to set the buffer size on an existing BufferedIndexInput", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This feature is only used as an optimization when reading skip lists. Since our default directory doesn't use buffering, I'd suggest removing it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9340", "change_description": ": SimpleBindings#add(SortField) has been removed.", "change_title": "Deprecate and remove the SimpleBindings.add(SortField) method", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.6", "detail_description": "This method is trappy, in that it only works for certain types of SortField and you only find out which at runtime.  We should deprecate it and encourage users to pass an equivalent DoubleValuesSource instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9462", "change_description": ": Fields without positions should still return MatchIterator.", "change_title": "Fields without positions should still return MatchIterator", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9516", "change_description": ": Removed the ability to replace the IndexingChain / DocConsumer\nin Lucenes IndexWriter. The interface is not sufficient to efficiently\nreplace the functionality with reasonable efforts.", "change_title": "Remove DocConsumer and IndexingChain from Lucene", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Disclaimer: This is a breaking change! We allow today to replace the entire indexing chain which is a fundamental part of our software. I personally don't know if there are any users of this API. but given the complexity I personally don't think we should further support this. If you are willing to implement this entire thing yourself I really wonder if you are better off building lucene from the source. An option like this on IWC might make users look into it while I am convinced they shouldn't. It's too complex and nothing is made for reuse down there. I wonder what others think but from my perspective it's time to remove it in 9.0", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9317", "change_description": "", "change_title": "Resolve package name conflicts for StandardAnalyzer to allow Java module system support", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": " To allow Lucene to be modularised there are a few preparatory tasks to be completed prior to this being possible.  The Java module system requires that jars do not use the same package name in different jars.  The lucene-core and lucene-analyzers-common both share the package org.apache.lucene.analysis.standard. Possible resolutions to this issue are discussed by Uwe on the mailing list here:  http://mail-archives.apache.org/mod_mbox/lucene-dev/202004.mbox/%3CCAM21Rt8FHOq_JeUSELhsQJH0uN0eKBgduBQX4fQKxbs49TLqzA%40mail.gmail.com%3E???? About StandardAnalyzer: Unfortunately I aggressively complained a while back when Mike McCandless wanted to move standard analyzer out of the analysis package into core (“for convenience”). This was a bad step, and IMHO we should revert that or completely rename the packages and everything. The problem here is: As the analysis services are only part of lucene-analyzers, we had to leave the factory classes there, but move the implementation classes in core. The package has to be the same. The only way around that is to move the analysis factory framework also to core (I would not be against that). This would include all factory base classes and the service loading stuff. Then we can move standard analyzer and some of the filters/tokenizers including their factories to core an that problem would be solved. There are two options here, either move factory framework into core or revert StandardAnalyzer back to lucene-analyzers.  In the email, the solution lands on reverting back as per the task list: Add some preparatory issues to cleanup class hierarchy: Move Analysis SPI to core / remove StandardAnalyzer and related classes out of core back to anaysis    ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9318", "change_description": "", "change_title": "Fix Codec API to not rely on package-private classes as part of changes to support java module system", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "To allow Lucene to be modularised with Java module system there are a few preparatory tasks to be completed prior to this being possible. These are detailed by Uwe on the mailing list here: http://mail-archives.apache.org/mod_mbox/lucene-dev/202004.mbox/%3c0a5e01d60ff2$563f9c80$02bed580$@thetaphi.de%3e  This task is: Fix Codec API to not rely on package-private classes, so we can have a completely public API with abstract classes for codecs, so stuff in backwards-codecs does not need to have access to package private stuff in core. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9319", "change_description": "", "change_title": "Clean up Sandbox project by retiring/delete functionality or move it to Lucene core", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "To allow Lucene to be modularised with Java module system there are a few preparatory tasks to be completed prior to this being possible. These are detailed by Uwe on the mailing list here: http://mail-archives.apache.org/mod_mbox/lucene-dev/202004.mbox/%3c0a5e01d60ff2$563f9c80$02bed580$@thetaphi.de%3e  The lucene-sandbox currently shares package names with lucene-core which is not allowed in the Java module system.  There are two ways to deal with this. Either prefix all packages with \"sandbox\" or retire the lucene-sandbox all together. As per the email: Cleanup sandbox to prefix all classes there with “sandbox” package and where needed remove package-private access. If it’s needed for internal access, WTF: Just move the stuff to core! We have a new version 9.0, so either retire/delete Sandbox stuff or make it part of Lucene core. The suggested way forward is to move sandbox code to core. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9558", "change_description": "", "change_title": "Clean up package name conflicts for analyzers-icu module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "analyzers-icu module shares the package names o.a.l.collation and o.a.l.collation.tokenattributes with analyzers-common; they need to be renamed. There could be two solutions: 1. rename \"o.a.l.collation\" to \"o.a.l.a.icu.collation\"  2. move classes under \"o.a.l.collation\" to \"o.a.l.a.icu\" and classes under \"o.a.l.collation.tokenattributes\" to \"o.a.l.a.icu.tokenattributes\", and delete \"o.a.l.collation\" from analyzers-icu I would prefer option 2. 1. may complicate the package hierarchy and there already exist o.a.l.a.icu.tokenattributes. (All classes under \"o.a.l.collation\" have prefix \"ICUCollation\", so I think we don't need to keep \"collation\" in the package name, do we?)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9600", "change_description": "", "change_title": "Clean up package name conflicts for misc module", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "misc module shares the package names o.a.l.document, o.a.l.index, o.a.l.search, o.a.l.store, and o.a.l.util with lucene-core. Those should be moved under o.a.l.misc (or some classed should be moved to core?).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9646", "change_description": ": Set BM25Similarity discountOverlaps via the constructor", "change_title": "Set BM25Similarity discountOverlaps via the constructor", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "BM25Similarity discountOverlaps parameter is true by default. It can be set with org.apache.lucene.search.similarities.BM25Similarity#setDiscountOverlaps method. But this method makes BM25Similarity mutable.  discountOverlaps should be set via the constructor and setDiscountOverlaps method should be removed to make BM25Similarity immutable.  PR https://github.com/apache/lucene-solr/pull/2161", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9480", "change_description": ": Make DataInput's skipBytes(long) abstract as the implementation was not performant.\nIndexInput's api is unaffected: skipBytes() is implemented via seek().", "change_title": "investigate slow DataInput.skipBytes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Currently DataInput has skipBytes(), but IndexInput also adds seek(). There isn't a clear reason about the differences in the two methods: why would you choose one over the other? It causes some performance issues: for example the default implementation actually reads bytes into a byte array and throws everything away. This is really silly for MMapDirectory: skipping bytes should only be a glorified +=. So when I look at latest LUCENE-9447 patch, I can't help but think a ton of waste is happening:", "patch_link": "https://issues.apache.org/jira/secure/attachment/13020763/LUCENE-9480.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9796", "change_description": ": SortedDocValues no longer extends BinaryDocValues, as binaryValue() was not performant.\nSee MIGRATE.md for details.", "change_title": "fix SortedDocValues to no longer extend BinaryDocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "SortedDocValues give ordinals and a way to derefence ordinal as a byte[] But currently they extend BinaryDocValues, which allows directly calling binaryValue(). This allows them to act as a \"slow\" BinaryDocValues, but it is a performance trap, especially now that terms bytes may be block-compressed (LUCENE-9663). I think this should be detangled to prevent performance traps like LUCENE-9795: SortedDocValues shouldn't have the trappy inherited binaryValue() method that implicitly derefs the ord for the doc, then the term bytes for the ord.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13020974/LUCENE-9796.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9853", "change_description": ": JapaneseAnalyzer should use CJKWidthCharFilter for full-width and half-width character normalization.", "change_title": "Use CJKWidthCharFilter as the default character normalizer for JapaneseAnalyzer instead of CJKWidthFilter", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Follow-up issue of LUCENE-9413. We now have CJKWidthCharFilter in analyzers-common. I believe in many situations it is recommended applying half-width/full-width character normalization before tokenization for consistency in analysis. The change slightly affects on the analyzer's outputs. We can provide a parameter to switch back to CJKWidthFilter for backward compatibility.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9387", "change_description": ": Removed CodecReader#ramBytesUsed.", "change_title": "Remove RAM accounting from LeafReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Context for this issue can be found at https://lists.apache.org/thread.html/r06b6a63d8689778bbc2736ec7e4e39bf89ae6973c19f2ec6247690fd%40%3Cdev.lucene.apache.org%3E. RAM accounting made sense when readers used lots of memory. E.g. when norms were on heap, we could return memory usage of the norms array and memory estimates would be very close to actual memory usage. However nowadays, readers consume very little memory, so RAM accounting has become less valuable. Furthermore providing good estimates has become incredibly complex as we can no longer focus on a couple main contributors to memory usage, but would need to start considering things that we historically ignored, such as field infos, segment infos, NIOFS buffers, etc. Let's remove RAM accounting from LeafReader?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9334", "change_description": ": Require consistency between data-structures on a per-field basis.\nA field across all documents within an index must be indexed with the same index\noptions and data-structures. As a consequence of this, doc values updates are\nonly applicable for fields that are indexed with doc values only.", "change_title": "Require consistency between data-structures on a per-field basis", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Follow-up of https://lists.apache.org/thread.html/r747de568afd7502008c45783b74cc3aeb31dab8aa60fcafaf65d5431%40%3Cdev.lucene.apache.org%3E. We would like to start requiring consitency across data-structures on a per-field basis in order to make it easier to do the right thing by default: range queries can run faster if doc values are enabled, sorted queries can run faster if points by indexed, etc. This would be a big change, so it should be rolled out in a major. Strict validation is tricky to implement, but we should still implement best-effort validation:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9047", "change_description": ": Directory API is now little endian.", "change_title": "Directory APIs should be little endian", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "We started discussing this on LUCENE-9027. It's a shame that we need to keep reversing the order of bytes all the time because our APIs are big endian while the vast majority of architectures are little endian.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9948", "change_description": ": No longer require the user to specify whether-or-not a field is multi-valued in\nLongValueFacetCounts (detect automatically based on what is indexed).", "change_title": "Automatically detect multi- vs. single-valued cases in LongValueFacetCounts", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "The public API in LongValueFacetCounts currently requires the user to specify whether-or-not a field being counted should be single- or multi-valued (i.e., is it NumericDocValues or SortedNumericDocValues). Since we can detect this automatically, it seems unnecessary to ask users to specify. Let's consider updating the implementation to auto-detect these cases and deprecate the ctors that allow explicit specification. This is a spin-off issue from LUCENE-9946.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9843", "change_description": ": Remove compression option on default codec's docvalues.", "change_title": "Remove compression option on doc values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Options on file formats add complexity and put a big tax on backward-compatibility testing. I'm the one who introduced it LUCENE-9378 but I would now like to think about what we can do to remove this option. For the record, compression was initially introduced because some binary fields have so much redundancy that it's wasteful not to compress them at all. But unfortunately, this slowed down some search workloads and we decided to introduce this option as a way to let users choose the trade-off they want.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13025041/LUCENE-9843.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9204", "change_description": ": SpanQuery and its subclasses have been moved from core/ into the\nqueries/ module.", "change_title": "Move span queries to the queries module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "We have a slightly odd situation currently, with two parallel query structures for building complex positional queries: the long-standing span queries, in core; and interval queries, in the queries module.  Given that interval queries solve at least some of the problems we've had with Spans, I think we should be pushing users more towards these implementations.  It's counter-intuitive to do that when Spans are in core though.  I've opened this issue to discuss moving the spans package as a whole to the queries module.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9454", "change_description": ": Analyzer no longer has a mutable version field.", "change_title": "Upgrade hamcrest to version 2.2", "detail_type": "Task", "detail_affect_versions": "9.0", "detail_fix_versions": "None", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9956", "change_description": ": Expose the getBaseQuery, getDrillDownQueries APIs from DrillDownQuery", "change_title": "Make getBaseQuery API from DrillDownQuery public", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "It would be great if users could access the baseQuery of a DrillDownQuery. I think this can be useful for folks who want to access/test the clauses of a BooleanQuery (for example) after they've already wrapped it into a DrillDownQuery. Currently the Query getBaseQuery() method is package private by default. If this proposed change does not make sense, or if this change breaks the semantic of the class, I am happy to explore other ways of doing this!  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-8143", "change_description": ": SpanBoostQuery has been removed.", "change_title": "Remove SpanBoostQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "I initially added it so that span queries could still be boosted, but this was actually a mistake: boosts are ignored on inner span queries, only the boost of the top-level span query, the one that performs scoring, is not ignored.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9998", "change_description": ": Remove unused parameter fis in StoredFieldsWriter.finish() and TermVectorsWriter.finish(),\nincluding those subclasses.", "change_title": "The param 'fis' in StoredFieldsWriter.finish(FieldInfos fis, int numDocs) is never used", "detail_type": "Improvement", "detail_affect_versions": "8.6.2", "detail_fix_versions": "9.0", "detail_description": "We can see the `FieldInfos fis` in StoredFieldsWriter.finish(FieldInfos fis, int numDocs) is never used. The class has four subclasses: Lucene90CompressingStoredFieldsWriter,Lucene50CompressingStoredFieldsWriter,CrankyStoredFieldsWriter,SimpleTextStoredFieldsWriter.  the function `finish` in all four subclasses never use `fis`, if we could remove it from the function. The same situation with the function  `finish` in TermVectorsWriter.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-7020", "change_description": ": TieredMergePolicy#setMaxMergeAtOnceExplicit has been removed.\nTieredMergePolicy no longer sets a limit on the maximum number of segments\nthat can be merged at once via a forced merge.", "change_title": "TieredMergePolicy - cascade maxMergeAtOnce setting to maxMergeAtOnceExplicit", "detail_type": "Improvement", "detail_affect_versions": "5.4.1", "detail_fix_versions": "8.10", "detail_description": "SOLR-8621 covers improvements in configuring a merge policy in Solr. Discussions on that issue brought up the fact that if large values are configured for maxMergeAtOnce and segmentsPerTier, but maxMergeAtOnceExplicit is not changed, then doing a forceMerge is likely to not work as expected. When I first configured maxMergeAtOnce and segmentsPerTier to 35 in Solr, I saw an optimize (forceMerge) fully rewrite most of the index twice in order to achieve a single segment, because there were approximately 80 segments in the index before the optimize, and maxMergeAtOnceExplicit defaults to 30.  On advice given via the solr-user mailing list, I configured maxMergeAtOnceExplicit to 105 and have not had that problem since. I propose that setting maxMergeAtOnce should also set maxMergeAtOnceExplicit to three times the new value – unless the setMaxMergeAtOnceExplicit method has been invoked, indicating that the user wishes to set that value themselves.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12787222/LUCENE-7020.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-10027", "change_description": ": Directory reader open API from indexCommit and leafSorter has been modified\nto add an extra parameter - minSupportedMajorVersion.", "change_title": "Custom order for leaves in DirectoryReader opened from IndexCommit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "This is a left-over from LUCENE-9507 to provide a leaf sorter also for directory readers opened from IndexCommit. This part was missed in  LUCENE-9507.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9620", "change_description": ": Added a (sometimes) faster implementation for IndexSearcher#count that relies on the new Weight#count API.\nThe Weight#count API represents a cleaner way for Query classes to optimize their counting method.", "change_title": "Add Weight#count(LeafReaderContext)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "We have IndexSearcher#count today, which tries to optimize counting for TermQuery and MatchAllDocsQuery, and falls back to BulkScorer + TotalHitCountCollector otherwise. I'm considering moving this to Weight instead, where it'd be a better place to add counting optimizations for other queries, e.g. pure disjunctions over single-valued fields or range queries on points. The default implementation could use a BulkScorer+TotalHitCountCollector like IndexSearcher#count does today.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-10089", "change_description": ": Add a method to SortField that allows to enable or disable numeric sort\noptimization to use the points index to skip over non-competitive documents,\nwhich is enabled by default from 9.0", "change_title": "Add a way to disable the sort optimizations to leverage points on numeric fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "jimczi romseygeek mayya and I were just looking at a case when it would be useful to be able to disable the sort optimization. Say you have a 8.x index that has a field that is indexed with IntPoint and NumericDocValuesField. This field is used for index sorting, and the SortField is created with SortField.Type.LONG. This was accepted in 8.x, but this is something that Lucene 9 would complain about: since the field is an integer, it should use SortField.Type.Int, not SortField.Type.LONG. If the field was not used for index sorting, then you could just switch to SortField.Type.INT and everything would work fine. However since the field is used for index sorting, if you switch to SortField.Type.INT, then index-time SortField objects are going to compare differently from search-time SortField objects, which in-turn will disable early termination of queries in TopFieldCollector. To be able to migrate from indices that fall in this scenario, it would be helpful to have an option that would disable the optimization to use points as well as the associated validation logic, similarly to what we do in 8.x to enable this optimization.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-10115", "change_description": ": Add an extension point, BaseQueryParser#getFuzzyDistance, to allow custom\nquery parsers to determine the similarity distance for fuzzy queries.", "change_title": "Add an extension point for custom query parsers to determine the similarity distance for fuzzy queries", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Add an extension point for custom query parsers to determine the similarity distance for fuzzy queries. This will be used by the elasticsearch query parser to customise how similarity distances are determined. This issue was filed in response to a question raise on the lucene-dev mailing list, see: https://lists.apache.org/thread.html/r831b11f918e4e7ead6f4a5fc7b28bec804ff449ae7d5acec9153c37a%40%3Cdev.lucene.apache.org%3E", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-10132", "change_description": ": Support addition of diagnostics by custom merge policies", "change_title": "Support addition of diagnostics by custom merge policies", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Allow custom merge policies to add additional diagnostics. This issue has been raised by a discussion on the lucene dev mailing list, see https://mail-archives.apache.org/mod_mbox/lucene-dev/202109.mbox/%3c2AF60548-2AD1-471D-B864-DBB09AB2A87C@elastic.co%3e", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9325", "change_description": ": Sort is now final, and the `setSort()` method has been removed", "change_title": "Sort and SortField are not immutable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "The `Sort` and `SortField` classes are currently mutable, which makes them dangerous to use in multiple threads.  In particular, you can set an index sort on an IndexWriterConfig and then change its internal sort fields while the index is being written to. We should make all member fields on these classes final, and in addition we should make `Sort` final itself.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-9431", "change_description": ": The UnifiedHighlighter's WEIGHT_MATCHES flag is now set by default, provided its\nrequirements are met.  It can be disabled via over-riding getFlags", "change_title": "UnifiedHighlighter: Make WEIGHT_MATCHES the default", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This mode uses Lucene's modern mechanism of exposing information that previously required complicated highlighting machinery.  It's also likely to generally work better out-of-the-box and with custom queries.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13034573/LUCENE-9431.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-10158", "change_description": ": Add a new interface Unwrappable to the utils package to allow code to\nunwrap wrappers/delegators that are added by Lucene's testing framework. This will allow\ntesting new MMapDirectory implementation based on JDK Project Panama.", "change_title": "Add a new interface Unwrappable to the utils package to ease migration to new MMAPDirectory and its testing", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "While creating the new MMapDirectory using Project Panama in the recent OpenJDK versions (not yet released, incubation only), I stumbled on our testing framework, which wraps many objects with AssertingXY. The problem with that is, mmap in project panama only works when the java.nio.files.Path is owned by the default file system provider. During testing we wrap it often with custom implementations emulating Windows or track open file handles. If you pass such a wrapped Path to the NIO2 Panama APIs, it will fail with exception, because it can't refer to file channel internal methods from it. In the final version of Panama this may go away and we can provide our own wrapper for memory mapping, but this is problematic with current testing. My plan is to release versions of MMapDirectory version 2 with different implementations of the Panama APIs for easy pluggin into Lucene, Solr, Elasticsearch by just adding a JAR file that fits your JDK version. To run tests, unfortunately the MMapDir impl must \"unrwap\" the Path wrappers added by the test system. To help with that and to make it more general, in my pull requests (e.g. https://github.com/apache/lucene/pull/177), I added a new interface org.apache.lucene.util.Unwrappable that allows to unwrap external code to get the \"original\" Path implementation. The same interface could be applied to many other Lucene/Test classes that needs unwrapping sometimes (e.g. around Directory or Queries), but for now it is only implemented for Test's FilterPath. The interface needs to be part of Lucene core and is used by production code to unwrap any test-framework FilterPath (or similar) wrappers. MMapDirectory version 2 uses it to get the original Path to be passed to MemorySegment.mapFile(). I'd like to get this into Lucene 9.0. It does not hurt, it is just an interface, which is implemented by test classes, ready for extension to other classes. It also provides the unwrapper method, which is generic.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "API Changes", "change_id": "LUCENE-10260", "change_description": ": LucenePackage class has been removed. The implementation string can be\nretrieved from Version.getPackageImplementationVersion().", "change_title": "Luke's about window no longer shows version number", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "This seems to be caused by the module change. It looks like the modularized (automodule) no longer can get the Implementation verison using class LuceneVersion (in root package).  We should fix this in case of respin, otherwise it is cosmetical only.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10234", "change_description": ": Added Automatic-Module-Name to all JARs. This is the first step to enable full Java\nmodule system (JMS) support in later Lucene versions. At the moment, the automatic names should\nnot be considered stable.", "change_title": "Add automatic module name to JAR manifests.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This is the first step to make Lucene a proper fit for the java module system. I chose a shorthand \"lucene.[x]\" module name convention, without the \"org.apache\" prefix.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10182", "change_description": ": TestRamUsageEstimator used RamUsageTester.sizeOf throughout, making some of the\ntests trivial. Now, it compares results from RamUsageEstimator with those from RamUsageTester.\nTo prevent this error in the future, RamUsageTester.sizeOf was renamed to ramUsed.", "change_title": "TestRamUsageEstimator asserts trivial equality", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "TestRamUsageEstimator.testStaticOverloads has serveral lines like: Both calls to sizeOf() fall back on RamUsageTester.sizeOf, making the 2 calls identical. Instead, we would want one of the calls to go to RamUsageEstimator.sizeOf.  This issue came up while working on LUCENE-10129. A possible solution, as per uschindler's suggestion, would be to remove the static import Instead, we could be explicit on which method we are calling, like: This could be replicated for other potentially confusing cases in the test class.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10129", "change_description": ": RamUsageEstimator overloads the shallowSizeOf method for primitive arrays\nto avoid falling back on shallowSizeOf(Object), which could lead to performance traps.", "change_title": "Add RamUsageEstimator shallowSizeOf(long[]) overload that just calls sizeOf(long[])?", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "See LUCENE-10128 for an example. The problem is there is only a sizeOf(long[]), so if the programmer uses shallowSizeOf instead of sizeOf then it falls back to shallowSizeOf(Object) which does a bunch of reflection. This is pretty crazy because it can create performance traps. Should we just add a shallowSizeOf(long[]) that calls sizeOf(long[]), so that things are fast? (same for other primitive arrays). It would solve the problem easily I think.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10139", "change_description": ": ExternalRefSorter returns a covariant with a subtype of BytesRefIterator\nthat is Closeable.", "change_title": "ExternalRefSorter should return a covariant with a subtype of BytesRefIterator  that is Closeable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This is a trivial cleanup to make ExternalRefSorter return a covariant subtype of BytesRefIterator  that is Closeable. This way callers of iterator() have a way of making sure I/O resources are closed without fully exhausting the iterator (in case of exceptions happening somewhere while scanning the byte refs, for example).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10135", "change_description": ": Correct passage selector behavior for long matching snippets", "change_title": "Correct passage selector behavior for long matching snippets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Passage selector can fail to provide a reasonable output on odd inputs such as very long matches and a small highlight window. It should properly truncate such highlights.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9960", "change_description": ": Avoid unnecessary top element replacement for equal elements in PriorityQueue.", "change_title": "Avoid unnecessary top element replacement for equal elements in PriorityQueue", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Currently the priority queue implementation always replaces the top (minimum) element, even if it is equal to the provided argument. This seems redundant. I've modified the condition and polished a few other minor things (unnecessary cast, size modification even if add throws AIOOB). Separately from the above, it is quite weird that a pq of size zero is allowed (and actually used!). The code is incorrect in this case, allowing add() to proceed and top() to return the added value. I understand it's a heavily used data structure but perhaps we should at least add an assertion to add() checking for zero-size?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9633", "change_description": ": Improve match highlighter behavior for degenerate intervals (on non-existing positions).", "change_title": "Improve match highlighter behavior for degenerate intervals (on non-existing positions)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Interval functions can produce match spans on non-existing or otherwise degenerate token positions. For example, would create an interval to the left and right of each term foo, regardless of whether such positions actually exist in the token stream. This issue improves match highlighter to still work in such cases. This is actually fun to play with  as you can highlight and visualize actual interval spans even for functions that expand or manipulate other sources' context.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9618", "change_description": ": Do not call IntervalIterator.nextInterval after NO_MORE_DOCS is returned.", "change_title": "Improve IntervalIterator.nextInterval's behavior/documentation/test", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I'm trying to play around with my own IntervalSource and found out that nextInterval method of IntervalIterator will be called sometimes even after nextDoc/ docID/ advance method returns NO_MORE_DOCS.     After I dug a bit more I found that FilteringIntervalIterator.reset is calling an inner iterator's nextInterval regardless of what the result of nextDoc, and also most (if not all) existing IntervalIterator's implementation do considered the case where nextInterval is called after nextDoc returns NO_MORE_DOCS.     We should probably update the javadoc and test if the behavior is necessary. Or we should change the current implementation to avoid this behavior  original email discussion thread: https://markmail.org/thread/aytal77bgzl2zafm", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9576", "change_description": ": Improve ConcurrentMergeScheduler settings by default, assuming modern I/O.\nPreviously Lucene was too conservative, jumping through hoops to detect if disks were SSD-backed.\nIn many common modern cases (VMs, RAID arrays, containers, encrypted mounts, non-Linux OS),\nthe pessimistic heuristics were wrong, resulting in slower indexing performance. Heuristics were\nalso complex and would trigger JDK issues even on unrelated mount points. Merge scheduler defaults\nare now modernized and the heuristics removed. Users with spinning disks that want to maximize I/O\nperformance should tweak ConcurrentMergeScheduler.", "change_title": "nuke SSD detection: was IndexWriter::commit() hangs when the server has a stale NFS mount.", "detail_type": "Bug", "detail_affect_versions": "8.5.2", "detail_fix_versions": "9.0", "detail_description": "Noticed IndexWriter::commit() hangs when the server has one or more stale NFS mounts. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13013608/LUCENE-9576.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9463", "change_description": ": Query match region retrieval component, passage scoring and formatting\nfor building custom highlighters.", "change_title": "Query match region retrieval component, passage scoring and formatting", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9370", "change_description": ": RegExp query is no longer lenient about inappropriate backslashes and\nfollows the Java Pattern policy for rejecting illegal syntax.", "change_title": "RegExpQuery should error for inappropriate use of \\ character in input", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "The RegExp class is too lenient in parsing user input which can confuse or mislead users and cause backwards compatibility issues as we enhance regex support. In normal regular expression syntax the backslash is used to: The leniency bug in RegExp is that it adds an extra rule to this list - any backslashed characters that don't satisfy the above rules are taken literally. For example, there's no reason to put a backslash in front of the letter \"p\" but we accept \\p as the letter p. Java's Pattern class will throw a parse exception given a meaningless backslash like \\p. We should too. In Lucene-9336 we added support for commonly supported regex expressions like `\\d`. Sadly this is a breaking change because of the leniency that has allowed \\d to be accepted as the letter d without an exception. Users were likely silently missing results they were hoping for and we made a BWC problem for ourselves in filling in the gaps. I propose we do like other RegEx parsers and error on inappropriate use of backslashes. This will be another breaking change so should target 9.0", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9336", "change_description": ": RegExp query now supports \\w \\W \\d \\D \\s \\S expressions.\nThis is a break with previous behaviour where these were (mis)interpreted\nas literally the characters w W d etc.", "change_title": "RegExp.java - add support for character classes like \\w", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Character classes commonly used in regular expressions like \\s for whitespace are not currently supported and may well be returning false negatives because they don't throw any \"unsupported\" errors.  The proposal is that the RegExp class add support for the set of character classes defined by Java's Pattern API I can work on a patch if we think this is something we want to consider.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-8757", "change_description": ": When provided with an ExecutorService to run queries across\nmultiple threads, IndexSearcher now groups small segments together, up to\n250k docs per slice.", "change_title": "Better Segment To Thread Mapping Algorithm", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "The current segments to threads allocation algorithm always allocates one thread per segment. This is detrimental to performance in case of skew in segment sizes since small segments also get their dedicated thread. This can lead to performance degradation due to context switching overheads.  A better algorithm which is cognizant of size skew would have better performance for realistic scenarios", "patch_link": "https://issues.apache.org/jira/secure/attachment/12969284/LUCENE-8757.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-8857", "change_description": ": Introduce Custom Tiebreakers in TopDocs.merge for tie breaking on\ndocs on equal scores. Also, remove the ability of TopDocs.merge to set shard\nindices", "change_title": "Refactor TopDocs#Merge To Take In Custom Tie Breakers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "In LUCENE-8829, the idea of having lambdas passed in to the API to allow finer control over the process was discussed. This JIRA tracks adding a parameter to the API which allows passing in lambdas to define custom tie breakers, thus allowing users to do custom algorithms when required. CC: jpountz simonw", "patch_link": "https://issues.apache.org/jira/secure/attachment/12972189/LUCENE-8857.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-8958", "change_description": ": Shared count early termination for relevance sorted indices", "change_title": "Add Shared Count Based Concurrent Early Termination For TopScoreDocCollector", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "LUCENE-8939 implements a shared count early termination collector manager for indices sorted by non relevance fields. This Jira tracks efforts for implementing the same for TopScoreDocCollector when the index is sorted by relevance", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-8937", "change_description": ": Avoid aggressive stemming on numbers in the FrenchMinimalStemmer.", "change_title": "Avoid agressive stemming on numbers in the FrenchMinimalStemmer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Here is the discussion on the mailing list : http://mail-archives.apache.org/mod_mbox/lucene-java-user/201907.mbox/browser The light stemmer removes the last character of a word if the last two  characters are identical.  We can see that here: https://github.com/apache/lucene-solr/blob/813ca77/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchLightStemmer.java#L263  In this light stemmer, there is a check to avoid altering the token if the  token is a number. The minimal stemmer also removes the last character of a word if the last  two characters are identical.  We can see that here: https://github.com/apache/lucene-solr/blob/813ca77/lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchMinimalStemmer.java#L77 But in this minimal stemmer there is no check to see if the character is a  letter or not.  So when we have numeric tokens with the last two characters identical they  are altered. For example \"1234567899\" will be stemmed as \"123456789\". It could be great of it's not altered. Here is the same issue for the LightStemmer : https://issues.apache.org/jira/browse/LUCENE-4063", "patch_link": "https://issues.apache.org/jira/secure/attachment/12976175/0001-LUCENE-8937-Avoid-agressive-stemming-on-numbers-in-t.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-8596", "change_description": ": Kuromoji user dictionary now accepts entries containing hash mark (#) that were\npreviously treated as beginning a line-ending comment", "change_title": "The replacement of comments is a bug, in \"UserDictionary.java\"", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "trunk", "detail_description": "https://github.com/apache/lucene-solr/blob/1d85cd783863f75cea133fb9c452302214165a4d/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java#L68  hi I think that this is bug. I think the following is correct  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9109", "change_description": ": Use StackWalker to implement TestSecurityManager's detection\nof JVM exit", "change_title": "Use Java 9+ StackWalker to implement TestSecurityManager's detection of JVM exit", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This is just a small improvement in Lucene/Solr master (Java 11) to detect exit of JVM in our test framework. There are other places in Lucene that use ineffective ways to inspect the stack trace. This one optimizes the implementation of TestSecurityManager#checkExit(status) to disallow all JVM exits outside of the official test runner by using StackWalker. In addition this needs no additional permissions, because we do not instruct StackWalker to fetch all crazy stuff like Class instances of stack elements. The way how this works is: Walk through stack trace: This can only be commited to master (9.0), as it requires Java 9.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9110", "change_description": ": Refactor stack analysis in tests to use generalized LuceneTestCase\nmethods that use StackWalker", "change_title": "Use StackWalker in tests instead of iterating through StackTraceElement arrays", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.5", "detail_description": "Followup of LUCENE-9109: There are a lot of tests (especially around IndexWriter) that look into stack traces to inject failures or check that some methods were called in their call stack. This issue will refactor all those tests by adding a few methods to LuceneTestCase that make it easy to verify if some method call/class is in stack trace. On master (Java 11) we can use StackWalker to do this checks, which has a speedup of sometimes >>2 times (depending on how deep you dive into call stack). There are a few tests (only 3) that do more complex stack trace analysis. Those should be refactored at some point. For now I added a deprecated method to get the whole StackTrace in Java 11, which is still 2 times faster than using an Exception. For branch 8.x i will apply the same patch, just the LuceneTestCase methods use the old \"Java 8\" way to inspect stack trace using the thread's stack trace (which is very expensive). So this issue is mainly about refactoring the tests to use a common method pattern to check the existence of stack frames. One important thing is: Using StackWalker makes sure that the stack is \"correct\". Stacks from Thread or Exception may miss some frames, as it does not deoptimize the code. So depending on JVMs and optimizations (e.g. Graal), call stacks may change if we still use old code for analysis. This is no longer an issue for Java 8, but may be in future.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9206", "change_description": ": IndexMergeTool gets additional options to control the merging.\nThis tool no longer forceMerge(1)s to a single segment by default. If you\nrely upon this behavior, pass -max-segments 1 instead.", "change_title": "improve IndexMergeTool", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This tool can have performance problems since it will only force merge the index down to one segment. Let's give it some better options and default behavior.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12992651/LUCENE-9206.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9220", "change_description": ": Upgrade snowball to 2.0. New snowball stemmers: Hindi, Indonesian,\nNepali, Serbian, and Tamil. New stoplist: Indonesian. Adds gradle 'snowball'\ntask to regenerate and ease future upgrades.", "change_title": "Upgrade Snowball version to 2.0", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "When working with Snowball-based stemmers, I realized that Lucene is currently using a pre-compiled version of Snowball, that seems from 12 years ago: https://github.com/snowballstem/snowball/tree/e103b5c257383ee94a96e7fc58cab3c567bf079b Snowball has just released v2.0 in 10/2019 with many improvements, new supported languages ( Arabic, Indonesian…) and new features ( stringdef notation for Unicode codepoints…). Details of the changes could be found here: https://github.com/snowballstem/snowball/blob/master/NEWS. I think these changes of Snowball could give a promising positive impact on Lucene. I wonder when Lucene should upgrade Snowball to the latest version ( v2.0).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12993567/snowball_53739a805cfa6c.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9354", "change_description": ": Improvements to snowball french stopwords list, so that it is less\naggressive.", "change_title": "Refresh French stop words to remove homonyms", "detail_type": "Improvement", "detail_affect_versions": "8.5.1", "detail_fix_versions": "trunk", "detail_description": "Sync French stop words with latest version from Snowball.  This new version removed some French homonyms from the list", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9114", "change_description": ": Improve ValueSourceScorer's Default Cost Implementation", "change_title": "Add FunctionValues.cost", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "The FunctionRangeQuery uses FunctionValues.getRangeScorer which returns a subclass of  ValueSourceScorer.  VSC's TwoPhaseIterator has a matchCost impl that returns a constant 100.  This is pretty terrible; the cost should vary based on the complexity of the ValueSource provided to FRQ.  ValueSource's are typically nested a number of levels, so they should aggregate. BTW there is a parallel concern for FunctionMatchQuery which works with DoubleValuesSource which doesn't have a cost either, and unsurprisingly there is a TPI with matchCost 100 there.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9074", "change_description": ": Introduce Slice Executor For Dynamic Runtime Execution Of Slices", "change_title": "Account for Executor's Queue Length When Planning Slices in IndexSearcher", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9280", "change_description": ": Add an ability for field comparators to skip non-competitive documents.\nCreating a TopFieldCollector with totalHitsThreshold less than Integer.MAX_VALUE\ninstructs Lucene to skip non-competitive documents whenever possible. For numeric\nsort fields the skipping functionality works when the same field is indexed both\nwith doc values and points. In this case, there is an assumption that the same data is\nstored in these points and doc values", "change_title": "Add ability to skip non-competitive documents on field sort", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.6", "detail_description": "Today collectors, once they collect enough docs, can instruct scorers to update their iterators to skip non-competitive documents. This is applicable only for a case when we need top docs by _score. It would be nice to also have an ability to skip non-competitive docs when we need top docs sorted by other fields different from _score.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9449", "change_description": ": Enhance DocComparator to provide an iterator over competitive\ndocuments when searching with \"after\". This iterator can quickly position\non the desired \"after\" document skipping all documents and segments before\n\"after\". Also redesign numeric comparators to provide skipping functionality\nby default.", "change_title": "Skip non-competitive documents when sort by _doc with search after", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "Enhance DocComparator to provide an iterator over competitive documents when search ing with \"after\" FieldDoc. This iterator can quickly position on the desired \"after\" document, and skip all documents or even segments that contain documents before \"after\" This is especially efficient when \"after\" is high.  Related to LUCENE-9280", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9527", "change_description": ": Upgrade javacc to 7.0.4, regenerate query parsers.", "change_title": "Upgrade javacc to 7.0.4", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9531", "change_description": ": Consolidated CharStream and FastCharStream classes: these have been moved\nfrom each query parser package to org.apache.lucene.queryparser.charstream", "change_title": "Consolidate duplicated generated classes CharStream and FastCharStream", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9450", "change_description": ": Use BinaryDocValues for the taxonomy index instead of StoredFields.\nAdd backwards compatibility tests for the taxonomy index.", "change_title": "Taxonomy index should use DocValues not StoredFields", "detail_type": "Improvement", "detail_affect_versions": "8.5.2", "detail_fix_versions": "9.0", "detail_description": "The taxonomy index that maps binning labels to ordinals was created before Lucene added BinaryDocValues. I've attached a WIP patch (does not pass tests currently) Issue suggested by mikemccand", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9605", "change_description": ": Update snowball to d8cf01ddf37a, adds Yiddish stemmer.", "change_title": "update snowball to latest (adds Yiddish stemmer)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "I'm trying to find time to upstream our snowball diffs... it helps to be reasonably up to date with their sources. Plus there is a new stemmer added.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-8982", "change_description": ": Make NativeUnixDirectory pure java with FileChannel direct IO flag,\nand rename to DirectIODirectory (Zach Chen, Uwe Schindler, Mike McCandless, Dawid Weiss).", "change_title": "Make NativeUnixDirectory pure java now that direct IO is possible", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "NativeUnixDirectory is a Directory implementation that uses direct IO to write newly merged segments.  Direct IO bypasses the kernel's buffer cache and write cache, making merge writes \"invisible\" to the kernel, though the reads for merging the N segments are still going through the kernel. But today, NativeUnixDirectory uses a small JNI wrapper to access the O_DIRECT flag to open ... since JDK9 we can now pass that flag in pure java code, so we should now fix NativeUnixDirectory to not use JNI anymore. We should also run some more realistic benchmarks seeing if this option really helps nodes that are doing concurrent indexing (merging) and searching.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9674", "change_description": ": Implement faster advance on VectorValues using binary search.", "change_title": "Faster advance on Vector Values", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "None", "detail_description": "The advance() function in the class Lucene90VectorReader does a linear search for the target document. To make it faster we can do a  binary search over the \"ordToDoc\" array which will make the advance operation take logarithmic time to search.This will make retrieving vectors for a sparse set of documents efficient.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9794", "change_description": ": Speed up implementations of DataInput.skipBytes().", "change_title": "Optimize skipBytes implementation in remaining DataInput subclasses", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "LUCENE-9480 introduced a more efficient method for byte skipping in IndexInput and its subclasses, but the rest of the DataInput implementations are still delegating to DataInput#skipBytesSlowly.  This issue tracks optimizing the remaining DataInput skipBytes implementations. Here's the list remaining:", "patch_link": "https://issues.apache.org/jira/secure/attachment/13021081/LUCENE-9794.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9898", "change_description": ": Removes no longer used scorePayload method from BM25Similarity", "change_title": "Remove no longer used scorePayload method from BM25Similarity", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "LUCENE-8038 introduced a new PayloadDecoder interface to score payloads. The scorePayload method in scorePayload is no longer called and should be removed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9850", "change_description": ": Switch to PFOR encoding for doc IDs (instead of FOR).", "change_title": "Explore PFOR for Doc ID delta encoding (instead of FOR)", "detail_type": "Task", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "It'd be interesting to explore using PFOR instead of FOR for doc ID encoding. Right now PFOR is used for positions, frequencies and payloads, but FOR is used for doc ID deltas. From a recent conversation on the dev mailing list, it sounds like this decision was made based on the optimization possible when expanding the deltas. I'd be interesting in measuring the index size reduction possible with switching to PFOR compared to the performance reduction we might see by no longer being able to apply the deltas in as optimal a way.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9929", "change_description": ": Add NorwegianNormalizationFilter, which does the same as ScandinavianNormalizationFilter except\nit does not fold oo->Ã¸ and ao->Ã¥.", "change_title": "NorwegianNormalizationFilter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "The ScandinavianNormalizationFilter applies foldings for aa, ao, ae, oe and oo. But all those five do not make sense for both Norwegian, Swedish and Danish. Implement a separate Norwegian variant, based on the Scandinavian: This would have the same rules as ScandinavianNormalizationFilter except it would not fold oo->ø and ao->å.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9535", "change_description": ": Improve DocumentsWriterPerThreadPool to prefer larger instances.", "change_title": "Investigate recent indexing slowdown for wikimedium documents", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "8.7", "detail_description": "Nightly benchmarks report a ~10% slowdown for 1kB documents as of September 9th: http://people.apache.org/~mikemccand/lucenebench/indexing.html. On that day, we added stored fields in DWPT accounting (LUCENE-9511), so I first thought this could be due to smaller flushed segments and more merging, but I still wonder whether there's something else. The benchmark runs with 8GB of heap, 2GB of RAM buffer and 36 indexing threads. So it's about 2GB/36 = 57MB of RAM buffer per thread in the worst-case scenario that all DWPTs get full at the same time. Stored fields account for about 0.7MB of memory, or 1% of the indexing buffer size. How can a 1% reduction of buffering capacity explain a 10% indexing slowdown? I looked into this further by running indexing benchmarks locally with 8 indexing threads and 128MB of indexing buffer memory, which would make this issue even more apparent if the smaller RAM buffer was the cause, but I'm not seeing a regression and actually I'm seeing similar number of flushes when I disabled memory accounting for stored fields. I ran indexing under a profiler to see whether something else could cause this slowdown, e.g. slow implementations of ramBytesUsed on stored fields writers, but nothing surprising showed up and the profile looked just like I would have expected. Another question I have is why the 4kB benchmark is not affected at all.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10000", "change_description": ": MultiCollectorManager now has parity with MultiCollector with respect to how it\nhandles CollectionTerminationException and setMinCompetitiveScore calls.", "change_title": "MultiCollectorManager should have parity with MultiCollector behavior", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "The behavior of MultiCollectorManager and MultiCollector differ with respect to how they handle CollectionTerminationException and minCompetitiveScore calls. In MultiCollector, remaining collectors will continue collecting when one or more have thrown a CollectionTerminationException, but MultiCollectorManager will allow the exception to propagate and stop collecting against all collectors as soon as one throws (see: LUCENE-6772). Also, MultiCollector properly handles setting of min competitive scores by ensuring the lowest set across all wrapped collectors is used, or by making it a no-op in cases where at least one collector is using ScoreMode.COMPLETE (see: LUCENE-9402). MultiCollectorManager will share the same Scorable across all collectors, which seems like the wrong thing to do. Finally, MultiCollector will cache scores when a Scorable is shared by more than one collector (see: LUCENE-6263). We should update MultiCollectorManager to use common behavior with MultiCollector in all these areas. It should be easy to do by delegating to MultiCollector within MultiCollectorManager.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10019", "change_description": ": Align file starts in CFS files to have proper alignment (8 bytes)", "change_title": "Align file starts in CFS files to have proper alignment (8 bytes)", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "While discussing about MMapDirectory and fast access to file contents through MMap (https://github.com/apache/lucene/pull/177 and previous versions of this draft, also), I figured out that for most Lucene files, the data inside is not aligned at all. We can't fix this easily and it's also not always important, but some files should really have a CPU fieldly alignment from beginning! This is escpecially important when we use slices(). I got many tests with aligned VarHandles to pass, but it broke instantly, if the file was inside a Compound CFS file. CompoundFormat.write() just appends all data to the IndexOutput and writes the offset to the entries file. The fix to make at least file starts aligned is to just write some null-bytes between the files, so startOffset is aligned to multiples of 8 bytes. At a later stage we could also think of aligning to LBA blocks/sectors/whatever to make OS paging work better. But for performance of index access, slices of compound files when memory mapped should at least align to 8 bytes. Fix is easy: Just add some modulo on startOffset and write some extra bytes before the next file is serialized. The change is only 2 lines. It does not even change index format! I'd like to get this in for 9.0 so we can at least say: our CFS files are aligned. Aligning other files like docvalues to better help CPU is then possible. I will provide a simple pull request for Lucene90CompoundFormat soon. If you don't see any problems, this is a no-brainer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9662", "change_description": ": Make CheckIndex concurrent by parallelizing index check across segments.", "change_title": "CheckIndex should be concurrent", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I am watching a nightly benchmark run slowly run its CheckIndex step, using a single core out of the 128 cores the box has. It seems like this is an embarrassingly parallel problem, if the index has multiple segments, and would finish much more quickly on concurrent hardware if we did \"thread per segment\". If wanted to get even further concurrency, each part of the Lucene index that is checked is also independent, so it could be \"thread per segment per part\".", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-9476", "change_description": ": Add new getBulkPath API to DirectoryTaxonomyReader to more efficiently retrieve FacetLabels for multiple\nfacet ordinals at once. This API is 2-4% faster than iteratively calling getPath.\nThe getPath API now throws an IAE instead of returning null if the ordinal is out of bounds.", "change_title": "Add a bulk ordinal->FacetLabel API", "detail_type": "Improvement", "detail_affect_versions": "8.6.1", "detail_fix_versions": "None", "detail_description": "This issue is a spillover from the PR for LUCENE 9450 The idea here is to share a single BinaryDocValues instance per leaf per query instead of creating a new one each time in the DirectoryTaxonomyReader. Suggested by mikemccand   ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10113", "change_description": ": Use VarHandles to access int/long/short primitive types in byte arrays.\nThis improves readability and performance of encoding/decoding of primitives to index\nfile format in input/output classes like DataInput / DataOutput and codecs.", "change_title": "Use VarHandles to access int/long/short types in byte arrays (e.g. ByteArrayDataInput)", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "LUCENE-10112 reminded me about something i wanted to do long ago: Basically for all IndexInputs/DataInputs we are able to natively read short, int, long using little endian with single CPU instructions (due to using ByteBuffer's methods that support primitive reads). Only ByteArrayDataInput still uses manual code beased on the the inherited byte-by-byte approach to read single bytes and combining the bytes using little endian. The approach here is to use Java 9+ VarHandles to allow reading int/long/short as single cpu instructions and not manually recombining the bytes. The trick is to make a \"view\" var handle which allows to access the byte array using the same mechanisms as ByteBuffers or JDK 17 MemorySegments (under the hood it uses Unsafe to use CPU instructions and optionally swap bytes if platform endianness is BE). In LUCENE-10112 there were similar stuff done with LZ4 and a microbenchmark was written that showed a significant speed improvement when accessing the types with VarHandle. P.S.: The same applies to FST.BytesReader and/or ByteSliceReader, but I am no sure if those use the int/short/long ones at all. At least this one does not override the methods to read ints, longs and shorts, so there is no optimization at all. FST seems to read bytes and byte[] only and ByteSliceReader mostly VInts.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10112", "change_description": ": Improve LZ4 Compression performance with direct primitive read/writes.", "change_title": "Improve LZ4 Compression performance with direct primitive read/writes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Summary Java9 introduced VarHandles as a tool to quickly read and write primitive types directly to byte arrays without bound checks. The LZ4 compressor class must consistently read ints from a byte array to analyze matches. The performance can be improved by reading these using a VarHandle. Additionally, the LZ4 compressor/decompressor methods currently individually read/write the bytes for LE shorts. Lucene's DataOutput/DataInput abstractions already have dedicated methods for reading/writing LE shorts. These methods are selectively optimized in certain implementations and will provide superior performance than individual byte reads. Concerns The DataOutput/DataInput readShort() and writeShort() methods do not call out that they are LE. It just looks to me that the DataOutput/DataInput are LE? Since this particular change does not appear to provide significant performance wins, maybe the patch is better leaving the explicit individual byte reads? Additionally, this patch changes read ints to read them in the platform native order which should be fine since it is just matching bytes. But I can change it to only read in the order the previous version did. Benchmarks I created JMH benchmarks which compresses 1MB of highly compressible JSON observability data. And compresses it 64KB at a time. In order to simulate the \"short\" changes, I use a forked version `ByteArrayDataOutput` which writes shorts using a VarHandle (to simulate fast writes that the ByteBuffer versions would get.) I also ran a benchmark without the short changes, just the reading ints using a VarHandle.   ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13033805/LUCENE-10112.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10125", "change_description": ": Optimize primitive writes in OutputStreamIndexOutput.", "change_title": "Investigate indexing throughput regression on NYC Taxis between 2021-04-12 and 2021-05-24", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "There's been a significant drop in indexing throughput between 2021-04-12 and 2021-05-24 on the NYC Taxis benchmark. Unfortunately several suspects have been merged during that period of time so we might need to git bisect to figure out which one is responsible for the regression. Interestingly the sorted index looks less affected than the non-sorted indexes. https://home.apache.org/~mikemccand/lucenebench/sparseResults.html#index_throughput", "patch_link": "https://issues.apache.org/jira/secure/attachment/13034199/LUCENE-10125_hack.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10143", "change_description": ": Delegate primitive writes in RateLimitedIndexOutput.", "change_title": "RateLimitedIndexOutput should delegate writeShort/writeInt/writeLong", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Otherwise merges are not taking advantage of LUCENE-10125. cc uschindler", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10145", "change_description": ",", "change_title": "Use VarHandles to speedup byte[] comparisons in some cases", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "I noticed that using Long#compareUnsigned on longs read via VarHandles is significantly faster than calling compareUnsigned directly. There are cases when this could be leveraged easily such as BKDWriter, which often keeps comparing 4-bytes or 8-bytes across byte[] arrays.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10153", "change_description": ",", "change_title": "More speedups for operations on byte[] via VarHandles", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "LUCENE-10145 leveraged VarHandles to speed up unsigned comparisons of byte[4] or byte[8]. But we could do more, such as speeding up the computation of common prefix lengths.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10201", "change_description": ": Spatial-Extras: Upgrading Spatial4j to 0.8 improving a varitety of minor things.\nSee release notes.", "change_title": "Upgrade Spatial4j to 0.8", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Spatial4j has been at 0.8 for some time.  We should upgrade. https://github.com/locationtech/spatial4j/releases/tag/spatial4j-0.8 ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "https://github.com/locationtech/spatial4j/releases/tag/spatial4j-0.8", "change_description": ": Spatial-Extras: Upgrading Spatial4j to 0.8 improving a varitety of minor things.\nSee release notes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Improvements", "change_id": "LUCENE-10062", "change_description": ": Switch taxonomy faceting to use numeric doc values for storing ordinals instead of binary doc values\nwith its own custom encoding.", "change_title": "Explore using SORTED_NUMERIC doc values to encode taxonomy ordinals for faceting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,10.0(main)", "detail_description": "We currently encode taxonomy ordinals using varint style packing in a binary doc values field. I suspect there have been a number of improvements to SortedNumericDocValues since taxonomy faceting was first introduced, and I plan to explore replacing the custom binary format we have today with a SORTED_NUMERIC type dv field instead. I'll report benchmark results and index size impact here.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9686", "change_description": ": Fix read past EOF handling in DirectIODirectory.", "change_title": "TestDirectIODirectory#testFloatsUnderflow can fail assertion", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Reproduction line: I didn't have the chance to look deeply, but it seems like the wrong exception type is being thrown:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-8663", "change_description": ": NRTCachingDirectory.slowFileExists may open a file while\nit's inaccessible.", "change_title": "NRTCachingDirectory.slowFileExists may open a file while it's inaccessible", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This build: https://builds.apache.org/job/Lucene-Solr-NightlyTests-master/1761/consoleText failed with the following stack trace: The problem in slowFileExists is that it probes for file existence by trying to open it in a directory. We don't allow opening files that are still open by another thread (for writing), hence the exception thrown is AccessDeniedException. I couldn't reproduce the problem with the same seed. Takes some luck to hit the right scenario in a multi-threaded run. The contract for Directory.fileLength is less strict so a straightforward patch is to use that instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956681/LUCENE-8663.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9117", "change_description": ": RamUsageEstimator hangs with AOT compilation. Removed any attempt to\nestimate Long.valueOf cache size.", "change_title": "RamUsageEstimator hangs with AOT compilation", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Mailing list report by Cleber Muramoto.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12990855/LUCENE-9117.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9290", "change_description": ": Don't assume that different XYPoint have different hash code", "change_title": "TestXYPoint#testEqualsAndHashCode test failure", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Test assumes that if two objects are different, then the has code must be different which is a wrong assumption.  Reproduce with:  Error: ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9372", "change_description": ": Fix paths for cygwin/msys before gradle wrapper jar lookup.", "change_title": "gradlew does not run on cygwin", "detail_type": "Bug", "detail_affect_versions": "8.5.1", "detail_fix_versions": "9.0", "detail_description": "The verification and downloading of the gradle wrapper jar added by LUCENE-9266 happens before classpaths are corrected for cygwin/msys. This causes the script to erroneously exit with a file not found error.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13003094/LUCENE-9372.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9365", "change_description": ": FuzzyQuery was missing matches when prefix length was equal to the term length", "change_title": "Fuzzy query has a false negative when prefix length == search term length", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "When using FuzzyQuery the search string `bba` does not match doc value `bbab` with an edit distance of 1 and prefix length of 3. In FuzzyQuery an automaton is created for the \"suffix\" part of the search string which in this case is an empty string. In this scenario maybe the FuzzyQuery should rewrite to a WildcardQuery of the following form : .. where there's an appropriate number of ? characters according to the edit distance.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9580", "change_description": ": Fix bug in the polygon tessellator when introducing collinear edges during polygon\nsplitting.", "change_title": "Tessellator failure for a certain polygon", "detail_type": "Bug", "detail_affect_versions": "8.5,8.6", "detail_fix_versions": "9.0", "detail_description": "This bug was discovered while using ElasticSearch (checked with versions 7.6.2 and 7.9.2). But I've created an isolated test case just for Lucene: https://github.com/apache/lucene-solr/pull/2006/files   The unit test fails with \"java.lang.IllegalArgumentException: Unable to Tessellate shape\".   The polygon contains two holes that share the same vertex and one more standalone hole. Removing any of them makes the unit test pass.    Changing the least significant digit in any coordinate of the \"common vertex\" in any of two first holes, so that these vertices become different in each hole - also makes unit test pass.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9930", "change_description": ": The Ukrainian analyzer was reloading its dictionary for every new\nTokenStreamComponents, which could lead to memory leaks.", "change_title": "UkrainianMorfologikAnalyzer reloads its Dictionary for every new TokenStreamComponents instance", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Large static data structures should be loaded in Analyzer constructors and shared between threads, but the UkrainianMorfologikAnalyzer is loading its dictionary in `createComponents`, which means it is reloaded and stored on every new analysis thread.  If you have a large dictionary and highly concurrent indexing then this can lead to you running out of memory as multiple copies of the dictionary are held in thread locals.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9940", "change_description": ": The order of disjuncts in DisjunctionMaxQuery does not matter\nfor equality checks", "change_title": "The order of disjuncts in DisjunctionMaxQuery affects equals() impl", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "DisjunctionMaxQuery stores its disjuncts in a java array, and its equals() implementation uses Arrays.equal() when checking equality.  This means that two queries with the same disjuncts but added in a different order will compare as different, even though their results will be identical.  We should replace the array with a Set.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9971", "change_description": ": Requesting facet counts for unseen dimensions in SortedSetDocValueFacetCounts and\nConcurrentSortedSetDocValueFacetCounts now returns null / -1 instead of throwing\nIllegalArgumentException as per Javadoc spec in Facets.", "change_title": "Inconsistent SSDVFF and Taxonomy facet behavior in case of unseen dimension", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Unlike other org.apache.lucene.facet.Facet interface implementations, SortedSetDocValuesFacetCounts and ConcurrentSortedSetDocValuesFacetCounts throw an exception in case of unseen dimension in the getTopChildren() method It is inconsistent with Facet interface documentation and not pleasant to deal with: in our case requested dimensions can be manually specified by the user, so we should always catch that and check the exception message to be sure of the exact cause. During the PR review, it was also decided to make similar changes for Taxonomy implementation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9823", "change_description": ": Prevent unsafe rewrites for SynonymQuery and CombinedFieldQuery. Before, rewriting\ncould slightly change the scoring when weights were specified.", "change_title": "SynonymQuery rewrite can change field boost calculation", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "SynonymQuery accepts a boost per term, which acts as a multiplier on the term frequency in the document. When rewriting a SynonymQuery with a single term, we create a BoostQuery wrapping a TermQuery. This changes the meaning of the boost: it now multiplies the final TermQuery score instead of multiplying the term frequency before it's passed to the score calculation. This is a small point, but maybe it's worth avoiding rewriting a single-term SynonymQuery unless the boost is 1.0. The same consideration affects CombinedFieldQuery in sandbox.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-10047", "change_description": ": Fix a value de-duping bug in LongValueFacetCounts and RangeFacetCounts", "change_title": "TestLongValueFacetCounts#testRandomMultiValued now failing", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Looks like this test is failing pretty consistently in nightly builds (e.g., https://jenkins.thetaphi.de/job/Lucene-main-Linux/31140/). I'll have a look.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-10101", "change_description": ",", "change_title": "getField vs getDeclaredField in analysis SPI", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Discussion between uschindler and romseygeek: > LUCENE-9281 moved the `lookupSPIName` method from > AbstractAnalysisFactory to AnalysisSPILoader; the method is mostly the same, > but one line has been changed from Class.getField() to Class.getDeclaredField(). > This can fall foul of the Security Manager, which wants a higher level of > permission for getDeclaredField.  Was this an intentional change? As I This was intentional because the previous code wasn't fully correct, because I had some safety check in mind: The main reason for the getDeclaredField() is to lookup the field only in this class; while getField() also looks into superclasses. E.g. if the superclass has a NAME field because of a programming error it would pick that up, which would be wrong. When investigating other implementations using \"named\" lookups out there (even in JDK), they used getDeclaredField() when accessing a static member. There are 2 solutions: Maybe also post your opinion about think fix #1 or fix #2 is better. I tend to go for fix #1. getDeclaredField() should theoretically be faster, but that won't matter here: If it goes the slow path (going up to superclass) it will fail anyways and that's the exceptional case. A correct factory should have a NAME field and its lookup is fast and the additional check introduced for the class is cheap. This is the issue to implement one of the solutions, preferably #1", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-9281", "change_description": ",", "change_title": "Retire SPIClassIterator from master because Java 9+ uses different mechanism to load services when module system is used", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "We currently have our own implementation of the service loader standard (SPI) fo several reasons: (1) In some older JDKs the order of classpath was not respected and this lead to wrong order of codecs implementing the same SPI name. This caused tests to sometimes use wrong class (we had this in Lucene 4 where we had a test-only read/write Lucene3 codec that was listed before the read-only one). That's no longer an issue, the order of loading does not matter. In addition, Java now does everything correct. (2) In Analysis, we require SPI classes to have a constructor taking args (a Map of params in our case). We also extract the NAME from a static field. Standard service loader does not support this, it tries to instantiate the class with default ctor. With Java 9+, the ServiceLoader now has a stream() method that allows to filter and preprocess classes: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html#stream() This allows us to use the new interface and just get the loaded class (which may come from module-info.class or a conventional SPI file): https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.Provider.html#type() This change allows us to convert Lucene to modules listing all SPIs in the module-info.java.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-10114", "change_description": ": Remove unused byte order mark in Lucene90PostingsWriter. This\nwas initially introduced by accident in Lucene 8.4.", "change_title": "Remove unused byte order mark in Lucene90PostingsWriter", "detail_type": "Task", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "While reviewing the byte order in lucene index, I found the following code in Lucene90PostingsWriter: Actually this byte is consumed nowhere, as the file is only used via seeking and the offsets are just 1 larger. We should remove this code. Why was this added?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Bug fixes", "change_id": "LUCENE-10140", "change_description": ": Fix cases where minimizing interval iterators could return\nincorrect matches", "change_title": "Minimizing intervals can give inaccurate positions for duplicate terms", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Minimizing intervals (maybe just ORDERED and AT_LEAST, but not sure) can move sub iterators to non-sub-match position inside match window, but CachingMatchesIterator logic relies on heuristic that any position inside matching interval is a sub-match. For example: ORDERED(\"a\", \"b\", \"a\") over \"a b a\" highlights (report sub-matches) only \"a <b>b</b> <b>a</b>\", and ORDERED(\"a\", \"b\", \"a\", \"b\", \"a\")  highlights only \"a b <b>a</b> <b>b</b> <b>a</b>\". Looks like there is no way to determine the right moment to cache from caching iterator perspective, so I propose to add an interface allowing minimizing IntervalIterators notify sub-sources positioned at sub-match positions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-9904", "change_description": ": regenerated UAX29URLEmailTokenizer and the corresponding analyzer with up-to-date top\nlevel domains. This may change the token sequence compared to previous Lucene versions.", "change_title": "Port GenerateJflexTLDMacros.java regeneration to gradle", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-9669", "change_description": ": DirectoryReader#open now accepts an argument to open indices created with versions\nolder than N-1. Lucene now can open indices created with a major version of N-2 in read-only mode.\nOpening an index created with a major version of N-2 with an IndexWriter is not supported.\nFurther does lucene only support file-format compatibilty which enables reading of old indices while\nsemantic changes like analysis or certain encoding on top of the file format are only supported on\na best effort basis.", "change_title": "N-2 compatibility for file formats", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Currently Lucene supports reading and writing indices that have been created with the current or previous (N-1) version of Lucene. Lucene refuses to open an index created by N-2 or earlier versions. I would like to propose that Lucene adds support for opening indices created by version N-2 in read-only mode. Here's what I have in mind: The reason I came up with these limitations is because I wanted to make the scope minimal in order to retain Lucene's ability to move forward. If there is consensus to move forward with this, I would like to target Lucene 9.0 with this change. This is a follow-up of the mailinglist thread here", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Changes in Backwards Compatibility Policy", "change_id": "LUCENE-10232", "change_description": ": Fix MultiRangeQuery to confirm all dimensions for a given range match.", "change_title": "MultiRangeQuery incorrectly matches docs that only match on a single dimension", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "When MultiRangeQuery iterates the multiple dimensions to see if any of them contain a point/range, it incorrectly short-circuits as soon as one dimension matches. It should instead confirm that all dimensions for that range match. Attached PR demonstrates the bug (in a test case) and includes a proposed fix.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Build", "change_id": "LUCENE-10198", "change_description": ":", "change_title": "Allow external JAVA_OPTS in gradlew scripts; use sane defaults (heap, stack and system proxies)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Build", "change_id": "LUCENE-10198", "change_description": ":", "change_title": "Allow external JAVA_OPTS in gradlew scripts; use sane defaults (heap, stack and system proxies)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Build", "change_id": "LUCENE-10163", "change_description": ": Move LICENSE and NOTICE files to top level to satisfy src artifact requirements", "change_title": "Review top-level *.txt and *.md files", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Some of them contain obsolete pointers and information (SYSTEM_REQUIREMENTS.md, etc.). Also, move the files that are distribution-specific (lucene/README.md) to the distribution project. Otherwise they give odd, incorrect information like:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-10122", "change_description": ": Use NumericDocValues to store taxonomy parent array", "change_title": "Explore using NumericDocValue to store taxonomy parent array", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,10.0(main)", "detail_description": "We currently use term position of a hardcoded term in a hardcoded field to represent the parent ordinal of each taxonomy label. That is an old way and perhaps could be dated back to the time where doc values didn't exist. We probably would want to use NumericDocValues instead given we have spent quite a lot of effort optimizing them.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-10136", "change_description": ": allow 'var' declarations in source code", "change_title": "Lift the restriction on using 'var' variables", "detail_type": "Wish", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Can we lift the restriction on using 'var' on the main branch? I know it's a double-edged sword and sometimes it leads to unreadable code, especially when you invoke a method, for example: but in many, many, many cases the var keyword shortens the code and the type is obvious from the context. This happens in loops, try-with-resources and local variables. I'd say - let's allow vars (and other language features) and use common sense. If something is not clear in the context, type the variable. If something is obvious, use shortcuts.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9570", "change_description": ",", "change_title": "Review code diffs after automatic formatting and correct problems before it is applied", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Review and correct all the javadocs before they're messed up by automatic formatting. Apply project-by-project, review diff, correct. Lots of diffs but it should be relatively quick. Reviewing diffs manually", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9564", "change_description": ",", "change_title": "Format code automatically and enforce it", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This is a trivial change but a bold move. And I'm sure it's not for everyone. I started using google java format [1] in my projects a while ago and have never looked back since. It is an oracle-style formatter (doesn't allow customizations or deviations from the defined 'ideal') - this takes some getting used to - but it also eliminates all the potential differences between IDEs, configs, etc.  And the formatted code typically looks much better than hand-edited one. It is also verifiable on precommit (so you can't commit code that deviates from what you'd get from automated formatting output). The biggest benefit I see is that refactorings become such a joy and keep the code neat, everywhere. Before you commit you just reformat everything automatically, no matter how much you messed it up. This isn't a change for everyone. I myself love hand-edited, neat code... but the reality is that with IDE support for automated code changes and so many people with different styles working on the same codebase keeping it neat is a big pain. Checkstyle and other tools are fine for ensuring certain rules but they don't take the burden of formatting off your shoulders. This tool does. Like I said - I had great reservations about using it at the beginning but over time got so used to it that I almost can't live without it now. It's like magic - you play with the code in any way you like, then run formatting and it's nice and neat. The downside is that automated formatting does imply potential merge problems in backward patches (or any currently existing branches). Like I said, it is a bold move. Just throwing this for your consideration. I've added a PR that adds spotless but it's not ready; some files would have to be excluded as they currently violate header rules. A more interesting thing is here where the current code is automatically reformatted - this branch is for eyeballing only. https://github.com/dweiss/lucene-solr/compare/LUCENE-9564...dweiss:LUCENE-9564-example [1] https://google.github.io/styleguide/javaguide.html", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9631", "change_description": ": Properly override slice() on subclasses of OffsetRange.", "change_title": "Properly override slice() on subclasses of OffsetRange", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Subclasses of OffsetRange should return a proper covariant on slice(); this ensures the sliced copy is still an instance of the same class. Passage should throw a RuntimeException from that method since it's not a sub-divisible (passages contain sub-ranges).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9391", "change_description": ": Upgrade HPPC to 0.8.2.", "change_title": "Upgrade to HPPC 0.8.2", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "HPPC 0.8.2 is out and exposes an Accountable-like interface using to estimate the memory usage. https://issues.carrot2.org/secure/ReleaseNote.jspa?projectId=10070&version=13522&styleName=Text We should upgrade to that if any of components using hppc need to estimate memory better.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-10021", "change_description": ": Upgrade HPPC to 0.9.0. Replace usage of ...ScatterMap to ...HashMap.", "change_title": "Upgrade HPPC to 0.9.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "HPPC 0.9.0 was out and we probably should upgrade. The ...ScatterMap was deprecated in 0.9.0 and I think we're still using them in a few places so probably we should measure the performance impact if there is. (According to release note there shouldn't be any)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-8768", "change_description": ": Fix Javadocs build in Java 11.", "change_title": "Javadoc search support", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Javadoc search is a new feature since Java 9.  (https://openjdk.java.net/jeps/225) I think there is no reason not to use it if the current Lucene Java version is 11. It can be a great help to developers looking at API documentation. (The elastic search also supports it now! https://artifacts.elastic.co/javadoc/org/elasticsearch/client/elasticsearch-rest-client/7.0.0/org/elasticsearch/client/package-summary.html)  ■ Before (Lucene Nightly Core Module Javadoc)  ■ After   I'll change two lines for this. 1) change Javadoc's noindex option from true to false. 2) add javadoc argument \"--no-module-directories\" Currently there is an issue like the following link in JDK 11, so we need \"--no-module-directories\" option.  (https://bugs.openjdk.java.net/browse/JDK-8215291)  ■ How to test I did \"ant javadocs-modules\" on lucene project and check Javadoc.  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12966492/LUCENE-8768.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9092", "change_description": ": upgrade randomizedtesting to 2.7.5", "change_title": "Upgrade randomizedtesting to 2.7.5 and Carrot2 to 3.16.2", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-8656", "change_description": ": Deprecations in FuzzyQuery and get compiler warnings out of\nqueryparser code", "change_title": "Deprecations in FuzzyQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "FuzzyQuery has a static helper method that translates float similarity values into integer edit distances, only currently used by queryparsers.  It has been deprecated since 4.0, but I think allowing query parsers to specify float similarities is actually fine - we should un-deprecate it, and just remove the deprecated float-valued defaultMinSimilarity member.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12956007/LUCENE-8656.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9344", "change_description": ": Convert .txt files to properly formatted .md files.", "change_title": "Convert  XXX.txt files to proper XXX.md", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "Text files that are (partially) written in markdown (such as \"README.txt\") can be converted to proper markdown files. This change was suggested on LUCENE-9321.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9267", "change_description": ": Update MatchingQueries documentation to correct\ntime unit.", "change_title": "The documentation of getQueryBuildTime function reports a wrong time unit.", "detail_type": "Task", "detail_affect_versions": "8.2,8.3,8.4", "detail_fix_versions": "9.0", "detail_description": "As per documentation, the MatchingQueries class returns both getQueryBuildTime and getSearchTime in milliseconds. The code shows searchTime returning milliseconds. However, the code shows buildTime returning nanoseconds. The patch changes the documentation of getQueryBuildTime to report nanoseconds instead of milliseconds.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12999375/LUCENE-9267.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9411", "change_description": ": Fail compilation on warnings, 9x gradle-only (Erick Erickson, Dawid Weiss)\nDeserves mention here as well as Lucene CHANGES.txt since it affects both.", "change_title": "Fail complation on warnings, 9x gradle-only", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Moving this over here from SOLR-11973 since it's part of the build system and affects Lucene as well as Solr. You might want to see the discussion there. We have a clean compile for both Solr and Lucene, no rawtypes, unchecked, try, etc. warnings. There are some peculiar warnings (things like SuppressFBWarnings, i.e. FindBugs) that I'm not sure about at all, but let's assume those are not a problem. Now I'd like to start failing the compilation if people write new code that generates warnings. From what I can tell, just adding the flag is easy in both the Gradle and Ant builds. I still have to prove out that adding -Werrors does what I expect, i.e. succeeds now and fails when I introduce warnings. But let's assume that works. Are there objections to this idea generally? I hope to have some data by next Monday. FWIW, the Lucene code base had far fewer issues than Solr, but common-build.xml is in Lucene.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13006250/LUCENE-9411.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9077", "change_description": "", "change_title": "Gradle build", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This task focuses on providing gradle-based build equivalent for Lucene and Solr (on master branch). See notes below on why this respin is needed. The code lives on gradle-master branch. It is kept with sync with master. Try running the following to see an overview of helper guides concerning typical workflow, testing and ant-migration helpers: gradlew :help A list of items that needs to be added or requires work. If you'd like to work on any of these, please add your name to the list. Once you have a patch/ pull request let me (dweiss) know - I'll try to coordinate the merges. Hard-to-implement stuff already investigated: Of lesser importance:  Note: this builds on the work done by Mark Miller and Cao Mạnh Đạt but also applies lessons learned from those two efforts: ", "patch_link": "https://issues.apache.org/jira/secure/attachment/12992787/LUCENE-9077-javadoc-locale-en-US.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9433", "change_description": "", "change_title": "Remove Ant support from trunk", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Items that may need to be addressed:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9215", "change_description": ": Replace checkJavaDocs.py with doclet", "change_title": "replace checkJavaDocs.py with doclet", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "The current checker runs regular expressions against html, and it breaks when newer java change html output. This is not particularly fun to fix: see LUCENE-9213 Java releases often now, and when i compared generated html of a simple class across 11,12,13 it surprised me how much changes. So I think we want to avoid parsing their HTML. Javadoc Xdoclint feature has a \"missing checker\": but it is black/white. Either everything is fully documented or its not. And while you can enable/disable doclint checks per-package, this also seems black/white (either all checks or no checks at all). On the other hand the python checker is able to check per-package at different granularities (package, class, method). It makes it possible to iteratively improve the situation. With doclet api we could implement checks in pure java, for example to match checkJavaDocs.py logic: If there are problems then they just appear as errors from the output of javadoc like usual:", "patch_link": "https://issues.apache.org/jira/secure/attachment/13010735/overrides.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9497", "change_description": ": Integrate Error Prone, a static analysis tool during compilation", "change_title": "Integerate Error Prone ( Static Analysis Tool ) during compilation", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Integrate https://github.com/google/error-prone during compilation of our source code to catch mistakes", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9544", "change_description": ": add regenerate gradle script for nori dictionary", "change_title": "Port Nori dictionary compilation", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "There is no script for Nori dictionary after the Ant build was deleted in LUCENE-9433.  I made a patch by referring to LUCENE-9155. (Thanks dweiss )", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9627", "change_description": ": Remove unused Lucene50FieldInfosFormat codec and small refactor some codecs\nto separate reading header/footer from reading content of the file.", "change_title": "Small refactor of codec classes", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "While working on LUCENE-9047, I had to refactor some classes in order to separate code that opens a file and reads the header/ footer from the code that reads the actual content of the file. Regardless of that issue, I think the refactor is a good thing.  In addition it seems Lucene50FieldInfosFormat is not used anywhere in the code so I propose to remove it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9773", "change_description": ": Upgrade icu to 68.2", "change_title": "upgrade to icu 68.2", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Upgrade to the latest release, especially now that it is becoming much easier to regenerate.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9822", "change_description": ": Add assertion to PFOR exception encoding, documenting the BLOCK_SIZE assumption.", "change_title": "Assert that ForUtil.BLOCK_SIZE can be encoded in a single byte in PForUtil", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0", "detail_description": "PForUtil assumes that ForUtil.BLOCK_SIZE can be encoded in a single byte when generating \"patch offsets\". If this assumption doesn't hold, PForUtil will silently encode incorrect positions. While the BLOCK_SIZE isn't particularly configurable, it would be nice to assert this assumption early in PForUtil in the even that the BLOCK_SIZE changes in some future codec version.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13021591/LUCENE-9822.patch", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9883", "change_description": ": Turn on ecj missingEnumCaseDespiteDefault setting.", "change_title": "Turn on ecj missingEnumCaseDespiteDefault setting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "This is a spin-off issue from discussion in https://github.com/apache/lucene/pull/25. In this issue, we would like to turn on org.eclipse.jdt.core.compiler.problem.missingEnumCaseDespiteDefault=enabled configuration, and fix problems that arise.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9705", "change_description": ": Make new versions of all index formats for the Lucene90 codec and move\nthe existing ones to the backwards codecs.", "change_title": "Move all codec formats to the o.a.l.codecs.Lucene90 package", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "Current formats are distributed in different packages, prefixed with the Lucene version they were created. With the upcoming release of Lucene 9.0, it would be nice to move all those formats to just the o.a.l.codecs.Lucene90 package (and of course moving the current ones to the backwards-codecs). This issue would actually facilitate moving the directory API to little endian (LUCENE-9047) as the only codecs that would need to handle backwards compatibility will be the codecs in backwards codecs. In addition, it can help formalising the use of internal versions vs format versioning ( LUCENE-9616) ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-9907", "change_description": ": Remove dependency on PackedInts#getReader() from the current codecs and move the\nmethod to backwards codec.", "change_title": "Remove dependency on PackedInts#getReader() in all current codecs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "PackedInts#getDirectWriter/Reader are really legacy and the way to go now is using DirectReader and DirectWriter. With LUCENE-9705, we should be able to remove them from the current codecs. This will help as well to move the Directory API to little endian/", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.0.0", "change_type": "Other", "change_id": "LUCENE-10024", "change_description": ": Catch NoSuchFileException when opening index directory with Luke.", "change_title": "Catch NoSuchFileException when trying to open an index directory which does not exist", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0", "detail_description": "When trying to open an index one can select from the dropdown \"Index Path\" (Dialog: \"Choose index directory path\") previously opened index directories. If such a previously opened index directory path has been deleted in the meantime, but one selects it from the dropdown, then the error message should tell that this directory does not exist. As an alternative Luke might be able to check the existence of the previously opened index directories before displaying in the dropdown", "patch_link": "https://issues.apache.org/jira/secure/attachment/13030754/LUCENE-10024-remove-nonexisting-path.patch", "patch_content": "none"}

{"library_version": "4.8.0", "change_type": "System Requirements", "change_id": "LUCENE-4747", "change_description": ",", "change_title": "java7 as a minimum requirement for lucene 5", "detail_type": "Task", "detail_affect_versions": "6.0", "detail_fix_versions": "6.0", "detail_description": "Spinoff from LUCENE-4746. I propose we make this change on trunk only.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574132/LUCENE-4747.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "System Requirements", "change_id": "LUCENE-5514", "change_description": ",", "change_title": "Backport Java 7 changes from trunk to Lucene 4.8", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "This issue tracks the backporting of various issues that are related to Java 7 to 4.8. It will also revert \"build fixes\" that worked around compile failures (especially stuff like Long/Integer.compare(). I will attach a patch soon (for review). Here is the vote thread: http://mail-archives.apache.org/mod_mbox/lucene-dev/201403.mbox/%3C02be01cf3ae9%24e3735090%24aa59f1b0%24%40thetaphi.de%3E Preliminary result: http://mail-archives.apache.org/mod_mbox/lucene-dev/201403.mbox/%3C001001cf3c45%248d2adc00%24a7809400%24%40thetaphi.de%3E", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634161/LUCENE-5514.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-5472", "change_description": ": IndexWriter.addDocument will now throw an IllegalArgumentException\nif a Term to be indexed exceeds IndexWriter.MAX_TERM_LENGTH.  To recreate previous\nbehavior of silently ignoring these terms, use LengthFilter in your Analyzer.", "change_title": "Long terms should generate a RuntimeException, not just infoStream", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "As reported on the solr-user list, when a term is greater then 2^15 bytes it is silently ignored at indexing time – a message is logged in to infoStream if enabled, but no error is thrown. seems like we should change this behavior (if nothing else starting in 5.0) to throw an exception.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12632712/LUCENE-5472.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5356", "change_description": ": Morfologik filter can accept custom dictionary resources.", "change_title": "Morfologik filter can accept custom dictionary resources", "detail_type": "Improvement", "detail_affect_versions": "4.6", "detail_fix_versions": "4.8,6.0", "detail_description": "I have little proposal for morfologik lucene module. Current module is tightly coupled with polish DICTIONARY enumeration. But other people (like me) can build own dictionaries to FSA and use it with lucene.  You can find proposal in attachment and also example usage in analyzer (SlovakLemmaAnalyzer). It uses dictionary property as String resource from classpath, not enumeration. One change is, that dictionary variable must be set in MofologikFilterFactory (no default value).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636329/LUCENE-5356.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5454", "change_description": ": Add SortedSetSortField to lucene/sandbox, to allow sorting\non multi-valued field.", "change_title": "SortField for SortedSetDV", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Currently, its not possible to sort by a sortedsetdv (e.g. a multi-valued field). So the idea is to provide some comparators that let you do this: by choosing a selector (e.g. min/max/median type stuff) that picks a value from the set as a representative sort value. The implementation is pretty simple, just actually wrap the SortedSet in a SortedDV that does the selection, and re-use the existing TermOrdValComparator logic. One issue is that, with the current sortedset API only 'min' is a viable option because its the only one that can be done in constant time. So this patch adds an optional extension (RandomAccessOrds) for codecs that can support random access. I added this to the default codec, diskdv, and direct, as they can all easily support it. While this could be useful for other purposes (e.g. min/max as valuesource or whatever), i think its best to be optional because it prevents some forms of encoding/compression. Anyway I'm targeting lucene/sandbox with this change...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629782/LUCENE-5454.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5478", "change_description": ": CommonTermsQuery now allows to create custom term queries\nsimilar to the query parser by overriding a newTermQuery method.", "change_title": "Allow CommonTermsQuery to create custom term queries", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "currently we create term queries with new TermQuery(..) directly in CommonTermsQuery I'd like to extend the creation of the term query just like you can do that in the the query parser.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631508/LUCENE-5478.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5477", "change_description": ": AnalyzingInfixSuggester now supports near-real-time\nadditions and updates (to change weight or payload of an existing\nsuggestion).", "change_title": "add near-real-time suggest building to AnalyzingInfixSuggester", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Because this suggester impl. is just a Lucene index under-the-hood, it should be straightforward to enable near-real-time additions/removals of suggestions.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631723/LUCENE-5477.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5482", "change_description": ": Improve default TurkishAnalyzer by adding apostrophe\nhandling suitable for Turkish.", "change_title": "improve default TurkishAnalyzer", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "Add a TokenFilter that strips characters after an apostrophe (including the apostrophe itself).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631822/LUCENE-5482.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5479", "change_description": ": FacetsConfig subclass can now customize the default\nper-dim facets configuration.", "change_title": "Make default dimension config in FacetConfig adjustable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Now it is hardcoded to DEFAULT_DIM_CONFIG. This may be useful for most standard approaches. However, I use lots of facets. These facets can be multivalued, I do not know that on beforehand. So what I would like to do is to change the default config to mulitvalued = true. Currently I have a working, but rather ugly workaround that subclasses FacetConfig, like this: I created a patch to illustrate what I would like to change. By making a protected method it is easier to create a custom subclass of FacetConfig. Also, maybe there are better way to accomplish my goal (easy default to multivalue?)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631697/LUCENE-5479.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5485", "change_description": ": Add circumfix support to HunspellStemFilter.", "change_title": "hunspell circumfix support", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "when the affix file has CIRCUMFIX <flag>, prefixes and suffixes with this flag really represent a single affix around the word, and must agree. in practice i've only seen this in the german dictionary (i unzipped a ton and looked).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12632061/LUCENE-5485.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5224", "change_description": ": Add iconv, oconv, and ignore support to HunspellStemFilter.", "change_title": "org.apache.lucene.analysis.hunspell.HunspellDictionary should implement ICONV and OCONV lines in the affix file", "detail_type": "Improvement", "detail_affect_versions": "4.0,4.4", "detail_fix_versions": "4.8,6.0", "detail_description": "There are some Hunspell dictionaries that need to emulate Unicode normalization and collation in order to get the correct stem of a word. The original Hunspell provides a way to do this with the ICONV and OCONV lines in the affix file. The Lucene HunspellDictionary ignores these lines right now. Please support these keys in the affix file. This bit of functionality is briefly described in the hunspell man page http://manpages.ubuntu.com/manpages/lucid/man4/hunspell.4.html This functionality is practically required in order to use a Korean dictionary because you want only some of the Jamos of a Hangul character (grapheme cluster) when using stemming. Other languages will find this to be helpful functionality. Here is an example for a .aff file: Here is the same example escaped.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12632334/LUCENE-5224.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5493", "change_description": ": SortingMergePolicy, and EarlyTerminatingSortingCollector\nsupport arbitrary Sort specifications.", "change_title": "Rename Sorter, NumericDocValuesSorter, and fix javadocs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Its not clear to users that these are for this super-expert thing of pre-sorting the index. From the names and documentation they think they should use them instead of Sort/SortField. These need to be renamed or, even better, the API fixed so they aren't public classes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633180/LUCENE-5493.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-3758", "change_description": ": Allow the ComplexPhraseQueryParser to search order or\nun-order proximity queries.", "change_title": "Allow the ComplexPhraseQueryParser to search order or un-order proximity queries.", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.8,6.0", "detail_description": "The ComplexPhraseQueryParser use SpanNearQuery, but always set the \"inOrder\" value hardcoded to \"true\". This could be configurable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634990/LUCENE-3758.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5530", "change_description": ": ComplexPhraseQueryParser throws ParseException for fielded queries.", "change_title": "ComplexPhraseQueryParser throws ParseException for fielded queries", "detail_type": "Bug", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "Queries using QueryParser's non-default field e.g. author:\"j* smith\" are not supported by ComplexPhraseQueryParser. For example following code snippet yields", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634998/LUCENE-5530.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5513", "change_description": ": Add IndexWriter.updateBinaryDocValue which lets\nyou update the value of a BinaryDocValuesField without reindexing the\ndocument(s).", "change_title": "Binary DocValues Updates", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "LUCENE-5189 was a great move toward. I wish to continue. The reason for having this feature is to have \"join-index\" - to write children docnums into parent's binaryDV. I can try to proceed the implementation, but I'm not so experienced in such deep Lucene internals. shaie, any hint to begin with is much appreciated.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635147/LUCENE-5513.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-4072", "change_description": ": Add ICUNormalizer2CharFilter, which lets you do unicode normalization\nwith offset correction before the tokenizer.", "change_title": "CharFilter that Unicode-normalizes input", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "I'd like to contribute a CharFilter that Unicode-normalizes input with ICU4J. The benefit of having this process as CharFilter is that tokenizer can work on normalised text while offset-correction ensuring fast vector highlighter and other offset-dependent features do not break. The implementation is available at following repository: https://github.com/ippeiukai/ICUNormalizer2CharFilter Unfortunately this is my unpaid side-project and cannot spend much time to merge my work to Lucene to make appropriate patch. I'd appreciate it if anyone could give it a go. I'm happy to relicense it to whatever that meets your needs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635693/LUCENE-4072.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5476", "change_description": ": Add RandomSamplingFacetsCollector for computing facets on a sampled\nset of matching hits, in cases where there are millions of hits.", "change_title": "Facet sampling", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "With LUCENE-5339 facet sampling disappeared. When trying to display facet counts on large datasets (>10M documents) counting facets is rather expensive, as all the hits are collected and processed. Sampling greatly reduced this and thus provided a nice speedup. Could it be brought back?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635559/LUCENE-5476.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-4984", "change_description": ": Add SegmentingTokenizerBase, abstract class for tokenizers\nthat want to do two-pass tokenization such as by sentence and then by word.", "change_title": "Fix ThaiWordFilter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "ThaiWordFilter is an offender in TestRandomChains because it creates positions and updates offsets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635710/LUCENE-4984.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5489", "change_description": ": Add Rescorer/QueryRescorer, to resort the hits from a\nfirst pass search using scores from a more costly second pass\nsearch.", "change_title": "Add query rescoring API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "When costly scoring factors are used during searching, a common approach is to do a cheaper / basic query first, collect the top few hundred hits, and then rescore those hits using the more costly query. It's not clear/simple to do this with Lucene today; I think we should make it easier.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635902/LUCENE-5489.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5528", "change_description": ": Add context to suggesters (InputIterator and Lookup\nclasses), and fix AnalyzingInfixSuggester to handle contexts.\nSuggester contexts allow you to filter suggestions.", "change_title": "Add context to AnalyzingInfixSuggester", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Spinoff from LUCENE-5350.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636231/LUCENE-5528-1.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5545", "change_description": ": Add SortRescorer and Expression.getRescorer, to\nresort the hits from a first pass search using a Sort or an\nExpression.", "change_title": "Add ExpressionRescorer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "In LUCENE-5489 we added QueryRescorer, to rescore first-pass hits using scores from a (usually) more expensive second-pass query. I think we should also add ExpressionRescorer, to compute the second pass score using an arbitrary JS expression.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636190/LUCENE-5545.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5558", "change_description": ": Add TruncateTokenFilter which truncates terms to\nthe specified length.", "change_title": "Add TruncateTokenFilter", "detail_type": "New Feature", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "I am using this filter as a stemmer for Turkish language. In many academic research (classification, retrieval) it is used and called as Fixed Prefix Stemmer or Simple Truncation Method or F5 in short. Among F3 TO F7, F5 stemmer (length=5) is found to work well for Turkish language in Information Retrieval on Turkish Texts. It is the same work where most of stopwords_tr.txt are acquired. ElasticSearch has truncate filter but it does not respect keyword attribute. And it has a use case similar to TruncateFieldUpdateProcessorFactory Main advantage of F5 stemming is : it does not effected by the meaning loss caused by ascii folding. It is a diacritics-insensitive stemmer and works well with ascii folding. Effects of diacritics on Turkish information retrieval Here is the full field type I use for \"diacritics-insensitive search\" for Turkish I  would like to get community opinions : 1) Any interest in this?  2) keyword attribute should be respected?  3) package name analysis.misc versus analyis.tr  4) name of the class TruncateTokenFilter versus FixedPrefixStemFilter", "patch_link": "https://issues.apache.org/jira/secure/attachment/12637754/LUCENE-5558.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-2446", "change_description": ": Added checksums to lucene index files. As of 4.8, the last 8\nbytes of each file contain a zlib-crc32 checksum. Small metadata files are\nverified on load. Larger files can be checked on demand via\nAtomicReader.checkIntegrity. You can configure this to happen automatically\nbefore merges by enabling IndexWriterConfig.setCheckIntegrityAtMerge.", "change_title": "Add checksums to Lucene segment files", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "It would be useful for the different files in a Lucene index to include checksums. This would make it easy to spot corruption while copying index files around; the various cloud efforts assume many more data-copying operations than older single-index implementations. This feature might be much easier to implement if all index files are created in a sequential fashion. This issue therefore depends on LUCENE-2373.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12637749/LUCENE-2446.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5580", "change_description": ": Checksums are automatically verified on the default stored\nfields format when performing a bulk merge.", "change_title": "Always verify stored fields' checksum on merge", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "I have seen a couple of index corruptions over the last months, and most of them happened on stored fields. The explanation might just be that since stored fields are usually most of the index size, they are just more likely to be corrupted due to a hardware/operating-system failure, but it might be as well a sneaky bug on our side. Lucene recently added checksums to index files, and you can enable integrity verification upon merge, but this comes with a cost since you need to read all index files twice instead of once. If you are merging a very large segment and your merges are I/O-bound, this might be noticeable. I would like to implement integrity checks for stored fields on merges on the fly, so that the stored fields files need to be read only once.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639164/LUCENE-5580.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5602", "change_description": ": Checksums are automatically verified on the default term\nvectors format when performing a bulk merge.", "change_title": "always verify term vectors on bulk merge", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Similar to LUCENE-5580: its scary we just copy bytes and dont verify anything. we should at least verify in the bulk merge case. This is just a quick hack: it would be nice, though scary to remove the clone() of the index input...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639805/LUCENE-5602.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5583", "change_description": ": Added DataInput.skipBytes. ChecksumIndexInput can now seek, but\nonly forward.", "change_title": "Add DataInput.skipBytes", "detail_type": "Bug", "detail_affect_versions": "4.8", "detail_fix_versions": "4.8,6.0", "detail_description": "I was playing with on-the-fly checksum verification and this made me stumble upon an issue with BufferedChecksumIndexInput. I have some code that skips over a DataInput by reading bytes into /dev/null, eg. It is fine to read into this static buffer, even from multiple threads, since the content that is read doesn't matter here. However, it breaks with BufferedChecksumIndexInput because of the way that it updates the checksum: If you are unlucky enough so that a concurrent call to skipBytes started modifying the content of b before the call to digest.update(b, offset, len) finished, then your checksum will be wrong. I think we should make BufferedChecksumIndexInput read into a private buffer first instead of relying on the user-provided buffer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639417/LUCENE-5583.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "New Features", "change_id": "LUCENE-5588", "change_description": ": Lucene now calls fsync() on the index directory, ensuring\nthat all file metadata is persisted on disk in case of power failure.\nThis does not work on all file systems and operating systems, but Linux\nand MacOSX are known to work. On Windows, fsyncing a directory is not\npossible with Java APIs.", "change_title": "We should also fsync the directory when committing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Since we are on Java 7 now and we already fixed FSDir.sync to use FileChannel (LUCENE-5570), we can also fsync the directory (at least try to do it). Unlike RandomAccessFile, which must be a regular file, FileChannel.open() can also open a directory: http://stackoverflow.com/questions/7694307/using-filechannel-to-fsync-a-directory-with-nio-2", "patch_link": "https://issues.apache.org/jira/secure/attachment/12639669/LUCENE-5588-nonexistfix.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5454", "change_description": ": Add RandomAccessOrds, an optional extension of SortedSetDocValues\nthat supports random access to the ordinals in a document.", "change_title": "SortField for SortedSetDV", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Currently, its not possible to sort by a sortedsetdv (e.g. a multi-valued field). So the idea is to provide some comparators that let you do this: by choosing a selector (e.g. min/max/median type stuff) that picks a value from the set as a representative sort value. The implementation is pretty simple, just actually wrap the SortedSet in a SortedDV that does the selection, and re-use the existing TermOrdValComparator logic. One issue is that, with the current sortedset API only 'min' is a viable option because its the only one that can be done in constant time. So this patch adds an optional extension (RandomAccessOrds) for codecs that can support random access. I added this to the default codec, diskdv, and direct, as they can all easily support it. While this could be useful for other purposes (e.g. min/max as valuesource or whatever), i think its best to be optional because it prevents some forms of encoding/compression. Anyway I'm targeting lucene/sandbox with this change...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12629782/LUCENE-5454.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5468", "change_description": ": Move offline Sort (from suggest module) to OfflineSort.", "change_title": "Hunspell very high memory use when loading dictionary", "detail_type": "Bug", "detail_affect_versions": "3.5", "detail_fix_versions": "4.8,6.0", "detail_description": "Hunspell stemmer requires gigantic (for the task) amounts of memory to load dictionary/rules files.  For example loading a 4.5 MB polish dictionary (with empty index!) will cause whole core to crash with various out of memory errors unless you set max heap size close to 2GB or more. By comparison Stempel using the same dictionary file works just fine with 1/8 of that (and possibly lower values as well). Sample error log entries: http://pastebin.com/fSrdd5W1 http://pastebin.com/Lmi0re7Z", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631599/LUCENE-5468.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5493", "change_description": ": SortingMergePolicy and EarlyTerminatingSortingCollector take\nSort instead of Sorter. BlockJoinSorter is removed, replaced with\nBlockJoinComparatorSource, which can take a Sort for ordering of parents\nand a separate Sort for ordering of children within a block.", "change_title": "Rename Sorter, NumericDocValuesSorter, and fix javadocs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Its not clear to users that these are for this super-expert thing of pre-sorting the index. From the names and documentation they think they should use them instead of Sort/SortField. These need to be renamed or, even better, the API fixed so they aren't public classes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633180/LUCENE-5493.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5516", "change_description": ": MergeScheduler#merge() now accepts a MergeTrigger as well as\na boolean that indicates if a new merge was found in the caller thread before\nthe scheduler was called.", "change_title": "Forward information that trigger a merge to MergeScheduler", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "Today we pass information about the merge trigger to the merge policy. Yet, no matter if the MP finds a merge or not we call the MergeScheduler who runs & blocks even if we didn't find a merge. In some cases we don't even want this to happen but inside the MergeScheduler we have no choice to opt out since we don't know what triggered the merge. We should forward the infos we have to the MergeScheduler as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633915/LUCENE-5516.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5487", "change_description": ": Separated bulk scorer (new Weight.bulkScorer method) from\nnormal scoring (Weight.scorer) for those queries that can do bulk\nscoring more efficiently, e.g. BooleanQuery in some cases.  This\nalso simplified the Weight.scorer API by removing the two confusing\nbooleans.", "change_title": "Can we separate \"top scorer\" from \"sub scorer\"?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "This is just an exploratory patch ... still many nocommits, but I think it may be promising. I find the two booleans we pass to Weight.scorer confusing, because they really only apply to whoever will call score(Collector) (just IndexSearcher and BooleanScorer). The params are pointless for the vast majority of scorers, because very, very few query scorers really need to change how top-scoring is done, and those scorers can only score top-level (throw throw UOE from nextDoc/advance).  It seems like these two types of scorers should be separately typed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633463/LUCENE-5487.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5519", "change_description": ": TopNSearcher now allows to retrieve incomplete results if the max\nsize of the candidate queue is unknown. The queue can still be bound in order\nto apply pruning while retrieving the top N but will not throw an exception if\ntoo many results are rejected to guarantee an absolutely correct top N result.\nThe TopNSearcher now returns a struct like class that indicates if the result\nis complete in the sense of the top N or not. Consumers of this API should assert\non the completeness if the bounded queue size is know ahead of time.", "change_title": "Make queueDepth enforcing optional in TopNSearcher", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "currently TopNSearcher enforces the maxQueueSize based on rejectedCount + topN. I have a usecase where I just simply don't know the exact limit and I am ok with a top N that is not 100% exact. Yet, if I don't specify the right upper limit for the queue size I get an assertion error when I run tests but the only workaround it to make the queue unbounded which looks odd while it would possibly work just fine. I think it's fair to add an option that just doesn't enforce the limit and if it shoudl be enforced we throw a real exception.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634188/LUCENE-5519.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-4984", "change_description": ": Deprecate ThaiWordFilter and smartcn SentenceTokenizer and WordTokenFilter.\nThese filters would not work correctly with CharFilters and could not be safely placed\nat an arbitrary position in the analysis chain. Use ThaiTokenizer and HMMChineseTokenizer\ninstead.", "change_title": "Fix ThaiWordFilter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "ThaiWordFilter is an offender in TestRandomChains because it creates positions and updates offsets.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635710/LUCENE-4984.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5543", "change_description": ": Remove/deprecate Directory.fileExists", "change_title": "Remove Directory.fileExists", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Since 3.0.x/3.6.x (see LUCENE-5541), Lucene has substantially removed its reliance on fileExists to the point where I think we can fully remove it now. Like the other iffy IO methods we've removed over time (touchFile, fileModified, seeking back during write, ...), File.exists is dangerous because a low level IO issue can cause it to return false when it should have returned true.  The fewer IO operations we rely on the more reliable/portable Lucene is.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635892/LUCENE-5543.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5573", "change_description": ": Move docvalues constants and helper methods to o.a.l.index.DocValues.", "change_title": "Deadlock during class loading/ initialization", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "It's always worth looking into those randomized failures. http://builds.flonkings.com/job/Lucene-trunk-Linux-Java7-64-test-only/81259/console Log quote: Robert looked into it and was wtf'd, quote: I looked into it and was wtf'd, quote (from my head): I looked deeper at the code and it's a beautiful and classic class loading deadlock. I don't think I've seen an example of this in real life ever, except for this one case. Problem description. 1. Thread A attempts to return a new instance of RandomAccessOrds: RandomAccessOrds extends SortedSetDocValues and has a final static field which in turn loads RandomAccessOrds (circular reference). 2. Thread B attempts to create: 3. If thread B starts loading SortedSetDocValues, it blocks other threads from doing so. If, at the same time, thread A starts loading RandomAccessOrds then thread A will eventually attempt to initialize SortedSetDocValues and both will wait for each other indefinitely. I attach a simpler example that does the same as a POC.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638879/LUCENE-5573.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "API Changes", "change_id": "LUCENE-5604", "change_description": ": Switched BytesRef.hashCode to MurmurHash3 (32 bit).\nTermToBytesRefAttribute.fillBytesRef no longer returns the hash\ncode.  BytesRefHash now uses MurmurHash3 for its hashing.", "change_title": "Should we switch BytesRefHash to MurmurHash3?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "MurmurHash3 has better hashing distribution than the current hash function we use for BytesRefHash which is a simple multiplicative function with 31 multiplier (same as Java's String.hashCode, but applied to bytes not chars).  Maybe we should switch ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640095/LUCENE-5604.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Optimizations", "change_id": "LUCENE-5468", "change_description": ": HunspellStemFilter uses 10 to 100x less RAM. It also loads\nall known openoffice dictionaries without error, and supports an additional\nlongestOnly option for a less aggressive approach.", "change_title": "Hunspell very high memory use when loading dictionary", "detail_type": "Bug", "detail_affect_versions": "3.5", "detail_fix_versions": "4.8,6.0", "detail_description": "Hunspell stemmer requires gigantic (for the task) amounts of memory to load dictionary/rules files.  For example loading a 4.5 MB polish dictionary (with empty index!) will cause whole core to crash with various out of memory errors unless you set max heap size close to 2GB or more. By comparison Stempel using the same dictionary file works just fine with 1/8 of that (and possibly lower values as well). Sample error log entries: http://pastebin.com/fSrdd5W1 http://pastebin.com/Lmi0re7Z", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631599/LUCENE-5468.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Optimizations", "change_id": "LUCENE-4848", "change_description": ": Use Java 7 NIO2-FileChannel instead of RandomAccessFile\nfor NIOFSDirectory and MMapDirectory. This allows to delete open files\non Windows if NIOFSDirectory is used, mmapped files are still locked.", "change_title": "Fix Directory implementations to use NIO2 APIs", "detail_type": "Task", "detail_affect_versions": "6.0", "detail_fix_versions": "4.8,6.0", "detail_description": "I have implemented 3 Directory subclasses using NIO2 API's (available on JDK7).  These may be suitable for inclusion in a Lucene contrib module. See the mailing list at http://lucene.markmail.org/thread/lrv7miivzmjm3ml5 for more details about this code and the advantages it provides. The code is attached as a zip to this issue.  I'll be happy to make any changes requested.  I've included some minimal smoke tests, but any help in how to use the normal Lucene tests to perform more thorough testing would be appreciated.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12574767/LUCENE-4848.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Optimizations", "change_id": "LUCENE-5515", "change_description": ": Improved TopDocs#merge to create a merged ScoreDoc\narray with length of at most equal to the specified size instead of length\nequal to at most from + size as was before.", "change_title": "Improve TopDocs#merge for pagination", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "If TopDocs#merge takes from and size into account it can be optimized to create a hits ScoreDoc array equal to size instead of from+size what is now the case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633850/LUCENE-5515.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Optimizations", "change_id": "LUCENE-5529", "change_description": ": Spatial search of non-point indexed shapes should be a little\nfaster due to skipping intersection tests on redundant cells.", "change_title": "Spatial: Small optimization searching on indexed non-point shapes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "When searching for indexed non-point shapes (such as polygons), there are redundant cells which can be skipped at the bottom \"detail level\" of the search.  This won't be a problem once LUCENE-4942 is fixed since there then won't be any but it's easy to fix now. This affects all predicates RecursivePrefixTreeStrategy uses except Contains.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12634980/LUCENE-5529_Skip_redundant_non-point_scanned_cells.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5483", "change_description": ": Fix inaccuracies in HunspellStemFilter. Multi-stage affix-stripping,\nprefix-suffix dependencies, and COMPLEXPREFIXES now work correctly according\nto the hunspell algorithm. Removed recursionCap parameter, as its no longer needed, rules for\nrecursive affix application are driven correctly by continuation classes in the affix file.", "change_title": "hunspell inaccuracies", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "I added a lot of tests and greatly refined this algorithm to match correct hunspell behavior. there were many bugs: I validated all testing against hunspell", "patch_link": "https://issues.apache.org/jira/secure/attachment/12631985/LUCENE-5483.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5497", "change_description": ": HunspellStemFilter properly handles escaped terms and affixes without conditions.", "change_title": "hunspell doesnt handle escaping or optional conditions properly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "These sometimes come across as errors today, though they are valid. Examples include newer Dutch, Romanian, Kannada dictionaries. Previously there was stuff like 'strictAffixParsing' option, but that was just a hack around some of this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633303/LUCENE-5497.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5505", "change_description": ": HunspellStemFilter ignores BOM markers in dictionaries and handles varying\ntypes of whitespace in SET/FLAG commands.", "change_title": "hunspell SET/FLAG whitespace/BOM handling", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Several dictionaries cannot be loaded today (Armenian, Papiamento, Macedonian, Russian, Urdu) because they have stuff like SET<tab>UTF-8, FLAG<space><space>UTF-8 or have a BOM marker on the first line (or even combinations of these). Also because SET need not be the first line in the file, we should ignore BOM markers on the first line in general (e.g. it might be something else like FLAG).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633545/LUCENE-5505.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5507", "change_description": ": Fix HunspellStemFilter loading of dictionaries with large amounts of aliases\netc before the encoding declaration.", "change_title": "fix hunspell affix file loading", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Some newer dictionaries cant be loaded (arabic, hungarian, turkmen) just because we do a hackish mark/reset thing to go find the SET to know the encoding and then revisit. Problem is: we would need a 2MB buffer for some of these newer ones, thats a little extreme. So instead we just copy to a tempfile and do 2 passes. Also fixes a bug where an alias that goes to no flags would cause an exception (this is ok).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633555/LUCENE-5507.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5111", "change_description": ": Fix WordDelimiterFilter to return offsets in correct order.", "change_title": "Fix WordDelimiterFilter", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "WordDelimiterFilter is documented as broken is TestRandomChains (LUCENE-4641). Given how used it is, we should try to fix it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12635324/LUCENE-5111.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5555", "change_description": ": Fix SortedInputIterator to correctly encode/decode contexts in presence of payload", "change_title": "SortedInputIterator does not encode/decode contexts when payload is present", "detail_type": "Bug", "detail_affect_versions": "4.8,6.0", "detail_fix_versions": "4.8,6.0", "detail_description": "There is a bug where the ordering in which contexts and payload are encoded/decoded in SortedInputIterator is wrong.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636818/LUCENE-5555.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5559", "change_description": ": Add missing argument checks to tokenfilters taking\nnumeric arguments.", "change_title": "Argument validation for TokenFilters having numeric constructor parameter(s)", "detail_type": "Improvement", "detail_affect_versions": "4.7", "detail_fix_versions": "4.8,6.0", "detail_description": "Some TokenFilters have numeric arguments in their constructors. They should throw IllegalArgumentException for negative or meaningless values. Here is some examples that demonstrates invalid/meaningless arguments :", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638933/LUCENE-5559.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5568", "change_description": ": Benchmark module's \"default.codec\" option didn't work.", "change_title": "Benchmark default.codec doesn't work", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "CreateIndexTask looks at the \"default.codec\" option and ultimately calls Codec.setDefault(...).  However that only applies to IndexWriterConfig's created after this point in time, but the IndexWriterConfig here was already created. This fix is to simply call iwc.setCodec(codec).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12638498/LUCENE-5568_benchmark_default_codec_does_not_work.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "SOLR-5983", "change_description": ": HTMLStripCharFilter is treating CDATA sections incorrectly.", "change_title": "HTMLStripCharFilter is treating CDATA sections incorrectly", "detail_type": "Bug", "detail_affect_versions": "4.7.1", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "I'm hammering on this Solr Instance.  I've got three cores that I'm using to store millions of small bits of reference data.  I'm using a heavily tweaked Tika to parse xml files and ingest them into Solr, while referencing this data.  So I'm making hundreds of query requests against solr, while also making some substantial posts. (I queue up the posts, in general sending in 100 documents at a time). Stack Trace: 4099640 [qtp39890933-24] WARN  org.eclipse.jetty.servlet.ServletHandler  – Error for /solr/us_patent_gran t/update java.lang.AssertionError: Attempting to read past the end of a segment.         at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter$TextSegment.nextChar(HTMLStripCharFi lter.java:30885)         at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter.zzDoEOF(HTMLStripCharFilter.java:311 50)         at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter.nextChar(HTMLStripCharFilter.java:31 802)         at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter.read(HTMLStripCharFilter.java:30829)         at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter.read(HTMLStripCharFilter.java:30842)        at org.apache.lucene.analysis.standard.std40.StandardTokenizerImpl40.zzRefill(StandardTokenizerImpl40.java:916)         at org.apache.lucene.analysis.standard.std40.StandardTokenizerImpl40.getNextToken(StandardTokenizerImpl40.java:1123)         at org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:17 5)         at org.apache.lucene.analysis.payloads.TokenOffsetPayloadTokenFilter.incrementToken(TokenOffsetPa yloadTokenFilter.java:45)         at org.apache.lucene.analysis.core.LowerCaseFilter.incrementToken(LowerCaseFilter.java:54)         at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:182)         at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:248)         at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:253)         at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:455)         at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)         at org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:236)         at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)         at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java: 69)         at org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java :51)         at org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProces sor.java:704)         at org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProces sor.java:858)         at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProces sor.java:557)         at org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java: 100)         at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:247)         at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:174)         at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)         at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.ja va:74)         at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)         at org.apache.solr.core.SolrCore.execute(SolrCore.java:1916)         at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:780)         at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:427)         at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:217)         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640593/SOLR-5983.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5615", "change_description": ": Validate per-segment delete counts at write time, to\nhelp catch bugs that might otherwise cause corruption", "change_title": "validate per-segment delete counts on write", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "When reading the segment infos, we already check if the delCount is out of bounds for a segment (< 0 or > docCount) and throw CorruptIndexException if so. I think we should catch this when writing as well, in case there are Lucene bugs that mess up the delete counts.  We have just an assert today...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640786/LUCENE-5615.4x.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5612", "change_description": ": NativeFSLockFactory no longer deletes its lock file. This cannot be done\nsafely without the risk of deleting someone else's lock file. If you use NativeFSLockFactory,\nyou may see write.lock hanging around from time to time: its harmless.", "change_title": "LockStressTest fails always with NativeFSLockFactory", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "I was looking at this, because i wanted to remove the static map inside NativeFSLockFactory (no particular reason: it just smells bad, we require java7, and you get overlappingexception as of java6 so its unnecessary). Before changing any code, i wanted to run lockstresstest first, just to ensure it works: but it fails always. Simple works fine always. Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: lock was double acquired at org.apache.lucene.store.VerifyingLockFactory$CheckedLock.verify(VerifyingLockFactory.java:67)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641017/LUCENE-5612.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5624", "change_description": ": Ensure NativeFSLockFactory does not leak file handles if it is unable\nto obtain the lock.", "change_title": "nightly 'test-lock-factory' may leak file handles", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "https://builds.apache.org/job/Lucene-Solr-NightlyTests-trunk/556/console", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641245/LUCENE-5624-test.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5626", "change_description": ": Fix bug in SimpleFSLockFactory's obtain() that sometimes throwed\nIOException (ERROR_ACESS_DENIED) on Windows if the lock file was created\nconcurrently. This error is now handled the same way like in NativeFSLockFactory\nby returning false.", "change_title": "SimpleFSLockFactory \"access denied\" on windows.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "This happened twice in jenkins: My windows machine got struck by lightning, so I cannot fix this easily.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641451/LUCENE-5626.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Bug fixes", "change_id": "LUCENE-5630", "change_description": ": Add missing META-INF entry for UpperCaseFilterFactory.", "change_title": "Improve TestAllAnalyzersHaveFactories", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "This test wasn't working at all, it would always pass. It is sensitive to the strings inside exception messages, if we change those, it might suddenly stop working. It would be great to improve this thing to be less fragile.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641790/LUCENE-5630.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Tests", "change_id": "LUCENE-5630", "change_description": ": Fix TestAllAnalyzersHaveFactories to correctly check for existence\nof class and corresponding Map<String,String> ctor.", "change_title": "Improve TestAllAnalyzersHaveFactories", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "This test wasn't working at all, it would always pass. It is sensitive to the strings inside exception messages, if we change those, it might suddenly stop working. It would be great to improve this thing to be less fragile.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641790/LUCENE-5630.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Test Framework", "change_id": "LUCENE-5592", "change_description": ": Incorrectly reported uncloseable files.", "change_title": "Incorrectly reported uncloseable files.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "As pointed out by Uwe, something dodgy is going on with unremovable file detection because they seem to cross a suite boundary, as in.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Test Framework", "change_id": "LUCENE-5577", "change_description": ": Temporary folder and file management (and cleanup facilities)", "change_title": "Temporary folder and file management (and cleanup facilities)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "This is a spinoff of the work markrmiller@gmail.com, Uwe and me have initiated in SOLR-5914. The core concept is this: Note the absence of \"create a temporary file/dir under a given directory\" method. These shouldn't be needed since any temporary folder created by the above is guaranteed to be empty. If one still needs temporary file creation use createTempDir() and then relevant File class methods.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Test Framework", "change_id": "LUCENE-5567", "change_description": ": When a suite fails with zombie threads failure marker and count\nis not propagated properly.", "change_title": "When a suite fails with zombie threads failure marker and count is not propagated properly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "This is a chicken-and-egg problem between thread leak detection and failure detection rule. I have an idea how to fix this. It'd prevent cascading failures resulting from previous zombie thread errors, like this one: http://builds.flonkings.com/job/Lucene-trunk-Linux-Java7-64-test-only/81259/consoleText", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Test Framework", "change_id": "LUCENE-5449", "change_description": ": Rename _TestUtil and _TestHelper to remove the leading _.", "change_title": "Two ancient classes renamed to be less peculiar: _TestHelper and _TestUtil", "detail_type": "Improvement", "detail_affect_versions": "4.6.1", "detail_fix_versions": "4.8,6.0", "detail_description": "_TestUtil and _TestHelper begin with _ for historical reasons that don't apply any longer. Lets eliminate those _'s.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Test Framework", "change_id": "LUCENE-5501", "change_description": ": Added random out-of-order collection testing (when the collector\nsupports it) to AssertingIndexSearcher.", "change_title": "Out-of-order collection testing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "Collectors have the ability to declare whether or not they support out-of-order collection, but since most scorers score in order this is not well tested.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12636031/LUCENE-5501-2.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Build", "change_id": "LUCENE-5463", "change_description": ": RamUsageEstimator.(human)sizeOf(Object) is now a forbidden API.", "change_title": "Make RamUsageEstimator.(human)sizeOf(Object) a forbidden API", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.8", "detail_description": "We have had a few issues with RamUsageEstimator recently so I think we should consider making the sizeOf(Object) and humanSizeOf(Object) methods forbidden under src/java (however still allowed for tests as it is handy to check the size computations which are done \"manually\"). However, sizeOf(byte[]), shallowSizeOf(Class), etc. remain useful so I think we should keep them allowed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12630816/LUCENE-5463-2.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Build", "change_id": "LUCENE-5512", "change_description": ": Remove redundant typing (use diamond operator) throughout\nthe codebase.", "change_title": "Remove redundant typing (diamond operator) in trunk", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12633908/LUCENE-5512.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Build", "change_id": "LUCENE-5614", "change_description": ": Enable building on Java 8 using Apache Ant 1.8.3 or 1.8.4\nby adding a workaround for the Ant bug.", "change_title": "Add workaround for ANT bug to enable compile with Java 8 on ANT 1.8.3. and 1.8.4", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "Ant has the following bug introduced in ANT 1.8.3 and 1.8.4. Unfortunately many computers are on this version (and MacOSX on 1.8.2): https://issues.apache.org/bugzilla/show_bug.cgi?id=53347 I would like to change to minimum ANT 1.9.0, but this would make MacOSX user unhappy. The provided patch is a workaround to set the compiler version to \"javac1.7\" via ANT property, if the broken versions are used with Java 1.8.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12640624/LUCENE-5614.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Build", "change_id": "LUCENE-5612", "change_description": ": Add a new Ant target in lucene/core to test LockFactory\nimplementations: \"ant test-lock-factory\".", "change_title": "LockStressTest fails always with NativeFSLockFactory", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.8,4.9,6.0", "detail_description": "I was looking at this, because i wanted to remove the static map inside NativeFSLockFactory (no particular reason: it just smells bad, we require java7, and you get overlappingexception as of java6 so its unnecessary). Before changing any code, i wanted to run lockstresstest first, just to ensure it works: but it fails always. Simple works fine always. Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: lock was double acquired at org.apache.lucene.store.VerifyingLockFactory$CheckedLock.verify(VerifyingLockFactory.java:67)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12641017/LUCENE-5612.patch", "patch_content": "none"}
{"library_version": "4.8.0", "change_type": "Documentation", "change_id": "LUCENE-5534", "change_description": ": Add javadocs to GreekStemmer methods.", "change_title": "GreekStemmer javadocs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.8,6.0", "detail_description": "Just an issue for tracking https://github.com/apache/lucene-solr/pull/43.patch", "patch_link": "none", "patch_content": "none"}

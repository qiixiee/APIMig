{"library_version": "9.7.0", "change_type": "API Changes", "change_id": "GITHUB#11840", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Issue: #11838", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "API Changes", "change_id": "GITHUB#12304", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In OpenSearch, we've updated to the recent Apache Lucene 9.7 snapshots and got an number of tests failing with java.security.AccessControlException . The culprit is org.apache.lucene.util.VirtualMethod class that does a number of unprivileged calls to Reflection APIs without using AccessController.doPrivileged , a sample stack trace is below: I surely know that SecurityManager & co are deprecated but Apache Lucene / OpenSearch / Elasticsearch are still relying on it for the time being. @uschindler @jpountz the fix is simple (happy to take it and submit a pull request) if there are no objections to make this change in general, cc @nknize Sample failing tests: Latest 9.7.0 snapshots (built of branch_9x ) The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "API Changes", "change_id": "GITHUB#12321", "change_description": ": DaciukMihovAutomatonBuilder has been marked deprecated in preparation of reducing its visibility in\na future release.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "There's some good suggestions/discussion around renaming this class in #12310 , but I wonder if we should consider making it pkg-private and exposing the build functionality through Automata instead? We already do this with Automata#makeStringUnion , so maybe we could shrink our API footprint and consolidate everything behind Automata ? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "API Changes", "change_id": "GITHUB#12268", "change_description": ": Add BitSet.clear() without parameters for clearing the entire set", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Depending on the implementation, this may be significantly faster than clear(0, length). In particular, SparseFixedBitSet can use a single Arrays.fill call instead of looping through the backing array.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "API Changes", "change_id": "GITHUB#12346", "change_description": ": add new IndexWriter#updateDocuments(Query, Iterable<Document>) API\nto update documents atomically, with respect to refresh and commit using a query.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Please see #12341", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "New Features", "change_id": "GITHUB#12257", "change_description": ": Create OnHeapHnswGraphSearcher to let OnHeapHnswGraph to be searched in a thread-safety manner.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Implemented a searcher specific to address the thread-safety problem on searching on OnHeapHnswGraph Idea: #12246 (comment) A test case comparing the search using 4 threads and new class with single thread and old one.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "New Features", "change_id": "GITHUB#12302", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "For years we have explored using the vector api to actually take advantage of SIMD units on the hardware. A couple of approaches have been attempted so far: Currently, unless the user is extremely technical, they can't make use of the vector support their hardware has, which is terribly sad. If they have immense resources/funding/etc, they can fork lucene and patch the source code, and maintain a fork, hooking in incubator openjdk stuff, but that's too hard on users. I think we have to draw a line in the sand, basically we can not rely upon openjdk to be managed as a performant project, their decisions make no sense, we have to play a little less nicer and apply some hacks! otherwise give up and switch to a different programming language with better perf! So I'd suggest to look at the work @uschindler has done with mmap and the preview apis, and let's carve out a path where we use the vector api IFF the user opts in via the command-line. Proposal (depends entirely upon user's jdk version and supported flags): Actually the system @uschindler developed I think is the correct design for this, the only trick is that the incubating api is more difficult than the preview api. So we need more build system support, it could require more stuff to be downloaded or build to be slower. But I think its the right decision? We don't want to have base64-encoded hackiness that is hard to maintain, at the same time, we need to give the users option to opt-in to actually making use their hardware. I think we should suffer the complexity to make this easy on them. It fucking sucks that openjdk makes this almost impossible, but we need to do it for our users. That's what being a library is all about. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "New Features", "change_id": "GITHUB#12311", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Leverage accelerated vector hardware instructions in Vector Search. Lucene already has a mechanism that enables the use of non-final JDK APIs, currently used for the Previewing Pamana Foreign API. This change expands this mechanism to include the Incubating Pamana Vector API. When the jdk.incubator.vector module is present at run time the Panamaized version of the low-level primitives used by Vector Search is enabled. If not present, the default scalar version of these low-level primitives is used (as it was previously). Currently, we're only targeting support for JDK 20. A subsequent PR should evaluate JDK 21, which is still in development.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "New Features", "change_id": "GITHUB#12363", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit enables the Panama Vector API for Java 21. The version of VectorUtilPanamaProvider for Java 21 is identical to that of Java 20. As such, there is no specific 21 version - the Java 20 version will be loaded from the MRJAR. Initial outdated approach (not merged):", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "New Features", "change_id": "GITHUB#12294", "change_description": ": Add support for Java 21 foreign memory API. If Java 19 up to 21 is used,\nMMapDirectory will mmap Lucene indexes in chunks of 16 GiB (instead of 1 GiB) and indexes\nclosed while queries are running can no longer crash the JVM. To disable this feature,\npass the following sysprop on Java command line:\n\"-Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\"", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is the Java 21 version of MemorySegments following JEP 442 . There are not many changes: I will make this PR a draft until JDK 21 is in final phase (RC).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "New Features", "change_id": "GITHUB#12252", "change_description": "Add function queries for computing similarity scores between knn vectors.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This proposal aims to enable function queries to calculate similarity functions between dense vectors using function scores. The function queries should accept vector fields or constant vectors as parameters. By leveraging function queries, users can compute similarity scores between dense vectors and apply these scores to rank or rerank search results. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12245", "change_description": ": Add support for Score Mode to `ToParentBlockJoinQuery` explain.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today, explain() returns the max score regardless of the value of toParentBlockJoin as documented in #12204 . This is a simple PoC to address the issue. Please note: I still need to write three more tests. I'm toying around with a few approaches and trying to be mindful of opportunity costs and readability given the size of the project. I am curious if anyone has any thoughts around parameterized tests vs individual methods for each score mode other than None , which was already tested.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12305", "change_description": ": Minor cleanup and improvements to DaciukMihovAutomatonBuilder.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This proposes some minor cleanup and improvements to DaciukMihovAutomatonBuilder I came across while starting to explore #12176 (exploring the idea of allowing this algorithm to directly build binary automata). I thought it would be good to independently ship these minor cleanups:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12325", "change_description": ": Parallelize AbstractKnnVectorQuery rewrite across slices rather than segments.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The concurrent query rewrite for knn vectory query introduced with #12160 requests one thread per segment to the executor. To align this with the IndexSearcher parallel behaviour, we should rather parallelize across slices. Also, we can reuse the same slice executor instance that the index searcher already holds, that way we are using a QueueSizeBasedExecutor when a thread pool executor is provided.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12333", "change_description": ": NumericLeafComparator#competitiveIterator makes better use of a \"search after\" value when paginating.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "If we specify after value for PagingFieldCollector which is out of min/max values for that field in that segment, PagingFieldCollector is not able to update its competitiveIterator because queueFull is never able to set to true. Code ref - NumericComparator.java$194 Due to this, it will spend lot of time comparing large number of hits here TopFieldCollector.java#264 . Instead it is supposed to short cut right away. No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12290", "change_description": ": Make memory fence in ByteBufferGuard explicit using `VarHandle.fullFence()`", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Current implementation of ByteBufferGuard uses an implementation-based assumption that AtomicInteger#lazySet causes full-barrrier (ie [prior stores] happen before [following loads & stores] is required). Since VarHandle appearance there have been explicit methods for performing memory fences including full fence thus it seems reasonable to rely on them rather than ungaranteed semantic. It's worth mentioning that unlike #9824 which proposes to use new explicit access modes on the field, this does not change the type of invalidated or its access methods but simply replaces the old non-obviously invoked barrier with an obvious one.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12320", "change_description": ": Add \"direct to binary\" option for DaciukMihovAutomatonBuilder and use it in TermInSetQuery#visit.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Adds the ability to directly build a binary automaton for a string union using the Daciuk-Mihov algorithm, and uses it to make the TermInSetQuery#visit implementation a little more optimal. I'm hoping we end up moving to an automaton approach in general for TermInSetQuery (see #12312 ), but I think this is a good iterative step for now, as suggested by @rmuir / @mikemccand in #12310 .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Improvements", "change_id": "GITHUB#12281", "change_description": ": Require indexed KNN float vectors and query vectors to be finite.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR adds argument checking to constructors of fields and query, so all vector values must be finite.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12324", "change_description": ": Speed up sparse block advanceExact with tiny step in IndexedDISI.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today Sparse#AdvanceExactWithinBlock always need to read next doc and seek back if a doc not exists. This could do harm to performance in dense hit queries. For example, a field exists in doc 1, 5. When advanceExact 2,3,4 it always need to read next doc (5) and seek back. I think caching the next existing doc in block can help dense hit queries without too much harm to other cases. I ran a benchmark with MatchAllDocsQuery on some fields with different sparsity: sparsity=n means field only exists when doc % n == 0", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12270", "change_description": "Don't generate stacktrace in CollectionTerminatedException.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Seems this exception is always ignored so there's no point in filling in a stack-trace for it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12160", "change_description": ": Concurrent rewrite for AbstractKnnVectorQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Issue #11862 AbstractKnnVectorQuery currently performs HNSW searches (one per-segment) iteratively Since this is done in a single thread, we can make it concurrent by spinning off per-segment searches to different threads (and make use of available processors) The actual search is performed in Query#rewrite , and support to allow concurrency there was added recently ( #11838 ) by passing an IndexSearcher (which wraps an IndexReader and Executor ) Proposing to achieve this by CompletableFuture :", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12286", "change_description": "Toposort use iterator to avoid stackoverflow.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "In Issue #12284 , I observed that Lucene limits the recursion level in the topoSortStatesRecurse method to avoid a StackOverflow error during automaton topological sorting. I propose we could use an iterative approach instead of recursion. I've implemented an iterative version as shown below. However, I noticed that the test TestAutomaton.java depends on the order in which recursive calls are made or items are added to and removed from the stack. To maintain the same order as the recursive version in the iterative approach, so I've used a particular technique in the pull request. To further improve this change, here are my plans:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12235", "change_description": ": Optimize HNSW diversity calculation.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently when we pop out the worst diverse node we go from the most distant node and calculate diversity one by one. But actually when we add the first batch of neighbours, we have already checked the diversity and rejected the non-diverse node. So the only nodes that are unchecked are only the ones that inserted by neighbours (using insertSorted ), we should be able to only check those \"unchecked\" nodes but not every node. I rely on current unit test to check the correctness but if that's not enough I'm pretty happy to write some test as well. KNNGraphTester bench based on mikemccand/luceneutil#218 :", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12328", "change_description": ": Optimize ConjunctionDISI.createConjunction", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is showing up as a little hot when profiling some queries (in ES enrich caching). Almost all the time spent in this method is just burnt on ceremony around stream indirections that don't inline.  Moving this to iterators, simplifying the check for same doc id and also saving one iteration (for the min cost) makes this method far cheaper and IMO easier to read.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12357", "change_description": ": Better paging when doing backwards random reads.  This speeds up\nqueries relying on terms in NIOFSDirectory and SimpleFSDirectory.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When reading data from outside the buffer, BufferedIndexInput always resets its buffer to start at the new read position.  If we are reading backwards (for example, using an OffHeapFSTStore for a terms dictionary) then this can have the effect of re-reading the same data over and over again. This commit changes BufferedIndexInput to use paging when reading backwards, so that if we ask for a byte that is before the current buffer, we read a block of data of bufferSize that ends at the previous buffer start. Fixes #12356", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12339", "change_description": ": Optimize part of duplicate calculation numDeletesToMerge in merge phase", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "as es issuse metioned when soft delete enable the numDeletesToMerge function is very time consume part. As the following picture show, there is actually calculate duplicate in there. This change want to reuse the numDeletesToMerge result to reduce the time used", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Optimizations", "change_id": "GITHUB#12334", "change_description": ": Honor after value for skipping documents even if queue is not full for PagingFieldCollector", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As mentioned in issue #12333 , if after value is out of range for a field in a segment, PagingFieldCollector is not able to put any item in the FieldhitQueue .  Due to which queueFull always stays false and it wont ever update PointValues based competitiveIterator to skip non-competitive hits. I think, it makes sense to update competitiveIterator even if queue is not full and the query is after query because we have at least topValue in after query always. That itself should be capable enough to skip non-competitive hits. It does not only help when after is out of range, also when after is within the range but filtering only based on the after value significantly reduces the number of hits to evaluate.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12291", "change_description": ": Skip blank lines from stopwords list.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Hi team, This issue is a spin-off from the java-user list thread . The stopwords.txt of SmartChineseAnalyzer contains two blank lines at L56 & L58 . As a result, SmartChineseAnalyzer.getDefaultStopSet() will produce an empty string stop word, but it makes no sense to have empty string as a stop word. Maybe we can improve it? The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11350", "change_description": ": Handle possible differences in FieldInfo when merging indices created with Lucene 8.x", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We have a long-standing index with some mandatory fields and some optional fields that has been through multiple lucene upgrades without a full rebuild and on testing out an upgrade from version 8.11.0 to 9.0.0, when open an IndexWriter we are hitting the exception Exception in thread \"main\" java.lang.IllegalArgumentException: cannot change field \"language\" from index options=NONE to inconsistent index options=DOCS at org.apache.lucene.index.FieldInfo.verifySameIndexOptions(FieldInfo.java:245) at org.apache.lucene.index.FieldInfos$FieldNumbers.verifySameSchema(FieldInfos.java:421) at org.apache.lucene.index.FieldInfos$FieldNumbers.addOrGet(FieldInfos.java:357) at org.apache.lucene.index.IndexWriter.getFieldNumberMap(IndexWriter.java:1263) at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1116) Where language is one of our optional fields. Presumably this is at least somewhat related to \"Index options can no longer be changed dynamically\" as mentioned at https://lucene.apache.org/core/9_0_0/MIGRATE.html although it fails before our code attempts to update the index, and we are not trying to change any index options. Adding some displays to IndexWriter and FieldInfos and logging rather than throwing the exception I see language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=DOCS language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=NONE language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS language     curr=NONE, other=DOCS where there is one line per segment.  It logs the exception whenever other=DOCS.  Subset with segment info: segment _x8(8.2.0):c31753/-1:[diagnostics={timestamp=1565623850605, lucene.version=8.2.0, java.vm.version=11.0.3+7, java.version=11.0.3, mergeMaxNumSegments=-1, os.version=3.1.0-1.2-desktop, java.vendor=AdoptOpenJDK, source=merge, os.arch=amd64, mergeFactor=10, java.runtime.version=11.0.3+7, os=Linux}]:[attributes={Lucene50StoredFieldsFormat.mode=BEST_SPEED}] language     curr=NONE, other=NONE segment _y9(8.7.0):c43531/-1:[diagnostics={timestamp=1604597581562, lucene.version=8.7.0, java.vm.version=11.0.3+7, java.version=11.0.3, mergeMaxNumSegments=-1, os.version=3.1.0-1.2-desktop, java.vendor=AdoptOpenJDK, source=merge, os.arch=amd64, mergeFactor=10, java.runtime.version=11.0.3+7, os=Linux}]:[attributes={Lucene87StoredFieldsFormat.mode=BEST_SPEED}] language     curr=NONE, other=DOCS NOT throwing java.lang.IllegalArgumentException: cannot change field \"language\" from index options=NONE to inconsistent index options=DOCS Migrated from LUCENE-10314 by Ian Lea, 2 votes The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12352", "change_description": ": [Tessellator] Improve the checks that validate the diagonal between two polygon nodes so\nthe resulting polygons are valid counter clockwise polygons.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "relates to #11986 More polygons reported to be failing to tessellate. The polygon data can be found here: Sharing the sample polygon data. https://raw.githubusercontent.com/amalantonygit/polygon-data/main/polygon.geojson  From a first look, the error seems related to the way we validate the diagonal chosen to split polygons here: lucene/lucene/core/src/java/org/apache/lucene/geo/Tessellator.java Line 1093\n      in 0c29390  The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.7.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10181", "change_description": ": Restrict GraphTokenStreamFiniteStrings#articulationPointsRecurse recursion depth.", "change_title": "GraphTokenStreamFiniteStrings#articulationPointsRecurse can run into stack overflows", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "If provided with a very long query string, GraphTokenStreamFiniteStrings#articulationPointsRecurse may run into a StackOverflowError.", "patch_link": "none", "patch_content": "none"}

{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7388", "change_description": ": Add point based IntRangeField, FloatRangeField, LongRangeField along with\nsupporting queries and tests", "change_title": "Add IntRangeField, FloatRangeField, LongRangeField", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "This is the follow on to LUCENE-7381 for adding support for indexing and querying on int, float, and long ranges.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12819709/LUCENE-7388.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7381", "change_description": ": Add point based DoubleRangeField and RangeFieldQuery for\nindexing and querying on Ranges up to 4 dimensions", "change_title": "Add new RangeField", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I've been tinkering with a new Point-based RangeField for indexing numeric ranges that could be useful for a number of applications. For example, a single dimension represents a span along a single axis such as indexing calendar entries start and end time, 2d range could represent bounding boxes for geometric applications (e.g., supporting Point based geo shapes), 3d ranges bounding cubes for 3d geometric applications (collision detection, 3d geospatial), and 4d ranges for space time applications. I'm sure there's applicability for 5d+ ranges but a first incarnation should likely limit for performance.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12819215/LUCENE-7381.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-6968", "change_description": ": LSH Filter", "change_title": "LSH Filter", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "I'm planning to implement LSH. Which support query like this Find similar documents that have 0.8 or higher similar score with a given document. Similarity measurement can be cosine, jaccard, euclid.. For example. Given following corpus 1. Solr is an open source search engine based on Lucene 2. Solr is an open source enterprise search engine based on Lucene 3. Solr is an popular open source enterprise search engine based on Lucene 4. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java We wanna find documents that have 0.6 score in jaccard measurement with this doc Solr is an open source search engine It will return only docs 1,2 and 3 (MoreLikeThis will also return doc 4)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12801204/LUCENE-6968.4.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7302", "change_description": ": IndexWriter methods that change the index now return a\nlong \"sequence number\" indicating the effective equivalent\nsingle-threaded execution order", "change_title": "IndexWriter should tell you the order of indexing operations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Today, when you use multiple threads to concurrently index, Lucene knows the effective order that those operations were applied to the index, but doesn't return that information back to you. But this is important to know, if you want to build a reliable search API on top of Lucene.  Combined with the recently added NRT replication (LUCENE-5438) it can be a strong basis for an efficient distributed search API. I think we should return this information, since we already have it, and since it could simplify servers (ES/Solr) on top of Lucene: Not returning this just hurts people who try to build servers on top with clear semantics on crashing/recovering ... I also struggled with this when building a simple \"server wrapper\" on top of Lucene (LUCENE-5376).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12806458/LUCENE-7032.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7335", "change_description": ": IndexWriter's commit data is now late binding,\nrecording key/values from a provided iterable based on when the\ncommit actually takes place", "change_title": "IndexWriter.setCommitData should be late binding", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Today, IndexWriter.setCommitData is early-binding: as soon as you call it, it clones the provided map and later on when commit is called, it uses that clone. But this makes it hard for some use cases where the app needs to record more timely information based on when specifically the commit actually occurs.  E.g., with LUCENE-7302, it would be helpful to store the max completed sequence number in the commit point: that would be a lower bound of operations that were after the commit. I think the most minimal way to do this would be to upgrade the existing method to take an Iterable<Map.Entry<String,String>, and document that it's now late binding, i.e. IW will pull an Iterator from that when it's time to write the segments file. Or we could also make an explicit interface that you pass (seems like overkill), or maybe have a listener or something (or you subclass IW) that's invoked when the commit is about to write the segments file, but that also seems like overkill.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12810399/LUCENE-7335.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7287", "change_description": ": UkrainianMorfologikAnalyzer is a new dictionary-based\nanalyzer for the Ukrainian language", "change_title": "New lemma-tizer plugin for ukrainian language.", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Hi all, I wonder whether you are interested in supporting a plugin which provides a mapping between ukrainian word forms and their lemmas. Some tests and docs go out-of-the-box =) . https://github.com/mrgambal/elasticsearch-ukrainian-lemmatizer It's really simple but still works and generates some value for its users. More: https://github.com/elastic/elasticsearch/issues/18303", "patch_link": "https://issues.apache.org/jira/secure/attachment/12811703/LUCENE-7287.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7373", "change_description": ": Directory.renameFile, which did both renaming and fsync\nof the directory metadata, has been deprecated; use the new separate\nmethods Directory.rename and Directory.syncMetaData instead", "change_title": "Break out Directory.syncMetaData from FSDirectory.renameFile", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Today, when you call FSDirectory.renameFile it also calls fsync on the directory. This is OK for Lucene's current usage of this method, to rename just the one segments_N file on commit. But in playing with adding NRT replication (LUCENE-5438) to the simple demo Lucene server (LUCENE-5376) I found that, on spinning disks, that fsync is very costly, because when copying over an NRT point, we write to N .tmp files and then rename many files (taking seconds) in the end. I think we should just deprecate/remove the existing method, and make a new rename method that does only renaming, and a separate syncMetaData to call fsync on the directory?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12817056/LUCENE-7373.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7355", "change_description": ": Added Analyzer#normalize(), which only applies normalization to\nan input string.", "change_title": "Leverage MultiTermAwareComponent in query parsers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "MultiTermAwareComponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. However, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816841/LUCENE-7355.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7380", "change_description": ": Add Polygon.fromGeoJSON for more easily creating\nPolygon instances from a standard GeoJSON string", "change_title": "Add Polygon.fromGeoJSON", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Working with Polygon is a bit tricky today because you typically must use an external dependency to convert e.g. a GeoJSON string into Lucene's Polygon class ... I think this is a weakness in our API, and it clearly confuses users: http://markmail.org/thread/mpge4wqo7cfqm4i5 So I created a simplistic GeoJSON parser to extract a single Polygon or MultiPolygon from a GeoJSON string, without any dependencies.  The parser only handles the various ways that a single Polygon or MultiPolygon can appear in a GeoJSON string, and throws an exception otherwise.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12817989/LUCENE-7380.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "LUCENE-7395", "change_description": ": PerFieldSimilarityWrapper requires a default similarity\nfor calculating query norm and coordination factor in Lucene 6.x.\nLucene 7 will no longer have those factors.", "change_title": "Query Norm and coordination factor not calculated when PerFieldSimilarityWrapper is used", "detail_type": "Bug", "detail_affect_versions": "5.3.1,5.4.1", "detail_fix_versions": "6.2", "detail_description": "If any kind of similarity is defined and therefore the SchemaSimilarityFactory is defined as global similarity the queryNorm is always 1.0 The PerFieldSimilarityWrapper delegates some of the methods to the desired Similarity but misses to delegate public float queryNorm(float valueForNormalization) Instead the IndexReader calls this method on the base class Similarity. The result is that all scores are much higher. I created a custom similarity which extends ClassicSimilarity. To have the calculation fixed I did a local \"hotfix\"  which always uses the default similarity. Also wrong for some cases but fine in my scenario. @Override   public float queryNorm(float valueForNormalization)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12820115/LUCENE-7395.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "New Features", "change_id": "SOLR-9279", "change_description": ": Queries module: new ComparisonBoolFunction base class", "change_title": "Add greater than, less than, etc in Solr function queries", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.2", "detail_description": "If you use the \"if\" function query, you'll often expect to be able to use greater than/less than functions. For example, you might want to boost books written in the past 7 years. Unfortunately, there's no \"greater than\" function query that will return non-zero when the lhs > rhs. Instead to get this, you need to create really awkward function queries like I do here (http://opensourceconnections.com/blog/2014/11/26/stepwise-date-boosting-in-solr/): if(min(0,sub(ms(mydatefield),sub(ms(NOW),315569259747))),0.8,1) The pull request attached to this Jira adds the following function queries (https://github.com/apache/lucene-solr/pull/49) -gt(lhs, rhs) (returns 1 if lhs > rhs, 0 otherwise) -lt(lhs, rhs) (returns 1 if lhs < rhs, 0 otherwise) -gte -lte -eq So instead of if(min(0,sub(ms(mydatefield),sub(ms(NOW),315569259747))),0.8,1) one could now write if(lt(ms(mydatefield),315569259747,0.8,1) (if mydatefield < 315569259747 then 0.8 else 1) A bit more readable and less puzzling", "patch_link": "https://issues.apache.org/jira/secure/attachment/12820490/SOLR-9279.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6662", "change_description": ": Fixed potential resource leaks.", "change_title": "Resource Leaks", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "6.2", "detail_description": "Several resource leaks were identified. I am merging all resource leak issues and creating a single patch as suggested.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743834/LUCENE-6662.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7340", "change_description": ": MemoryIndex.toString() could throw NPE; fixed. Renamed to toStringDebug().", "change_title": "MemoryIndex.toString is broken if you enable payloads", "detail_type": "Bug", "detail_affect_versions": "5.4.1,6.0.1,7.0", "detail_fix_versions": "6.2", "detail_description": "Noticed this as we use Luwak which creates a MemoryIndex(true, true) storing both offsets and payloads (though in reality we never put any payloads in it). We used to use MemoryIndex.toString() for debugging and noticed it broke in Lucene 5.x  and beyond.  I think LUCENE-6155 broke it when it added support for payloads? Creating default memoryindex (as all the tests currently do) works fine, as does one with just offsets, it is just the payload version which is broken.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816311/LUCENE-7340.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7382", "change_description": ": Fix bug introduced by", "change_title": "Wrong default attribute factory in use", "detail_type": "Bug", "detail_affect_versions": "6.2,7.0", "detail_fix_versions": "6.2,7.0", "detail_description": "Originally reported to the mailing list: http://mail-archives.apache.org/mod_mbox/lucene-java-user/201607.mbox/%3cCAJ0VynnMAH7N7byPevTV9Htxo-Nk-B7mwUwRgP4X8gN=V4pYBg@mail.gmail.com%3e LUCENE-7355 made a change to CustomAnalyzer.createComponents() such that it uses a different AttributeFactory. https://github.com/apache/lucene-solr/commit/e92a38af90d12e51390b4307ccbe0c24ac7b6b4e#diff-b39a076156e10aa7a4ba86af0357a0feL122 The previous default was TokenStream.DEFAULT_TOKEN_ATTRIBUTE_FACTORY which uses PackedTokenAttributeImpl while the new default is now AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY which does not use PackedTokenAttributeImpl. thetaphi Asked me to open an issue for this.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12818176/LUCENE-7382.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7355", "change_description": ": Fix bug introduced by", "change_title": "Leverage MultiTermAwareComponent in query parsers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "MultiTermAwareComponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. However, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816841/LUCENE-7355.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7389", "change_description": ": Fix FieldType.setDimensions(...) validation for the dimensionNumBytes\nparameter.", "change_title": "Validation issue in FieldType#setDimensions?", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "It compares if the dimensionCount is larger than PointValues.MAX_NUM_BYTES while this constant should be compared to dimensionNumBytes instead? So this if statement: Should be:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12819632/LUCENE-7383.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7391", "change_description": ": Fix performance regression in MemoryIndex's fields() introduced\nin Lucene 6.", "change_title": "MemoryIndexReader.fields() performance regression", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "6.2", "detail_description": "While upgrading our codebase from Lucene 4 to Lucene 6 we found a significant performance regression - a 5x slowdown On profiling the code, the method MemoryIndexReader.fields() shows up as one of the hottest methods Looking at the method, it just creates a copy of the inner fields Map before passing it to MemoryFields. It does this so that it can filter out fields with numTokens <= 0. The simplest \"fix\" would be to just remove the copying of the map completely, and pass fields directly to MemoryFields.  It's simple and removes any slowdown caused by this method.  It does potentially change behaviour though, but none of the unit tests seem to test that behaviour so I wonder whether it's necessary (I looked at the original ticket LUCENE-7091 that introduced this code, I can't find much in way of an explanation). I'm going to attach a patch to this effect anyway and we can take things from there", "patch_link": "https://issues.apache.org/jira/secure/attachment/12819836/LUCENE-7391.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7395", "change_description": ",", "change_title": "Query Norm and coordination factor not calculated when PerFieldSimilarityWrapper is used", "detail_type": "Bug", "detail_affect_versions": "5.3.1,5.4.1", "detail_fix_versions": "6.2", "detail_description": "If any kind of similarity is defined and therefore the SchemaSimilarityFactory is defined as global similarity the queryNorm is always 1.0 The PerFieldSimilarityWrapper delegates some of the methods to the desired Similarity but misses to delegate public float queryNorm(float valueForNormalization) Instead the IndexReader calls this method on the base class Similarity. The result is that all scores are much higher. I created a custom similarity which extends ClassicSimilarity. To have the calculation fixed I did a local \"hotfix\"  which always uses the default similarity. Also wrong for some cases but fine in my scenario. @Override   public float queryNorm(float valueForNormalization)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12820115/LUCENE-7395.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "SOLR-9315", "change_description": ",", "change_title": "SchemaSimilarityFactory should delegate queryNorm and coord to the default similarity", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This is a follow-up to the discussion with upayavira on LUCENE-6590: SchemaSimilarityFactory can easily build similarities that apply the idf twice.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12818756/SOLR-9315.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "SOLR-9413", "change_description": ": Fix analysis/kuromoji's CSVUtil.quoteEscape logic, add TestCSVUtil test.", "change_title": "String.replace Function result is ignored in lucene/analysis/kuromoji CSVUtil.quoteEscape", "detail_type": "Bug", "detail_affect_versions": "6.1", "detail_fix_versions": "6.2,7.0", "detail_description": "Hello! Code in the method CSVUtil. quoteEscape ignores the return value of the String.replace method. Probably, is should be: This possible defect found by static code analyzer AppChecker", "patch_link": "https://issues.apache.org/jira/secure/attachment/12824124/SOLR-9413.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7419", "change_description": ": Fix performance bug with TokenStream.end(), where it would lookup\nPositionIncrementAttribute every time.", "change_title": "performance bug in tokenstream.end()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.5,6.2,7.0", "detail_description": "TokenStream.end() calls getAttribute(), which is pretty costly to do per-stream. It does its current hack, because in the ctor of TokenStream is \"too early\". Instead, we can just add a variant of clear(), called end() to AttributeImpl. For most attributes it defers to clear, but for PosIncAtt it can handle the special case.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12824445/LUCENE-7419.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7323", "change_description": ": Compound file writing now verifies the incoming\nsub-files' checkums and segment IDs, to catch hardware issues or\nfilesytem bugs earlier", "change_title": "Compound file writing should verify checksum of its sub-files", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "For larger segments, there is a non-trivial window, from when IW writes sub-files, to when it then builds the CFS, during which the files can become corrupted (from external process, bad filesystem, hardware, etc.) Today we quietly build the CFS even if the sub-files are corrupted, but we can easily detect it, letting users catch corruption earlier (write time instead of read time).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809163/LUCENE-7323.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-6766", "change_description": ": Index time sorting has graduated from the misc module\nto core, is much simpler to use, via\nIndexWriter.setIndexSort, and now works with dimensional points.", "change_title": "Make index sorting a first-class citizen", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Today index sorting is a very expert feature. You need to use a custom merge policy, custom collectors, etc. I would like to explore making it a first-class citizen so that:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12803201/LUCENE-6766.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-5931", "change_description": ": Detect when an application tries to reopen an\nIndexReader after (illegally) removing the old index and\nreindexing", "change_title": "DirectoryReader.openIfChanged(oldReader, commit) incorrectly assumes given commit point has deletes/field updates", "detail_type": "Bug", "detail_affect_versions": "4.6.1", "detail_fix_versions": "6.2,7.0", "detail_description": "StandardDirectoryReader assumes that the segments from commit point have deletes, when they may not, yet the original SegmentReader for the segment that we are trying to reuse does. This is evident when running attached JUnit test case with asserts enabled (default): or, if asserts are disabled then it falls through into NPE:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809541/LUCENE-5931.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-6171", "change_description": ": Lucene now passes the StandardOpenOption.CREATE_NEW\noption when writing new files so the filesystem enforces our\nwrite-once architecture, possibly catching externally caused\nissues sooner", "change_title": "Make lucene completely write-once", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Today, lucene is mostly write-once, but not always, and these are just very exceptional cases. This is an invitation for exceptional bugs: (and we have occasional test failures when doing \"no-wait close\" because of this). I would prefer it if we didn't try to delete files before we open them for write, and if we opened them with the CREATE_NEW option by default to throw an exception, if the file already exists. The trickier parts of the change are going to be IndexFileDeleter and exceptions on merge / CFS construction logic. Overall for IndexFileDeleter I think the least invasive option might be to only delete files older than the current commit point? This will ensure that inflateGens() always avoids trying to overwrite any files that were from an aborted segment. For CFS construction/exceptions on merge, we really need to remove the custom \"sniping\" of index files there and let only IndexFileDeleter delete files. My previous failed approach involved always consistently using TrackingDirectoryWrapper, but it failed, and only in backwards compatibility tests, because of LUCENE-6146 (but i could never figure that out). I am hoping this time I will be successful Longer term we should think about more simplifications, progress has been made on LUCENE-5987, but I think overall we still try to be a superhero for exceptions on merge?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809804/LUCENE-6171.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7318", "change_description": ": StandardAnalyzer has been moved from the analysis\nmodule into core and is now the default analyzer in\nIndexWriterConfig", "change_title": "Graduate StandardAnalyzer out of analyzers module into core", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,6.2.1,7.0", "detail_description": "Spinoff from LUCENE-7314: StandardAnalyzer has progressed substantially since we broke out the analyzers module ... it now follows a real Unicode standard (UAX #29 Unicode Text Segmentation).  It's also much faster than it used to be, since it switched to JFlex a while back.  Many bug fixes, etc. I think it would make a good default for most Lucene users, and we should graduate it from the analyzers module into core, and make it the default for IndexWriter. It's really quite crazy that users must go digging in the analyzers module to get started with Lucene ... we don't make them dig through the codecs module to find a good default codec ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12828082/LUCENE-7318-backwards.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7345", "change_description": ": RAMDirectory now enforces write-once files as well", "change_title": "Make RAMDir write-once for files", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "See http://jenkins.thetaphi.de/job/Lucene-Solr-master-MacOSX/3349 The bug here is that we do this crazy copyFrom of all the index files without fsyncing it. So MockDirectoryWrapper treats it as corrupted. But while investigating I noticed that RAMdir is incredibly lenient, will happily truncate existing files and so on. I think we should fix that here too.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12811808/LUCENE-7345.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7337", "change_description": ": MatchNoDocsQuery now scores with 0 normalization factor\nand empty boolean queries now rewrite to MatchNoDocsQuery instead of\nvice/versa", "change_title": "MultiTermQuery are sometimes rewritten into an empty boolean query", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "MultiTermQuery are sometimes rewritten to an empty boolean query (depending on the rewrite method), it can happen when no expansions are found on a fuzzy query for instance. It can be problematic when the multi term query is boosted.  For instance consider the following query: `((title:bar~1)^100 text:bar)` This is a boolean query with two optional clauses. The first one is a fuzzy query on the field title with a boost of 100.  If there is no expansion for \"title:bar~1\" the query is rewritten into: `(()^100 text:bar)` ... and when expansions are found: `((title:bars | title:bar)^100 text:bar)` The scoring of those two queries will differ because the normalization factor and the norm for the first query will be equal to 1 (the boost is ignored because the empty boolean query is not taken into account for the computation of the normalization factor) whereas the second query will have a normalization factor of 10,000 (100*100) and a norm equal to 0.01. This kind of discrepancy can happen in a single index because the expansions for the fuzzy query are done at the segment level. It can also happen when multiple indices are requested (Solr/ElasticSearch case). A simple fix would be to replace the empty boolean query produced by the multi term query with a MatchNoDocsQuery but I am not sure that it's the best way to fix. WDYT ?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12811578/LUCENE-7337.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7359", "change_description": ": Add equals() and hashCode() to Explanation", "change_title": "Add equals() and hashcode() to Explanation", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2", "detail_description": "I don't think there's any reason not to add these?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12814126/LUCENE-7359.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7353", "change_description": ": ScandinavianFoldingFilterFactory and\nScandinavianNormalizationFilterFactory now implement MultiTermAwareComponent.", "change_title": "ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory should implement MultiTermAwareComponent", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "These token filters are safe to apply for multi-term queries.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-2605", "change_description": ": Add classic QueryParser option setSplitOnWhitespace() to\ncontrol whether to split on whitespace prior to text analysis.  Default\nbehavior remains unchanged: split-on-whitespace=true.", "change_title": "queryparser parses on whitespace", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2", "detail_description": "The queryparser parses input on whitespace, and sends each whitespace separated term to its own independent token stream. This breaks the following at query-time, because they can't see across whitespace boundaries: Its also rather unexpected, as users think their charfilters/tokenizers/tokenfilters will do the same thing at index and querytime, but in many cases they can't. Instead, preferably the queryparser would parse around only real 'operators'.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816326/LUCENE-2605-dont-split-by-default.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7276", "change_description": ": MatchNoDocsQuery now includes an optional reason for\nwhy it was used", "change_title": "Add an optional reason to the MatchNoDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "It's sometimes difficult to debug a query that results in a MatchNoDocsQuery. The MatchNoDocsQuery is always rewritten in an empty boolean query. This patch adds an optional reason and implements a weight in order to keep track of the reason why the query did not match any document. The reason is printed on toString and when an explanation for noMatch is asked.   For instance the query: new MatchNoDocsQuery(\"Field not found\").toString() => 'MatchNoDocsQuery[\"field 'title' not found\"]'", "patch_link": "https://issues.apache.org/jira/secure/attachment/12812475/LUCENE-7276.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7355", "change_description": ": AnalyzingQueryParser now only applies the subset of the analysis\nchain that is about normalization for range/fuzzy/wildcard queries.", "change_title": "Leverage MultiTermAwareComponent in query parsers", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "MultiTermAwareComponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. However, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12816841/LUCENE-7355.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7376", "change_description": ": Add support for ToParentBlockJoinQuery to fast vector highlighter's\nFieldQuery.", "change_title": "Add ToParentBlockJoinQuery support to FVH's FieldQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12817611/LUCENE_7376.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7385", "change_description": ": Improve/fix assert messages in SpanScorer.", "change_title": "SpanScorer's assert message strings should reference spans.toString()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2", "detail_description": "SpanScorer.setFreqCurrentDoc has a bunch of assert statements, and they refer to this.toString().  I'm pretty confident the intention was for this to actually be spans.toString(), not \"this\" which is a SpanScorer that doesn't even have a custom toString.  It was probably correct once but after some refactoring of Spans got messed up, probably in LUCENE-6919 (Lucene 5.5).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12818727/LUCENE_7385.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7393", "change_description": ": Add ICUTokenizer option to parse Myanmar text as syllables instead of words,\nbecause the ICU word-breaking algorithm has some issues. This allows for the previous\ntokenization used before Lucene 5.", "change_title": "Incorrect ICUTokenization on South East Asian Language", "detail_type": "Bug", "detail_affect_versions": "5.5", "detail_fix_versions": "6.2,7.0", "detail_description": "Lucene 4.10.3 correctly tokenize a syllable into one token.  However in Lucune 5.5.0 it end up being two tokens which is incorrect.  Please let me know segmentation rules are implemented by native speakers of a particular language? In this particular example, it is M-y-a-n-m-a-r language.  I have understood that L-a-o, K-m-e-r and M-y-a-n-m-a-r fall into ICU category.  Thanks a lot.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12819855/LUCENE-7393.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Improvements", "change_id": "LUCENE-7409", "change_description": ": Changed MMapDirectory's unmapping to work safer, but still with\nno guarantees. This uses a store-store barrier and yields the current thread\nbefore unmapping to allow in-flight requests to finish. The new code no longer\nuses WeakIdentityMap as it delegates all ByteBuffer reads throgh a new\nByteBufferGuard wrapper that is shared between all ByteBufferIndexInput clones.", "change_title": "Look into making mmapdirectory's unmap safer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "I have seen a few bugs around this recently: of course its a bug in application code but a JVM crash is not good. I think we should see if we can prevent the crashes better than the current weak map, e.g. make it a safer option. I made an ugly prototype here: https://github.com/apache/lucene-solr/compare/master...rmuir:ace?expand=1 It has a test that crashes the JVM without the patch but passes with. Hacky patch only implements readBytes() but has no problems with the luceneutil benchmark (1M): We should do more testing. Maybe its totally the wrong tradeoff, maybe we only need handles for getters and everything inlines correctly, rather than needing a ton for every getXYZ() method...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12823505/LUCENE-7409.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7330", "change_description": ",", "change_title": "Speed up conjunctions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "I am digging into some performance regressions between 4.x and 5.x which seem to be due to how we always run conjunctions with ConjunctionDISI now while 4.x had FilteredQuery, which was optimized for the case that there are only two clauses or that one of the clause supports random access. I'd like to explore the former in this issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809493/LUCENE-7330.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7339", "change_description": ",", "change_title": "Bring back RandomAccessFilterStrategy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "FiteredQuery had 3 ways of running conjunctions: leap-frog, query first and random-access filter. We still use leap-frog for conjunctions and we now have a better \"query-first\" strategy through two-phase iteration. However, we don't have any equivalent for the random-access filter strategy.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12811048/LUCENE-7339.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7356", "change_description": ": SearchGroup tweaks.", "change_title": "SearchGroup tweaks", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12813103/LUCENE-7356.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7351", "change_description": ": Doc id compression for points.", "change_title": "BKDWriter should compress doc ids when all values in a block are the same", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "BKDWriter writes doc ids using 4 bytes per document. I think it should compress similarly to postings when all docs in a block have the same packed value. This can happen either when a field has a default value which is common across documents or when quantization makes the number of unique values so small that a large index will necessarily have blocks that all contain the same value (eg. there are only 63490 unique half-float values).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12815504/LUCENE-7351.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7371", "change_description": ": Point values are now better compressed using run-length\nencoding.", "change_title": "BKDReader could compress values better", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "For compressing values, BKDReader only relies on shared prefixes in a block. We could probably easily do better. For instance there are only 256 possible values for the first byte of the dimension that the values are sorted by, yet we use a block size of 1024. So by using something simple like run-length compression we could save 6 bits per value on average.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12817414/LUCENE-7371.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7311", "change_description": ": Cached term queries do not seek the terms dictionary anymore.", "change_title": "TermWeight shoud seek terms lazily", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Currently the terms are seeked eagerly in TermQuery.createWeight when creating the TermContext. This might be wasteful when scores are not needed since the query might be cached on some segments, thus seeking the term on these segments is not needed. We could change TermWeight to only seek terms in Weight.scorer when scores are not needed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12817626/LUCENE-7311.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7396", "change_description": ",", "change_title": "Speed up flush of points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "1D points already have an optimized merge implementation which works when points come in order. So maybe we could make IndexWriter's PointValuesWriter sort before feeding the PointsFormat and somehow propagate the information to the PointsFormat? The benefit is that flushing could directly stream points to disk with little memory usage.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12820740/LUCENE-7396.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7399", "change_description": ",", "change_title": "Speed up flush of points v2", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "There are improvements we can make on top of LUCENE-7396 to get ever better flush performance.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12821541/LUCENE-7399.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7406", "change_description": ": Automaton and PrefixQuery tweaks (fewer object (re)allocations).", "change_title": "Automaton and PrefixQuery tweaks", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Automaton: PrefixQuery:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12822114/LUCENE-7406.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Other", "change_id": "LUCENE-4787", "change_description": ": Fixed some highlighting javadocs.", "change_title": "The QueryScorer.getMaxWeight method is not found.", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "6.2,7.0", "detail_description": "The following API documents refer to the QueryScorer.getMaxWeight method: http://lucene.apache.org/core/4_1_0/highlighter/org/apache/lucene/search/highlight/package-summary.html \"The QueryScorer.getMaxWeight method is useful when passed to the GradientFormatter constructor to define the top score which is associated with the top color.\" http://lucene.apache.org/core/4_1_0/highlighter/org/apache/lucene/search/highlight/GradientFormatter.html \"See QueryScorer.getMaxWeight which can be used to calibrate scoring scale\" However, the QueryScorer class does not declare a getMaxWeight method in lucene 4.1, according to its document: http://lucene.apache.org/core/4_1_0/highlighter/org/apache/lucene/search/highlight/QueryScorer.html Instead, the class declares a getMaxTermWeight method. Is that the correct method in the preceding two documents? If it is, please revise the two documents.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12653033/LUCENE-4787.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Other", "change_id": "LUCENE-7334", "change_description": ": Update ASM dependency to 5.1.", "change_title": "Update ASM to 5.1 (expressions,...)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.0", "detail_description": "We should update ASM to version 5.1. The change is easy, no real code changes needed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12809658/LUCENE-7334.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Other", "change_id": "LUCENE-7346", "change_description": ": Update forbiddenapis to version 2.2.", "change_title": "Update forbiddenapis to version 2.2", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "Forbidden-Apis 2.2 was released an hour ago: https://github.com/policeman-tools/forbidden-apis/wiki/Changes This version supports/fixes the following important stuff: I will post & commit patch soon", "patch_link": "https://issues.apache.org/jira/secure/attachment/12811722/LUCENE-7346.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Other", "change_id": "LUCENE-7360", "change_description": ": Explanation.toHtml() is deprecated.", "change_title": "Remove Explanation.toHtml()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "This seems to be something of a relic.  It's still used in Solr, but I think it makes more sense to move it directly into the ExplainAugmenter there rather than having it in Lucene itself.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12814143/LUCENE-7360.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Other", "change_id": "LUCENE-7372", "change_description": ": Factor out an org.apache.lucene.search.FilterWeight class.", "change_title": "factor out a org.apache.lucene.search.FilterWeight class", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.2,7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12817681/LUCENE-7372.patch", "patch_content": "none"}
{"library_version": "6.2.0", "change_type": "Other", "change_id": "LUCENE-7384", "change_description": ": Removed ScoringWrapperSpans. And tweaked SpanWeight.buildSimWeight() to\nreuse the existing Similarity instead of creating a new one.", "change_title": "Remove ScoringWrapperSpans", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.2", "detail_description": "In LUCENE-6919 (Lucene 5.5), ScoringWrapperSpans was modified in such a way that made the existence of this class pointless, and possibly broke anyone who was using it as it's SimScorer argument isn't used anymore.  We should now delete it.  SpanWeight has getSimScorer() so people can customize the SimScorer that way. Another small change I observe to improve is have SpanWeight.buildSimWeight's last line use the existing Similarity that has already been populated on the field?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12818683/LUCENE_7384.patch", "patch_content": "none"}

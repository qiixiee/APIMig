{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6308", "change_description": ",", "change_title": "Spans to extend DocIdSetIterator; was: SpansEnum, deprecate Spans", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "5.2,6.0", "detail_description": "An alternative for Spans that looks more like PositionsEnum and adds two phase doc id iteration", "patch_link": "https://issues.apache.org/jira/secure/attachment/12707989/LUCENE-6308-changeapi.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6385", "change_description": ",", "change_title": "NullPointerException from Highlighter.getBestFragment()", "detail_type": "Bug", "detail_affect_versions": "5.2", "detail_fix_versions": "5.2,6.0", "detail_description": "When testing against the 5.1 nightly snapshots I've come across a NullPointerException in highlighting when nothing would be highlighted. This does not happen with 5.0. I've written a small unit test and used git bisect to narrow the regression to the following commit: The problem looks quite simple, WeightedSpanTermExtractor.extractWeightedSpanTerms() needs an early return if SpanQuery.getSpans() returns null. All other callers check against this. Unit test and fix (against the regressed commit) attached.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708698/LUCENE-6385.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6391", "change_description": ",", "change_title": "Give SpanScorer two-phase iterator support.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Fix SpanScorer to use any two-phase iterator support of the underlying Spans. This means e.g. a spans in a booleanquery, or a spans with a filter can be faster. In order to do this, we have to clean up this class a little bit:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12709259/LUCENE-6391.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6393", "change_description": ": Add two-phase support to SpanPositionCheckQuery\nand its subclasses: SpanPositionRangeQuery, SpanPayloadCheckQuery,\nSpanNearPayloadCheckQuery, SpanFirstQuery.", "change_title": "Add two-phase support to SpanPositionCheckQuery and subclasses", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "This hits an assert in SpanScorer because it breaks the javadocs contract of Spans.nextStartPosition():", "patch_link": "https://issues.apache.org/jira/secure/attachment/12709444/LUCENE-6393.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6394", "change_description": ": Add two-phase support to SpanNotQuery and refactor\nFilterSpans to just have an accept(Spans candidate) method for\nsubclasses.", "change_title": "Add two-phase support to SpanNotQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "This query is actually a lot like SpanPositionCheckQuery, except it checks that each inclusion Spans does not come near the exclusion side. Two-phase iteration should just work the inclusion side, deferring positions (the overlap checking against exclusion) until necessary.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12723311/LUCENE-6394.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6373", "change_description": ": SpanOrQuery shares disjunction logic with boolean\nqueries, and supports two-phased iterators to avoid loading\npositions when possible.", "change_title": "Complete two phase doc id iteration support for Spans", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Spin off from LUCENE-6308, see comments there from about 23 March 2015.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12728224/LUCENE-6373-20150426.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6352", "change_description": ",", "change_title": "Add global ordinal based query time join", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Global ordinal based query time join as an alternative to the current query time join. The implementation is faster for subsequent joins between reopens, but requires an OrdinalMap to be built. This join has certain restrictions and requirements:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708645/LUCENE-6352.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6472", "change_description": ",", "change_title": "Add min and max document options to global ordinal join", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "This feature allows to only match \"to\" documents that have at least between min and max matching  \"from\" documents.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12731411/LUCENE-6472.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-5879", "change_description": ": Added experimental auto-prefix terms to BlockTree terms\ndictionary, exposed as AutoPrefixPostingsFormat", "change_title": "Add auto-prefix terms to block tree terms dict", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "This cool idea to generalize numeric/trie fields came from Adrien: Today, when we index a numeric field (LongField, etc.) we pre-compute (via NumericTokenStream) outside of indexer/codec which prefix terms should be indexed. But this can be inefficient: you set a static precisionStep, and always add those prefix terms regardless of how the terms in the field are actually distributed.  Yet typically in real world applications the terms have a non-random distribution. So, it should be better if instead the terms dict decides where it makes sense to insert prefix terms, based on how dense the terms are in each region of term space. This way we can speed up query time for both term (e.g. infix suggester) and numeric ranges, and it should let us use less index space and get faster range queries. This would also mean that min/maxTerm for a numeric field would now be correct, vs today where the externally computed prefix terms are placed after the full precision terms, causing hairy code like NumericUtils.getMaxInt/Long.  So optos like LUCENE-5860 become feasible. The terms dict can also do tricks not possible if you must live on top of its APIs, e.g. to handle the adversary/over-constrained case when a given prefix has too many terms following it but finer prefixes have too few (what block tree calls \"floor term blocks\").", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708812/LUCENE-5879.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-5579", "change_description": ": New CompositeSpatialStrategy combines speed of RPT with\naccuracy of SDV. Includes optimized Intersect predicate to avoid many\ngeometry checks. Uses TwoPhaseIterator.", "change_title": "Spatial, enhance RPT to differentiate confirmed from non-confirmed hits, then validate with SDV", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "If a cell is within the query shape (doesn't straddle the edge), then you can be sure that all documents it matches are a confirmed hit. But if some documents are only on the edge cells, then those documents could be validated against SerializedDVStrategy for precise spatial search. This should be much faster than using RPT and SerializedDVStrategy independently on the same search, particularly when a lot of documents match. Perhaps this'll be a new RPT subclass, or maybe an optional configuration of RPT.  This issue is just for the Intersects predicate, which will apply to Disjoint.  Until resolved in other issues, the other predicates can be handled in a naive/slow way by creating a filter that combines RPT's filter and SerializedDVStrategy's filter using BitsFilteredDocIdSet. One thing I'm not sure of is how to expose to Lucene-spatial users the underlying functionality such that they can put other query/filters in-between RPT and the SerializedDVStrategy.  Maybe that'll be done by simply ensuring the predicate filters have this capability and are public. It would be ideal to implement this capability after the PrefixTree term encoding is modified to differentiate edge leaf-cells from non-edge leaf cells. This distinction will allow the code here to make more confirmed matches.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12704805/LUCENE-5579_SPT_leaf_covered.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-5989", "change_description": ": Allow passing BytesRef to StringField to make it easier\nto index arbitrary binary tokens, and change the experimental\nStoredFieldVisitor.stringField API to take UTF-8 byte[] instead of\nString", "change_title": "Allow StringField to take BytesRef value, to index a single binary token", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "5 years ago (LUCENE-1458) we \"enabled\" fully binary terms in the lowest levels of Lucene (the codec APIs) yet today, actually adding an arbitrary byte[] binary term during indexing is far from simple: you must make a custom Field with a custom TokenStream and a custom TermToBytesRefAttribute, as far as I know. This is supremely expert, I wonder if anyone out there has succeeded in doing so? I think we should make indexing a single byte[] as simple as indexing a single String. This is a pre-cursor for issues like LUCENE-5596 (encoding IPv6 address as byte[16]) and LUCENE-5879 (encoding native numeric values in their simple binary form).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12723596/LUCENE-5989.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6389", "change_description": ": Added ScoreMode.Min that aggregates the lowest child score\nto the parent hit.", "change_title": "Add ScoreMode.Min", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "In addition to the existing score modes add 'min' score mode that aggregates the lowest child score to the parent hit.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12723576/LUCENE-6389.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6423", "change_description": ": New LimitTokenOffsetFilter that limits tokens to those before\na configured maximum start offset.", "change_title": "New LimitTokenOffsetFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "It would be nice to have a token filter that limited based on the offset.  I suggest the start offset.  It should be named LimitTokenOffsetFilter to have a name similar to the other LimitToken*Filter choices, including a \"consumeAllTokens\" option.  I plan to use this filter in LUCENE-6392 (to limit tokens from analyzed text for applicable methods in TokenSources) and in LUCENE-6375.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12726419/LUCENE-6423_LimitTokenOffsetFilter.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6422", "change_description": ": New spatial PackedQuadPrefixTree, a generally more efficient\nchoice than QuadPrefixTree, especially for high precision shapes.\nWhen used, you should typically disable RPT's pruneLeafyBranches option.", "change_title": "Add PackedQuadPrefixTree", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "This task introduces a PackedQuadPrefixTree that includes two things: 1.  A packed 8 byte representation for a QuadCell, including more efficient implementations of the SPT API than the existing QuadPrefixTree or GeoHashPrefixTree. 2.  An alternative implementation to RPT's \"pruneLeafyBranches\" that streams the cells without buffering them all, which is way more memory efficient.  However pruning is limited to the target detail level, where it accomplishes the most good. Future improvements over this approach may include the generation of the packed cells using an AutoPrefixAutomaton", "patch_link": "https://issues.apache.org/jira/secure/attachment/12727300/LUCENE-6422.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6451", "change_description": ": Expressions now support bindings keys that look like\nzero arg functions", "change_title": "Support non-static methods in the Javascript compiler", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Allow methods such as date.getMonth() or string.getOrdinal() to be added in the same way expression variables are now (forwarded to the bindings for processing).  This change will only allow non-static methods that have zero arguments due to current limitations in the architecture, and to keep the change simple.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12727724/LUCENE-6451.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6083", "change_description": ": Add SpanWithinQuery and SpanContainingQuery that return\nspans inside of / containing another spans.", "change_title": "Span containing/contained queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "SpanContainingQuery reducing a spans to where it is containing another spans. SpanContainedQuery reducing a spans to where it is contained in another spans.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12728889/LUCENE-6083.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6454", "change_description": ": Added distinction between member variable and method in\nexpression helper VariableContext", "change_title": "Support Member Methods in VariableContext", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "The Javascript compiler now supports simple member methods being processed by expression Bindings.  The VariableContext should also support being able to parse member methods.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12728524/LUCENE-6454.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6196", "change_description": ": New Spatial \"Geo3d\" API with partial Spatial4j integration.\nIt is a set of shapes implemented using 3D planar geometry for calculating\nspatial relations on the surface of a sphere. Shapes include Point, BBox,\nCircle, Path (buffered line string), and Polygon.", "change_title": "Include geo3d package, along with Lucene integration to make it useful", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "I would like to explore contributing a geo3d package to Lucene.  This can be used in conjunction with Lucene search, both for generating geohashes (via spatial4j) for complex geographic shapes, as well as limiting results resulting from those queries to those results within the exact shape in highly performant ways. The package uses 3d planar geometry to do its magic, which basically limits computation necessary to determine membership (once a shape has been initialized, of course) to only multiplications and additions, which makes it feasible to construct a performant BoostSource-based filter for geographic shapes.  The math is somewhat more involved when generating geohashes, but is still more than fast enough to do a good job.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725741/LUCENE-6196-fixes.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "New Features", "change_id": "LUCENE-6464", "change_description": ": Add a new expert lookup method to\nAnalyzingInfixSuggester to accept an arbitrary BooleanQuery to\nexpress how contexts should be filtered.", "change_title": "Allow possibility to group contexts in AnalyzingInfixSuggester.loockup()", "detail_type": "Improvement", "detail_affect_versions": "5.1", "detail_fix_versions": "5.2,6.0", "detail_description": "This is an enhancement to LUCENE-6050  LUCENE-6050 added which allowed to do something like (A OR B AND C OR D ...) In our use-case, we realise that we need grouping i.e (A OR B) AND (C OR D) AND (...) In other words, we need the intersection of multiple contexts. The attached patch allows to pass in a varargs of map, each one representing the each group. Looks a bit heavy IMHO. This is an initial patch. The question to mikemccand and janechang is: is it better to expose a FilteredQuery/Query and let the user build their own query instead of passing a map? Exposing a filteredQuery will probably give the best flexibility to the end-users.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12732999/LUCENE-6464.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6379", "change_description": ": IndexWriter.deleteDocuments(Query...) now detects if\none of the queries is MatchAllDocsQuery and just invokes the much\nfaster IndexWriter.deleteAll in that case", "change_title": "IndexWriter's delete-by-query should optimize/specialize MatchAllDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.1,6.0", "detail_description": "We can short-circuit this to just IW.deleteAll (Solr already does so I think).  This also has the nice side effect of clearing Lucene's low-schema (FieldInfos).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708462/LUCENE-6379.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6388", "change_description": ": Optimize SpanNearQuery when payloads are not present.", "change_title": "Optimize SpanNearQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "After the big spans overhaul in LUCENE-6308, we can speed up SpanNearQuery a little more:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708921/LUCENE-6388.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6421", "change_description": ": Defer reading of positions in MultiPhraseQuery until\nthey are needed.", "change_title": "Add two-phase support to MultiPhraseQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Two-phase support currently works for both sloppy and exact Scorers but it does not work if you have multiple terms at the same position (MultiPhraseQuery). This is because UnionPostingsEnum.nextDoc() aggressively reads and merges all the positions. Even making this initialization lazy might just be enough?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725147/LUCENE-6421_luceneutil.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6392", "change_description": ": Highligher- reduce memory of tokens in\nTokenStreamFromTermVector, and add maxStartOffset limit.", "change_title": "Add offset limit to Highlighter's TokenStreamFromTermVector", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "The Highlighter's TokenStreamFromTermVector utility, typically accessed via TokenSources, should have the ability to filter out tokens beyond a configured offset. There is a TODO there already, and this issue addresses it.  New methods in TokenSources now propagate a limit. This patch also includes some memory saving optimizations, to be described shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12709274/LUCENE-6392_highlight_term_vector_maxStartOffset.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6456", "change_description": ": Queries that generate doc id sets that are too large for the\nquery cache are not cached instead of evicting everything.", "change_title": "Don't cache queries that are too large for the query cache", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "We have a default query cache with a reasonable size: 32MB. However if you happen to have a large index (eg. 1B docs), this might be too small even to store a single cached filter. In such cases we should not even try to cache instead of generating cache entries and trashing them immediately.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12728869/LUCENE-6456.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6455", "change_description": ": Require a minimum index size to enable query caching in order\nnot to cache eg. on MemoryIndex.", "change_title": "Do not cache queries on small indices", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "The query cache currently only caches on segments that are more than 3% of the total index size currently. Another good condition could be to require a minimum index size so that we don't end up caching on things like MemoryIndex.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12728856/LUCENE-6455.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6330", "change_description": ": BooleanScorer (used for top-level disjunctions) does not decode\nnorms when not necessary anymore.", "change_title": "BooleanScorer should not call score() when scores are not needed", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "BooleanScorer (and maybe others?) still call score() when scores are not needed. While this does not make results wrong, it is sub-optimal since it still forces to decode norms while they are not needed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12729086/LUCENE-6330.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6350", "change_description": ": TermsQuery is now compressed with PrefixCodedTerms.", "change_title": "switch TermsQuery to prefixcodedterms", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "This will save ram and cleanup a lot of the code. Unfortunately the code is still a mess, it has a custom iterator api, and prefixcodedterms has yet another custom iterator api (seriously, maybe the worst one ever).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12730798/LUCENE-6350.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6458", "change_description": ": Multi-term queries matching few terms per segment now execute\nlike a disjunction.", "change_title": "MultiTermQuery's FILTER rewrite method should support skipping whenever possible", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Today MultiTermQuery's FILTER rewrite always builds a bit set fom all matching terms. This means that we need to consume the entire postings lists of all matching terms. Instead we should try to execute like regular disjunctions when there are few terms.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12734432/LUCENE-6458-2.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Optimizations", "change_id": "LUCENE-6360", "change_description": ": TermsQuery rewrites to a disjunction when there are 16 matching\nterms or less.", "change_title": "TermsQuery should rewrite to a ConstantScoreQuery over a BooleanQuery when there are few terms", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "TermsQuery helps when there are lot of terms from which you would like to compute the union, but it is a bit harmful when you have few terms since it cannot really skip: it always consumes all documents matching the underlying terms. It would certainly help to rewrite this query to a ConstantScoreQuery over a BooleanQuery when there are few terms in order to have actual skip support. As usual the hard part is probably to figure out the threshold.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12734573/LUCENE-6360.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-329", "change_description": ": Fix FuzzyQuery defaults to rank exact matches highest.", "change_title": "Fuzzy query scoring issues", "detail_type": "Bug", "detail_affect_versions": "1.2", "detail_fix_versions": "5.3,6.0", "detail_description": "Queries which automatically produce multiple terms (wildcard, range, prefix,  fuzzy etc)currently suffer from two problems: 1) Scores for matching documents are significantly smaller than term queries  because of the volume of terms introduced (A match on query Foo~ is 0.1  whereas a match on query Foo is 1). 2) The rarer forms of expanded terms are favoured over those of more common  forms because of the IDF. When using Fuzzy queries for example, rare mis- spellings typically appear in results before the more common correct spellings. I will attach a patch that corrects the issues identified above by  1) Overriding Similarity.coord to counteract the downplaying of scores  introduced by expanding terms. 2) Taking the IDF factor of the most common form of expanded terms as the  basis of scoring all other expanded terms.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12733790/LUCENE-329.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6378", "change_description": ": Fix all RuntimeExceptions to throw the underlying root cause.", "change_title": "Fix RuntimeExceptions that are thrown without the root cause", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "In the lucene/solr codebase I can see 15 RuntimeExceptions that are thrown without wrapping the root cause. We should fix them to wrap the root cause before throwing it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708405/LUCENE-6378.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6415", "change_description": ": TermsQuery.extractTerms is a no-op (used to throw an\nUnsupportedOperationException).", "change_title": "TermsQuery.extractTerms should not throw an UOE", "detail_type": "Bug", "detail_affect_versions": "5.1", "detail_fix_versions": "5.2,6.0", "detail_description": "TermsQuery inherits the default impl of extractTerms which throws an UnsupportedOperationException.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12724572/LUCENE-6415.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6416", "change_description": ": BooleanQuery.extractTerms now only extracts terms from scoring\nclauses.", "change_title": "BooleanQuery should only extract terms from scoring clauses", "detail_type": "Bug", "detail_affect_versions": "5.1,6.0", "detail_fix_versions": "5.2,6.0", "detail_description": "BooleanQuery should not extract terms from FILTER clauses.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12724563/LUCENE-6416.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6409", "change_description": ": Fixed integer overflow in LongBitSet.ensureCapacity.", "change_title": "LongBitSet.ensureCapacity overflows on numBits > Integer.MaxValue", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "LongBitSet.ensureCapacity calculates the number of longs required to store the number of bits correctly and allocates a long[] accordingly, but then shifts the array length (which is an int!) left by 6 bits.  The int should be cast to long before performing the shift.", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6424", "change_description": ",", "change_title": "DirectoryStream<Path> doesnt wrap with FilterPath", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "This can cause various mayhem e.g. globs with Files.newDirectoryStream may not work correctly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725457/LUCENE-6424.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6430", "change_description": ",", "change_title": "FilterPath needs hashCode/equals", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Its defined here: https://docs.oracle.com/javase/7/docs/api/java/nio/file/Path.html#equals%28java.lang.Object%29 Currently we always use Object.equals/hashcode", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725882/LUCENE-6430.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6426", "change_description": ": Fix FieldType's copy constructor to also copy over the numeric\nprecision step.", "change_title": "FieldType copy constructor does not copy the numeric precision step", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "If you create a new FieldType with new FieldType(existingFieldType), the precision step will not be copied over.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725578/LUCENE-6426.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6345", "change_description": ": Null check terms/fields in Lucene queries", "change_title": "null check all term/fields in queries", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "See the mail thread \"is this lucene 4.1.0 bug in PerFieldPostingsFormat\". If anyone seriously thinks adding a null check to ctor will cause measurable slowdown to things like regexp or wildcards, they should have their head examined. All queries should just check this crap in ctor and throw exceptions if parameters are invalid.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12704945/LUCENE-6345.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6400", "change_description": ": SolrSynonymParser should preserve original token instead\nof replacing it with a synonym, when expand=true and there is no\nexplicit mapping", "change_title": "SynonymParser should encode 'expand' correctly.", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Today SolrSynonymParser encodes something like A, B, C with 'expand=true' like this: A -> A, B, C (includeOrig=false) B -> B, A, C (includeOrig=false) C -> C, A, B (includeOrig=false) This gives kinda buggy output (synfilter sees it all as replacements, and makes all the terms with type synonym, positionLength isnt supported, etc) and it wastes space in the FST (includeOrig is just one bit). Example with \"spiderman, spider man\" and analysis on 'spider man' Trunk: term=spider,startOffset=0,endOffset=6,positionIncrement=1,positionLength=1,type=SYNONYM term=spiderman,startOffset=0,endOffset=10,positionIncrement=0,positionLength=1,type=SYNONYM term=man,startOffset=7,endOffset=10,positionIncrement=1,positionLength=1,type=SYNONYM You can see this is confusing, all the words have type SYNONYM, because spider and man got deleted, and totally replaced by new terms (Which happen to have the same text). Patch: term=spider,startOffset=0,endOffset=6,positionIncrement=1,positionLength=1,type=word term=spiderman,startOffset=0,endOffset=10,positionIncrement=0,positionLength=2,type=SYNONYM term=man,startOffset=7,endOffset=10,positionIncrement=1,positionLength=1,type=word", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725953/LUCENE-6400.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6449", "change_description": ": Don't throw NullPointerException if some segments are\nmissing the field being highlighted, in PostingsHighlighter", "change_title": "NullPointerException in PostingsHighlighter", "detail_type": "Bug", "detail_affect_versions": "5.1", "detail_fix_versions": "5.2,6.0", "detail_description": "In case an index segment does not have any docs with the field requested for highlighting indexed, there should be a null check immediately following this line (in PostingsHighlighter.java): Terms t = r.terms(field); Looks like the null check was moved in the 5.1 release and this is occasionally causing a NullPointerException in my near-realtime searcher.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12727693/postingshighlighter.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6427", "change_description": ": Added assertion about the presence of ghost bits in\n(Fixed|Long)BitSet.", "change_title": "BitSet fixes - assert on presence of 'ghost bits' and others", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Fixes after reviewing org.apache.lucene.util.FixedBitSet, LongBitSet and corresponding tests:", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6468", "change_description": ": Fixed NPE with empty Kuromoji user dictionary.", "change_title": "Empty kuromoji user dictionary -> NPE", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Kuromoji user dictionary takes Reader and allows for comments and other lines to be ignored. But if its \"empty\" in the sense of no actual entries, the returned FST will be null, and it will throw a confusing NPE. JapaneseTokenizer and JapaneseAnalyzer apis already treat null UserDictionary as having none at all, so I think the best fix is to fix the UserDictionary api from UserDictionary(Reader) to UserDictionary.open(Reader) or similar, and return null if the FST is empty.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12731710/LUCENE-6468.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6483", "change_description": ": Ensure core closed listeners are called on the same cache key as\nthe reader which has been used to register the listener.", "change_title": "Core closed listeners should be called with the same cache key as the one returned by getCoreCacheKey", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Core cache keys are typically used for caching, for instance we use them in the query cache and when uninverting. However, since FilterLeafReader delegates addCoreClosedListener by default, these listeners will always be called with the cache key of the wrapper reader. So if you happen to use a filter reader that overrides the core cache key, you are going to try to evict on a different key than the one that you used for caching. We should ensure that core closed listeners are always called on the same key as the one returned by #getCoreCacheKey", "patch_link": "https://issues.apache.org/jira/secure/attachment/12733096/LUCENE-6483.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6486", "change_description": "DocumentDictionary iterator no longer skips\ndocuments with no payloads and now returns an empty BytesRef instead", "change_title": "DocumentDictionary entry iterator skips items with optional null payload field", "detail_type": "Bug", "detail_affect_versions": "4.10.3", "detail_fix_versions": "5.2,6.0", "detail_description": "As denoted in the ticket SOLR-7086 the DocumentDictionary entry iterator shouldn't skip entries from the dictionary having null value for the payload field due to the fact that this field is optional. This behaviour causes inconsistencies in the Solr suggester which simply skips valid documents due to the fact that they don't have values for the payload field. As agreed with mikemccand I am attaching a patch to this Lucene issue.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12734152/LUCENE-6486.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6505", "change_description": ": NRT readers now reflect segments_N filename and commit\nuser data from previous commits", "change_title": "NRT readers don't always reflect last commit", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Two cases here:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12735708/LUCENE-6505.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-6507", "change_description": ": Don't let NativeFSLock.close() release other locks", "change_title": "NativeFSLock.close() can invalidate other locks", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "the lock API in Lucene is super trappy since the lock that we return form this API must first be obtained and if we can't obtain it the lock should not be closed since we might ie. close the underlying channel in the NativeLock case which releases all lock for this file on some operating systems. I think the makeLock method should try to obtain and only return a lock if we successfully obtained it. Not sure if it's possible everywhere but we should at least make the documentation clear here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12736146/LUCENE-6507-410x.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6377", "change_description": ": SearcherFactory#newSearcher now accepts the previous reader\nto simplify warming logic during opening new searchers.", "change_title": "Pass previous reader to SearcherFactory", "detail_type": "Improvement", "detail_affect_versions": "5.0", "detail_fix_versions": "5.2,6.0", "detail_description": "SearcherFactory is often used as advertised for warming segments for newly flushed segments or for searchers that are opened for the first time (generally where merge warmers don't apply). To make this simpler we should pass the previous reader to the factory as well to know what needs to be warmed.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12708683/LUCENE-6377.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6410", "change_description": ": Removed unused \"reuse\" parameter to\nTerms.iterator.", "change_title": "Remove unused \"reuse\" param to Terms.iterator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Terms.iterator takes a reuse param but no impls in fact reuse it.  I think callers can just hang onto a TermsEnum and reuse themselves instead?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12723933/LUCENE-6410.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6425", "change_description": ": Replaced Query.extractTerms with Weight.extractTerms.", "change_title": "Move extractTerms to Weight", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Today we have extractTerms on Query, but it is supposed to only be called after the query has been specialized to a given IndexReader using Query.rewrite(IndexReader) to allow some complex queries to replace terms \"matchers\" with actual terms (eg. WildcardQuery). However, we already have an abstraction for indexreader-specialized queries: Weight. So I think it would make more sense to have extractTerms on Weight. This would also remove the trap of calling extractTerms on a query which is not rewritten yet. Since Weights know about whether scores are needed or not, I also hope this would help improve the extractTerms semantics. We currently have 2 use-cases for extractTerms: distributed IDF and highlighting. While the former only cares about terms which are used for scoring, it could make sense to highlight terms that were used for matching, even if they did not contribute to the score (eg. if wrapped in a ConstantScoreQuery or a BooleanQuery FILTER clause). So highlighters could do searcher.createNormalizedWeight(query, false).extractTerms(termSet) to get all terms that were used for matching the query while distributed IDF would instead do searcher.createNormalizedWeight(query, true).extractTerms(termSet) to get scoring terms only.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725590/LUCENE-6425.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6446", "change_description": ": Simplified Explanation API.", "change_title": "Simplify Explanation API", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "We should make this API easier to consume, for instance:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12726689/LUCENE-6446.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6445", "change_description": ": Two new methods in Highlighter's TokenSources; the existing\nmethods are now marked deprecated.", "change_title": "Highlighter TokenSources simplification; just one getAnyTokenStream()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "The Highlighter \"TokenSources\" class has quite a few utility methods pertaining to getting a TokenStream from either term vectors or analyzed text.  I think it's too much:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12728016/LUCENE-6445_TokenSources_simplification.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6484", "change_description": ": Removed EliasFanoDocIdSet, which was unused.", "change_title": "Remove EliasFanoDocIdSet", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "EliasFanoDocIdSet is currently unused, remove it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12733114/LUCENE-6484.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "API Changes", "change_id": "LUCENE-6497", "change_description": ": Allow subclasses of FieldType to check frozen state", "change_title": "Allow subclasses of FieldType to check frozen state", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "checkIfFrozen() is currently private. We should this protected, so subclasses of FieldType can add additional state that is protected by freezing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12734892/LUCENE-6497.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Other", "change_id": "LUCENE-6413", "change_description": ": Test runner should report the number of suites completed/\nremaining.", "change_title": "Test runner should report the number of suites completed/ remaining", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "Pretty much as suggested by Shawn on the dev list. The number of individual tests cannot be printed (it's not available globally to the runner until the suite is actually executed). There is no ETA remaining for similar reasons (the variance on each suite's execution time is unpredictable).", "patch_link": "none", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Other", "change_id": "LUCENE-5439", "change_description": ": Add 'ant jacoco' build target.", "change_title": "Add Jacoco option for Test Coverage", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Jacoco (http://www.jacoco.org/) is a much cleaner and simpler to use code coverage tool than clover and additionally doesn't require having a third party license since it is open source.  It also has nice Jenkins integration tools that make it incredibly easy to see what is and isn't tested.  We should convert the Lucene and Solr builds to use Jacoco instead of Clover.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12724138/LUCENE-5439.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Other", "change_id": "LUCENE-6315", "change_description": ": Simplify the private iterator Lucene uses internally\nwhen resolving deleted terms to matched docids.", "change_title": "Simplify custom Term iterator used to resolve deletions", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.3,6.0", "detail_description": "In LUCENE-6161 we added yet-another-term-iterator ... this patch tries to simplify that by re-using the existing BytesRefIterator instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12701686/LUCENE-6315.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Other", "change_id": "LUCENE-6399", "change_description": ": Benchmark module's QueryMaker.resetInputs should call setConfig\nso queries can react to property changes in new rounds.", "change_title": "Benchmark's AbstractQueryMaker.resetInputs should call setConfig", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2", "detail_description": "DocMaker.resetInput() will call setConfig.  QueryMaker should too for the same reason â€“ so that it can respond to properties that change per round.  DocMaker is concrete but QueryMaker is an interface, but this behavior can be put into AbstractQueryMaker. I found this as some benchmarking was driving me crazy as I couldn't get spatial benchmark queries to see changes I had!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12723455/LUCENE-6399_Benchmark_QueryMaker_resetInput_setConfig.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Other", "change_id": "LUCENE-6382", "change_description": ": Lucene now enforces that positions never exceed the\nmaximum value IndexWriter.MAX_POSITION.", "change_title": "Don't allow position = Integer.MAX_VALUE going forward", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Spinoff from LUCENE-6308, where Integer.MAX_VALUE position is now used as a sentinel during position iteration to indicate that there are no more positions. Where IW now detects int overflow of position, it should now also detect == Integer.MAX_VALUE. And CI should note corruption if a segment's version is >= 5.2 and has Integer.MAX_VALUE position.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725277/LUCENE-6382.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Other", "change_id": "LUCENE-6372", "change_description": ": Simplified and improved equals/hashcode of span queries.", "change_title": "Simplify hashCode/equals for SpanQuery subclasses", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Spin off from LUCENE-6308, see the comments there from around 23 March 2015.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12730799/LUCENE-6372.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Build", "change_id": "LUCENE-6420", "change_description": ": Update forbiddenapis to v1.8", "change_title": "Update forbiddenapis to 1.8", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "Update forbidden-apis plugin to 1.8: This will for now only update the dependency and remove the additional forbid by shalinmangar for MessageFormat (which is now shipped with forbidden). But we should review and for example suppress forbidden failures in command line tools using @SuppressForbidden (or similar annotation). The discussion is open, I can make a patch.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12726538/LUCENE-6420-anno.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Test Framework", "change_id": "LUCENE-6419", "change_description": ": Added two-phase iteration assertions to AssertingQuery.", "change_title": "Add AssertingQuery / two-phase iteration to AssertingScorer", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "I am working on a similar issue with Spans (LUCENE-6411). AssertingScorer is currently only used as a top-level wrapper, and it doesnt support asserting for asTwoPhaseIterator (wouldn't help at the moment, the way it is currently used). Today some good testing of this is done, but only when RandomApproximationQuery is explicitly used. I think we should add AssertingQuery that can wrap a query with asserts and we can then have checks everywhere in a complicated tree?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12725288/LUCENE-6419.patch", "patch_content": "none"}
{"library_version": "5.2.0", "change_type": "Test Framework", "change_id": "LUCENE-6437", "change_description": ": Randomly set CPU core count and spins, derived from\ntest's master seed, used by ConcurrentMergeScheduler to set dynamic\ndefaults, for better test randomization and to help tests reproduce", "change_title": "Improve test reproducibility of ConcurrentMergeScheduler", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.2,6.0", "detail_description": "If things go wrong based on merges with concurrentmergescheduler in tests, its always more difficult to reproduce. But we make it worse, CMS can base its behavior on: We should just allow a (undocumented, for testing) sysprop backdoor to override both of these. This way, it can be set to values completely based on the random seed in LuceneTestCase init, and its not a reproducibility trap. Its true we try to set explicit values for LuceneTestCase.newIndexWriterConfig() and so forth. But not all tests use randomized IW configs.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12726377/LUCENE-6437.patch", "patch_content": "none"}

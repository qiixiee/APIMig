{"library_version": "6.0.0", "change_type": "System Requirements", "change_id": "LUCENE-5950", "change_description": ": Move to Java 8 as minimum Java version.", "change_title": "Move to Java 8 in trunk", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "The dev list thread \"[VOTE] Move trunk to Java 8\" passed. http://markmail.org/thread/zcddxioz2yvsdqkc This issue is to actually move trunk to java 8.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12682835/LUCENE-5950-javadocpatcher.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "System Requirements", "change_id": "LUCENE-6069", "change_description": ": Lucene Core now gets compiled with Java 8 \"compact1\" profile,\nall other modules with \"compact2\".", "change_title": "compile with compact profiles", "detail_type": "Task", "detail_affect_versions": "6.0", "detail_fix_versions": "6.0", "detail_description": "If we clean up the 'alignment' calculator in RamUsageEstimator, we can compile core with compact1, and the rest of lucene (except tests) with compact2.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12698466/LUCENE-6069.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6631", "change_description": ": Lucene Document classification", "change_title": "Lucene Document Classification", "detail_type": "Improvement", "detail_affect_versions": "5.2.1", "detail_fix_versions": "6.0", "detail_description": "Currently the Lucene Classification module supports the classification for an input text using the Lucene index as a trained model. This improvement is adding to the module a set of components to provide Document classification ( where the Document is a Lucene document ). All selected fields from the Document will have their part in the classification ( including the use of the proper Analyzer per field).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12766541/LUCENE-6631.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6747", "change_description": ": FingerprintFilter is a TokenFilter that outputs a single\ntoken which is a concatenation of the sorted and de-duplicated set of\ninput tokens. Useful for normalizing short text in clustering/linking\ntasks.", "change_title": "FingerprintFilter - a TokenFilter for clustering/linking purposes", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "5.4,6.0", "detail_description": "A TokenFilter that emits a single token which is a sorted, de-duplicated set of the input tokens. This approach to normalizing text is used in tools like OpenRefine[1] and elsewhere [2] to help in clustering or linking texts. The implementation proposed here has a an upper limit on the size of the combined token which is output. [1] https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth [2] https://rajmak.wordpress.com/2013/04/27/clustering-text-map-reduce-in-python/", "patch_link": "https://issues.apache.org/jira/secure/attachment/12752231/fingerprintv4.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6711", "change_description": ": Use CollectionStatistics.docCount() for IDF and average field\nlength computations, to avoid skew from documents that don't have the field.", "change_title": "Instead of docCount(), maxDoc() is used for numberOfDocuments in SimilarityBase", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "SimilarityBase.java has the following line : It seems like collectionStats.docCount(), which returns the total number of documents that have at least one term for this field, is more appropriate statistics here.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748645/LUCENE-6711.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6758", "change_description": ": Use docCount+1 for DefaultSimilarity's IDF, so that queries\ncontaining nonexistent fields won't screw up querynorm.", "change_title": "Adding a SHOULD clause to a BQ over an empty field clears the score when using DefaultSimilarity", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "6.0", "detail_description": "Patch with unit test to show the bug will be attached. I've narrowed this change in behavior with git bisect to the following commit:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754613/LUCENE-6758.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "SOLR-7876", "change_description": ": The QueryTimeout interface now has a isTimeoutEnabled method\nthat can return false to exit from ExitableDirectoryReader wrapping at\nthe point fields() is called.", "change_title": "Support disabling ExitableDirectory wrapper when not needed", "detail_type": "Bug", "detail_affect_versions": "5.0,5.1,5.2,5.2.1", "detail_fix_versions": "6.0", "detail_description": "ExitableDirectoryReader can cause some overhead in Solr for some use cases, even when not really used. There should be a way of not using it when not needed. See http://search-lucene.com/m/l6pAi1HLrodLhNUd", "patch_link": "https://issues.apache.org/jira/secure/attachment/12753163/SOLR-7876.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6825", "change_description": ": Add low-level support for block-KD trees", "change_title": "Add multidimensional byte[] indexing support to Lucene", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "I think we should graduate the low-level block KD-tree data structure from sandbox into Lucene's core? This can be used for very fast 1D range filtering for numerics, removing the 8 byte (long/double) limit we have today, so e.g. we could efficiently support BigInteger, BigDecimal, IPv6 addresses, etc. It can also be used for > 1D use cases, like 2D (lat/lon) and 3D (x/y/z with geo3d) geo shape intersection searches. The idea here is to add a new part of the Codec API (DimensionalFormat maybe?) that can do low-level N-dim point indexing and at runtime exposes only an \"intersect\" method. It should give sizable performance gains (smaller index, faster searching) over what we have today, and even over what auto-prefix with efficient numeric terms would do. There are many steps here ... and I think adding this is analogous to how we added FSTs, where we first added low level data structure support and then gradually cutover the places that benefit from an FST. So for the first step, I'd like to just add the low-level block KD-tree impl into oal.util.bkd, but make a couple improvements over what we have now in sandbox: This is already hard enough   After that we can build the DimensionalFormat on top, then cutover existing specialized block KD-trees.  We also need to fix OfflineSorter to use Directory API so we don't fill up /tmp when building a block KD-tree. A block KD-tree is at heart an inverted data structure, like postings, but is also similar to auto-prefix in that it \"picks\" proper N-dimensional \"terms\" (leaf blocks) to index based on how the specific data being indexed is distributed.  I think this is a big part of why it's so fast, i.e. in contrast to today where we statically slice up the space into the same terms regardless of the data (trie shifting, morton codes, geohash, hilbert curves, etc.) I'm marking this as trunk only for now... as we iterate we can see if it could maybe go back to 5.x...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12767473/LUCENE-6825.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6852", "change_description": ",", "change_title": "Add DimensionalFormat to Codec", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "This is phase 2 for adding dimensional indexing in Lucene, so we can (eventually) do efficient numeric range filtering, BigInteger/Decimal and IPv6 support, and \"point in shape\" spatial searching (2D or 3D). It's the follow-on from LUCENE-6825 (phase 1). This issue \"just\" adds DimensionalFormat (and Reader/Writer) to Codec and the IndexReader hierarchy, to IndexWriter and merging, and to document API (DimensionalField). I also implemented dimensional support for SimpleTextCodec, and added a test case showing that you can in fact use SimpleTextCodec to do multidimensional shape intersection (seems to pass a couple times!). Phase 3 will be adding support to the default codec as well (\"just\" wrapping BKDWriter/Reader), phase 4 is then fixing places that use the sandbox/spatial3d BKD tree to use the codec instead and maybe exposing sugar for numerics, things like BigInteger/Decimal, etc. There are many nocommits still, but I think it's close-ish ... I'll commit to a branch and iterate there.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12768600/LUCENE-6852.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6975", "change_description": ",", "change_title": "Add dimensional \"equals\" query to match docs containing precisely a given value", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Today, you can make a dimensional range query using e.g. DimensionalRangeQuery.new1DIntRange, etc., plus a direct ctor for \"expert\" (2D, 3D, etc.) usages, but matching a single value is awkward and users ask about it from time to time. We could maybe rename DimensionalRangeQuery to DimensionalQuery and add new \"factories\" like newIntEqualsQuery or something. Or, we could make new classes, DimensionalIntEqualsQuery etc., and you get to use ordinary constructors? Or something else?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12783554/LUCENE-6975.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6861", "change_description": ": Create Lucene60Codec, supporting points.", "change_title": "Default codec should support dimensional values", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "This is phase 3, follow-on from LUCENE-6852, to add dimensional values support to Lucene, so we can index large \"numeric\" values like BigInteger, BigDecimal, IPv6, and multi-dimensional things like 2d and 3d geo. I created a new Lucene60Codec, implemented DimensionalFormat for it, and a new FieldInfosFormat (to write/read the dimension settings for each field), and moved Lucene54Codec to backwards-codecs. I also fixed CheckIndex to do basic testing of the dimensional values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12769313/LUCENE-6861.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6879", "change_description": ": Allow to define custom CharTokenizer instances without\nsubclassing using Java 8 lambdas or method references.", "change_title": "Allow to define custom CharTokenizer using Java 8 Lambdas/Method references", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "6.0", "detail_description": "As a followup from LUCENE-6874, I thought about how to generate custom CharTokenizers wthout subclassing. I had this quite often and I was a bit annoyed, that you had to create a subclass every time. This issue is using the pattern like ThreadLocal or many collection methods in Java 8: You have the (abstract) base class and you define a factory method named fromXxxPredicate (like ThreadLocal.withInitial(() -> value). This would allow to define a new CharTokenizer with a single line statement using any predicate: I know this would not help Solr users that want to define the Tokenizer in a config file, but for real Lucene users this Java 8-like way would be easy and elegant to use. It is fast as hell, as it is just a reference to a method and Java 8 is optimized for that. The inverted factories fromSeparatorCharPredicate() are provided to allow quick definition without lambdas using method references. In lots of cases, like WhitespaceTokenizer, predicates are on the separator chars (isWhitespace(int), so using the 2nd set of factories you can define them without the counter-intuitive negation. Internally it just uses Predicate#negate(). The factories also allow to give the normalization function, e.g. to Lowercase, you may just give Character::toLowerCase as IntUnaryOperator reference.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770701/LUCENE-6879.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6881", "change_description": ": Cutover all BKD implementations to points", "change_title": "Cutover all BKD tree implementations to the codec", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "This is phase 4 for enabling indexing dimensional values in Lucene ... follow-on from LUCENE-6861. This issue removes the 3 pre-existing specialized experimental BKD implementations (BKD* in sandbox module for 2D lat/lon geo, BKD3D* in spatial3d module for 3D x/y/z geo, and range tree in sandbox module) and instead switches over to having the codec index the dimensional values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12770778/LUCENE-6881.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6837", "change_description": ": Add N-best output support to JapaneseTokenizer.", "change_title": "Add N-best output capability to JapaneseTokenizer", "detail_type": "Improvement", "detail_affect_versions": "5.3", "detail_fix_versions": "6.0", "detail_description": "Japanese morphological analyzers often generate mis-segmented tokens. N-best output reduces the impact of mis-segmentation on search result. N-best output is more meaningful than character N-gram, and it increases hit count too. If you use N-best output, you can get decompounded tokens (ex: \"シニアソフトウェアエンジニア\" => ) and overwrapped tokens (ex: \"数学部長谷川\" => ), depending on the dictionary and N-best parameter settings.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12774603/LUCENE-6837.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6962", "change_description": ": Add per-dimension min/max to points", "change_title": "Add per-dimension min/max to dimensional values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "It can be useful for apps to know the min/max value for a given field for each dimension, to give the global bounding box.  E.g. an app can know that a given range filter excludes all documents in a segment/index and skip searching it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12780718/LUCENE-6962.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6975", "change_description": ": Add ExactPointQuery, to match a single N-dimensional\npoint", "change_title": "Add dimensional \"equals\" query to match docs containing precisely a given value", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Today, you can make a dimensional range query using e.g. DimensionalRangeQuery.new1DIntRange, etc., plus a direct ctor for \"expert\" (2D, 3D, etc.) usages, but matching a single value is awkward and users ask about it from time to time. We could maybe rename DimensionalRangeQuery to DimensionalQuery and add new \"factories\" like newIntEqualsQuery or something. Or, we could make new classes, DimensionalIntEqualsQuery etc., and you get to use ordinary constructors? Or something else?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12783554/LUCENE-6975.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-6989", "change_description": ": Add preliminary support for MMapDirectory unmapping in Java 9.", "change_title": "Implement MMapDirectory unmapping for coming Java 9 changes", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "5.5.4,6.0,6.4", "detail_description": "Originally, the sun.misc.Cleaner interface was declared as \"critical API\" in JEP 260 Unfortunately the decission was changed in favor of a oficially supported java.lang.ref.Cleaner API. A side effect of this change is to move all existing sun.misc.Cleaner APIs into a non-exported package. This causes our forceful unmapping to no longer work, because we can get the cleaner instance via reflection, but trying to invoke it will throw one of the new Jigsaw RuntimeException because it is completely inaccessible. This will make our forceful unmapping fail. There are also no changes in Garbage collector, the problem still exists. For more information see this mailing list thread. This commit will likely be done, making our unmapping efforts no longer working. Alan Bateman is aware of this issue and will open a new issue at OpenJDK to allow forceful unmapping without using the now private sun.misc.Cleaner. The idea is to let the internal class sun.misc.Cleaner implement the Runable interface, so we can simply cast to runable and call the run() method to unmap. The code would then work. This will lead to minor changes in our unmapper in MMapDirectory: An instanceof check and casting if possible. I opened this issue to keep track and implement the changes as soon as possible, so people will have working unmapping when java 9 comes out. Current Lucene versions will no longer work with Java 9.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12844298/LUCENE-6989-v3-testFixes.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-7040", "change_description": ": Upgrade morfologik-stemming to version 2.1.0.", "change_title": "Upgrade morfologik-stemming to version 2.1.0", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "New Features", "change_id": "LUCENE-7048", "change_description": ": Add XXXPoint.newSetQuery, to create a query that\nefficiently matches all documents containing any of the specified\npoint values.  This is the analog of TermsQuery, but for points\ninstead.", "change_title": "Add XXXPoint.newSetQuery to match a set of points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "This is the analog of TermsQuery for dimensional points, to (relatively) efficiently match any docs whose point value is in the specified set.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12789915/LUCENE-7048.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7094", "change_description": ": BBoxStrategy and PointVectorStrategy now support\nPointValues (in addition to legacy numeric trie).  Their APIs\nwere changed a little and also made more consistent.  PointValues/Trie\nis optional, DocValues is optional, stored value is optional.", "change_title": "spatial-extras BBoxStrategy and (confusingly!) PointVectorStrategy use legacy numeric encoding", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "We need to deprecate these since they work on the old encoding and provide points based alternatives.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796099/LUCENE_7094.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6067", "change_description": ": Accountable.getChildResources has a default\nimplementation returning the empty list.", "change_title": "Change Accountable.getChildResources to return empty list by default", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "This is the typical case, and defaulting to it makes this accounting api much less invasive on the codebase.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12682893/LUCENE-6067.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6583", "change_description": ": FilteredQuery has been removed. Instead, you can construct a\nBooleanQuery with one MUST clause for the query, and one FILTER clause for\nthe filter.", "change_title": "Remove FilteredQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Now that BooleanQuery can handle filters, FilteredQuery should be removed in trunk and deprecated in 5.x.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12740369/LUCENE-6583.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6651", "change_description": ": AttributeImpl#reflectWith(AttributeReflector) was made\nabstract and has no reflection-based default implementation anymore.", "change_title": "Remove private field reflection (setAccessible) in AttributeImpl#reflectWith", "detail_type": "Improvement", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3,6.0", "detail_description": "In AttributeImpl we currently have a \"default\" implementation of reflectWith (which is used by toString() and other methods) that uses reflection to list all private fields of the implementation class and reports them to the AttributeReflector (used by Solr and Elasticsearch to show analysis output). Unfortunately this default implementation needs to access private fields of a subclass, which does not work without doing Field#setAccessible(true). And this is done without AccessController#doPrivileged()! There are 2 solutions to solve this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12743343/LUCENE-6651-5x.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6706", "change_description": ": PayloadTermQuery and PayloadNearQuery have been removed.\nInstead, use PayloadScoreQuery to wrap any SpanQuery.", "change_title": "Support Payload scoring for all SpanQueries", "detail_type": "New Feature", "detail_affect_versions": "5.2.1", "detail_fix_versions": "5.3", "detail_description": "I need a way to have payloads influence the score of SpanOrQuery's.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12748012/LUCENE-6706.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6829", "change_description": ": OfflineSorter, and the classes that use it (suggesters,\nhunspell) now do all temporary file IO via Directory instead of\ndirectly through java's temp dir.  Directory.createTempOutput\ncreates a uniquely named IndexOutput, and the new\nIndexOutput.getName returns its name", "change_title": "OfflineSorter should use Directory API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "I think this is a blocker for LUCENE-6825, because the block KD-tree makes heavy use of OfflineSorter and we don't want to fill up tmp space ... This should be a straightforward cutover, but there are some challenges, e.g. the test was failing because virus checker blocked deleting of files.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12766178/LUCENE-6829.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6917", "change_description": ": Deprecate and rename NumericXXX classes to\nLegacyNumericXXX in favor of points", "change_title": "Deprecate and rename NumericField/RangeQuery to LegacyNumeric", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "DimensionalValues seems to be better across the board (indexing time, indexing size, search-speed, search-time heap required) than NumericField, at least in my testing so far. I think for 6.0 we should move IntField, LongField, FloatField, DoubleField and NumericRangeQuery to backward-codecs, and rename with Legacy prefix?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12776826/LUCENE-6917.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6947", "change_description": ": SortField.missingValue is now protected. You can read its\nvalue using the new SortField.getMissingValue getter.", "change_title": "SortField.missingValue should not be public", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Today we have SortField.setMissingValue that tries to perform validation of the missing value, except that given that SortField.missingValue is public, it is very easy to bypass it. Let's make it protected (some sub-classes use it) and add a getter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12779047/LUCENE-6947.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7028", "change_description": ": Remove duplicate method in LegacyNumericUtils.", "change_title": "Remove useless clone of method in Lucene 6' LegacyNumericUtils", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "While working on LUCENE-7027 I noticed, that NumericUtils contains the same method 2 times (same signature), one just delegating to the other. I will remove the duplicate for 6.0.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12787821/LUCENE-7028.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7052", "change_description": ",", "change_title": "BytesRefHash.sort should always sort in unicode code point order", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Today BytesRefHash.sort takes a custom Comparator but we always pass it BytesRef.getUTF8SortedAsUnicodeComparator().", "patch_link": "https://issues.apache.org/jira/secure/attachment/12790351/LUCENE-7052-cleanup1.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7053", "change_description": ",", "change_title": "Remove deprecated BytesRef#getUTF8SortedAsUTF16Comparator(); remove natural comparator in favour of Java 8 one", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Followup from LUCENE-7052: This removes the legacy, deprecated getUTF8SortedAsUTF16Comparator() in the BytesRef class. I know originally we added the different comparators to be able to allow the index term dict to be sorted in different order. This never proved to be useful, as many Lucene queries rely on the default order. The only codec that used another byte order internally was the Lucene 3 one (but it used the unicode spaghetti algorithm to reorder its term enums at runtime). This patch also removes the BytesRef-Comparator completely and just implements compareTo. So all code can rely on natural ordering. This patch also cleans up other usages of natural order comparators, e.g. in ArrayUtil, because Java 8 natively provides a comparator.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12790364/LUCENE-7053.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7060", "change_description": ": Update Spatial4j to 0.6.  The package com.spatial4j.core\nis now org.locationtech.spatial4j.", "change_title": "Update Spatial4j 0.5 to 0.6 (includes change in package)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "Spatial4j 0.6 was released the 26th of February and I want to upgrade to it.  The most impactful change is that the package moves from com.spatial4j.core to org.locationtech.spatial4j.  For that reason, it would be ideal to do this for Lucene 6.0 https://github.com/locationtech/spatial4j/blob/master/CHANGES.md", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791001/LUCENE_7060_spatial4j_0_6_upgrade.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7058", "change_description": ": Add getters to various Query implementations", "change_title": "Add getters for the properties of several Query implementations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "Hi! At Hibernate Search, we are currently working on an Elasticsearch backend (aside from the existing Lucene backend). As part of this effort, to provide a smooth migration path, we need to be able to rewrite the Lucene queries as Elasticsearch queries. We know it will be neither perfect or comprehensive but we want it to be the best possible experience. It works well in many cases but several implementations of Query don't have the necessary getters to be able to extract the information from the Query. The attached patch add getters to several implementations of Query. It would be nice if it could be applied. Any chance it could be applied to the next point release too? (probably not but I'd better ask).", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7064", "change_description": ": MultiPhraseQuery is now immutable and should be constructed\nwith MultiPhraseQuery.Builder.", "change_title": "Make MultiPhraseQuery immutable", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "See LUCENE-6531 Mutable queries are an issue for automatic filter caching since modifying a query after it has been put into the cache will corrupt the cache. We should make all queries immutable (up to the boost) to avoid this issue.", "patch_link": "none", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7072", "change_description": ": Geo3DPoint always uses WGS84 planet model.", "change_title": "Geo3dPointField should always use WGS84 planet model", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Right now it takes any PlanetModel, but I think that's dangerous. First, WGS84 is the most accurate one we have, and second, the planet model is secretly baked into the index via the \"planetMax\" normalization. I think for Lucene we should only expose WGS84 ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791687/LUCENE-7072.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-7056", "change_description": ": Geo3D classes are in different packages now.", "change_title": "Separate Geo3DPoint into another package from the rest of Geo3D", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "The original description follows; it's greater in scope than the new title: This is a proposal for the \"spatial3d\" module to be purely about the shape/geometry implementations it has.  In Lucene 5 that's actually all it has.  In Lucene 6 at the moment its ~76 files have 2 classes that I think should go elsewhere: Geo3DPoint and PointInGeo3DShapeQuery.  Specifically lucene-spatial-extras (which doesn't quite exist yet so lucene-spatial) would be a suitable place due to the dependency.   Eventually I see this module migrating elsewhere be it on its own or a part of something else more spatial-ish.  Even if that never comes to pass, non-Lucene users who want to use this module for it's geometry annoyingly have to exclude the Lucene dependencies that are there because this module also contains these two classes. In a comment I'll suggest some specifics.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791784/LUCENE_7056__split_spatial3d_package.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "LUCENE-6952", "change_description": ": These classes are now abstract: FilterCodecReader, FilterLeafReader,\nFilterCollector, FilterDirectory.  And some Filter* classes in\nlucene-test-framework too.", "change_title": "All Filter* delegating classes should be abstract", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "I think it's confusing that FilterLeafReader (and it's Filter* inner classes) are not abstract.  By making them abstract, we clarify to users how to use them by virtue of them being abstract.  It seems only a couple tests directly instantiate them.  This applies to other Filter* classes as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791799/LUCENE_6952.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "API Changes", "change_id": "SOLR-8867", "change_description": ": FunctionValues.getRangeScorer now takes a LeafReaderContext instead\nof an IndexReader, and avoids matching documents without a value in the field\nfor numeric fields.", "change_title": "frange / ValueSourceRangeFilter / FunctionValues.getRangeScorer should not match documents w/o a value", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.6,6.0", "detail_description": "currently can match documents w/o a value (because of a default value of 0). This only existed historically because we didn't have info about what fields had a value for numerics, and didn't have exists() on FunctionValues.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12794050/SOLR-8867.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6891", "change_description": ": Use prefix coding when writing points in\neach leaf block in the default codec, to reduce the index\nsize", "change_title": "Lucene60DimensionalFormat should use block prefix coding when writing values", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Today we write the whole value for every doc in one leaf block in the BKD tree, but that's crazy because the whole point of that leaf block is all the docs inside it have values that are very close together. So I changed this to write the common prefix for the whole block up front in each block.  This requires more index-time and search-time work, but gives nice index size reductions: On the 2D (London, UK) lat/lon benchmark: On the 1D (just \"lat\" from the above test) benchmark: Index time can be a bit slower since there are two passes now per leaf block (first to find the common prefix per dimension, and second pass must then strip those prefixes). Query time is slower because there's more work per hit that needs value filtering, i.e. collating the suffixes onto the prefixes, per dimension.  This affects 2D much more than 1D because 1D has fewer leaf blocks that need filtering (typically 0, 1 or 2, unless there are many duplicate values in the index). I suspect the index size savings is use-case dependent, e.g. if you index a bunch of ipv4 addresses along with a few ipv6 addresses, you'd probably see sizable savings. I also suspect the more docs you index, the greater the savings, because the cells will generally be smaller. Net/net I think the opto is worth it, even if it slows query time a bit.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12771721/LUCENE-6891.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6901", "change_description": ": Optimize points indexing: use faster\nIntroSorter instead of InPlaceMergeSorter, and specialize 1D\nmerging to merge sort the already sorted segments instead of\nre-indexing", "change_title": "Optimize 1D dimensional value indexing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Dimensional values give a smaller index, and faster search times, for indexing ordered byte[] values across one or more dimensions, vs our existing approaches, but the indexing time is substantially slower. Since the 1D case is so important/common (numeric fields, range query) I think it's worth optimizing its indexing time.  It should also be possible to optimize the N > 1 dimensions case too, but it's more complex ... we can postpone that. So for the 1D case, I changed the merge method to do a merge sort (like postings) of the already sorted segments dimensional values, instead of simply re-indexing all values from the incoming segments, and this was a big speedup. I also changed from InPlaceMergeSorter to IntroSorter (this is what postings use, and it's faster but still safe) and this was another good speedup, which should also help the > 1D cases. Finally, I added a BKDReader.verify method (currently it's dark: NOT called) that walks the index and then check that every value in each leaf block does in fact fall within what the index expected/claimed.  This is useful for finding bugs!  Maybe we can cleanly fold it into CheckIndex somehow later.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12773720/LUCENE-6901.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Optimizations", "change_id": "LUCENE-6793", "change_description": ": LegacyNumericRangeQuery.hashCode() is now less subject to hash\ncollisions.", "change_title": "NumericRangeQuery.hashCode() produces frequent collisions", "detail_type": "Bug", "detail_affect_versions": "4.6,5.3", "detail_fix_versions": "6.0", "detail_description": "We have a user who is developing a Solr plugin and needs to store NumericRangeQuery objects in a hash table.  They found that NumericRangeQuery.hashCode() produces extremely frequent collisions.  I understand that the contract for hashCode doesn't (and can't) guarantee unique hash codes for every value, but the distribution of this method seems particularly bad with an affinity for the hash value 897548010. Out of a set of 31 ranges, hashCode returned 897548010 14 times. This is going to result in very inefficient distribution of the objects in the hash table. The standard \"times 31\" hash function recommended by Effective Java fares quite a bit better, although it still produces quite a few collisions.  Here's a test case that compares the results of the current hashCode function with the times 31 method.  An even better method, like Murmur3 might be found here: http://floodyberry.com/noncryptohashzoo/", "patch_link": "https://issues.apache.org/jira/secure/attachment/12790141/LUCENE-6793.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7050", "change_description": ": TermsQuery is now cached more aggressively by the default\nquery caching policy.", "change_title": "Improve the query cache heuristic to detect costly queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Term queries, phrase queries and their combinations through boolean queries should not be cached too agressively since they can efficiently make use of skip lists. However we also have a number of queries that in practice need to visit all matches anyway like PrefixQuery, TermsQuery, PointInSetQuery, PointRangeQuery, so caching them more agressively can help avoid computing all documents that match in the whole index again and again.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12790166/LUCENE-7050.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Optimizations", "change_id": "LUCENE-7066", "change_description": ": PointRangeQuery got optimized for the case that all documents\nhave a value and all points from the segment match.", "change_title": "Optimize the \"everything matches\" case for point range queries", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Now that points have a getDocCount statistic, we could optimize execution of range queries in the case that the range's lower bound is less that the field's min value and the range's upper bound is greater than the field's max value. I would expect such an optimization to kick in frequently for users who store time series in their indices as it is quite frequent for ranges to cover entire segments or even entire indices.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791486/LUCENE-7066.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6789", "change_description": ": IndexSearcher's default Similarity is changed to BM25Similarity.\nUse ClassicSimilarity to get the old vector space DefaultSimilarity.", "change_title": "change IndexSearcher default similarity to BM25", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Since Lucene 4.0, the statistics needed for this are always present, so we can make the change without any degradation. I think the change should be a 6.0 change only: it will prevent any surprises. DefaultSimilarity is renamed to ClassicSimilarity to prevent confusion. No indexing change is needed as we use the same norm format, its just a runtime switch. Users can just do IndexSearcher.setSimilarity(new ClassicSimilarity()) to get the old behavior.  I did not change solr's default here, I think that should be a separate issue, since it has more concerns: e.g. factories in configuration files and so on. One issue was the generation of synonym queries (posinc=0) by QueryBuilder (used by parsers). This is kind of a corner case (query-time synonyms), but we should make it nicer. The current code in trunk disables coord, which makes no sense for anything but the vector space impl. Instead, this patch adds a SynonymQuery which treats occurrences of any term as a single pseudoterm. With english wordnet as a query-time synonym dict, this query gives 12% improvement in MAP for title queries on BM25, and 2% with Classic (not significant). So its a better generic approach for synonyms that works with all scoring models. I wanted to use BlendedTermQuery, but it seems to have problems at a glance, it tries to \"take on the world\", it has problems like not working with distributed scoring (doesn't consult indexsearcher for stats). Anyway this one is a different, simpler approach, which only works for a single field, and which calls tf(sum) a single time.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12754824/LUCENE-6789.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6886", "change_description": ": Reserve the .tmp file name extension for temp files,\nand codec components are no longer allowed to use this extension", "change_title": "IndexWriter gets angry at leftover temp files (e.g. from BKD)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "I was trying to run performance test for the new dimensional values and hit this crazy exception: It happened because I killed my indexing process while BKD was writing temp files.  On starting up again, IW would have removed these unreferenced files, except inflateGens got confused by their names. This bug only happens on trunk.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12771002/LUCENE-6886.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Changes in Runtime Behavior", "change_id": "LUCENE-6835", "change_description": ": Directory.listAll now returns entries in sorted order,\nto not leak platform-specific behavior, and \"retrying file deletion\"\nis now the responsibility of Directory.deleteFile, not the caller.", "change_title": "Directory.deleteFile should \"own\" retrying deletions on Windows", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Rob's idea: Today, we have hairy logic in IndexFileDeleter to deal with Windows file systems that cannot delete still open files. And with LUCENE-6829, where OfflineSorter now must deal with the situation too ... I worked around it by fixing all tests to disable the virus checker. I think it makes more sense to push this \"platform specific problem\" lower in the stack, into Directory?  I.e., its deleteFile method would catch the access denied, and then retry the deletion later.  Then we could re-enable virus checker on all these tests, simplify IndexFileDeleter, etc. Maybe in the future we could further push this down, into WindowsDirectory,  and fix FSDirectory.open to return WindowsDirectory on windows ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12786123/LUCENE-6835.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Tests", "change_id": "LUCENE-7009", "change_description": ": Add expectThrows utility to LuceneTestCase. This uses a lambda\nexpression to encapsulate a statement that is expected to throw an exception.", "change_title": "Add expectThrows utility to LuceneTestCase", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "In junit5, a neat assertion method is added which makes testing expected failures a little more straightforward. The block of code that is expected to throw is passed in with a lambda expression, and the caught exception returned for inspection. The usage looks something like this: We should add this to LuceneTestCase until junit5 is available.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12785955/LUCENE-7009.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7065", "change_description": ": Fix the explain for the global ordinals join query. Before the\nexplain would also indicate that non matching documents would match.\nOn top of that with score mode average, the explain would fail with a NPE.", "change_title": "Fix explain for global ordinal query time join", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.2,5.6,6.0", "detail_description": "The explain methods for the global ordinal join is broken, because even in the case that a document doesn't match with the query it tries to create an explain that tells it does. In the case when score mode 'avg' is used this causes a NPE and in the other cases the return explanation indicates that a document matches while it doesn't.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12791435/LUCENE_7065.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7101", "change_description": ": OfflineSorter had O(N^2) merge cost, and used too many\ntemporary file descriptors, for large sorts", "change_title": "OfflineSorter's merging is O(N^2) cost for large sorts", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "Our OfflineSorter acts just like Lucene, writing small initial segments of sorted values (from what it was able to sort at once in heap), periodically merging them when there are too many, and doing a forceMerge(1) in the end. But the merge logic is too simplistic today, resulting in O(N^2) cost.  Smallish sorts usually won't hit it, because the default 128 merge factor is so high, but e.g. the new 2B points tests do hit the N^2 behavior.  I suspect the high merge factor hurts performance (OS struggles to use what free RAM it has to read-ahead on 128 files), and also risks file descriptor exhaustion. I think we should implement a simple log merge policy for it, and drop its default merge factor to 10.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12793380/LUCENE-7101.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7111", "change_description": ": DocValuesRangeQuery.newLongRange behaves incorrectly for\nLong.MAX_VALUE and Long.MIN_VALUE", "change_title": "DocValuesRangeQuery.newLongRange behaves incorrectly for Long.MAX_VALUE and Long.MIN_VALUE", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.2,5.6,6.0,6.1,7.0", "detail_description": "It seems that the following queries return all documents, which is unexpected: In Solr, floats and doubles are converted to longs and -0d gets converted to Long.MIN_VALUE, and queries like {-0d TO 0d] could fail due to this, returning all documents in the index.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12793908/LUCENE-7111.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7139", "change_description": ": Fix bugs in geo3d's Vincenty surface distance\nimplementation", "change_title": "Geo3d Vincenty distance method broken", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.2,5.6,6.0,6.1,7.0", "detail_description": "There is a PlanetModel.surfaceDistance() method in Geo3D that computes the Vincenty distance.  This is mostly a convenience but is also reasonable to  include for tests.  The current implementation, however, is broken.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795398/LUCENE-7139.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7112", "change_description": ": WeightedSpanTermExtractor.extractUnknownQuery is only called\non queries that could not be extracted.", "change_title": "WeightedSpanTermExtractor should not always call extractUnknownQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "5.5.1,6.0", "detail_description": "WeightedSpanTermExtractor always calls extractUnknownQuery, even if term extraction already succeeded because the query is eg. a phrase query. It should only call this method if it could not find how to extract terms otherwise (eg. in case of a custom query).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12793986/LUCENE-7112.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7126", "change_description": ": Remove GeoPointDistanceRangeQuery. This query was implemented\nwith boolean NOT, and incorrect for multi-valued documents.", "change_title": "GeoPointDistanceRangeQuery not valid for multi-valued docs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "This query currently takes minimum and maximum range and rewrites to a boolean query of two distance queries (big NOT little). The problem is, this rewrite is not correct if a document has multiple values in the field. Do we really need to support this query? What is the use case? Is it a relic of people doing things like pagination or distance faceting in a very slow way? We can do these things more efficiently with other mechanisms (e.g. distance sort for LatLonPoint works well with searchAfter, can be ported to geopoint).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795559/LUCENE-7126.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7158", "change_description": ": Consistently use earth's WGS84 mean radius wherever our\ngeo search implementations approximate the earth as a sphere", "change_title": "Haversin should use the earth's mean radius, not its max (equitorial)?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "Across our spatial modules we seem to disagree about the earth's radius when we model it as a sphere. I think in our haversin implementation we use equitorial (maximum) radius, but maybe in spatial3d we use the earth's mean radius. I think mean makes more sense: the earth is actually a squashed sphere, so it's polar radius is shorter than its equitorial radius. I think it's important, when we model the earth as a sphere, that we pick one radius and try to use that one consistently?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12796431/LUCENE-7158.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Other", "change_id": "LUCENE-7035", "change_description": ": Upgrade icu4j to 56.1/unicode 8.", "change_title": "upgrade icu4j to latest (unicode 8)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "See LUCENE-6993. We want to bring all these tokenizers up to date. The icu part can be done independently.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12788343/LUCENE-7035.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Other", "change_id": "LUCENE-7087", "change_description": ": Let MemoryIndex#fromDocument(...) accept 'Iterable<? extends IndexableField>'\nas document instead of 'Document'.", "change_title": "Change MemoryIndex#fromDocument(...) helpers to accept Iterable<IndexableField> as document", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0,6.1,7.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12792472/LUCENE_7087.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Other", "change_id": "LUCENE-7091", "change_description": ": Add doc values support to MemoryIndex", "change_title": "Add doc values support to MemoryIndex", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "Sometimes queries executed via the MemoryIndex require certain things to be stored as doc values. Today this isn't possible because the memory index doesn't support this and these queries silently return no results.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12793394/LUCENE-7091.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Other", "change_id": "LUCENE-7093", "change_description": ": Add point values support to MemoryIndex", "change_title": "MemoryIndex does not support points", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "I realized this glancing at LUCENE-7091. I think this should have points support or else people cannot move off of the deprecated LegacyXXX encodings?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12793535/LUCENE-7093.patch", "patch_content": "none"}
{"library_version": "6.0.0", "change_type": "Other", "change_id": "LUCENE-7095", "change_description": ": Add point values support to the numeric field query time join.", "change_title": "join module only supports legacy numerics", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.0", "detail_description": "The join module supports numeric query-time joins but only with the old encoding. We need to deprecate the current support and implement a Points-based alternative.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12795424/LUCENE_7095.patch", "patch_content": "none"}

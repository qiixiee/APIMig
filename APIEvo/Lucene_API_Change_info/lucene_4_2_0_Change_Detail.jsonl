{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4602", "change_description": ": FacetFields now stores facet ordinals in a DocValues field,\nrather than a payload. This forces rebuilding existing indexes, or do a\none time migration using FacetsPayloadMigratingReader. Since DocValues\nsupport in-memory caching, CategoryListCache was removed too.", "change_title": "Use DocValues to store per-doc facet ord", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Spinoff from LUCENE-4600 DocValues can be used to hold the byte[] encoding all facet ords for the document, instead of payloads.  I made a hacked up approximation of in-RAM DV (see CachedCountingFacetsCollector in the patch) and the gains were somewhat surprisingly large: I didn't think payloads were THAT slow; I think it must be the advance implementation? We need to separately test on-disk DV to make sure it's at least on-par with payloads (but hopefully faster) and if so ... we should cutover facets to using DV.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564995/LUCENE-4602.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4697", "change_description": ": FacetResultNode is now a concrete class with public members\n(instead of getter methods).", "change_title": "FacetResultNode should be a simple bin, not interface", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "FacetResultNode is an interface with a bunch of getter methods. The purpose, I believe, was to return an object that does not allow you modify it. But that's overly defensive I think. I.e., we return to users ScoredDoc and they can happily modify 'doc' and 'score'. If users modify the members' values, they can only affect themselves, as this object is returned after the search has completed. Anyway, today it doesn't even defend itself right, because you can call getSubResults and remove/add elements from the list ... I want to make it a simple bin, w/ public members and get rid of MutableFacetResultNode. Will keep the class not final, since it might be useful for someone to extend it and add additional members, for his/her FacetsCollector purposes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565764/LUCENE-4697.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4600", "change_description": ": FacetsCollector is now an abstract class with two\nimplementations: StandardFacetsCollector (the old version of\nFacetsCollector) and CountingFacetsCollector. FacetsCollector.create()\nreturns the most optimized collector for the given parameters.", "change_title": "Explore facets aggregation during documents collection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Today the facet module simply gathers all hits (as a bitset, optionally with a float[] to hold scores as well, if you will aggregate them) during collection, and then at the end when you call getFacetsResults(), it makes a 2nd pass over all those hits doing the actual aggregation. We should investigate just aggregating as we collect instead, so we don't have to tie up transient RAM (fairly small for the bit set but possibly big for the float[]).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565734/LUCENE-4600.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4700", "change_description": ": OrdinalPolicy is now per CategoryListParams, and is no longer\nan interface, but rather an enum with values NO_PARENTS and ALL_PARENTS.\nPathPolicy was removed, you should extend FacetFields and DrillDownStream\nto control which categories are added as drill-down terms.", "change_title": "OrdinalPolicy and PathPolicy should be per-CategoryListParams", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Today these two are per FacetIndexingParams, which means that once defined, they affect all category lists. This prevents for example to index one category list with OrdinalPolicy.NO_PARENTS and another with OrdinalPolicy.ALL_PARENTS. Especially now that we know NO_PARENTS is faster (see LUCENE-4600), it will be good if users can control this setting per CategoryListParams, and index only the few facets which a document has more than once (e.g. Author) in a separate category list.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566129/LUCENE-4700.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4547", "change_description": ": DocValues improvements:", "change_title": "DocValues field broken on large indexes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I tried to write a test to sanity check LUCENE-4536 (first running against svn revision 1406416, before the change). But i found docvalues is already broken here for large indexes that have a PackedLongDocValues field:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552474/test.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4757", "change_description": ": Cleanup and refactoring of FacetsAccumulator, FacetRequest,\nFacetsAggregator and FacetResultsHandler API. If your application did\nFacetsCollector.create(), you should not be affected, but if you wrote\nan Aggregator, then you should migrate it to the per-segment\nFacetsAggregator. You can still use StandardFacetsAccumulator, which works\nwith the old API (for now).", "change_title": "Cleanup FacetsAccumulator API path", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "FacetsAccumulator and FacetRequest expose too many things to users, even when they are not needed, e.g. complements and partitions. Also, Aggregator is created per-FacetRequest, while in fact applied per category list. This is confusing, because if you want to do two aggregations, e.g. count and sum-score, you need to separate the two dimensions into two different category lists at indexing time. It's not so easy to refactor everything in one go, since there's a lot of code involved. So in this issue I will: There will be follow-on issues to migrate more features to the new API, and more cleanups ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568412/LUCENE-4757.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4761", "change_description": ": Facet packages reorganized. Should be easy to fix your import\nstatements, if you use an IDE such as Eclipse.", "change_title": "Facets package reorganization", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Facets packages have a weird structure IMO. I think that we should organize the packages by feature, and not by functionality (index/search). For example: The motivation â€“ if I want to handle all associations related code, it should be very easy to locate it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4750", "change_description": ": Convert DrillDown to DrillDownQuery, so you can initialize it\nand add drill-down categories to it.", "change_title": "Convert DrillDown to DrillDownQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "DrillDown is a utility class for creating drill-down queries over a base query and a bunch of categories. We've been asked to support AND, OR and AND of ORs. The latter is not so simple as a static utility method though, so instead we have some sample code ... Rather, I think that we can just create a DrillDownQuery (extends Query) which takes a baseQuery in its ctor and exposes add(CategoryPath...), such that every such group of categories is AND'ed with other groups, and internally they are OR'ed. It's very similar to how you would construct a BooleanQuery, only simpler and specific to facets. Internally, it would build a BooleanQuery and delegate rewrite, createWeight etc to it. That will remove the need for the static utility methods .. or we can keep static term() for convenience.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568735/LUCENE-4750.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4759", "change_description": ": remove FacetRequest.SortBy; result categories are always\nsorted by value, while ties are broken by category ordinal.", "change_title": "Remove FacetRequest.SortBy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "FacetRequest.SortBy lets you specify two sort-by: ORDINAL and VALUE. While VALUE is the default (and breaks ties by ordinal), it's not very clear what's the use of SortBy.ORDINAL. In practice, if you choose to do that, you'd get the first K categories that are the descendant of the requested one, from smallest to highest, or vice versa. But that seems quite useless ... someone could just traverse the counts array (for instance) and filter out all counts==0? Or also, someone can write a FacetResultsHandler which does that... My motivation to remove that is to reduce the number of PQ combinations we have: MinValue, MaxValue (SortBy.VALUE, SortOrder.ASCENDING/DESCENDING) and MinOrdinal, MaxOrdinal. Now there are 4 PQs and I'd like to separately split them out to PQs that handle int vs float. Because today these PQs call Double.compare(), which you need to for floating-point values, but is just a waste for integer values. So removing SortBy will both simplify the API and halve the number of PQs we need to write. Plus ... it doesn't seem such a useful option, to let the user even spend 10 seconds to read the differences between VALUE and ORDINAL.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568739/LUCENE-4759.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4772", "change_description": ": Facet associations moved to new FacetsAggregator API. You\nshould override FacetsAccumualtor and return the relevant aggregator,\nfor aggregating the association values.", "change_title": "Move Facet associations to new FacetsAggregator API", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Move facets associations to the new bulk FacetsAggregator API. Also, today when you index categories with associations, the categories are written to two fields redundantly - the associations field and the counting list field. However, when you aggregate them, you only need to read the associations field. The counting list field is redundant here. If an app requires indexing the categories into two lists, it can do so by adding the categories w/ associations using AssociationFacetFields and the plain categories (w/ their hierarchy etc.) using FacetFields. I will post a patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568951/LUCENE-4772.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4748", "change_description": ": A FacetRequest on a non-existent field now returns an\nempty FacetResult instead of skipping it.", "change_title": "Add DrillSideways helper class to Lucene facets module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This came out of a discussion on the java-user list with subject \"Faceted search in OR\": http://markmail.org/thread/jmnq6z2x7ayzci5k The basic idea is to count \"near misses\" during collection, ie documents that matched the main query and also all except one of the drill down filters. Drill sideways makes for a very nice faceted search UI because you don't \"lose\" the facet counts after drilling in.  Eg maybe you do a search for \"cameras\", and you see facets for the manufacturer, so you drill into \"Nikon\". With drill sideways, even after drilling down, you'll still get the counts for all the other brands, where each count tells you how many hits you'd get if you changed to a different manufacturer. This becomes more fun if you add further drill-downs, eg maybe I next drill down into Resolution=10 megapixels\", and then I can see how many 10 megapixel cameras all other manufacturers, and what other resolutions Nikon cameras offer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570680/LUCENE-4748.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Changes in backwards compatibility policy", "change_id": "LUCENE-4806", "change_description": ": The default category delimiter character was changed\nfrom U+F749 to U+001F, since the latter uses 1 byte vs 3 bytes for\nthe former.  Existing facet indices must be reindexed.", "change_title": "change FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR to U+001F", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "The current delim char takes 3 bytes as UTF-8 ... but U+001F (= INFORMATION_SEPARATOR, which seems appropriate) takes only 1 byte.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12571439/LUCENE-4806.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4687", "change_description": ": BloomFilterPostingsFormat now lazily initializes delegate\nTermsEnum only if needed to do a seek or get a DocsEnum.", "change_title": "Lazily initialize TermsEnum in BloomFilterPostingsFormat", "detail_type": "Improvement", "detail_affect_versions": "4.0,4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "BloomFilteringPostingsFormat initializes its delegate TermsEnum directly inside the Terms#iterator() call which can be a pretty heavy operation if executed thousands of times. I suspect that bloom filter postings are mainly used for primary keys etc. which in turn is mostly a seekExact. Given that, most of the time we don't even need the delegate termsenum since most of the segments won't contain the key and the bloomfilter will likely return false from seekExact without consulting the delegate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565131/LUCENE-4687.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4677", "change_description": ",", "change_title": "Use vInt to encode node addresses inside FST", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Today we use int, but towards enabling > 2.1G sized FSTs, I'd like to make this vInt instead.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564519/LUCENE-4677.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4682", "change_description": ",", "change_title": "Reduce wasted bytes in FST due to array arcs", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "When a node is close to the root, or it has many outgoing arcs, the FST writes the arcs as an array (each arc gets N bytes), so we can e.g. bin search on lookup. The problem is N is set to the max(numBytesPerArc), so if you have an outlier arc e.g. with a big output, you can waste many bytes for all the other arcs that didn't need so many bytes. I generated Kuromoji's FST and found it has 271187 wasted bytes vs total size 1535612 = ~18% wasted. It would be nice to reduce this. One thing we could do without packing is: in addNode, if we detect that number of wasted bytes is above some threshold, then don't do the expansion. Another thing, if we are packing: we could record stats in the first pass about which nodes wasted the most, and then in the second pass (paack) we could set the threshold based on the top X% nodes that waste ... Another idea is maybe to deref large outputs, so that the numBytesPerArc is more uniform ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564592/LUCENE-4682.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4678", "change_description": ": FST now uses a paged byte[] structure instead of a\nsingle byte[] internally, to avoid large memory spikes during\nbuilding", "change_title": "FST should use paged byte[] instead of single contiguous byte[]", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "The single byte[] we use today has several limitations, eg it limits us to < 2.1 GB FSTs (and suggesters in the wild are getting close to this limit), and it causes big RAM spikes during building when a the array has to grow. I took basically the same approach as LUCENE-3298, but I want to break out this patch separately from changing all int -> long for > 2.1 GB support.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564595/LUCENE-4678.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-3298", "change_description": ": FST can now be larger than 2.1 GB / 2.1 B nodes.", "change_title": "FST has hard limit max size of 2.1 GB", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "The FST uses a single contiguous byte[] under the hood, which in java is indexed by int so we cannot grow this over Integer.MAX_VALUE.  It also internally encodes references to this array as vInt. We could switch this to a paged byte[] and make the far larger. But I think this is low priority... I'm not going to work on it any time soon.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12564634/LUCENE-3298.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4690", "change_description": ": Performance improvements and non-hashing versions\nof NumericUtils.*ToPrefixCoded()", "change_title": "Optimize NumericUtils.*ToPrefixCoded(), add versions that don't hash", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests).  If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself). edit: Uwe pointed out they were used in one place.  Other places still don't need it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566206/LUCENE-4690.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4715", "change_description": ": CategoryListParams.getOrdinalPolicy now allows to return a\ndifferent OrdinalPolicy per dimension, to better tune how you index\nfacets. Also added OrdinalPolicy.ALL_BUT_DIMENSION.", "change_title": "Add OrdinalPolicy.ALL_BUT_DIMENSION", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "With the move of OrdinalPolicy to CategoryListParams, NonTopLevelOrdinalPolicy was nuked. It might be good to restore it, as another enum value of OrdinalPolicy. It's the same like ALL_PARENTS, only doesn't add the dimension ordinal, which could save space as well as computation time. It's good for when you don't care about the count of Date/, but only about its children counts.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12567111/LUCENE-4715.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4740", "change_description": ": Don't track clones of MMapIndexInput if unmapping\nis disabled. This reduces GC overhead.", "change_title": "Weak references cause extreme GC churn", "detail_type": "Bug", "detail_affect_versions": "3.6.1", "detail_fix_versions": "4.4,6.0", "detail_description": "We are running a set of independent search machines, running our custom software using lucene as a search library. We recently upgraded from lucene 3.0.3 to 3.6.1 and noticed a severe degradation of performance. After doing some heap dump digging, it turns out the process is stalling because it's spending so much time in GC. We noticed about 212 million WeakReference, originating from WeakIdentityMap, originating from MMapIndexInput. Our problem completely went away after removing the clones weakhashmap from MMapIndexInput, and as a side-effect, disabling support for explictly unmapping the mmapped data.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12567571/LUCENE-4740.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4733", "change_description": ": The default Lucene 4.2 codec now uses a more compact\nTermVectorsFormat (Lucene42TermVectorsFormat) based on\nCompressingTermVectorsFormat.", "change_title": "Make CompressingTermVectorsFormat the new default term vectors format?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2", "detail_description": "In LUCENE-4599, I wrote an alternate term vectors format which has a more compact format, and I think it could replace the current Lucene40TermVectorsFormat for the next (4.2) release?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12567369/LUCENE-4733-tests.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-3729", "change_description": ": The default Lucene 4.2 codec now uses a more compact\nDocValuesFormat (Lucene42DocValuesFormat). Sorted values are stored in an\nFST, Numerics and Ordinals use a number of strategies (delta-compression,\ntable-compression, etc), and memory addresses use MonotonicBlockPackedWriter.", "change_title": "Allow using FST to hold terms data in DocValues.BYTES_*_SORTED", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12532250/LUCENE-3729.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4792", "change_description": ": Reduction of the memory required to build the doc ID maps used\nwhen merging segments.", "change_title": "Smaller doc maps", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2", "detail_description": "MergeState.DocMap could leverage MonotonicAppendingLongBuffer to save memory.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570353/LUCENE-4792.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Optimizations", "change_id": "LUCENE-4794", "change_description": ": Spatial RecursivePrefixTreeStrategy's search filter: Skip calls\nto termsEnum.seek() when the next term is known to follow the current cell.", "change_title": "Refactor Spatial RecursivePrefixTreeFilter algorithm for extension", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "In the process of implementing algorithms on the SpatialPrefixTree indexed field like \"Within\" or some variations of Intersects that say collect the distance as side-effect, I find that I need near-copies of the code in RecursivePrefixTreeFilter.  RPTF is pretty intense with lots of optimizations.  So I refactored out the algorithm such that it makes implementing new algorithms much easier yet benefits from the logic in there. Patch to follow...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570417/LUCENE-4794_Spatial_PrefixTree_traversal_abstraction.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4686", "change_description": ": New specialized DGapVInt8IntEncoder for facets (now the\ndefault).", "change_title": "Write a specialized DGapVIntEncoder/Decoder for facets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Today the default encoder/decoder for facets is DGap(VInt). That is a DGapEncoder wrapping a VIntEncoder. Instead of this wrapping, we can write a specialized DGapVIntEncoder which does it all in one call.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565123/LUCENE-4686.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4703", "change_description": ": Add simple PrintTaxonomyStats tool to see summary\ninformation about the facets taxonomy index.", "change_title": "Add basic tool to print some summary stats about your taxonomy index", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I built a Wikipedia index w/ 9 dimensions but I don't know how many ords each child contributes / how many immediate children under each dim / etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565800/LUCENE-4703.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4599", "change_description": ": New oal.codecs.compressing.CompressingTermVectorsFormat which\ncompresses term vectors into chunks of documents similarly to\nCompressingStoredFieldsFormat.", "change_title": "Compressed term vectors", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.2", "detail_description": "We should have codec-compressed term vectors similarly to what we have with stored fields.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565633/solr.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4695", "change_description": ": Added LiveFieldValues utility class, for getting the\ncurrent (live, real-time) value for any indexed doc/field.  The\nclass buffers recently indexed doc/field values until a new\nnear-real-time reader is opened that contains those changes.", "change_title": "Add utility class for getting live values for a given field during NRT indexing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This is a simple utility/wrapper class, that holds the field values for recently indexed documents until the NRT reader has refreshed, and exposes a \"get\" API to get the last indexed value per id. For example one could use this to look up the \"version\" field for a given id, even when that id was just indexed and not yet visible in the NRT reader. The implementation is fairly simple: it just watches the gen coming out of NRTManager and updates/prunes accordingly. The class is abstract: you must subclass it and impl the lookupFromSearcher method...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566213/LUCENE-4695.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4723", "change_description": ": Add AnalyzerFactoryTask to benchmark, and enable analyzer\ncreation via the resulting factories using NewAnalyzerTask.", "change_title": "Add AnalyzerFactoryTask to benchmark, and enable analyzer creation via the resulting factories using NewAnalyzerTask", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.2", "detail_description": "Benchmark algorithms can't currently use analysis factories.  Instead, one must rely on pre-existing analyzers, or write specialized tasks to construct them. Now that all analysis components have factories, benchmark algorithms should be able to use them.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566692/LUCENE-4723.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4728", "change_description": ": Unknown and not explicitly mapped queries are now rewritten\nagainst the highlighting IndexReader to obtain primitive queries before\ndiscarding the query entirely. WeightedSpanTermExtractor now builds a\nMemoryIndex only once even if multiple fields are highlighted.", "change_title": "Allow CommonTermsQuery to be highlighted", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "Add support for CommonTermsQuery to all highlighter impls.  This might add a dependency (query-jar) to the highlighter so we might think about adding it to core?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568001/LUCENE-4728.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4035", "change_description": ": Added ICUCollationDocValuesField, more efficient\nsupport for Locale-sensitive sort and range queries for\nsingle-valued fields.", "change_title": "Collation via docvalues", "detail_type": "Improvement", "detail_affect_versions": "4.0-ALPHA", "detail_fix_versions": "4.2,6.0", "detail_description": "Currently collated sort is via an Analyzer into an indexedfield, which is uninverted in the fieldcache. Instead we could support this with docvalues, and take advantage of future improvements like LUCENE-3729.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12525784/LUCENE-4035.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4547", "change_description": ": Added MonotonicBlockPacked(Reader/Writer), which provide\nefficient random access to large amounts of monotonically increasing\npositive values (e.g. file offsets). Each block stores the minimum value\nand the average gap, and values are encoded as signed deviations from\nthe expected value.", "change_title": "DocValues field broken on large indexes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I tried to write a test to sanity check LUCENE-4536 (first running against svn revision 1406416, before the change). But i found docvalues is already broken here for large indexes that have a PackedLongDocValues field:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552474/test.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4547", "change_description": ": Added AppendingLongBuffer, an append-only buffer that packs\nsigned long values in memory and provides an efficient iterator API.", "change_title": "DocValues field broken on large indexes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I tried to write a test to sanity check LUCENE-4536 (first running against svn revision 1406416, before the change). But i found docvalues is already broken here for large indexes that have a PackedLongDocValues field:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552474/test.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4540", "change_description": ": It is now possible for a codec to represent norms with\nless than 8 bits per value. For performance reasons this is not done\nby default, but you can customize your codec (e.g. pass PackedInts.DEFAULT\nto Lucene42DocValuesConsumer) if you want to make this tradeoff.", "change_title": "Allow packed ints norms", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I was curious what the performance would be, because it might be useful option to use packedints for norms if you have lots of fields and still want good scoring: Today the smallest norm per-field-per-doc you can use is a single byte, and if you have f fields with norms enabled and n docs, it uses f * n bytes of space in RAM. Especially if you aren't using index-time boosting (or even if you are, but not with ridiculous values), this could be wasting a ton of RAM. But then I noticed there was no clean way to allow you to do this in your Similarity: its a trivial patch.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552198/LUCENE-4540.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4764", "change_description": ": A new Facet42Codec and Facet42DocValuesFormat provide\nfaster but more RAM-consuming facet performance.", "change_title": "Faster but more RAM/Disk consuming DocValuesFormat for facets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "The new default DV format for binary fields has much more RAM-efficient encoding of the address for each document ... but it's also a bit slower at decode time, which affects facets because we decode for every collected docID.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12569001/LUCENE-4764.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4769", "change_description": ": Added OrdinalsCache and CachedOrdsCountingFacetsAggregator\nwhich uses the cache to obtain a document's ordinals. This aggregator\nis faster than others, however consumes much more RAM.", "change_title": "Add a CountingFacetsAggregator which reads ordinals from a cache", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Mike wrote a prototype of a FacetsCollector which reads ordinals from a CachedInts structure on LUCENE-4609. I ported it to the new facets API, as a FacetsAggregator. I think we should offer users the means to use such a cache, even if it consumes more RAM. Mike tests show that this cache consumed x2 more RAM than if the DocValues were loaded into memory in their raw form. Also, a PackedInts version of such cache took almost the same amount of RAM as straight int[], but the gains were minor. I will post the patch shortly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568992/LUCENE-4769.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4778", "change_description": ": Add a getter for the delegate in RateLimitedDirectoryWrapper.", "change_title": "Add a getter for the delegate in RateLimitedDirectoryWrapper.", "detail_type": "Improvement", "detail_affect_versions": "4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4765", "change_description": ": Add a multi-valued docvalues type (SORTED_SET). This is equivalent\nto building a FieldCache.getDocTermOrds at index-time.", "change_title": "Multi-valued docvalues field", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "The general idea is basically the docvalues parallel to FieldCache.getDocTermOrds/UninvertedField Currently this stuff is used in e.g. grouping and join for multivalued fields, and in solr for faceting.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12569698/LUCENE-4765.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4780", "change_description": ": Add MonotonicAppendingLongBuffer: an append-only buffer for\nmonotonically increasing values.", "change_title": "MonotonicAppendingLongBuffer", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2", "detail_description": "IndexWriter uses AppendingLongBuffer in several places, and in a few of them the mapping is monotonically increasing so we could save additional space by only encoding the delta from a linear projection.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12569616/LUCENE-4780.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "New Features", "change_id": "LUCENE-4748", "change_description": ": Added DrillSideways utility class for computing both\ndrill-down and drill-sideways counts for a DrillDownQuery.", "change_title": "Add DrillSideways helper class to Lucene facets module", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This came out of a discussion on the java-user list with subject \"Faceted search in OR\": http://markmail.org/thread/jmnq6z2x7ayzci5k The basic idea is to count \"near misses\" during collection, ie documents that matched the main query and also all except one of the drill down filters. Drill sideways makes for a very nice faceted search UI because you don't \"lose\" the facet counts after drilling in.  Eg maybe you do a search for \"cameras\", and you see facets for the manufacturer, so you drill into \"Nikon\". With drill sideways, even after drilling down, you'll still get the counts for all the other brands, where each count tells you how many hits you'd get if you changed to a different manufacturer. This becomes more fun if you add further drill-downs, eg maybe I next drill down into Resolution=10 megapixels\", and then I can see how many 10 megapixel cameras all other manufacturers, and what other resolutions Nikon cameras offer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570680/LUCENE-4748.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "API Changes", "change_id": "LUCENE-4709", "change_description": ": FacetResultNode no longer has a residue field.", "change_title": "Nuke FacetResultNode.residue", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "The residue is the count of all categories that did not make it to the top K. But, this is a senseless statistic. Take for example the following case: two documents with categories [A/1, A/2, A/3] and [A/1, A/4, A/5]. If you ask for top-1 category of \"A\", you'll get A (count=2), A/1 (count=2), but A's residue will be 4! As a user, that number doesn't tell you anything, except maybe when you index only one category per document for a given dimension. But in that case, the residue is root.value - sum(topK.value), which the application can compute if it needs to. In short, we're just wasting CPU cycles for that statistic, so I'm going to remove it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566110/LUCENE-4709.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "API Changes", "change_id": "LUCENE-4716", "change_description": ": DrillDown.query now takes Occur, allowing to specify if\ncategories should be OR'ed or AND'ed.", "change_title": "Add OR support to DrillDown", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "DrillDown provides helper methods to wrap a baseQuery with drill-down categories. All the categories are AND'ed, and it has been asked on the user list for OR support. While users can construct their own BooleanQuery, it would be useful if DrillDown helped them doing that. I think that a simple Occur additional parameter to DrillDown.query will help to some extent.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566383/LUCENE-4716.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "API Changes", "change_id": "LUCENE-4695", "change_description": ": ReferenceManager.RefreshListener.afterRefresh now takes\na boolean indicating whether a new reference was in fact opened, and\na new beforeRefresh method notifies you when a refresh attempt is\nstarting.", "change_title": "Add utility class for getting live values for a given field during NRT indexing", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This is a simple utility/wrapper class, that holds the field values for recently indexed documents until the NRT reader has refreshed, and exposes a \"get\" API to get the last indexed value per id. For example one could use this to look up the \"version\" field for a given id, even when that id was just indexed and not yet visible in the NRT reader. The implementation is fairly simple: it just watches the gen coming out of NRTManager and updates/prunes accordingly. The class is abstract: you must subclass it and impl the lookupFromSearcher method...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566213/LUCENE-4695.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "API Changes", "change_id": "LUCENE-4794", "change_description": ": Spatial RecursivePrefixTreeFilter replaced by\nIntersectsPrefixTreeFilter and some extensible base classes.", "change_title": "Refactor Spatial RecursivePrefixTreeFilter algorithm for extension", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "In the process of implementing algorithms on the SpatialPrefixTree indexed field like \"Within\" or some variations of Intersects that say collect the distance as side-effect, I find that I need near-copies of the code in RecursivePrefixTreeFilter.  RPTF is pretty intense with lots of optimizations.  So I refactored out the algorithm such that it makes implementing new algorithms much easier yet benefits from the logic in there. Patch to follow...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570417/LUCENE-4794_Spatial_PrefixTree_traversal_abstraction.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4705", "change_description": ": Pass on FilterStrategy in FilteredQuery if the filtered query is\nrewritten.", "change_title": "FilteredQuery always uses default FilterStrategy if the filtered query is rewritten", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "the rewrite method doesn't pass on the filterstrategy in FilteredQuery and we don't have a test for it. grrr....", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565938/LUCENE-4705.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4712", "change_description": ": MemoryIndex#normValues() throws NPE if field doesn't exist.", "change_title": "NullPointer Exception in MemoryIndex.MemoryIndexReader", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "NullPointer Exceptions when searching on an index with a query that has a field that's not in the index. The NullPointer is thrown at line 1141: public DocValues normValues(String field) {       if (fieldInfos.get(field).omitsNorms())  <---- //If fieldInfos doesn't contain the field then a NullPointer is thrown.         return null;", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566154/LUCENE-4712.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4550", "change_description": ": Shapes wider than 180 degrees would use too much accuracy for the\nPrefixTree based SpatialStrategy. For a pathological case of nearly 360\ndegrees and barely any height, it would generate so many indexed terms\n(> 500k) that it could even cause an OutOfMemoryError. Fixed.", "change_title": "For extremely wide shapes (> 180 degrees) distErrPct is not used correctly", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.2,6.0", "detail_description": "When a shape is given to a PrefixTreeStrategy (index or query time), it needs to know how many levels down the prefix tree to go for a target precision (distErrPct).  distErrPct is basically a fraction of the radius of the shape, defaulting to 2.5% (0.0025). If the shape presented is extremely wide, > 180 degrees, then the internal calculations in SpatialArgs.calcDistanceFromErrPct(...) will wrongly measure the shape's size as having width < 180 degrees, yielding more accuracy than intended.  Given that this happens for unrealistic shape sizes and results in more accuracy, I am flagging this as \"minor\", but a bug nonetheless.  Indeed, this was discovered as a result of someone using lucene-spatial incorrectly, not for an actual shape they have.  But in the extreme [erroneous] case they had, they had 566k terms  generated, when it should have been ~1k tops.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565987/LUCENE-4550__fix_SpatialArgs_calcDistanceFromErrPct.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4704", "change_description": ": Make join queries override hashcode and equals methods.", "change_title": "TermsIncludingScoreQuery and TermsQuery should implement hashcode() and equals()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "none", "patch_link": "https://issues.apache.org/jira/secure/attachment/12565846/LUCENE-4704.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4724", "change_description": ": Fix bug in CategoryPath which allowed passing null or empty\nstring components. This is forbidden now (throws an exception). Note that if\nyou have a taxonomy index created with such strings, you should rebuild it.", "change_title": "TaxonomyReader drops empty string component from CategoryPath", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I ran the new PrintTaxonomyStats on a Wikipedia facets index, and it hit an AIOOBE because there was a child of the /categories path that had only one component ... this was created because I had added new CategoryPath(\"categories\", \"\") during indexing. I think TaxoReader should preserve and return that empty string from .getPath?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566746/LUCENE-4724.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4732", "change_description": ": Fixed TermsEnum.seekCeil/seekExact on term vectors.", "change_title": "Test TermsEnum.seek on term vectors", "detail_type": "Test", "detail_affect_versions": "4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "We don't have test cases for this method and it looks broken with both Lucene40TermVectorsFormat and CompressiongTermVectorsFormat.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12567011/LUCENE-4732.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4739", "change_description": ": Fixed bugs that prevented FSTs more than ~1.1GB from\nbeing saved and loaded", "change_title": "FST cannot be loaded if it's larger than Integer.MAX_VALUE / 2 bytes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This is really quite awful, but the test I created for > 2.1 GB FSTs never tested save/load ... and ... it doesn't work.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12567425/LUCENE-4739.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4717", "change_description": ": Fixed bug where Lucene40DocValuesFormat would sometimes write\nan extra unused ordinal for sorted types. The bug is detected and corrected\non-the-fly for old indexes.", "change_title": "Lucene40's DocValues (sometimes?) have a bogus extra ordinal", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "I committed the following commented out check in CheckIndex: I'd really like to have this check in CheckIndex, and so it would be great to understand the conditions when the bug happens, and if we can correct it on-the-fly in Lucene40DocValuesReader in LUCENE-4547 branch... otherwise we will have to conditionalize the check based on when the segment was written (it will ultimately be corrected on merge, just annoying)", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4547", "change_description": ": Fixed bug where Lucene40DocValuesFormat was unable to encode\nsegments that would exceed 2GB total data. This could happen in some surprising\ncases, for example if you had an index with more than 260M documents and a\nVAR_INT field.", "change_title": "DocValues field broken on large indexes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "I tried to write a test to sanity check LUCENE-4536 (first running against svn revision 1406416, before the change). But i found docvalues is already broken here for large indexes that have a PackedLongDocValues field:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12552474/test.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4775", "change_description": ": Remove SegmentInfo.sizeInBytes() and make\nMergePolicy.OneMerge.totalBytesSize thread safe", "change_title": "OneMerge.totalBytesSize is trappy", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Spinoff from LUCENE-3051.  This method is not thread-safe unless you hold IndexWriter's lock.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12569068/LUCENE-4775.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4770", "change_description": ": If spatial's TermQueryPrefixTreeStrategy was used to search\nindexed non-point shapes, then there was an edge case where a query should\nfind a shape but it didn't. The fix is the removal of an optimization that\nsimplifies some leaf cells into a parent. The index data for such a field is\nnow ~20% larger. This optimization is still done for the query shape, and for\nindexed data for RecursivePrefixTreeStrategy. Furthermore, this optimization\nis enhanced to roll up beyond the bottom cell level.", "change_title": "GeoShape intersects filter omitted matching docs", "detail_type": "Bug", "detail_affect_versions": "4.0,4.1", "detail_fix_versions": "4.2,6.0", "detail_description": "SpatialPrefixTree#recursiveGetNodes uses an optimization that prevents recursion into the deepest tree level if a parent node in the penultimate level covers all its children.  This produces a bug if the optimization happens both at indexing and at query/filter time. Original post", "patch_link": "https://issues.apache.org/jira/secure/attachment/12569252/LUCENE-4770_Spatial_make_prefixTree_simplification_configurable.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4790", "change_description": ": Fix FieldCacheImpl.getDocTermOrds to not bake deletes into the\ncached datastructure. Otherwise this can cause inconsistencies with readers\nat different points in time.", "change_title": "FieldCache.getDocTermOrds back to the future bug", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Found while working on LUCENE-4765: FieldCache.getDocTermOrds unsafely \"bakes in\" liveDocs into its structure. This means in cases if you have readers at two points in time (r1, r2), and you happen to call getDocTermOrds first on r2, then call it on r1, the results will be incorrect. Simple fix is to make DocTermOrds uninvert take liveDocs explicitly: FieldCacheImpl always passes null, Solr's UninvertedField just keeps doing what its doing today (since its a top-level reader, and cached somewhere else). Also DocTermOrds had a telescoping ctor that was uninverting twice.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570170/LUCENE-4790.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4791", "change_description": ": A conjunction of terms (ConjunctionTermScorer) scanned on\nthe lowest frequency term instead of skipping, leading to potentially\nlarge performance impacts for many non-random or non-uniform\nterm distributions.", "change_title": "ConjunctionTermScorer scans instead of skips on first scorer", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "As discovered by John Wang, it looks like a bug was introduced when ConjunctionTermScorer was first introduced in 7/2011 that causes scanning instead of skipping on the lowest frequency term. http://markmail.org/message/wuukqzbhe7zgkfmf", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570314/LUCENE-4791.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4798", "change_description": ": PostingsHighlighter's formatter sometimes didn't highlight\nmatched terms.", "change_title": "PostingsHighlighter's formatter sometimes doesnt highlight matched terms", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This can happen if you have a sentence where the query terms match many times in the same sentence: for example if you query on \"testing highlighter\" but you have \"Testing highlighters is sometimes harder than testing other things.\" The issue is that the formatter receives all 3 matches, but in this order: Testing (first occurrence) testing (second occurrence) highlighters The formatter expects the matches to be in sorted order by offset (not by term, then offset). This is how the javadocs say they should be. But there is currently a bug, a stupid side effect of how the ranking is done. Because of this, in this example \"highlighters\" isnt marked up in bold.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570922/LUCENE-4798.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4796", "change_description": ",", "change_title": "NamedSPILoader.reload needs to be synchronized", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Spun off of SOLR-4373: as discsused with uwe on IRC, NamedSPILoader.reload is not thread safe: it reads from this.services at the beginging of hte method, makes additions based on the method input, and then overwrites this.services at the end of the method.  if the method is called by two threads concurrently, the entries added by threadB could be lost if threadA enters the method before threadB and exists the method after threadB", "patch_link": "https://issues.apache.org/jira/secure/attachment/12570866/LUCENE-4796.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "SOLR-4373", "change_description": ",", "change_title": "In multicore, lib directives in solrconfig.xml cause conflict and clobber directives from earlier cores", "detail_type": "Bug", "detail_affect_versions": "4.1,4.2", "detail_fix_versions": "4.2", "detail_description": "Having lib directives in the solrconfig.xml files of multiple cores can cause problems when using multi-threaded core initialization â€“ which is the default starting with Solr 4.1. The problem manifests itself as init errors in the logs related to not being able to find classes located in plugin jars, even though earlier log messages indicated that those jars had been added to the classpath. One work around is to set coreLoadThreads=\"1\" in your solr.xml file â€“ forcing single threaded core initialization.  For example... (Similar problems may occur if multiple cores are initialized concurrently using the /admin/cores handler)", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4802", "change_description": ": Don't compute norms for drill-down facet fields.", "change_title": "FacetFields should omitNorms for $facets field", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "We are indexing norms today but we only ever filter by these fields ...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12571006/LUCENE-4802.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-4804", "change_description": ": PostingsHighlighter sometimes applied terms to the wrong passage,\nif they started exactly on a passage boundary.", "change_title": "PostingsHighlighter sometimes applies term to the wrong passage", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "There is an off-by-one if the term starts exactly on a sentence boundary.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12571067/LUCENE-4804.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Documentation", "change_id": "LUCENE-4718", "change_description": ": Fixed documentation of oal.queryparser.classic.", "change_title": "Default field in query syntax documentation has confusing error", "detail_type": "Bug", "detail_affect_versions": "4.0", "detail_fix_versions": "4.2", "detail_description": "The explanation of default search fields uses two different queries that are supposed to be semantically the same, but the query text changes between the two examples.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12566493/SOLR-4357.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Documentation", "change_id": "LUCENE-4784", "change_description": ",", "change_title": "Out of date API document-ValueSourceQuery", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "None", "detail_description": "The following API documents talk about ValueSourceQuery: http://lucene.apache.org/core/4_1_0/queries/org/apache/lucene/queries/CustomScoreProvider.html http://lucene.apache.org/core/4_1_0/queries/org/apache/lucene/queries/CustomScoreQuery.html However, ValueSourceQuery is deleted in lucene 4.1, according to the following migration guide. http://lucene.apache.org/core/4_1_0/MIGRATE.html The following lists the replacement classes for those removed: ...  o.a.l.search.function.ValueSourceQuery -> o.a.l.queries.function.FunctionQuery Please update the API documents to reflect the latest code.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Documentation", "change_id": "LUCENE-4785", "change_description": ",", "change_title": "Out of date API document-RangeQuery", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "None", "detail_description": "The following API documents talk about RangeQuery: http://lucene.apache.org/core/4_1_0/queryparser/org/apache/lucene/queryparser/classic/QueryParserBase.html http://lucene.apache.org/core/4_1_0/core/org/apache/lucene/document/DateTools.html However, RangeQuery is deleted in lucene 4.1, according to the change log: http://lucene.apache.org/core/4_1_0/changes/Changes.html LUCENE-1944, LUCENE-1856, LUCENE-1957, LUCENE-1960, LUCENE-1961, LUCENE-1968, LUCENE-1970, LUCENE-1946, LUCENE-1971, LUCENE-1975, LUCENE-1972, LUCENE-1978, LUCENE-944, LUCENE-1979, LUCENE-1973, LUCENE-2011: Remove deprecated methods/constructors/classes: ...       Remove RangeQuery, RangeFilter and ConstantScoreRangeQuery. Please update the API documents to reflect the latest code.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Documentation", "change_id": "LUCENE-4786", "change_description": ",", "change_title": "Out of date API document-SinkTokenizer", "detail_type": "Bug", "detail_affect_versions": "4.1", "detail_fix_versions": "None", "detail_description": "The following API document talks about SinkTokenizer: http://lucene.apache.org/core/4_1_0/analyzers-common/org/apache/lucene/analysis/sinks/package-summary.html However, SinkTokenizer is deleted and replaced by TeeSinkTokenFilter in lucene 4.1, according to the change log: http://lucene.apache.org/core/4_1_0/changes/Changes.html LUCENE-1422, LUCENE-1693: New TokenStream API that uses a new class called AttributeSource instead of the Token class, which is now a utility class that holds common Token attributes. All attributes that the Token class had have been moved into separate classes: TermAttribute, OffsetAttribute, PositionIncrementAttribute, PayloadAttribute, TypeAttribute and FlagsAttribute. The new API is much more flexible; it allows to combine the Attributes arbitrarily and also to define custom Attributes. The new API has the same performance as the old next(Token) approach. For conformance with this new API Tee-/SinkTokenizer was deprecated and replaced by a new TeeSinkTokenFilter. Please update the API documents to reflect the latest code.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Build", "change_id": "LUCENE-4654", "change_description": ": Test duration statistics from multiple test runs should be\nreused.", "change_title": "Test duration statistics from multiple test runs should be reused (locally).", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "This is trivial to accomplish: when somebody (or jenkins) runs tests multiple times the execution statistics could be reused to improve load balancing on the local machine (local hardware and settings) in favor of the precached values currently version in the svn repo. At this moment we already do this, but keep the stats under build/ and every ant clean effectively removes them. I could move those stats under an svn-ignored folder elsewhere so that these stats are not lost and reused for balancing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Build", "change_id": "LUCENE-4636", "change_description": ": Upgrade ivy to 2.3.0", "change_title": "Upgrade ivy for IVY-1388 - build hangs at \"resolve:\"", "detail_type": "Improvement", "detail_affect_versions": "3.6,4.0", "detail_fix_versions": "4.2,6.0", "detail_description": "For certain failures during a lucene/solr build, or if you press ctrl-c at the wrong moment during the build, ivy may leave a lockfile behind.  The next time you run a build, ivy will hang with \"resolve:\" on the screen. The ivy project has a fix, currently not yet released.  When it does get released, the version installed by the ivy-bootstrap target needs to be updated.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12567478/LUCENE-4636.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Build", "change_id": "LUCENE-4570", "change_description": ": Use the Policeman Forbidden API checker, released separately\nfrom Lucene and downloaded via Ivy.", "change_title": "Release ForbiddenAPI checker on Google Code", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Currently there is source code in lucene/tools/src (e.g. Forbidden APIs checker ant task). It would be convenient if you could download this thing in your ant build from ivy (especially if maybe it included our definitions .txt files as resources). In general checking for locale/charset violations in this way is a pretty general useful thing for a server-side app. Can we either release lucene-tools.jar as an artifact, or maybe alternatively move this somewhere else as a standalone project and suck it in ourselves?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12569699/LUCENE-4570-maven-inherited.patch", "patch_content": "none"}
{"library_version": "4.2.0", "change_type": "Build", "change_id": "LUCENE-4758", "change_description": ": 'ant jar', 'ant compile', and 'ant compile-test' should\nrecurse.", "change_title": "'ant jar', 'ant compile' and 'ant compile-test' should recurse", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.2,6.0", "detail_description": "Right now, 'ant compile' at the top level compiles Lucene core, all non-test Solr sources, and all Lucene modules on which Solr depends.  lucene/codecs/, e.g., doesn't get compiled, because 'ant compile' is an alias for 'ant compile-core' under lucene/. Similarly for 'ant jar' (except there is no top-level target for this right now), with some problems under solr/. There is no top-level 'ant compile-test'. All these targets should recurse at all levels. Under lucene/ and solr/, 'ant jar-core' and 'ant compile-core' should be aliased to running the operation under core/.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12568261/LUCENE-4758.patch", "patch_content": "none"}

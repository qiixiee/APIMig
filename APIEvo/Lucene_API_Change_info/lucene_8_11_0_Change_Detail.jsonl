{"library_version": "8.11.0", "change_type": "Improvements", "change_id": "LUCENE-9662", "change_description": ": Make CheckIndex concurrent by parallelizing index check across segments.", "change_title": "CheckIndex should be concurrent", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I am watching a nightly benchmark run slowly run its CheckIndex step, using a single core out of the 128 cores the box has. It seems like this is an embarrassingly parallel problem, if the index has multiple segments, and would finish much more quickly on concurrent hardware if we did \"thread per segment\". If wanted to get even further concurrency, each part of the Lucene index that is checked is also independent, so it could be \"thread per segment per part\".", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Improvements", "change_id": "LUCENE-10103", "change_description": ": Make QueryCache respect Accountable queries.", "change_title": "QueryCache not estimating query size properly", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "QueryCache seems estimating the cached query size using a constant, it will cause OOM error in some extreme cases where queries cached will use far more memories than assumed. (The default QueryCache tries to use only 5% of heap) One example of such memory-eating query is AutomatonQuery, it will each carry a RunAutomaton , which consumes a good amount of memory in exchange for the speed. On the other hand, we actually have a good implementation of Accountable interface for AutomatonQuery (though it will become a bit more complicated later since this query will eventually be rewritten to something else), so maybe QueryCache could use those estimation directly (using an instanceof check)? Or moreover we could make all Query implement {Accountable}}, and maybe the default implementation could just be returning the current constant we're using, and only override the method of the potential troublesome queries?", "patch_link": "https://issues.apache.org/jira/secure/attachment/13033731/query_cache_error_demo.patch", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Optimizations", "change_id": "LUCENE-9673", "change_description": ": Substantially improve RAM efficiency of how MemoryIndex stores\npostings in memory, and reduced a bit of RAM overhead in\nIndexWriter's internal postings book-keeping", "change_title": "The level of IntBlockPool slice is always 1", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "First slice is allocated by IntBlockPoo.newSlice(), and its level is 1,    If one slice is not enough, IntBlockPoo.allocSlice() is called to allocate more slices, as the following code shows, level is 1, newLevel is NEXT_LEVEL_ARRAY[0] which is also 1.  The result is the level of IntBlockPool slice is always 1, the first slice is  2 bytes long, and all subsequent slices are 4 bytes long.  ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13019041/LUCENE-9673.patch", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Optimizations", "change_id": "LUCENE-10196", "change_description": ": Improve IntroSorter with 3-ways partitioning.", "change_title": "Improve IntroSorter with 3-ways partitioning", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.11", "detail_description": "I added a SorterBenchmark to evaluate the performance of the various Sorter implementations depending on the strategies defined in BaseSortTestCase (random, random-low-cardinality, ascending, descending, etc). By changing the implementation of the IntroSorter to use a 3-ways partitioning, we can gain a significant performance improvement when sorting low-cardinality lists, and with additional changes we can also improve the performance for all the strategies. Proposed changes:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10111", "change_description": ": Missing calculating the bytes used of DocsWithFieldSet in NormValuesWriter.", "change_title": "Missing calculating the bytes used of DocsWithFieldSet in NormValuesWriter", "detail_type": "Improvement", "detail_affect_versions": "8.9", "detail_fix_versions": "8.11", "detail_description": "Basing on the implement of updateBytesUsed() in SortedDocValuesWriter and the other DocValuesWriters, it seems to lack of calculating the BytesUsed of DocsWithFieldSet in NormValuesWriter？  ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10116", "change_description": ": Missing calculating the bytes used of DocsWithFieldSet and currentValues in SortedSetDocValuesWriter.", "change_title": "Missing calculating the bytes used of DocsWithFieldSet and currentValues in SortedSetDocValuesWriter", "detail_type": "Improvement", "detail_affect_versions": "8.9", "detail_fix_versions": "8.11", "detail_description": "After LUCENE-10111 ,  I read the bytes counting in SortedSetDocValuesWriter, Comparing SortedNumericDocValuesWriter, it seems to lack of bytes used of DocsWithFieldSet and currentValues .  Sorry for pushing a new commit with same issue      ", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10070", "change_description": "Skip deleted docs when accumulating facet counts for all docs.", "change_title": "\"count all\" faceting functionality counts deleted docs for multiple implementations", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "A few different Facets implementations supporting a \"count all\" style constructor that allows the user to not pass in a FacetsCollector instance. It advertises that it's equivalent to using a FacetsCollector populated with a MatchAllDocsQuery, but more efficient. It looks like, with the exception of FastTaxonomyFacetCounts, none of the implementations correctly account for deleted documents (have a look at FastTaxonomyFacetCounts for a correct example that consults \"live docs.\" From what I can tell, the affected implementations are: I'll attach a PR shortly illustrating unit tests I wrote that confirm the bug.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10134", "change_description": ": ConcurrentSortedSetDocValuesFacetCounts shouldn't share liveDocs Bits across threads.", "change_title": "TestSortedSetDocValuesFacets fails with Bits shared between threads", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "Repro:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10008", "change_description": ": Respect ignoreCase in CommonGramsFilterFactory", "change_title": "CommonGramsFilterFactory doesn't respect ignoreCase=true when default stopwords are used", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "CommonGramsFilterFactory's use of the \"words\" and \"ignoreCase\" config options is inconsistent with how StopFilterFactory uses them - leading to \"ignoreCase=true\" not being respected unless \"words\" is specified... StopFilterFactory... CommonGramsFilterFactory...", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10154", "change_description": ": NumericLeafComparator to define getPointValues.", "change_title": "NumericLeafComparator to define getPointValues", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "NumericLeafComparator must have a method getPointValues similar how it has getNumericDocValues. Numeric Sort optimization with points relies on the assumption that points and doc values record the same information, as we substitute iterator over doc_values with one over points. If we extend getNumericDocValues it almost certainly means that whatever PointValues NumericComparator is going to look at shouldn't be used to skip non-competitive documents. Returning null for pointValues in this case will force comparator NOT to use sort optimization with points, and continue with a traditional way of iterating over doc values.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10208", "change_description": ": Ensure that the minimum competitive score does not decrease in concurrent search.", "change_title": "Minimum score can decrease in concurrent search", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "TestLatLonPointDistanceFeatureQuery#testCompareSorting started to fail sporadically after https://github.com/apache/lucene/pull/331.  The test change added in this PR exposes an existing bug in top docs collector.  They re-set the minimum score multiple times per segment when a bulk scorer is used. In practice this is not a problem because the local minimum score cannot decrease.  However when concurrent search is used,  the global minimum score is updated after the local one so that breaks the assertion.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Build", "change_id": "LUCENE-10104", "change_description": ",", "change_title": "Update forbiddenapis to 3.2", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.9", "detail_fix_versions": "9.0,8.11", "detail_description": "Forbiddenapis was released in version 3.2: https://github.com/policeman-tools/forbidden-apis/wiki/Changes#version-32-released-2021-09-15 This issue will track the update (it's a one-line change) in LuSolr 8.x, Lucene 9, Solr 9.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Build", "change_id": "SOLR-15631", "change_description": ",", "change_title": "Update forbiddenapis to 3.2", "detail_type": "Improvement", "detail_affect_versions": "9.0,8.9", "detail_fix_versions": "9.0,8.11", "detail_description": "Forbiddenapis was released in version 3.2: https://github.com/policeman-tools/forbidden-apis/wiki/Changes#version-32-released-2021-09-15 This issue will track the update (it's a one-line change) in LuSolr 8.x, Lucene 9, Solr 9.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.11.0", "change_type": "Other", "change_id": "LUCENE-10098", "change_description": ": Add docs/links to GermanAnalyzer describing how to decompound nouns.", "change_title": "Add note/link to GermanAnalyzer for decompounding nouns", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.11", "detail_description": "The GermanAnalyzer doesn't split compound nouns. Doing this requires some auxiliary data files with strange licenses. But uschindler has documented and packaged everything up to make this easy: https://github.com/uschindler/german-decompounder We added a Lucene API example (using CustomAnalyzer) to the README: https://github.com/uschindler/german-decompounder/pull/6 So I think it would be nice to link to this from the javadocs, it makes it really easy to download the datafiles and configure an appropriate analyzer, if you are OK with Latex and LGPL licenses for the data files (which many folks might be).", "patch_link": "none", "patch_content": "none"}

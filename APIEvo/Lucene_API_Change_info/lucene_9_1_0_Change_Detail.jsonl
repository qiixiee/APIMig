{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10244", "change_description": ": MultiCollector::getCollectors is now public, allowing users to access the wrapped\ncollectors.", "change_title": "Please consider opening MultiCollector::getCollectors for public use", "detail_type": "Improvement", "detail_affect_versions": "8.10.1", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "The usage of MultiCollector sometimes is very convenient along with CollectorManagers instances (as an alternative to MultiCollectorManager), for example: Unfortunately, the reduce() phase is lacking the access to  MultiCollector::getCollectors method, since it is package protected at the moment. Making MultiCollector::getCollectors  public would make it convenient to use MultiCollector + CollectorManager and implement the reduce phase by accessing individual collectors.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10197", "change_description": ": UnifiedHighlighter now has a Builder to construct it.  The UH's setters are now\ndeprecated.", "change_title": "UnifiedHighlighter should use builders for thread-safety", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "UnifiedHighlighter is not thread-safe due to the presence of setters. We can move the fields to builder so that the class becomes thread-safe.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13035278/LUCENE-10197.patch", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10301", "change_description": ": the test framework is now a module. All the classes have been moved from\norg.apache.lucene.* to org.apache.lucene.tests.* to avoid package name conflicts with the\ncore module.", "change_title": "The test-framework as a module (or a separate test-framework-module)", "detail_type": "Sub-task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "The test framework has split packages. It's a follow-up to introducing modules but eventually the modular test subprojects will need something like the test framework too. I'm not sure whether we should start a new subproject for this or try to refactor the test framework, but it's a follow-up once the modules themselves are working and testable, I think.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10183", "change_description": ": KnnVectorsWriter#writeField to take KnnVectorsReader instead of VectorValues.", "change_title": "KnnVectorsWriter#writeField should take a KnnVectorsReader, not a VectorValues instance", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "By taking a VectorValues instance, KnnVectorsWriter#write doesn't let implementations iterate over vectors multiple times if needed. It should take a KnnVectorReaders similarly to doc values, where the writer takes a DocValuesProducer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10335", "change_description": ": Deprecate helper methods for resource loading in IOUtils and StopwordAnalyzerBase\nthat are not compatible with module system (Class#getResourceAsStream() and Class#getResource()\nare caller sensitive in Java 11). Instead add utility method IOUtils#requireResourceNonNull(T)\nto test existence of resource based on null return value.", "change_title": "IOUtils.getDecodingReader(Class<?>, String) is broken with modules", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "This method calls clazz.getResourceAsStream() but in a modular application the method won't see any of the resources in clazz's module, causing an NPE. We should deprecate or even remove this method entirely, leaving only getDecodingReader(InputStream) and opening the resource on the caller's side.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13037865/LUCENE-10335-1.patch", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10349", "change_description": ": WordListLoader methods now return unmodifiable CharArraySets.", "change_title": "Cleanup WordListLoader to use try-with-resources and make the default stop words unmodifiable", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "The WordlistLoader can be refactored to use try-with-resources. In addition I figured out that the default stopword files in the analyszers module are documented to be unmodifiable, but in fact the wrapping around the CharArraySet to make them unmodifiable is missing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10377", "change_description": ": SortField.getComparator() has changed signature. The second parameter is now\na boolean indicating whether or not skipping should be enabled on the comparator.", "change_title": "Replace 'sortPos' parameter in SortField.getComparator()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "SortField.getComparator() takes two parameters: the number of hits we are collecting, so that the comparator can size its internal priority queue; and the position of this sortfield in the overall sort.  This second parameter is only actually used to determine whether or not the SortField should enable various skipping operations, building comparative iterators etc.  However, it's not clear from the API that this is why it is passed, and as a consequence all sorts of consumers of this API faithfully pass through their sort positions even when they aren't actually using skipping at all.  In particular, CheckIndex does this when checking index sorts, and this in fact leads to a bug, where it will attempt to build skipping structures over fields from 8x-created indexes that did not have the same type constraints that 9x indexes do and incorrectly throw an error. I think we should replace this second parameter with a simple boolean, `enableSkipping`, that the TopHits collectors can pass as `true` for the first sortfield.  Other APIs that use sorts but not skipping, like CheckIndex or the grouping collectors, can always pass `false` so we don't waste time building skipping structures that never get used.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10381", "change_description": ": Require users to provide FacetsConfig for SSDV faceting.", "change_title": "Require users to provide FacetsConfig with initializing DefaultSortedSetDocValuesReaderState", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "In LUCENE-10250, we added the ability for users to provide their FacetsConfig when constructing DefaultSortedSetDocValuesReaderState. This is necessary for some of the new functionality and bug fixes in SSDV faceting (e.g., we need to know if a dim is configured as hierarchical, multivalued and/or requiring dim counts). To remain backwards-compatible, existing ctors were maintained and users don't need to provide config, which means we have to fall back to some default (pre-existing) behavior if we don't have the config. We should make the config mandatory going forward so that users always explicitly provide their config. This is consistent with taxonomy faceting where users must provide config. It's also pretty trappy to let users setup their state without config as they might just overlook the need to provide it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10368", "change_description": ": IntTaxonomyFacets has been deprecated and is no longer a supported extension point\nfor user-created faceting implementations.", "change_title": "Reduce visibility of IntTaxonomyFacets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Opening this issue as a follow up from the conversation over here: https://github.com/apache/lucene/pull/578#discussion_r779353869 I'd propose we reduce the visibility to pkg-private and do a little cleanup. This is essentially indicating that IntTaxonomyFacets is an internal implementation detail of taxonomy faceting (shared common logic) as opposed to an intentional extension point for users.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10400", "change_description": ": Add constructors that take external resource Paths to dictionary classes in Kuromoji and Nori:\nConnectionCosts, TokenInfoDictionary, and UnknownDictionary. Old constructors that take resource scheme and\nresource path in those classes are deprecated; These are replaced with the new constructors and planned to be\nremoved in a future release.", "change_title": "Clean up the constructors' API signature of dictionary classes in kuromoji and nori", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "It was suggested in a few issues/pr comments. Before working on LUCENE-8816 or LUCENE-10393, we'd need to sort the protected constructor APIs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10050", "change_description": ": Deprecate DrillSideways#search(Query, Collector) in favor of\nDrillSideways#search(Query, CollectorManager). This reflects the change (", "change_title": "Remove DrillSideways#search(DrillDownQuery,Collector) in favor of DrillSideways#search(DrillDownQuery,CollectorManager)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "With similar motivation to LUCENE-10002, we should consider doing away with the ability to directly provide a Collector to DrillSideways in favor of always accepting a CollectorManager. Just like with IndexSearcher, it's trappy that you can provide an Executor when setting up DrillSideways and then not leverage it by directly providing a single Collector.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10002", "change_description": ": Deprecate DrillSideways#search(Query, Collector) in favor of\nDrillSideways#search(Query, CollectorManager). This reflects the change (", "change_title": "Remove IndexSearcher#search(Query,Collector) in favor of IndexSearcher#search(Query,CollectorManager)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "It's a bit trappy that you can create an IndexSearcher with an executor, but that it would always search on the caller thread when calling IndexSearcher#search(Query,Collector). Let's remove IndexSearcher#search(Query,Collector), point our users to IndexSearcher#search(Query,CollectorManager) instead, and change factory methods of our main collectors (e.g. TopScoreDocCollector#create) to return a CollectorManager instead of a Collector?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10420", "change_description": ": Move functional interfaces in IOUtils to top-level interfaces.", "change_title": "Move functional interfaces in IOUtils to top-level interfaces", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Suggested at https://github.com/apache/lucene/pull/643#discussion_r802285404.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10398", "change_description": ": Add static method for getting Terms from LeafReader.", "change_title": "Add static method for getting Terms from LeafReader", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Hi all, LeafReader has methods like getBinaryDocValues(String field) that return null values if the field is not indexed. These methods also have equivalent DocValues static methods, such as DocValues.getBinary(), which return an emptyBinary() rather than a null if there is no field. I noticed that Terms does not have an equivalent static method for LeafReader.terms() like Terms.terms() or something similar. I was wondering if there was a reason for this, or if a method like this could be useful. Thanks!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10440", "change_description": ": TaxonomyFacets and FloatTaxonomyFacets have been deprecated and are no longer\nsupported extension points for user-created faceting implementations.", "change_title": "Reduce visibility of TaxonomyFacets and FloatTaxonomyFacets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Similar to what we did in LUCENE-10379, let's reduce the public visibility of TaxonomyFacets and FloatTaxonomyFacets to pkg-private since they're really implementation details housing common logic and not really intended as extension points for user faceting.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10431", "change_description": ": MultiTermQuery.setRewriteMethod() has been deprecated, and constructor\nparameters for the various implementations added.", "change_title": "AssertionError in BooleanQuery.hashCode()", "detail_type": "Bug", "detail_affect_versions": "8.11.1", "detail_fix_versions": "None", "detail_description": "Hello devs, the constructor of BooleanQuery can under some circumstances trigger a hash code computation before \"clauseSets\" is fully filled. Since BooleanClause is using its query field for the hash code too, it can happen that the \"wrong\" hash code is stored, since adding the clause to the set triggers its hashCode(). If assertions are enabled the check in BooleanQuery, which recomputes the hash code, will notice it and throw an error. exception: I noticed this while trying to upgrade the NetBeans maven indexer modules from lucene 5.x to 8.x https://github.com/apache/netbeans/pull/3558", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "API Changes", "change_id": "LUCENE-10171", "change_description": ": OpenNLPOpsFactory.getLemmatizerDictionary(String, ResourceLoader) now returns a\nDictionaryLemmatizer object instead of a raw String serialization of the dictionary.", "change_title": "Caching issue on dictionary-based OpenNLPLemmatizerFilterFactory", "detail_type": "Bug", "detail_affect_versions": "9.0,7.7.3,8.10", "detail_fix_versions": "9.1", "detail_description": "When providing a lemmas.txt dictionary file, OpenNLPLemmatizerFilterFactory caches internally only the string format of the dictionary, and not the DictionaryLemmatizer object. This results in parsing and creating a new DictionaryLemmatizer object every time the OpenNLPLemmatizerFilterFactory.create() is called. In our case, with a large lemmas.txt file (5MB) and the OpenNLPLemmatizerFilter used in many fields across our setup and in multiple collections (we use Solr), we had several random OOM issues and generally high server load due to GC activity. After heap dump analysis we noticed few thousands of DictionaryLemmatizer instances of around 80MB each. By switching the caching to the DictionaryLemmatizer instead of the String, we were able to resolve these issues. I will be attaching a PR for review, please let me know of any comments. Thanks!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10255", "change_description": ": Lucene JARs are now proper modules, with module descriptors and dependency information.", "change_title": "Fully embrace the java module system", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I've experimented a bit trying to move the code to the JMS. It is surprisingly difficult... A PoC that almost passes all checks is here: -https://github.com/dweiss/lucene/tree/jms- https://github.com/dweiss/lucene/tree/jms2 Here are my conclusions so far:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10342", "change_description": ": Lucene Core now depends on java.logging (JUL) module and reports\nif MMapDirectory cannot unmap mapped ByteBuffers or RamUsageEstimator's object size\ncalculations may be off. This was added especially for users running Lucene with the\nJava Module System where some optional features are not available by default or supported.\nFor all apps using Lucene it is strongly recommended, to explicitely require non-standard\nJDK modules: jdk.unsupported (unmapping) and jdk.management (OOP size for RAM usage calculatons).\nIt is also recommended to install JUL logging adapters to feed the log events into your app's\nlogging system.", "change_title": "Add (very limited) java.util.logging to Lucene Core", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "At the moment we have some parts in Lucene where we check for features of the JDK and use them, only if available: As those checks require \"optional\" modules in the java module system, which may not even be available by default as they are unsupported, we do the check dynamically. In Classpath mode, this are all nobrainers, because the modules are available by default. But as soon as downstream code switches to module mode, the whole things may suddenly stop working (because module was forgotten) for the application and nobody notices. A user of Lucene may then need to add those modules to the module descriptor of the application or pass on command line (to make it optional if the JVM supports it, e.g. what happens if \"jdk.unsupported\" is not available for custom JDK xy by provider Z?). If heshe misses to add the module or for some other reason it does not work (like feature is not available in J9 instead of Hotspot), we silently disable MMapDirectory or for memory usage we may be off by an factor of 2. My suggestion is now to enable java.util.logging in Lucene's core (maybe also in other modules, too - luke is already) and report such warnings using java.util.logging. Any downstream code will see the logging then depending on the used logging system (log4j, slf4j with correct wrapper module). I know this suggestion may cause a bit a flame war because of disagreement about pros and cons about if logging is needed, so I'd like to limit this to warnings+. We won't ever emit info/debug/trace. To make sure we only log \"warn\" and \"error\"/\"severe\" events, I will add a forbiddenapis rule to deny all other loglevels. The added logging will only appear once at application startup because I will add them only to static initializers. As a side effect I will also add an IndexWriter log stream implementation for java.util.logging to make it easy to log infostream events during indexing through a logging system. I will provide a PR adding 2 warnings and the InfoStream implementation.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10330", "change_description": ": Make MMapDirectory tests fail by default, if unmapping does not work.", "change_title": "Make the mmap directory tests fail by default, if unmapping does not work", "detail_type": "Improvement", "detail_affect_versions": "9.0,9.1,10.0(main)", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Since the recent refactoring with Java modules, we figured out that we have no tests, that MMapDirectory always works correctly and is choosen as default, e.g. because you have a module setup where jdk.unsupported is missing (last seen on Luke) or since the refactoring of tests after LUCENE-10255 (see also discussion about how tests should be ran with module system in LUCENE-10328). Currently TestMMapDirectory and similar ones silently disable themselves if the \"Unmap Hack\" is not detected. The warning is hidden by gradle, so that's a bad default. We should simply fail the test when unmapping does not work and add an option to \"still run tests if you know what you're doing\". All current Java releases are OpenJDK based (at least those in main/9.x as those use JDK 11), so the unmap hack always works. Of course MMapDirectory still disables itsself at runtime (e.g. when security policy is preventing it or if user has module system and does not add it – we should add this to the warning message), but we should make sure that we test it on known platforms. Another option would be to check the assume and modify it to also check the version tring for \"OpenJDK\", \"Oracle\" and enforce that test is enabled. But this is too risky, so I would simply fail test if unmapping does not work with a system property to disable checks.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10223", "change_description": ": Add interval function support to StandardQueryParser. Add min-should-match operator\nsupport to StandardQueryParser. Update and clean up package documentation in flexible query parser\nmodule.", "change_title": "Add interval function support to standard query parser", "detail_type": "New Feature", "detail_affect_versions": "9.1", "detail_fix_versions": "9.1", "detail_description": "This issue adds syntactic support for expressing interval sub-clauses within queries that StandardQueryParser can parse.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10220", "change_description": ": Add an utility method to get IntervalSource from analyzed text (or token stream).", "change_title": "Add an utility method to get IntervalSource from analyzed text (or token stream)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "The Intervals has a number of utility methods that provide an IntervalSource for tokens, phrases, etc. But it's missing an important bit: an interval source matching tokens that are a result of some string applied to a full analysis chain. This corresponds to actually resides in the index and is hard to predict from the outside. This is an important omission in Intervals as a utility class. I borrowed the implementation from the then-ASL-licensed Elasticsearch code at: https://github.com/elastic/elasticsearch/blob/7.10/server/src/main/java/org/elasticsearch/index/query/IntervalBuilder.java#L54-L106 I also modified it slightly to fit the static-method-based Lucene API. I also added a small test that showcases how this method can be used in practice (and why it's hard to accomplish the same result with existing methods). The only thing I'm not sure is how to attribute Elasticsearch properly - in the notice file, perhaps?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10085", "change_description": ": Added Weight#count on DocValuesFieldExistsQuery to speed up the query if terms or\npoints are indexed.", "change_title": "Implement Weight#count on DocValuesFieldExistsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Now that we require all documents to use the same features (LUCENE-9334) we could implement Weight#count to return docCount if either terms or points are indexed.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10263", "change_description": ": Added Weight#count to NormsFieldExistsQuery to speed up the query if all\ndocuments have the field..", "change_title": "Implement Weight#count() on NormsFieldExistsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Similar to LUCENE-10085, we can quickly get a count of documents that have a text field present by reading docCount() from the field's Terms instance if there are no deleted documents.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10248", "change_description": ": Add SpanishPluralStemFilter, for precise stemming of Spanish plurals.\nFor more information, see", "change_title": "Add SpanishPluralStemFilter", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.1", "detail_description": "We propose a new Spanish stemmer just for stemming plural to singular whilst maintaining gender: the SpanishPluralStemmer. Our goal is to provide a lightweight algorithmic approach with better precision and recall than current approaches. In the following article we made a comparison of different Spanish Stemmers and use cases and which value adds our contribution Our Solution is an algorithmic approach Spanish rules for building plural forms based on rules defined in wikilengua Some characteristics:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "https://s.apache.org/spanishplural", "change_description": ": Add SpanishPluralStemFilter, for precise stemming of Spanish plurals.\nFor more information, see", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10243", "change_description": ": StandardTokenizer, UAX29URLEmailTokenizer, and HTMLStripCharFilter have\nbeen upgraded to Unicode 12.1", "change_title": "increase unicode versions of tokenizers to unicode 12.1", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Followup from LUCENE-10239 Bump the Unicode version of these tokenizers from Unicode 9 to 12.1, which is the most recent supported by the jflex release.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10335", "change_description": ": Add ModuleResourceLoader as complement to ClasspathResourceLoader.", "change_title": "IOUtils.getDecodingReader(Class<?>, String) is broken with modules", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "This method calls clazz.getResourceAsStream() but in a modular application the method won't see any of the resources in clazz's module, causing an NPE. We should deprecate or even remove this method entirely, leaving only getDecodingReader(InputStream) and opening the resource on the caller's side.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13037865/LUCENE-10335-1.patch", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10245", "change_description": ": MultiDoubleValues(Source) and MultiLongValues(Source) were added as multi-valued\nversions of DoubleValues(Source) and LongValues(Source) to the facets module. LongValueFacetCounts,\nLongRangeFacetCounts and DoubleRangeFacetCounts were augmented to support these new multi-valued\nabstractions. DoubleRange and LongRange also support creating queries from these multi-valued\nsources.", "change_title": "Allow users to provide DocValues to (some) Facets Implementations", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "We have some Facets implementations that do aggregations over numeric values associated with each doc (e.g., DoubleRangeFacetCounts, LongValueFacetCounts, etc.). These implementations allow the user to either specify, 1) a field to source the doc values from, or 2) a custom source of values in the form of DoubleValueSource, LongValueSource, etc. Unfortunately, these xxValueSource implementations don't support multiple values per doc, while indexed doc values do. So if the user has indexed doc values they want to facet over, it works just fine if docs are multi-valued. But if they want to provide a custom source of data, they're restricted to single-value only. We should consider allowing users to provide their own doc values instances in favor of the xxValueSource instances. (I'd propose we deprecate the xxValueSource approach)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10250", "change_description": ": Add support for arbitrary length hierarchical SSDV facets.", "change_title": "Add hierarchical labels to SSDV facets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Hi all, I recently added a new benchmarking task to luceneutil to count facets on a random word chosen from each document which would give us a very high cardinality facet benchmarking compared to the faceting benchmarks we already had. After being merged, mikemccand pointed out some interesting results in the nightly benchmarks where the BrowseRandomLabelSSDVFacets task was much faster than the BrowseRandomLabelTaxoFacets task. I was thinking that using SSDV facets instead of taxonomy facets for our use case at Amazon Product Search could potentially lead to some increases in QPS and decreases in index size, but the issue is we use hierarchical labels, and as I understand it, SSDV faceting only supports a 2 level hierarchy as of today. This leads to my question of why is there a limitation like this on SSDV facets? Is hierarchical labels just a feature that hasn't been implemented in SSDV facets yet, or is there some more complex reason that we can't add hierarchical labels to SSDV facets? Thanks!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10395", "change_description": ": Add support for TotalHitCountCollectorManager, a collector manager\nbased on TotalHitCountCollector that allows users to parallelize counting the\nnumber of hits.", "change_title": "Add support for TotalHitCountCollectorManager", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "With LUCENE-10002 we are looking at replacing usages of IndexSearcher#search(Query, Collector) with the corresponding method that takes a CollectorManager instead of a Collector. As part of this effort we would like to add support for a collector manager that encapsulates TotalHitCountCollector which is commonly used in our tests and possibly by users.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10403", "change_description": ": Add ArrayUtil#grow(T[]).", "change_title": "Add ArrayUtil#grow(T[])", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "ArrayUtil exposes convenience methods for all primitive array types that don't require the user to provide a min size (defaulting to the current array length + 1 as the min size). It would be nice to include the same paradigm for the generically-typed version.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10414", "change_description": ": Add fn:fuzzyTerm interval function to flexible query parser", "change_title": "Add fn:fuzzyTerm interval function to flexible query parser", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Searching for \"fuzzy\" terms within interval expressions is currently impossible. The Intervals class does expose the necessary low-level machinery to make it happen though.  PR: https://github.com/apache/lucene/pull/668/files", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10378", "change_description": ": Implement Weight#count for PointRangeQuery to provide a faster way to calculate\nthe number of matching range docs when each doc has at-most one point and the points are 1-dimensional.", "change_title": "Implement Weight#count on PointRangeQuery", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "When there are no deletions and the field is single-valued (docCount == size) and has a single-dimension, PointRangeQuery could implement Weight#count by only counting matches on the two leaves that cross with the query.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10415", "change_description": ": FunctionScoreQuery and IndexOrDocValuesQuery delegate Weight#count.", "change_title": "FunctionScoreQuery and IndexOrDocValuesQuery should delegate Weight#count", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "We have a number of query wrappers that do not modify the set of matching documents like FunctionScoreQuery and IndexOrDocValuesQuery. These queries should delegate Weight#count.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10382", "change_description": ": Add support for filtering in KnnVectorQuery. This allows for finding the\nnearest k documents that also match a query.", "change_title": "Allow KnnVectorQuery to operate over a subset of liveDocs", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.1", "detail_description": "Currently the KnnVectorQuery selects the top K vectors from all live docs.  This ticket will change the interface to make it possible for the top K vectors to be selected from a subset of the live docs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "New Features", "change_id": "LUCENE-10237", "change_description": ": Add MergeOnFlushMergePolicy to sandbox.", "change_title": "Add merge on flush merge policy to Lucene Sandbox", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Add MergeOnCommitTieredMergePolicy to lucene sandbox. We use it within Amazon product search, which enables 1.  easy to configure merge-on-commit merges 2. ensures no merged segment is accidentally too big a percentage of the total index thus harming effective within-query concurrency and long-pole query latencies", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10313", "change_description": ": use java util logging in Luke. Add dynamic log filtering. Drop\nthe persistent log previously written to ~/.luke.d/luke.log. Configure Java's default\nlogging handlers to persist Luke logs according to your needs.", "change_title": "Remove log4j from dependencies and switch to java logging (in luke)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "This seems to be a simpler solution at this moment https://issues.apache.org/jira/browse/LUCENE-10303 https://issues.apache.org/jira/browse/LUCENE-10308 ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10238", "change_description": ": Upgrade icu4j dependency to 70.1.", "change_title": "Update icu4j to 70.1", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-9820", "change_description": ": Extract BKD tree interface and move intersecting logic to the\nPointValues abstract class.", "change_title": "Separate logic for reading the BKD index from logic to intersecting it.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently the class BKDReader contains all the logic for traversing the KD tree and the logic to read the actual index. This makes difficult to develop new visiting strategies, for example LUCENE-9619, where it is proposed to move Points from a visitor API to a custor-style API. The first step is to isolate the logic the read the index from the logic that visits the the tree. Another benefit of doing this, is that it will help evolving the index, for example moving the current index format to backwards codec without moving the visiting logic.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10262", "change_description": ": Lift up restrictions for navigating PointValues#PointTree\nadded in", "change_title": "Lift up restrictions for navigating PointValues#PointTree", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Following up LUCENE-9820, we currently do not allow some navigation combinations on the PointTree which might make a bit trappy to use it in some cases. For example we should be calling moveToChild after calling moveToParent. I had a second thought and realise it can be very cheap to remove this limitation. We only need a new array of integers that holds the position when reading the node data in each level (very similar to what we are doing in rightNodePositions. Then we only need to check we are in the right position whenever we navigate down the tree.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-9820", "change_description": ": Lift up restrictions for navigating PointValues#PointTree\nadded in", "change_title": "Separate logic for reading the BKD index from logic to intersecting it.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently the class BKDReader contains all the logic for traversing the KD tree and the logic to read the actual index. This makes difficult to develop new visiting strategies, for example LUCENE-9619, where it is proposed to move Points from a visitor API to a custor-style API. The first step is to isolate the logic the read the index from the logic that visits the the tree. Another benefit of doing this, is that it will help evolving the index, for example moving the current index format to backwards codec without moving the visiting logic.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-9538", "change_description": ": Detect polygon self-intersections in the Tessellator.", "change_title": "Tessellator should provide a better error message for self-intersecting shapes", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Self-intersecting shapes cannot be tessellated and currently throw a generic like:    In case of Self-intersecting shapes we can do better and try to give a more useful message by detecting the self-intersection position and provide that information to the user.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10275", "change_description": ": Speed up MultiRangeQuery by using an interval tree.", "change_title": "Use interval tree in MultiRangeQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently MultiRangeQuery uses a naive approach that is iterating over a list of clauses to compute relationships between the list of ranges and the kd tree. This clearly does not scale for a large number of clauses. This problem has already been solved in geo (see https://github.com/apache/lucene/blob/main/lucene/core/src/java/org/apache/lucene/geo/ComponentTree.java). I think it should be easy to make a version of the interval tree for the n-dimensional case. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10229", "change_description": ": Unify behaviour of match offsets for interval queries on fields\nwith or without offsets enabled.", "change_title": "Match offsets should be consistent for fields with positions and fields with offsets", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.2", "detail_description": "This is a follow-up of LUCENE-10223 in which it was discovered that fields with offsets don't highlight some more complex interval queries properly.  Alan says: It's because it returns the position of the inner match, but the offsets of the outer.  And so if you're re-analyzing and retrieving offsets by looking at the positions, you get the 'right' thing.  It's not obvious to me what the correct response is here, but thinking about it the current behaviour is kind of the worst of both worlds, and perhaps we should change it so that you get offsets of the inner match as standard, and then the outer match is returned as part of the sub matches. Intervals are nicely separated into \"basic intervals\" and \"filters\" which restrict some other source of intervals, here is the original documentation: https://github.com/apache/lucene/blob/main/lucene/queries/src/java/org/apache/lucene/queries/intervals/package-info.java#L29-L50 My experience from an extended period of using interval queries in a frontend where they're highlighted is that filters are restrictions that should not be highlighted - it's the source intervals that people care about. Filters are what you remove or where you give proper context to source intervals. The test code contributed in LUCENE-10223 contains numerous query-highlight examples (on fields with positions) where this intuition is demonstrated on all kinds of interval functions: https://github.com/apache/lucene/blob/main/lucene/highlighter/src/test/org/apache/lucene/search/matchhighlight/TestMatchHighlighter.java#L335-L542 This issue is about making the internals work consistently for fields with positions and fields with offsets.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10054", "change_description": "Make HnswGraph hierarchical", "change_title": "Handle hierarchy in HNSW graph", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently HNSW graph is represented as a single layer graph.   We would like to extend it to handle hierarchy as per discussion.  TODO tasks:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Improvements", "change_id": "LUCENE-10371", "change_description": ": Make IndexRearranger able to arrange segment in a determined order.", "change_title": "Make IndexRearranger able to arrange segment into a determined order", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Previously when I tried to make change to luceneutil to let it use IndexRearranger for a faster deterministic index construction, I found that even each segment contains the same documents set, the order of segments will impact the estimated hit number (using BMW): https://markmail.org/message/zl6zsqvbg7nwfq6w At that time the discussion tend to tolerant the small hit count difference to resolve the issue, after some time when I discuss this issue again with mikemccand , we thought it might also be a good idea to just add ability of rearranging the segments order to IndexRearranger, so that we can ensure each time the rearranged index is truly the same.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10329", "change_description": ": Use computed block mask for DirectMonotonicReader#get.", "change_title": "Use Computed Mask For DirectMonotonicReader#get", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "I saw DirectMonotonicReader#get was the hot method during when running luceneutil test, So using a computed mask for DirectMonotonicReader#get instead of computing it for every call may make a bit sense", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10280", "change_description": ": Optimize BKD leaves' doc IDs codec when they are continuous.", "change_title": "Store BKD blocks with continuous ids more efficiently", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "For scenes that index is sorted on the field, it could be a common situation that blocks have continuous ids. Maywe can handle this situation more efficiently (only write the first id of this block). And we can just check to see if ids are continuous, the check should be very fast", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10233", "change_description": ": Store BKD leaves' doc IDs as bitset in some cases (typically for low cardinality fields\n or sorted indices) to speed up addAll.", "change_title": "Store docIds as bitset when doc IDs are strictly sorted and dense", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "In low cardinality points cases, id blocks will usually store doc ids that have the same point value, and intersect will get into addAll logic. If we store ids as bitset, and give the IntersectVisitor bulk visiting ability, we can speed up addAll because we can just execute the 'or' logic between the result and the block ids. Optimization will be triggered when the following conditions are met at the same time: I mocked a field that has 10,000,000 docs per value and search it with a 1 term PointInSetQuery, the build scorer time decreased from 151ms to 5ms.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10225", "change_description": ": Improve IntroSelector with 3-ways partitioning.", "change_title": "Improve IntroSelector with 3-way partitioning", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "The same way we improved IntroSorter, we can improve IntroSelector with Bentley-McIlroy 3-way partitioning. The gain in performance is roughly the same. With this new approach, we always use medians-of-medians (Tukey's Ninther), so there is no real gain to fallback to a slower medians-of-medians technique as an introspective protection (like the existing implementation does). Instead we can simply shuffle the sub-range if we exceed the recursive max depth (this does not change the speed as this intro-protective mechanism almost never happens - maybe only in adversarial cases).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10321", "change_description": ": Tweak MultiRangeQuery interval tree creation to skip \"pulling up\" mins.", "change_title": "MultiRangeQuery doesn't need to \"pull up\" mins when building an interval tree", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "When creating an interval tree for the search intervals, the min values are being \"pulled up\" to keep internal node bounding boxes consistent. I don't believe this should be necessary though given the intersection/contains algorithms. I stumbled across this when comparing the impl to ComponentTree, which is serving a similar role for specialized 2D cases.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10252", "change_description": ": ValueSource.asDoubleValues and asLongValues should not compute the score unless\nasked to -- typically never.  This fixes a performance regression since 7.3", "change_title": "ValueSource.asDoubleValues shouldn't fetch score", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "The ValueSource.asDoubleValuesSource() method bridges the old API to the new one.  It's rather important because boosting a query no longer has an old API; in its place is using this method and passing to FunctionScoreQuery.boostByValue.  Unfortunately, asDoubleValuesSource will fetch/compute the score for the document in order to expose it in a Scorable on the \"scorer\" key of the context Map.  AFAICT nothing in Lucene or Solr actually uses this.  If it should be kept, the Scorable's score() method could fetch it at that time (e.g. on-demand).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-8099", "change_description": ": ValueSource.asDoubleValues and asLongValues should not compute the score unless\nasked to -- typically never.  This fixes a performance regression since 7.3", "change_title": "Deprecate CustomScoreQuery, BoostedQuery and BoostingQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.3", "detail_description": "After LUCENE-7998, these three queries can all be replaced by a FunctionScoreQuery.  Using lucene-expressions makes them much easier to use as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12902659/LUCENE-8099.patch", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10346", "change_description": ": Optimize facet counting for single-valued TaxonomyFacetCounts.", "change_title": "Specially treat SingletonSortedNumericDocValues in FastTaxonomyFacetCounts#countAll()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "CPU profile often tells SingletonSortedNumericDocValues#nextDoc() is using a high percentage of CPU when running luceneutil, but the nextDoc() of dense cases should be rather simple. So I suspect that it is too many layers of abstraction (and wrap) that cause the stress of JVM. Unwraping it to NumericDocvalues shows around 30% speed up. Baseline Candidate", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10356", "change_description": ": Further optimize facet counting for single-valued TaxonomyFacetCounts.", "change_title": "Special-case singleton doc values for general taxonomy facet counting", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Inspired by https://github.com/apache/lucene/pull/574, we should also special-case singleton dvs in the general count path (#573 specialized it for countAll).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10379", "change_description": ": Count directly into the dense values array in FastTaxonomyFacetCounts#countAll.", "change_title": "Count directly into the values array in FastTaxonomyFacetCounts#countAl", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "This breaks out one of the two optimizations proposed in LUCENE-10350. As part of trying to track down the regressions caused by LUCENE-10350, I propose we push just this one optimization independently (see LUCENE-10374).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10375", "change_description": ": Speed up HNSW vectors merge by first writing combined vector\ndata to a file.", "change_title": "Speed up HNSW merge by writing combined vector data", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "When merging segments together, the HNSW writer creates a VectorValues instance that gives a merged view of all the segments' VectorValues. This merged instance is used when constructing the new HNSW graph. Graph building needs random access, and the merged VectorValues support this by mapping from merged ordinals -> segments and segment ordinals. This mapping seems to add overhead. The nightly indexing benchmarks sometimes show substantial time in Arrays.binarySearch (used to map an ordinal to a segment): https://blunders.io/jfr-demo/indexing-1kb-vectors-2022.01.09.18.03.19/top_down_cpu_samples. Instead of using a merged VectorValues to create the graph, maybe we could first write all the segment vectors to a file, and use that file to build the graph.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10388", "change_description": ": Remove MultiLevelSkipListReader#SkipBuffer to make JVM less confused.", "change_title": "Remove MultiLevelSkipListReader#SkipBuffer", "detail_type": "Improvement", "detail_affect_versions": "9.1", "detail_fix_versions": "9.1", "detail_description": "Previous talk can be found in https://github.com/apache/lucene/pull/592", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10367", "change_description": ": Optimize CoveringQuery for the case when the minimum number of\nmatching clauses is a constant.", "change_title": "Use WANDScorer in CoveringQuery Can accelerate scorer time", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "When using CoveringQuery In Elasticsearch with terms_set query, it takes too much time in CoveringScore and major cost in matain the DisiPriorityQueue: subScorers. But when minimumNumberMatch is ConstantLongValuesSource, we can use WANDScorer to optimize it.  i do a mini benchmark with 1m docs, which code in LUCENE-10367.patch TestCoveringQuery.java  testRandomBench() it shows: TEST: WAND elapsed 67ms TEST: NOWAND elapsed 163ms My testing environment is macBook with Intel Core i7 16GMem.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13038816/LUCENE-10367.patch", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10412", "change_description": ": More `Query#rewrite` optimizations for MatchNoDocsQuery.", "change_title": "Improve handling of MatchNoDocsQuery in rewrite rules", "detail_type": "Wish", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Having MatchNoDocsQuery in your query tree usually doesn't make the query slower, but by recognizing it in rewrite rules, we could perform rewrites which would then sometimes unlock other rewrite rules. For instance if you have a boolean query with 2 should clauses where one is a MatchAllDocsQuery and the other one is a MatchNoDocsQuery, we would naively run it as a disjunction today, while we could rewrite it to a MatchAllDocsQuery and leverage its specialized bulk scorer.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10408", "change_description": "Better encoding of doc Ids in vectors.", "change_title": "Better dense encoding of doc Ids in Lucene91HnswVectorsFormat", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently we write doc Ids of all documents that have vectors as is.  We should improve their encoding either using delta encoding or bitset.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10424", "change_description": ",", "change_title": "Optimize the \"everything matches\" case for count query in PointRangeQuery", "detail_type": "Improvement", "detail_affect_versions": "9.1", "detail_fix_versions": "9.1", "detail_description": "In Implement of Weight#count in PointRangeQuery, Whether additional consideration is needed that when PointValues#getDocCount() == IndexReader#maxDoc() and the range's lower bound is less that the field's min value and the range's upper bound is greater than the field's max value, then return reader.maxDoc() directly?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10439", "change_description": ",", "change_title": "Support multi-valued and multiple dimensions for count query in PointRangeQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Follow-up of LUCENE-10424, it also works with fields that have multiple dimensions and/or that are multi-valued.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10084", "change_description": ",", "change_title": "Rewrite DocValuesFieldExistsQuery to a MatchAllDocsQuery when docCount == maxDoc", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Now that we require all documents to use the same features (LUCENE-9334) we could rewrite DocValuesFieldExistsQuery to a MatchAllDocsQuery whenever terms or points have a docCount that is equal to maxDoc.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10435", "change_description": ",", "change_title": "Break loop early while checking whether DocValuesFieldExistsQuery can be rewrite to MatchAllDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "In the implementation of Query#rewrite in DocValuesFieldExistsQuery, when one Segment can't match the condition occurs, maybe we should break loop directly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10442", "change_description": ": When indexQuery or/and dvQuery be a MatchAllDocsQuery\nthen IndexOrDocValuesQuery should be rewrite to MatchAllDocsQuery.", "change_title": "When indexQuery or/and dvQuery be a MatchAllDocsQuery  then IndexOrDocValuesQuery should be rewrite to MatchAllDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "IndexOrDocValuesQuery is typically useful for range queries, When indexQuery was rewrite to MatchAllDocsQuery and if IndexOrDocValuesQuery not be a lead iterator , it most likely that dvQuery will supply the Scorer not indexQuery.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10450", "change_description": ": IndexSortSortedNumericDocValuesRangeQuery could be rewrite to MatchAllDocsQuery.", "change_title": "IndexSortSortedNumericDocValuesRangeQuery could be rewrite to MatchAllDocsQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "I see IndexOrDocValuesQuery as a fallbackQuery of IndexSortSortedNumericDocValuesRangeQuery in Elasticsearch, so After LUCENE-10442, we should rewrite IndexSortSortedNumericDocValuesRangeQuery to MatchAllDocsQuery if fallbackQuery could be as MatchAllDocsQuery?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10453", "change_description": ": Indexing and search speedup with KNN vectors when using\neuclidean distance.", "change_title": "Speed up VectorUtil#squareDistance", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "VectorUtil#squareDistance is used in conjunction with VectorSimilarityFunction#EUCLIDEAN. It didn't get as much love as dot products (LUCENE-9837) yet there seems to be room for improvement. I wrote a quick JMH benchmark to run some comparisons: https://github.com/jpountz/vector-similarity-benchmarks. While it's not as fast as using the vector API (which makes squareDistance computations more than 2x faster), we can get a ~25% speedup by unrolling the loop in a similar way to what dot product does.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Optimizations", "change_id": "LUCENE-10455", "change_description": ": IndexSortSortedNumericDocValuesRangeQuery now implements the scorerSupplier API.", "change_title": "IndexSortSortedNumericDocValuesRangeQuery should implement Weight#scorerSupplier(LeafReaderContext)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "IndexOrDocValuesQuery was used as a fallbackQuery of IndexSortSortedNumericDocValuesRangeQuery in Elasticsearch, but When IndexSortSortedNumericDocValuesRangeQuery can't take advantage of index sort, the fallbackQuery(IndexOrDocValuesQuery)  always only supply Scorer by indexQuery, beacuse IndexSortSortedNumericDocValuesRangeQuery did not implement Weight#scorerSupplier(LeafReaderContext).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Changes in runtime behavior", "change_id": "LUCENE-10291", "change_description": ": Lucene now only writes files for terms and postings if at least\none field is indexed with postings.", "change_title": "Only read/write postings when there is at least one indexed field", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Unlike points, norms, term vectors or doc values which only get written to the directory when at least one of the fields uses the data structure, postings always get written to the directory. While this isn't hurting much, it can be surprising at times, e.g. if you index with SimpleText you will have a file for postings even though none of the fields indexes postings. This inconsistency is hidden with the default codec due to the fact that it uses PerFieldPostingsFormat, which only delegates to any of the per-field codecs if any of the fields is actually indexed, so you don't actually get a file if none of the fields is indexed. We noticed this behavior by creating a codec that throws UnsupportedOperationException for postings since it's not expected to have postings, and it always fails writing or reading data. While it's easy to work around this issue on top of Lucene by using a dummy postings format, it would be better to fix Lucene to handle postings consistently with other data structures?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Changes in runtime behavior", "change_id": "LUCENE-10311", "change_description": ": FixedBitSet#approximateCardinality now trades accuracy for\nspeed instead of delegating to FixedBitSet#cardinality.", "change_title": "Should DocIdSetBuilder have different implementations for point and terms?", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "DocIdSetBuilder has two API implementations, one for terms queries and one for point values queries. In each cases they are used in totally different way. For terms the API looks like:   For Point Values it looks like:    This is becoming trappy for new developments in the PointValue API. 1) When we call #grow() from the PointValues API, we are not telling the builder how many docs we are going to add (as we don't really know it) but the number of points we are about to visit. This number can be bigger than Integer.MAX_VALUE. Until now, we get around this issue by making sure we don't call this API when we need to add more than Integer.MAX_VALUE points. In that case we will navigate the tree down until the number of points is reduced and they can fit in an int. This has work well until now because we are calling grow from inside the BKD reader, and the BKD writer/reader makes sure than the number of points in a leaf can fit in an int. In LUCENE-, we re moving into a cursor-like API which does not enforce that the number of points on a leaf needs to fit in an int.  This causes friction and inconsistency in the API.  2) This a secondary issue that I found when thinking in this issue. In Lucene- we added the possibility to add a `DocIdSetIterator` from the PointValues API.  Therefore there are two ways to add those kind of objects to a DocIdSetBuilder which can end up in different results:   I wonder if we need to rethink this API, should we have different implementation for Terms and Point values?    ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10316", "change_description": ": fix TestLRUQueryCache.testCachingAccountableQuery failure.", "change_title": "fix TestLRUQueryCache.testCachingAccountableQuery failure", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "I saw this build failure: https://jenkins.thetaphi.de/job/Lucene-9.x-Linux/348/ with following stack trace It does not reproduce on my laptop on current main branch, but since the test is comparing an estimation with a 10% slack, it can fail for sure sometime.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10279", "change_description": ": Fix equals in MultiRangeQuery.", "change_title": "Fix equal and hashCode in MultiRange query", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "I notice that the implementation of equals is using rangeClauses.equals(other.rangeClauses) bit a RangeClause is not implementing equals so it will likely always be false.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10349", "change_description": ": Fix all analyzers to behave according to their documentation:\ngetDefaultStopSet() methods now return unmodifiable CharArraySets.", "change_title": "Cleanup WordListLoader to use try-with-resources and make the default stop words unmodifiable", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "The WordlistLoader can be refactored to use try-with-resources. In addition I figured out that the default stopword files in the analyszers module are documented to be unmodifiable, but in fact the wrapping around the CharArraySet to make them unmodifiable is missing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10352", "change_description": ": Add missing service provider entries: KoreanNumberFilterFactory,\nDaitchMokotoffSoundexFilterFactory", "change_title": "Convert TestAllAnalyzersHaveFactories and TestRandomChains to a global integration test and discover classes to check from module system", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Currently TestAllAnalyzersHaveFactories and TestRandomChains only work on the analysis-commons module, but e.g. we do not do a random chain with kuromoji and ICU. Also both tests rely on some hacky classpath-inspection and the tests fail if ran on a JAR file. This issue tracks progress I am currently doing to refactor this: While doing this I disovered some bad things:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10352", "change_description": ": Fixed ctor argument checks: JapaneseKatakanaStemFilter,\nDoubleMetaphoneFilter", "change_title": "Convert TestAllAnalyzersHaveFactories and TestRandomChains to a global integration test and discover classes to check from module system", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Currently TestAllAnalyzersHaveFactories and TestRandomChains only work on the analysis-commons module, but e.g. we do not do a random chain with kuromoji and ICU. Also both tests rely on some hacky classpath-inspection and the tests fail if ran on a JAR file. This issue tracks progress I am currently doing to refactor this: While doing this I disovered some bad things:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10236", "change_description": ": Stop duplicating norms when scoring in CombinedFieldQuery.", "change_title": "CombinedFieldsQuery to use fieldAndWeights.values() when constructing MultiNormsLeafSimScorer for scoring", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "This is a spin-off issue from discussion in https://github.com/apache/lucene/pull/418#issuecomment-967790816, for a quick fix in CombinedFieldsQuery scoring. Currently CombinedFieldsQuery would use a constructed fields object to create a MultiNormsLeafSimScorer for scoring, but the fields object may contain duplicated field-weight pairs as it is built from looping over fieldTerms, resulting into duplicated norms being added during scoring calculation in MultiNormsLeafSimScorer. E.g. for CombinedFieldsQuery with two fields and two values matching a particular doc: I would imagine the scoring to be based on the following: but the current logic would use the following for scoring:  In addition, this differs from how MultiNormsLeafSimScorer is constructed from CombinedFieldsQuery explain function, which uses fieldAndWeights.values() and does not contain duplicated field-weight pairs.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10353", "change_description": ": Add random null injection to TestRandomChains.", "change_title": "Add null injection to analyzer integration tests (e.g. TestRandomChains)", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "These tests inject random parameter values (from argumentProviders). Some generated values may be illegal and IllegalArgumentException is \"allowed\" if the constructor returns it. None of the values should cause failures at runtime. But for object types, we never inject null values (unless the argumentProvider were to do it itself). We should do this some low % of the time, and \"allow\" ctors to return NPE too. I see bugs in some of the analyzers where they are just a missing null check in the constructor. It is important to fail on invalid configuration up-front in the ctor, rather than failing e.g. at index time.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10377", "change_description": ": CheckIndex could incorrectly throw an error when checking index sorts\ndefined on older indexes.", "change_title": "Replace 'sortPos' parameter in SortField.getComparator()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "SortField.getComparator() takes two parameters: the number of hits we are collecting, so that the comparator can size its internal priority queue; and the position of this sortfield in the overall sort.  This second parameter is only actually used to determine whether or not the SortField should enable various skipping operations, building comparative iterators etc.  However, it's not clear from the API that this is why it is passed, and as a consequence all sorts of consumers of this API faithfully pass through their sort positions even when they aren't actually using skipping at all.  In particular, CheckIndex does this when checking index sorts, and this in fact leads to a bug, where it will attempt to build skipping structures over fields from 8x-created indexes that did not have the same type constraints that 9x indexes do and incorrectly throw an error. I think we should replace this second parameter with a simple boolean, `enableSkipping`, that the TopHits collectors can pass as `true` for the first sortfield.  Other APIs that use sorts but not skipping, like CheckIndex or the grouping collectors, can always pass `false` so we don't waste time building skipping structures that never get used.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9952", "change_description": ": Address inaccurate dim counts for SSDV faceting in cases where a dim is configured\nas multi-valued.", "change_title": "FacetResult#value can be inaccurate in SortedSetDocValueFacetCounts", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.1", "detail_description": "As described in a dev@ list thread, the value of FacetResult#value can be incorrect in SSDV faceting when docs are multi-valued (affects both SortedSetDocValueFacetCounts and ConcurrentSortedSetDocValueFacetCounts). If a doc has multiple values in the same dimension, it will be counted multiple times when populating the counts of FacetResult#value. We should either provide an accurate count, or provide -1 if we don't have an accurate count (like we do in taxonomy faceting). I think this change will be a bit involved though as SSDV facet counting likely needs to be made aware of FacetConfig. NOTE: I've updated this description to describe only the SSDV case after spinning off LUCENE-9953 to track the LongValueFacetCounts case.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10401", "change_description": ": Fix lookups on empty doc-value terms dictionaries to no longer\nthrow an ArrayIndexOutOfBoundsException.", "change_title": "Seeking on empty doc-value terms dictionaries fails with AIOOBE", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "This got reported at SOLR-15856.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10402", "change_description": ": Prefix intervals should declare their automaton as binary, otherwise prefixes\ncontaining multibyte characters will not correctly match.", "change_title": "Intervals.prefix() handles multicharacter unicode incorrectly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Intervals.prefix() takes its input and builds a CompiledAutomaton, before passing this on to a new MultiTermAutomatonIntervalsSource.  However, while PrefixQuery.toAutomaton() builds a binary automaton, we don't tell CompiledAutomaton this and so it (incorrectly) does some unicode conversion on its input.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10407", "change_description": ": Containing intervals could sometimes yield incorrect matches when wrapped\nin a disjunction.", "change_title": "ContainingIntervalIterator can incorrectly report positions after exhaustion", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "ContainedByIntervalIterator and OverlappingIntervalIterator set their 'is the filter interval exhausted' flag to `false` once it has returned NO_MORE_POSITIONS on a document, so that subsequent calls to `startPosition()` will also return NO_MORE_POSITIONS.  ContainingIntervalIterator omits to do this, and so it can incorrectly report matches, for example when used in a disjunction. cc dweiss", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10405", "change_description": ": When using the MemoryIndex, binary and Sorted doc values are stored\n as BytesRef instead of BytesRefHash so they don't have a limit on size.", "change_title": "MemoryIndex: Binary and Sorted doc values should not be added to a BytesRefHash", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently when we add a binary or sorted doc value in the MemoryIndex, it will get stored in a BytesRefHash. This is not necessary as we only expect one doc value per document so they don't need to be deduped. In addition  a BytesRefHash has a limit on term size (~32kb) which those doc values don't have in normal codecs. Therefore as it s a different behaviour it can be considered a bug. We should store those doc values as a plain byte[] (or BytesRef).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10428", "change_description": ": Queries with a misbehaving score function may no longer cause\ninfinite loops in their parent BooleanQuery.", "change_title": "getMinCompetitiveScore method in MaxScoreSumPropagator fails to converge leading to busy threads in infinite loop", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Customers complained about high CPU for Elasticsearch cluster in production. We noticed that few search requests were stuck for long time Flame graphs indicated that CPU time is mostly going into getMinCompetitiveScore method in MaxScoreSumPropagator. After doing some live JVM debugging found that org.apache.lucene.search.MaxScoreSumPropagator.scoreSumUpperBound method had around 4 million invocations every second Figured out the values of some parameters from live debugging: Example code snippet:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10431", "change_description": ": MultiTermQuery no longer includes its rewrite method in its hashcode\ncalculation, as this could cause problems with wrapper queries like BooleanQuery which\nexpect their child queries hashcodes to be stable.", "change_title": "AssertionError in BooleanQuery.hashCode()", "detail_type": "Bug", "detail_affect_versions": "8.11.1", "detail_fix_versions": "None", "detail_description": "Hello devs, the constructor of BooleanQuery can under some circumstances trigger a hash code computation before \"clauseSets\" is fully filled. Since BooleanClause is using its query field for the hash code too, it can happen that the \"wrong\" hash code is stored, since adding the clause to the set triggers its hashCode(). If assertions are enabled the check in BooleanQuery, which recomputes the hash code, will notice it and throw an error. exception: I noticed this while trying to upgrade the NetBeans maven indexer modules from lucene 5.x to 8.x https://github.com/apache/netbeans/pull/3558", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Bug Fixes", "change_id": "LUCENE-10469", "change_description": ": Fix ScoreMode propagation by ConstantScoreQuery.", "change_title": "ConstantScoreQuery doesn't propagate its score mode correctly", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "We just noticed a performance bug on Elasticsearch that if you issue a search request sorted by field and the query is a MatchAllDocsQuery then everything works as expected. But if you change the query to be a MatchAllDocsQuery within a ConstantScoreQuery then the query suddenly visits all matching documents. This is due to the fact that ConstantScoreQuery always passes COMPLETE_NO_SCORES to the inner weight, and thet the MatchAllDocsQuery's optimized bulk scorer performs a brute-force for-loop over the documents to score unless the score mode is not exhaustive. The fix consists of making ConstantScoreQuery propagate a non-exhaustive score mode to the inner weight when the score mode that is passed to the ConstantScoreQuery is not exhaustive itself.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10273", "change_description": ": Deprecate SpanishMinimalStemFilter in favor of SpanishPluralStemFilter.", "change_title": "Deprecate SpanishMinimalStemFilter in favor of SpanishPluralStemFilter", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "The new SpanishPluralStemFilter is in fact much more minimal. Analysis: https://s.apache.org/spanishplural Original Issue: LUCENE-10248 Discussion on PR: https://github.com/apache/lucene/pull/461 Because the new stemmer does this job better (without side-effects such as normalizing ñ -> n), I think we should deprecate and point to the new one. We can deprecate in 9.1 and remove in 11?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10284", "change_description": ": Upgrade morfologik-stemming to 2.1.8.", "change_title": "Upgrade morfologik-stemming to 2.1.8", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10310", "change_description": ": TestXYDocValuesQueries#doRandomDistanceTest does not produce random circles with radius\nwith '0' value any longer.", "change_title": "CI error in TestXYDocValuesQueries#testRandomDistanceHuge", "detail_type": "Test", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Reproducible error in: ./gradlew :lucene:core:test --tests \"org.apache.lucene.search.TestXYDocValuesQueries.testRandomDistanceHuge\" -Ptests.jvms=4 -Ptests.jvmargs=-XX:TieredStopAtLevel=1 -Ptests.seed=1491381ABA418A11 -Ptests.nightly=true -Ptests.file.encoding=ISO-8859-1  Error is: org.apache.lucene.search.TestXYDocValuesQueries > testRandomDistanceHuge FAILED java.lang.IllegalArgumentException: radius must be bigger than 0, got 0.0 It seems the test can produce circles with radius 0 which is not allowed, I push a fix shortly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10352", "change_description": ": Removed duplicate instances of StringMockResourceLoader and migrated class to\ntest-framework.", "change_title": "Convert TestAllAnalyzersHaveFactories and TestRandomChains to a global integration test and discover classes to check from module system", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Currently TestAllAnalyzersHaveFactories and TestRandomChains only work on the analysis-commons module, but e.g. we do not do a random chain with kuromoji and ICU. Also both tests rely on some hacky classpath-inspection and the tests fail if ran on a JAR file. This issue tracks progress I am currently doing to refactor this: While doing this I disovered some bad things:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10352", "change_description": ": Convert TestAllAnalyzersHaveFactories and TestRandomChains to a global integration test\nand discover classes to check from module system. The test now checks all analyzer modules,\nso it may discover new bugs outside of analysis:common module.", "change_title": "Convert TestAllAnalyzersHaveFactories and TestRandomChains to a global integration test and discover classes to check from module system", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.1,10.0(main)", "detail_description": "Currently TestAllAnalyzersHaveFactories and TestRandomChains only work on the analysis-commons module, but e.g. we do not do a random chain with kuromoji and ICU. Also both tests rely on some hacky classpath-inspection and the tests fail if ran on a JAR file. This issue tracks progress I am currently doing to refactor this: While doing this I disovered some bad things:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10413", "change_description": ": Make Ukrainian default stop words list available as a public getter.", "change_title": "Ukrainian default stopword set should be easily available", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "It is not currently possible to create a UkrainianMorfologikAnalyzer with the default stop word set but a custom stem exclusion set, because the default stop set is held in a private static variable.  We should add a static method to make this default stop set generally available.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.1.0", "change_type": "Other", "change_id": "LUCENE-10437", "change_description": ": Polygon tessellator throws a more informative error message when the provided polygon\ndoes not contain enough no-collinear points.", "change_title": "Improve error message in the Tessellator for polygon with all points collinear", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.1", "detail_description": "Currently the error that is throws only says that it is not possible to tessellate but this check is trivial and we can give better information to the user.", "patch_link": "none", "patch_content": "none"}

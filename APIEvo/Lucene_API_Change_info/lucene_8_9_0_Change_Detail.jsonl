{"library_version": "8.9.0", "change_type": "API Changes", "change_id": "LUCENE-9680", "change_description": ": Removed deprecation warning from IndexWriter#getFieldNames().", "change_title": "Re-add IndexWriter.getFieldNames", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "IndexWriter.getFieldNames was deprecated in LUCENE-8909. It is useful to have this information exposed by IW to cap (or report) when too many fields have been created. getFieldNames was introduced in LUCENE-7659.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9575", "change_description": ": PatternTypingFilter has been added to allow setting a type attribute on tokens based on\na configured set of regular expressions", "change_title": "Add PatternTypingFilter", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "One of the key asks when the Library of Congress was asking me to develop the Advanced Query Parser was to be able to recognize arbitrary patterns that included punctuation such as POW/MIA or 401(k) or C++ etc. Additionally they wanted 401k and 401(k) to match documents with either style reference, and NOT match documents that happen to have isolated 401 or k tokens (i.e. not documents about the http status code) And of course we wanted to give up as little of the text analysis features they were already using. This filter in conjunction with the filters from LUCENE-9572, LUCENE-9574 and one solr specific filter in SOLR-14597 that re-analyzes tokens with an arbitrary analyzer defined for a type in the solr schema, combine to achieve this. This filter has the job of spotting the patterns, and adding the intended synonym as at type to the token (from which minimal punctuation has been removed). It also sets flags on the token which are retained through the analysis chain, and at the very end the type is converted to a synonym and the original token(s) for that type are dropped avoiding the match on 401 (for example) The pattern matching is specified in a file that looks like: That file would match match legal reference patterns such as 401(k), 401k, 501(c)3 and C++ The format is: <flagsInt> <pattern> ::: <replacement> and groups in the pattern are substituted into the replacement so the first line above would create synonyms such as:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9572", "change_description": ": TypeAsSynonymFilter has been enhanced support ignoring some types, and to allow\nthe generated synonyms to copy some or all flags from the original token", "change_title": "Allow TypeAsSynonymFilter to propagate selected flags and Ignore some types", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "(Breaking this off of SOLR-14597 for independent review) TypeAsSynonymFilter converts types attributes to a synonym. In some cases the original token may have already had flags set on it and it may be useful to propagate some or all of those flags to the synonym we are generating. This ticket provides that ability and allows the user to specify a bitmask to specify which flags are retained. Additionally there may be some set of types that should not be converted to synonyms, and this change allows the user to specify a comma separated list of types to ignore (most common case will be to ignore a common default type of 'word' I suspect)", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9574", "change_description": "A token filter to drop tokens that match all specified flags.", "change_title": "Add a token filter to drop tokens based on flags.", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "(Breaking this off of SOLR-14597 for independent review) A filter that tests flags on tokens vs a bitmask and drops tokens that have all specified flags.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9537", "change_description": ":  Added smoothingScore method and default implementation to\nScorable abstract class.  The smoothing score allows scorers to calculate a\nscore for a document where the search term or subquery is not present.  The\nsmoothing score acts like an idf so that documents that do not have terms or\nsubqueries that are more frequent in the index are not penalized as much as\ndocuments that do not have less frequent terms or subqueries and prevents\nscores which are the product or terms or subqueries from going to zero. Added\nthe implementation of the Indri AND and the IndriDirichletSimilarity from the\nacademic Indri search engine:", "change_title": "Add Indri Search Engine Functionality to Lucene", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "Indri (http://lemurproject.org/indri.php) is an academic search engine developed by The University of Massachusetts and Carnegie Mellon University.  The major difference between Lucene and Indri is that Indri will give a document a \"smoothing score\" to a document that does not contain the search term, which has improved the search ranking accuracy in our experiments.  I have created an Indri patch, which adds the search code needed to implement the Indri AND logic as well as Indri's implementation of Dirichlet Smoothing.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13011883/LUCENE-INDRI.patch", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "http://www.lemurproject.org/indri.php.", "change_description": ":  Added smoothingScore method and default implementation to\nScorable abstract class.  The smoothing score allows scorers to calculate a\nscore for a document where the search term or subquery is not present.  The\nsmoothing score acts like an idf so that documents that do not have terms or\nsubqueries that are more frequent in the index are not penalized as much as\ndocuments that do not have less frequent terms or subqueries and prevents\nscores which are the product or terms or subqueries from going to zero. Added\nthe implementation of the Indri AND and the IndriDirichletSimilarity from the\nacademic Indri search engine:", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9694", "change_description": ": New tool for creating a deterministic index to enable benchmarking changes\non a consistent multi-segment index even when they require re-indexing.", "change_title": "New tool for creating a deterministic index", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.9", "detail_description": "Lucene's index is segmented, and sometimes number of segments and documents arrangement greatly impact performance. Given a stable index sort, our team create a tool that records document arrangement (called index map) of an index and rearrange another index (consists of same documents) into the same structure (segment num, and documents included in each segment). This tool could be also used in lucene benchmarks for a faster deterministic index construction (if I understand correctly lucene benchmark is using a single thread manner to achieve this).  We've already had some discussion in email https://markmail.org/message/lbtdntclpnocmfuf And I've implemented the first method, using IndexWriter.addIndexes and a customized FilteredCodecReader to achieve the goal. The index construction time is about 25min and time executing this tool is about 10min.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9385", "change_description": ": Add FacetsConfig option to control which drill-down\nterms are indexed for a FacetLabel", "change_title": "Skip indexing facet drill down terms", "detail_type": "New Feature", "detail_affect_versions": "8.5.2", "detail_fix_versions": "9.0,8.9", "detail_description": "FacetsConfig creates index terms from the Facet dimension and path automatically for the purpose of supporting drill-down queries. An application that does not need drill-down ends up paying the index cost of the extra terms. Ideally an option to skip indexing these drill down terms should be exposed to the application.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9507", "change_description": ": Custom order for leaves in IndexReader and IndexWriter", "change_title": "Custom order for leaves in DirectoryReader, IndexWriter and searcher", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.9", "detail_description": "Now that we're able to skip documents efficiently when sorting by a numeric field, I was wondering if we could optimize sorted queries further by also sorting the leaf readers based on the primary sort. For time-based indices in Elasticsearch, we've implemented an optimization that does that at query time. If the query is sorted by a numeric docvalue field, prior to search, we sort the leaves according to the query sort. When sorting by timestamp this small optimization can have a big impact since early termination can be reached much faster if the sort values in the segments don't overlap too much. Applying this optimization at query time is challenging , it has the benefit to work on any numeric field sort and order but it requires to use a multi-reader that will reorganize the segments. It can also be deceptive that after a force merge to 1 segment sorted queries may be slower since there is nothing to sort anymore. So, another option that I look at is to add the ability to provide a leaf order directly in the IndexWriter and DirectoryReader. That could be similar to an index sort or even complementary to it since sorting segments based on the index sort could also help at query time. For time-based indices that cannot afford index sorting but have lots of sorted queries on timestamp, forcing the order of segments could speed up sorted queries significantly. The advantage of forcing a single leaf sort in the writer/reader is that we can also use it to influence the merges by putting the segments with the highest value first. That would help with the case of indices that are merged to a single segment but would like to keep the sorted queries fast but also for the multi-segments case since big segments would have more chance to have highest values first too.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "New Features", "change_id": "LUCENE-9950", "change_description": ": New facet counting implementation for general string doc value fields\n(SortedSetDocValues / SortedDocValues) not created through FacetsConfig", "change_title": "Support both single- and multi-value string fields in facet counting (non-taxonomy based approaches)", "detail_type": "Improvement", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.9", "detail_description": "Users wanting to facet count string-based fields using a non-taxonomy-based approach can use SortedSetDocValueFacetCounts, which accumulates facet counts based on a SortedSetDocValues field. This requires the stored doc values to be multi-valued (i.e., SORTED_SET), and doesn't work on single-valued fields (i.e., SORTED). In contrast, if a user wants to facet count on a stored numeric field, they can use LongValueFacetCounts, which supports both single- and multi-valued fields (and in LUCENE-9948, we now auto-detect instead of asking the user to specify). Let's update SortedSetDocValueFacetCounts to also support, and automatically detect single- and multi-value fields. Note that this is a spin-off issue from LUCENE-9946, where rcmuir points out that this can essentially be a one-line change, but we may want to do some class renaming at the same time. Also note that we should do this in ConcurrentSortedSetDocValuesFacetCounts while we're at it.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Improvements", "change_id": "LUCENE-9687", "change_description": ": Hunspell support improvements: add API for spell-checking and suggestions, support compound words,\nfix various behavior differences between Java and C++ implementations, improve performance", "change_title": "Hunspell support improvements", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.9", "detail_description": "I'd like Lucene's Hunspell support to be on a par with the native C++ Hunspell for spellchecking and suggestions, at least for some languages. So I propose to:", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Improvements", "change_id": "LUCENE-9725", "change_description": ": BM25FQuery was extended to handle similarities beyond BM25Similarity. It\nwas renamed to CombinedFieldQuery to reflect its more general scope.", "change_title": "Allow BM25FQuery to use other similarities", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "From a high level, BM25FQuery works as follows: The steps are (1) compute statistics that represent the combined field content, and (2) pass these to a similarity function. There is nothing really specific to BM25Similarity in this approach. In step 2, we could use another similarity, for example BooleanSimilarity or those based on language models like LMDirichletSimilarity. The main restriction is that norms have to be additive (the norm of the combined field must be the sum of the field norms). Maybe we could unhardcode BM25Similarity in BM25FQuery and instead use the one configured on IndexSearcher. We could think of this as providing a sensible default approach to cross-field scoring for many similarities. It's an incremental step towards LUCENE-8711, which would give similarities more fine-grained control over how stats/ scores are combined across fields.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Improvements", "change_id": "LUCENE-9663", "change_description": ": Add compression to terms dict from SortedSet/Sorted DocValues.", "change_title": "Adding compression to terms dict from SortedSet/Sorted DocValues", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "Elasticsearch keyword field uses SortedSet DocValues. In our applications, “keyword” is the most frequently used field type. LUCENE-7081 has done prefix-compression for docvalues terms dict. We can do better by replacing prefix-compression with LZ4. In one of our application, the dvd files were ~41% smaller with this change(from 1.95 GB to 1.15 GB).  I've done simple tests based on the real application data, comparing the write/merge time cost, and the on-disk *.dvd file size(after merge into 1 segment). This feature is only for the high-cardinality fields.   I'm doing the benchmark test based on luceneutil. Will attach the report and patch after the test.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Improvements", "change_id": "LUCENE-9877", "change_description": ": Reduce index size by increasing allowable exceptions in PForUtil from\n3 to 7.", "change_title": "Explore increasing the allowable exceptions in PForUtil", "detail_type": "Task", "detail_affect_versions": "9.0", "detail_fix_versions": "8.9", "detail_description": "Piggybacking a little off of the investigation I was doing over in LUCENE-9850 I thought it might also be worth-while exploring the impact of increasing the number of allowable exceptions in PForUtil. The aim of this investigation is to see if we could reduce index size by allowing for more exceptions without significant negative impact to performance. PForUtil currently allows for up to 3 exceptions, and it only uses 3 bits to encode the number of exceptions (using the remaining 3 bits of the byte used to also encode the number of bits-per-value, which requires 5 bits). Each exception used is encoded with a two full bytes, using a maximum of 6 bytes per block. It seems to me like 7 might be a more ideal number of exceptions if index size is the driving motivation. My thought process is that, in the worst-case, 7 exceptions would be used to save only a single bit-per-value in the corresponding block. With 128 entries per block, this would save 16 bytes. So with 14 bytes used to encode the exception values (7 x 2 bytes per exception), we would save a two bytes in total (just slightly better than breaking even). If we need fewer than the 7 exceptions, or if we're able to save more than 1 bit-per-value, it's all additional savings. I suppose the question is what kind of performance hit we might observe due to decoding more exceptions. Also note that 7 exceptions is the max we can encode with the 3 bits we currently have available for the number of exceptions. So moving to 8 exceptions would not only take 16 bytes to encode the exceptions (if using all of them), but we'd need one more byte per block to encode the exception count. So in the worst case of using all 8 exceptions to save 1 bit per value, we'd actually be worse off. I'll post some results here for discussion or at least for public record of my work for future reference.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Improvements", "change_id": "LUCENE-9935", "change_description": ": Enable bulk merge for stored fields with index sort.", "change_title": "Bulk merges for stored fields when index sorting is enabled", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.10", "detail_description": "Today stored fields disable bulk merges entirely when index sorting is enabled. However when sorting by low-cardinality fields or when the index sort is correlated with the order in which documents get indexed, we could likely still have efficient bulk merges. For instance, if you are merging two segments that are sorted on a field that can only take 2 values, one could bulk merge the first half of the first segment, then the first half of the second segment, then the second half of the first segment, and finally the second half of the second segment.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Optimizations", "change_id": "LUCENE-9932", "change_description": ": Performance improvement for BKD index building", "change_title": "Performance improvement for BKD index building", "detail_type": "Improvement", "detail_affect_versions": "8.8.2", "detail_fix_versions": "8.9", "detail_description": "In BKD index building, the input bytes must be sorted before calling BKD writer related API. The sorting method leverages MSB Radix Sort algorithm, and the comparing method takes both the bytes itself and the DocId, but in real cases, DocIds are usually monotonically increasing. This could yield one possible performance enhancer. I found this enhancement when I dig into one performance issue in our system. Then I research on the possible solution. DocId is usually increased by one when building index in a thread-safe way, by assuming such condition, the comparing method can eliminate the unnecessary comparing input - DocId, only leave the bytes itself to compare. In order to do so, MSB radix sorting and its fallback sorting method must be stable, so that when elements are the same, the sorting method maintains its original order when added, which makes DocId still monotonically increasing. To make MSB Radix Sort stable, it needs a trivial update; to make fallback sort table, use merge sort instead of quick sort. Meanwhile, there should introduce a switch which is able to turn the stable option on or off. To validate how much performance could be gained. I make a benchmark taking down only the time elapsed in MutablePointsReaderUtils.sort stage. Test environment:   MacBook Pro (Retina, 15-inch, Mid 2015), 2.2 GHz Intel Core i7, 16 GB 1600 MHz DDR3 Java version:  java version \"1.8.0_161\"  Java(TM) SE Runtime Environment (build 1.8.0_161-b12)  Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) Testcase:  bytesPerDim = [1, 2, 3, 4, 8, 16, 32]  dim = 1  doc num = 2,000,000  warm up 5 time, run 10 times to calculate average time used. Result:   Result shows that, by disabling sort DocId, sorting runs 1.73x to 37x faster when there are many duplicate bytes (bytesPerDim = 1 or 2 or 3). When data cardinality is high (bytesPerDim >= 4, test cases will generate random bytes which are more scatter, not likely to be duplicate), the performance does not go backward, still a little better. In conclusion, in the end to end process for building BKD index, which relies on BKDWriter for some data types, performance could be better by ignoring DocId if they are already monotonically increasing.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Optimizations", "change_id": "LUCENE-9827", "change_description": ": Speed up merging of stored fields and term vectors for smaller segments.", "change_title": "Small segments are slower to merge due to stored fields since 8.7", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.9", "detail_description": "dm and dimitrisli looked into an interesting case where indexing slowed down after upgrading to 8.7. After digging we identified that this was due to the merging of stored fields, which had become slower on average. This is due to changes to stored fields, which now have top-level blocks that are then split into sub-blocks and compressed using shared dictionaries (one dictionary per top-level block). As the top-level blocks are larger than they were before, segments are more likely to be considered \"dirty\" by the merging logic. Dirty segments are segments were 1% of the data or more consists of incomplete blocks. For large segments, the size of blocks doesn't really affect the dirtiness of segments: if you flush a segment that has 100 blocks or more, it will never be considered dirty as only the last block may be incomplete. But for small segments it does: for instance if your segment is only 10 blocks, it is very likely considered dirty given that the last block is always incomplete. And the fact that we increased the top-level block size means that segments that used to be considered clean might now be considered dirty. And indeed benchmarks reported that while large stored fields merges became slightly faster after upgrading to 8.7, the smaller merges actually became slower. See attached chart, which gives the total merge time as a function of the number of documents in the segment. I don't know how we can address this, this is a natural consequence of the larger block size, which is needed to achieve better compression ratios. But I wanted to open an issue about it in case someone has a bright idea how we could make things better.", "patch_link": "https://issues.apache.org/jira/secure/attachment/13022359/log-and-lucene-9827.patch", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9791", "change_description": ": BytesRefHash.equals/find is now thread safe, fixing a\nLuwak/Monitor bug causing registered queries to sometimes fail to\nmatch.", "change_title": "Monitor (aka Luwak) has concurrency issues related to BytesRefHash#find", "detail_type": "Bug", "detail_affect_versions": "9.0,8.7,8.8", "detail_fix_versions": "9.0,8.9", "detail_description": "org.apache.lucene.monitor.Monitor can sometimes NOT match a document that should be matched by one of registered queries if match operations are run concurrently from multiple threads. This is because sometimes in a concurrent environment TermFilteredPresearcher might not select a query that could later on match one of documents being matched. Internally TermFilteredPresearcher is using a term acceptor: an instance of org.apache.lucene.monitor.QueryIndex.QueryTermFilter. QueryTermFilter is correctly initialized under lock and its internal state (a map of org.apache.lucene.util.BytesRefHash instances) is correctly published. Later one when those instances are used concurrently a problem with org.apache.lucene.util.BytesRefHash#find is triggered since it is not thread safe. org.apache.lucene.util.BytesRefHash#find internally is using a private org.apache.lucene.util.BytesRefHash#equals method, which is using an instance field scratch1 as a temporary buffer to compare its ByteRef parameter with contents of ByteBlockPool. This is not thread safe and can cause incorrect answers as well as ArrayOutOfBoundException. __ ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13021127/LUCENE-9791_example.patch", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9887", "change_description": ": Fixed parameter use in RadixSelector.", "change_title": "error param use in RadixSelector", "detail_type": "Improvement", "detail_affect_versions": "8.8", "detail_fix_versions": "8.9", "detail_description": "There is a param use error in org.apache.lucene.util.RadixSelector#select(int, int, int, int, int). What is we expected in this method is: if the range becomes narrow or when the maximum level of recursion has been exceeded, then we get a fall-back selector(it's a IntroSelector). So, we should use the recursion level(param f)  compare to LEVEL_THRESHOLD. NOT the byte index of value(param d). effect: This bug will not affect the correctness of the program. but affect performance in some bad case. In average, RadixSelector and IntroSelector are all in linear time. This bug will let we choose a fall-back selector too early, then the constant of O will be bigger.  other evidence: verification:  Thanks for your read. I'm new of lucene. So please reply me if I am wrong. Or fix it in future.  I will do benchmark. But I can't promised the result is better. If you need the result. Ask for me. ", "patch_link": "https://issues.apache.org/jira/secure/attachment/13023108/LUCENE-9887.patch", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9958", "change_description": ": Fixed performance regression for boolean queries that configure a\nminimum number of matching clauses.", "change_title": "Performance regression when a minimum number of matching SHOULD clauses is required", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "Opening this issue on behalf of mattweber, who reported this at https://discuss.elastic.co/t/es-7-7-1-es-7-12-0-wand-performance-issue/272854. It looks like the fact that we introduced dynamic pruning for queries that already have a minimum number of SHOULD clauses configured makes things slower, at least in some cases.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9953", "change_description": ": LongValueFacetCounts should count each document at most once when determining\nthe total count for a dimension. Prior to this fix, multi-value docs could contribute a > 1\ncount to the dimension count.", "change_title": "FacetResult#value is inaccurate in LongValueFacetCounts for multi-value docs", "detail_type": "Bug", "detail_affect_versions": "8.9", "detail_fix_versions": "8.9", "detail_description": "As described in a dev@ list thread, the value of FacetResult#value should reflect the number of docs containing at least one value in a given facet path. LongValueFacetCounts counts the number of values contributed by all docs. In cases where all docs contain a single value, this is fine, but if a doc contains multiple values, FacetResult#value will be incorrect. This is a simple fix so I think we can include it in 8.9. Note: Spinning this off from LUCENE-9952 since fixing this for all cases (particularly SSDV) is trickier and may require non-backwards compatible changes.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Bug Fixes", "change_id": "LUCENE-9991", "change_description": ": Fix edge case failure in TestStringValueFacetCounts", "change_title": "Fix TestStringValueFacetCounts", "detail_type": "Bug", "detail_affect_versions": "9.0", "detail_fix_versions": "9.0,8.9", "detail_description": "As reported by julietibs in LUCENE-9950, there's a randomized test failure in TestStringValueFacetCounts. It's actually an issue with the test itself. Since count ties are broken in StringValueFacetCounts by ordinal, but the test doesn't know anything about the ordinals, the test breaks ties by the value itself before comparing results. The edge-case is if we only request a topN of 1, but the top result ties in count with other results. In this scenario, the result returned by the Facets might be one that sorts higher than another when secondarily sorted by value, but the test can't solve for this since it only sees the one result. Should be a fairly simple fix in the test case itself. Will do so shortly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Other", "change_id": "LUCENE-9836", "change_description": ": Removed the pure Maven build. It is no longer possible to build\nartifacts using Maven (this feature was no longer working correctly). Due to\nmigration to Gradle for Lucene/Solr 9.0, the maintenance of the Maven build\nwas no longer reasonable. POM files are generated for deployment to Maven\nCentral only. Please use \"ant generate-maven-artifacts\" to produce and deploy\nartifacts to any repository.", "change_title": "Fix 8.x Maven Validation and publication to work with Maven Central and HTTPS again; remove pure Maven build (did not work anymore)", "detail_type": "Improvement", "detail_affect_versions": "8.x,8.9", "detail_fix_versions": "8.x,8.9", "detail_description": "Currenty the Maven related stuff in 8.x completely fails, because Maven-Ant-Tasks is so outdated, that it has hardcoded Maven Central without HTTPS. This makes downloading fail. You can mostly fix this with an additional remote repository, so it can fallback to that one. I'd like to do the following on 8.x: I already tried some heavy committing, but the only way to solve this is to replace maven-ant-tasks with the followup ant task. I am not sure if this worth the trouble! What do others think? Should we maybe simply disable the Maven Dependency checker?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Other", "change_id": "LUCENE-9836", "change_description": ": Migrate Maven tasks to use \"maven-resolver-ant-tasks\"\ninstead of the no longer maintained \"maven-ant-tasks\".", "change_title": "Fix 8.x Maven Validation and publication to work with Maven Central and HTTPS again; remove pure Maven build (did not work anymore)", "detail_type": "Improvement", "detail_affect_versions": "8.x,8.9", "detail_fix_versions": "8.x,8.9", "detail_description": "Currenty the Maven related stuff in 8.x completely fails, because Maven-Ant-Tasks is so outdated, that it has hardcoded Maven Central without HTTPS. This makes downloading fail. You can mostly fix this with an additional remote repository, so it can fallback to that one. I'd like to do the following on 8.x: I already tried some heavy committing, but the only way to solve this is to replace maven-ant-tasks with the followup ant task. I am not sure if this worth the trouble! What do others think? Should we maybe simply disable the Maven Dependency checker?", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Other", "change_id": "LUCENE-9985", "change_description": ": Upgrade jetty to 9.4.41", "change_title": "Upgrade Jetty to 9.4.41", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "9.0,8.9", "detail_description": "As Solr is upgrading jetty dependency in 8.9 (shared with lucene), Lucene main should also do the same", "patch_link": "none", "patch_content": "none"}
{"library_version": "8.9.0", "change_type": "Other", "change_id": "LUCENE-9976", "change_description": ": Fix WANDScorer assertion error.", "change_title": "WANDScorer assertion error in ensureConsistent", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "8.9", "detail_description": "Build fails and is reproducible: https://ci-builds.apache.org/job/Lucene/job/Lucene-NightlyTests-main/283/console", "patch_link": "none", "patch_content": "none"}

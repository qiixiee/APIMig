{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12578", "change_description": ": Deprecate IndexSearcher#getExecutor in favour of executing concurrent tasks using\nthe TaskExecutor that the searcher holds, retrieved via IndexSearcher#getTaskExecutor", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We have recently introduced a TaskExecutor abstraction, which is meant to be used to execute concurrent tasks using the executor provided to the IndexSearcher constructor. All concurrenct tasks should be executed using the TaskExecutor, and there shoul be no reason to retrieve the Executor directly from the IndexSearcher.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12556", "change_description": ": StoredFieldVisitor has a new expert method StoredFieldVisitor#binaryField(FieldInfo, DataInput, int)\nthat allows implementors to read binary values directly from the DataInput without having to allocate a byte[].\nThe default implementation allocates an ew byte array and call StoredFieldVisitor#binaryField(FieldInfo, byte[]).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Similarly to #12459 , When reading a binary stored field, we allocate a byte[] in memory in order to read it: lucene/lucene/core/src/java/org/apache/lucene/codecs/lucene90/compressing/Lucene90CompressingStoredFieldsReader.java Line 243\n      in 008a0d4  Blindly allocating those arrays are dangerous as the allocations can be humongous and will put pressure on the GC. I wonder if we can provide this value so the user can decide the best way to serialize this value into memory. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12592", "change_description": ": Add RandomAccessInput#length method to the RandomAccessInput interface. In addition deprecate\nByteBuffersDataInput#size in favour of this new method.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "It is currently not possible to read all bytes from a RandomAccessInput without previous knowledge of how many bytes were written. I would like to propose that RandomAccessInput can provide the user the information on how many bytes it contains (similary to IndexInput). The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12718", "change_description": ": Make IndexSearcher#getSlices final as it is not expected to be overridden", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "IndexSearcher exposes a public getSlices method, that is used to retriefve the slices that the searcher executes queries against, as well as slices, which is supposed to be overridden to customize the creation of slices. I believe that getSlices should be final: there is no reason to override the method. Also, it is too easy to confuse the two and end up overriding the wrong one by mistake. This change was discussed here: #12606 (comment) .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12427", "change_description": ": Automata#makeStringUnion #makeBinaryStringUnion now accept Iterable<BytesRef> instead of\nCollection<BytesRef>. They also now explicitly throw IllegalArgumentException if input data is not properly sorted\ninstead of relying on assert.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Closes #12319", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12180", "change_description": ": Add TaxonomyReader#getBulkOrdinals method to more efficiently retrieve facet ordinals for multiple\nFacetLabel at once.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "LUCENE-9476 added the ability to do bulk ordinal -> path lookups, but we have no bulk lookup in the other direction. I think we can find some efficiency gains if we add bulk lookup for paths, which we could then put behind a Facets#getSpecificValues API for retrieving facet values for multiple paths in bulk. There's some nuance down in the depths of MultiTerms, so maybe there's nothing to be gained here, but I think there ought to be (I'll try to look in more detail later, but wanted to open an issue to track the idea in case I forget). A few thoughts: The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12816", "change_description": ": Add HumanReadableQuery which takes a description parameter for debugging purposes.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We use this only in KnnByte/FloatVectorQuery toString method so the benchmarker can disambiguate between different KnnFloatVectorQuery/KnnByteVectorQuery queries. Closes #12487 and should enable VectorQuery fixes to the benchmarker in mikemccand/luceneutil#226 . More details about the problem in the issues.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12646", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently FSTCompiler and FST has a circular dependencies to each other. FSTCompiler creates an instance of FST, and on adding node ( add(IntsRef input, T output) ), it delegates to FST.addNode() and passing itself as a variable. This introduces a circular dependency and mixes up the FST constructing and traversing code. To make matter worse, this implies one can call FST.addNode with an arbitrary FSTCompiler (as it's a parameter), but in reality it should be the compiler which creates the FST. This PR move the addNode method to FSTCompiler instead.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12690", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Follow-up of #12646 . NodeHash still depends on both FSTCompiler and FST. With the current method signature, one can create the NodeHash with a FST and call addNode with different FSTCompiler. This PR simplifies the signature and allow a consistent behavior.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12709", "change_description": ": Consolidate FSTStore and BytesStore in FST. Created FSTReader which contains the common methods\nof the two", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Consolidate the FSTStore and BytesStore in FST. The two are similar, except that FSTStore has an init() method, which is not needed for BytesStore. Thus I extracted the common methods to FSTReader (maybe there is better name). FST no longer needs to have if-else conditional logics to choose between the two. Also fix the numBytes() method which would throw NullPointerException if the FST is FSTStore-backed. I'm not sure if this needs a new entry in CHANGES.txt, but it can be backported to 9.x", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12735", "change_description": ": Remove FSTCompiler#getTermCount() and FSTCompiler.UnCompiledNode#inputCount", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Clean-up inputCount as it no longer has an active use. FSTCompiler.getTermCount() is removed, I don't see any usage of this method, but it's public and might be used by users.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12799", "change_description": ": Make TaskExecutor constructor public and use TaskExecutor for concurrent\nHNSW graph build.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR proposes to :", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12758", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Eventually we would want to remove/deprecate the constructors with DataInput metaIn . This will also make it easier to stream the FST to disk, as a FST written with DataOutput won't be able to read and only contain the metadata. The flow would then be: Note: We also might want to remove the constructor with FSTStore completely, and users need to call init() themselves?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "API Changes", "change_id": "GITHUB#12803", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fixes #12822 Remove FST constructors with DataInput for metadata, in favor of the new constructor with FSTMetadata, added in #12758", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "New Features", "change_id": "GITHUB#12548", "change_description": ": Added similarityToQueryVector API to compute vector similarity scores\nwith DoubleValuesSource.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR addresses the issue #12394. It adds an API similarityToQueryVector to DoubleValuesSource to compute vector similarity scores between the query vector and the KnnByteVectorField/KnnFloatVectorField for documentsusing the 2 new DVS implementations(ByteVectorSimilarityValuesSourceforbytevectorsandFloatVectorSimilarityValuesSource for float vectors). Below are the method signaturesaddedto DVS in this PR:DoubleValuessimilarityToQueryVector(LeafReaderContextctx,float[]queryVector,vectorField)(usesByteVectorSimilarityValuesSource)DoubleValuessimilarityToQueryVector(LeafReaderContextctx,byte[]queryVector,StringvectorField)(uses FloatVectorSimilarityValuesSource)Closes #12394", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "New Features", "change_id": "GITHUB#12685", "change_description": ": Lucene now records if documents have been indexed as blocks in SegmentInfo. This is recorded on a per\nsegment basis and maintained across merges. The property is exposed via LeafReaderMetadata.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "If the add/updateDocuments(List<>) API is used, lucene guarantees that all documents are indexed in the same segment with consecutive document IDs. This enables features like nested documents etc. This change records the usage of this API in SegmentInfo and preserves this property across merges. Relates to #12665", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "New Features", "change_id": "GITHUB#12582", "change_description": ": Add int8 scalar quantization to the HNSW vector format. This optionally allows for more compact lossy\nstorage for the vectors, requiring about 75% memory for fast HNSW search.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As with most codec changes, this is an eye popping number of LoC and the design isn't finished yet. Initial benchmarking (utilizing non-normalized cohere embeddings + max-inner product, which is a particularly difficult case for naive quantization), I get 10-20% faster search and ~4x smaller storage used for the search (I am keeping the raw vectors around...we can debate if we want to do that). Recall@10 with 100 fanout = 0.804 Recall@100 with 200 fanout = 0.9. I am reaching the point where the design needs to be finalized and I wanted to reachout for feedback. Some design discussion points that I am unsure about are:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "New Features", "change_id": "GITHUB#12660", "change_description": ": HNSW graph now can be merged with multiple thread. Configurable in Lucene99HnswVectorsFormat.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR still contains all kinds of magic number and names I need to improve. But I think we can start review it as it is not that hacky and I think is more on acceptable side. For sure I will add quite more javadocs to make the code readable, especially the add graph node part. The main idea is that we merely need to care about the thread safety of OnHeapHnswGraph after the #12651 (still, the entry node election is a bit complex), so I just create one RW lock for each NeighbourArray and lock them when necessary. The main change happens in HnswGraphBuilder#addGraphNode where I changed the sequence of updating the graph such that inserting a new node won't impact searching the previous graph, then I just hardcoded a thread pool in addVectors method to test the change. And it shows a good 4x speed up on forceMerge when I'm using 8 threads, and 3x overall indexing speed with 1M docs. A new ConcurrentHnswMerger to create the HnswConcurrentMergeBuilder , which will contain several workers ( ConcurrentMergeWorker ) which essentially each is a graph builder and we will spawn one thread for each of them. Then main logic still just as stated in the description, I make the OnHeapHnswGraph mostly thread-safe, and modified the way we add the graph node from the level 0 and goes up such that when a search is initiated concurrently, this new added node will not lead to a dead-end (because if the new node is discovered, it is guaranteed to have all out-going edges already and it is guaranteed to be able to go to the lower level) Promoting entry node is a bit tricky because essentially we need to change both node and entry level at the same time. So I made a new class and use AtomicReference to switch it. It seems to be working ok. One (I think worst) side effect of this change is that we have to manually set the entry node, because we add node to graph from top most level, then add neighbour to the node from the level 0, so we only want to set the new node to be entry node after we have done all this, so it cannot be set automatically when the node is first added at the highest level. 50MB writer buffer, force-merge to 1 segment at last. 384 dimension of vectors. Update: 100k doc with contention time infomation (note the first few time was for single thread overhead of locks)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "New Features", "change_id": "GITHUB#12729", "change_description": ": Add new Lucene99FlatVectorsFormat for writing vectors in a flat format and refactor\nLucene99HnswVectorsFormat to reuse the flat format. Added new Lucene99HnswQuantizedVectorsFormat that uses\nquantized vectors for its flat storage.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Currently the HNSW codec does too many things, it not only indexes vectors, but stores them and determines how to store them given the vector type. This PR extracts out the vector storage into a new format Lucene99FlatVectorsFormat and adds new base class called FlatVectorsFormat . This allows for some additional helper functions that allow an indexing codec (like HNSW) take advantage of the flat formats. Additionally, this PR refactors the new Lucene99ScalarQuantizedVectorsFormat to be a FlatVectorsFormat . Now, Lucene99HnswVectorsFormat is constructed with a Lucene99FlatVectorsFormat and a new Lucene99HnswScalarQuantizedVectorsFormat that uses Lucene99ScalarQuantizedVectorsFormat", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12523", "change_description": ": TaskExecutor waits for all tasks to complete before returning when Exceptions\nare thrown during concurrent operations", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The TaskExecutor used by IndexSearcher and AbstractKnnVectorQuery waits until all tasks have finished before returning, even when one or more of the tasks throw an Exception. Relates #12278", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12574", "change_description": ": Make TaskExecutor public so that it can be retrieved from the searcher and used\noutside of the o.a.l.search package", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "TaskExecutor is currently package private. We have scenarios where we want to parallelize the execution and reuse it outside of its package, hence this commit makes it public (and experimental). Note that its constructor remains package private as it is supposed to be created by the index searcher, and later retrieved from it via the appropriate getter, which is also made public as part of this commit. This is a pre-requisite for #12183 .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12603", "change_description": ": Simplify the TaskExecutor API by exposing a single invokeAll method that takes a\ncollection of callables, executes them and returns their results", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We recently made TaskExecutor public. It currently exposes two methods: one to create tasks given a collection of callables, and one to execute all tasks created at step 1. We can rather expose a single public method that takes a collection of callables which internally creates the appropriate tasks. This simplifies the API, and stops us from leaking the internal Task abstraction which can be kept private. Note that this is backwards compatible as we have not released yet a version where the TaskExecutor was made public. It is marked experimental anyways.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12606", "change_description": ": Create a TaskExecutor when an executor is not provided to the IndexSearcher, in\norder to simplify consumer's code", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As we introduce more places where we add concurrency (there are currently three) there is a common pattern around checking whether there is an executor provided, and then going sequential on the caller thread or parallel relying on the executor. That can be improved by internally creating a TaskExecutor that relies on an executor that executes tasks on the caller thread, which ensures that the task executor is never null, hence the common conditional is no longer needed, as the concurrent path that uses the task executor would be the default and only choice for operations that can be parallelized.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12676", "change_description": ": Improve logging of vector support if vector module was enabled but Java version\nis too old. It also logs partial vectorization support if old CPU or disabled AVX.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This improves logging a bit: Robert and I hit this when tetsing the benchmark.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12677", "change_description": ": Better detect vector module in non-default setups (e.g., custom module layers).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Hi @ChrisHegarty , while working on #12667 and #12676 I noticed that the code which detects if vector module is enabled and readable has some limitations because it only looks into the boot layer. If you create your own module layer with Lucene and the incubator module, Lucene does not detect it correctly. This code uses the ModuleLayer of the Lucene module and uses findModule to check if it is there (we do this in other Lucene code in the same way, e.g. to detect sun.misc.Unsafe ,...). Only in case of unnamed module (then getLayer() returns null), then the code uses boot layer. In addition to #12676 it also only calls thisModule.addReads(vectorModule) when it actually wants to enable it. For the pure check (e.g., java 17) ist does not add a read.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12634", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Let's improve the testing for the boundary cases and check them explicitly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12632", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "gives a little love to the mac for dot-product and binary-cosine. I see on m1 some improvement: You can reproduce with java -jar target/vectorbench.jar Binary -p size=1024 See https://github.com/rmuir/vectorbench with has a README now! edit: originally I tried to optimize the 256/512-bit path, but it caused slowdowns on avx-512 so i reverted the changes. Sorry for confusion. hopefully we get some improvement to something that can stick :)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12680", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Now that we have integrated benchmarks, it is easier to take care of this code. This is pretty straightforward change: I see the same performance (no logic was changed) Note: not much can be done with cosine method, for obvious reasons that it is a slow complex nightmare, IMO we shouldn't support it anyway. But we can't let that get in the way of trying to improve the other ones.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12681", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This builds on #12680 so please review that one first to make it easier. The advantage there is we split out vector kernels into smaller manageable methods, making it easy to specialize the avx-512 cases. This speeds up all the binary functions on avx-256, without slowing anything down on avx-512 (it explicitly avoids using 512-bit integer multiply). the squareDistance is sped up a bit on avx-512, only because avx-512 users were suffering from downclocking already (already doing 512-bit multiply), but we might as well clean up the other math. 512-bit multiply can't be easily avoided in this function due to the formula. We have to think about testing. I don't want to rely upon various hardware for correctness. I think there's a way to alter the code so that we can test the correctness of everything (albeit slowly), e.g. exercise 128, 256, and 512 in our tests.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12731", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The intel fma is nice, and its easier to reason about when looking at assembly. We basically reduce the error for free where its available. Along with another change (reducing the unrolling for cosine, since it has 3 fma ops already), we can speed up cosine from 6 -> 8 uops/us. On the arm the fma leads to slight slowdowns, so we don't use it. Its not much, just something like 10%, but seems like the wrong tradeoff. If you run the code with -XX-UseFMA there's no slowdown, but no speedup either. And obviously, no changes for ARM here.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12737", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Apply same logic to float scalar functions as we have for vector functions. Really doing the same instructions after all, just different width. So they shouldn't be so crazy different... The scalar functions currently have various ad-hoc unrolling that doesn't keep the CPU's FPUs properly busy, and have data dependencies and stuff that clogs everything up and prevents parallelization: this fixes that. This gives e.g. 2X speedup to cosine on both x86 and arm, but all the functions get speedups of some sort on both architectures. FMA is used on x86 where available, to really keep the cpu busy, same as the vector case. It is not used on ARM. main (skylake): patch (skylake): Good speedup across the board, e.g. 2x for cosine patch (skylake, -XX:-UseFMA): Still decent speedup even without using FMA, just not as much Main (arm): Patch (arm): Good speedups, 2x for cosine). FMA is not used.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12586", "change_description": ": Remove over-counting of deleted terms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "BufferedUpdates used to count deleted terms without deduplication to respect IndexWriterConfig.setMaxBufferedDeleteTerms . As IndexWriterConfig.setMaxBufferedDeleteTerms is removed since LUCENE-7868 , the overcounting can be avoided now. Previous talk: https://github.com/apache/lucene/pull/12573/files#r1332924157", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12705", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "See the dev thread by @msokolov @ https://lists.apache.org/thread/qts8wvrjs54gkgz04pk4p93fg0wjbq3o The handling of NPE is very special in ByteBufferIndexInput and also MemorySegmentIndexInput: To signal a closed input we set the buffers to NULL so any code trying to work on the inputs hits an NullPointerException. This is to avoid any null checks or isOpen checks everywhere in the code, which might be expensive, as the variable/field is not constant. But it must on the other hand be avoided that the NPE gets visible outside of the IndexInputs, because it looks like a buggy null checks and may cause support issues. So we have to hide it by all means, as the NPE is not an error but a signal that the IndexInput was closed. There are a few cases where a real NPE can still happen, e.g., when somebody accidentally passes a null array to one of the read  methods. In that case the NPE is important and should be re-thrown unmodified for the visibility of the caller. The workaround here to not add the NPE as a cause (and making it visible to call er outside code) is to do a safety check when the excapetion actually happens (so it does not slow down anything): If the NPE is catched, the code in this PR checks that the \"closed\" condition applies to current state of IndexInput (buffers are null or the byte buffer guard is invalided). If and only if the \"closed\" check is true, it throws AlreadyCosedException. In all other cases it rethrows the original NPE. I also added a test which failed before my change to demonstrate that it works. We may possibly add this check also to BaseDirectoryTestCase, The changes were applied to all MemorySegmentIndexInput variants in the MR-JAR and the original ByteBufferIndexInput (+ its guard).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12705", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "See the dev thread by @msokolov @ https://lists.apache.org/thread/qts8wvrjs54gkgz04pk4p93fg0wjbq3o The handling of NPE is very special in ByteBufferIndexInput and also MemorySegmentIndexInput: To signal a closed input we set the buffers to NULL so any code trying to work on the inputs hits an NullPointerException. This is to avoid any null checks or isOpen checks everywhere in the code, which might be expensive, as the variable/field is not constant. But it must on the other hand be avoided that the NPE gets visible outside of the IndexInputs, because it looks like a buggy null checks and may cause support issues. So we have to hide it by all means, as the NPE is not an error but a signal that the IndexInput was closed. There are a few cases where a real NPE can still happen, e.g., when somebody accidentally passes a null array to one of the read  methods. In that case the NPE is important and should be re-thrown unmodified for the visibility of the caller. The workaround here to not add the NPE as a cause (and making it visible to call er outside code) is to do a safety check when the excapetion actually happens (so it does not slow down anything): If the NPE is catched, the code in this PR checks that the \"closed\" condition applies to current state of IndexInput (buffers are null or the byte buffer guard is invalided). If and only if the \"closed\" check is true, it throws AlreadyCosedException. In all other cases it rethrows the original NPE. I also added a test which failed before my change to demonstrate that it works. We may possibly add this check also to BaseDirectoryTestCase, The changes were applied to all MemorySegmentIndexInput variants in the MR-JAR and the original ByteBufferIndexInput (+ its guard).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12785", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Unfortunately the solution in #12707 was not working well with concurrency. The is alive status of MemorySegment.Scope may be stale. In that case the IllegalStateException was catched, but the confirmation using isAlive() did not work, so the original IllegalStateException was rethrown instead of the required AlreadyClosedException . This PR reverts #12707 and falls back to the naive approach by checking exception message for \"closed\". I am in contact with @mcimadamore to find a better solution to confirm the closed status without relying on Exception message.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12689", "change_description": ": TaskExecutor to cancel all tasks on exception to avoid needless computation.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When operations are parallelized, like query rewrite, or search, or createWeight, one of the tasks may throw an exception. In that case we wait for all tasks to be completed before re-throwing the exception that were caught. Tasks that were not started when the exception is captured though can be safely skipped. Ideally we would also cancel ongoing tasks but I left that for another time.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12754", "change_description": ": Refactor lookup of Hotspot VM options and do not initialize constants with NULL\nif SecurityManager prevents access.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This code was previously in RamUsageEstimator and also in PanamaVectorUtilSupport . In addition this moves detection of Client VM and fast FMA support to Constants class (in preparation for #12737 ).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12801", "change_description": ": Remove possible contention on a ReentrantReadWriteLock in\nMonitor which could result in searches waiting for commits.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Within Lucene Monitor, there is a thread contention issue that manifests as multi-second latencies in the Monitor.match function when it is called concurrently with Monitor.register/Monitor.deleteById and QueryIndex.purgeCache . The source of the issue is the usage of purgeLock , a ReentrantReadWriteLock. Within the QueryIndex, there are 3 usages of the purgeLock . They are commit (called by Monitor.register/Monitor.deleteById ), ‘search’ (called by Monitor.match), and purgeCache (called by a background thread at a consistent interval). Within commit and search , the read lock is acquired, and within purgeCache , the write lock is acquired. search calls are very fast and only hold the purgeLock for short durations. However, commit calls are quite long and will hold the purgeLock for multi-second durations. Ostensibly because both commit and search only need the read lock, they should be able to execute concurrently and the long duration of commit should not impact search . However, when purgeCache attempts to acquire the write lock this no longer holds true. This is because once purgeCache is waiting to acquire the write lock, attempts to acquire the read lock probabilistically might wait for the write lock to be acquired + released (otherwise attempts to acquire the write lock would be starved). What this means is that if commit is holding the read lock, and then purgeCache attempts to acquire the write lock, some amount of calls to match that want to acquire the read lock will be queued behind the purgeCache attempt to acquire the write lock. Because purgeCache cannot acquire the write lock until commit releases the read lock, and because those search calls cannot acquire the read lock until purgeCache acquires + releases the write lock, those search calls end up having to wait for commit to complete. This makes it possible for search calls to take as long as commit to complete (multi-second rather than millisecond). For illustration purposes I have the following 2 diagrams of the problem. The first diagram demonstrates the wait order for the ReentrantReadWriteLock in the high latency match scenario. For the purpose of the diagram I am assuming that commit takes 3 seconds, purgeCache takes 3 milliseconds, and search takes 3 milliseconds as well. Most of the calls to search will not end up in this scenario, but without the fix included in this PR some of the calls to search will end up in this scenario and be slow. The way in which the calls to search end up being slow is more clearly shown in the second diagram which shows a timeline that has a multisecond delayed search call - making the same assumptions about the runtime of the 3 operations as the first diagram. In the below diagram, Search Thread A avoids the problem and has expected latency, but Search Thread B runs into the problem and has very high latency. This issue can be resolved by ensuring that purgeCache never attempts to acquire the write lock when commit is holding the read lock. This can be done by  simply using a mutual exclusion lock between the two since: To demonstrate the problem I have this gist that runs commit and search concurrently while recording the durations of the calls to search . Then it runs commit , search and purgeCache concurrently while recording the durations of search . Comparing the p99 - p100 durations between when purgeCache wasn’t run concurrently vs when it was run concurrently shows the high latency behavior for combining all 3 operations. Without this change, running the gist on my laptop gives: With this change, running the gist on my laptop gives: Even though the latencies in this test are noisy, the order of magnitude difference in the mean p99 thru p100 when using this change with commit , search and purgeCache run concurrently is statistically significant. This issue is important to fix because it prevents Lucene Monitor from having reliable low latency match performance. If this PR gets merged, can you please use my dcook96@bloomberg.net email address for the squash+merge. Thank you.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#11277", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Update OpenNLP dependency to version 1.9.4. Migrated from LUCENE-10241 by Jeff Zemerick Pull requests: #448 The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "LUCENE-10241", "change_description": ",", "change_title": "Update OpenNLP to 1.9.4", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "Update OpenNLP dependency to version 1.9.4.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12542", "change_description": ": FSTCompiler can now approximately limit how much RAM it uses to share\nsuffixes during FST construction using the suffixRAMLimitMB method.  Larger values\nresult in a more minimal FST (more common suffixes are shard).  Pass\nDouble.POSITIVE_INFINITY to use as much RAM as is needed to create a purely\nminimal FST.  Inspired by this Rust FST implemention:", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "[Spinoff from this comment about the cool approach Tantivy's FST implementation uses to limit memory during construction.  See also this awesome detailed blog post .] The most RAM/CPU costly part of constructing an FST is recording all suffixes seen so far so that when you see a new suffix, if it was already seen before, you can share the previous suffix.  To guarantee a minimal (smallest number of states) FST, you must record every such suffix.  But this is crazy costly when compiling many keys, and in practice you can accept loss of minimality in exchange for trimming how many / which suffixes you store. Lucene's FSTCompiler.Builder has three hairy integer options for this ( minSuffixCount1 , minSuffixCount2 , and sharedMaxTailLength ), but 1) nobody really knows exactly these do, 2) they are horribly indirect and heavily quantized ways to tune RAM/CPU.  You don't know how much RAM/CPU you really are saving. Whereas the approach in Tantivy's FST implementation (which was forked original from this implementation by Andrew Gallant ) is a simple bounded HashMap keeping only the commonly reused suffixes.  Then one could tune how large this is (Andrew suggests ~10K is large enough in practice) against how minimal you really need your FST to be. I think we should replace the three confusing integers with this approach, which requires only a single integer.  We could even make the single integer a bound on RAM required to make it even more clearly meaningful to users.  We should experiment with some \"typical\" corpora in how Lucene uses FSTs (encode synonyms, terms, etc.) to find a good default.  Today Lucene defaults to \"save everything so you get the truly minimal FST\". The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "https://blog.burntsushi.net/transducers", "change_description": ": FSTCompiler can now approximately limit how much RAM it uses to share\nsuffixes during FST construction using the suffixRAMLimitMB method.  Larger values\nresult in a more minimal FST (more common suffixes are shard).  Pass\nDouble.POSITIVE_INFINITY to use as much RAM as is needed to create a purely\nminimal FST.  Inspired by this Rust FST implemention:", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12738", "change_description": ": NodeHash now stores the FST nodes data instead of just node addresses", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fix #12714 First attempt to introduce value-based LRU cache in NodeHash. There are some inefficiencies, but the functionalities work. See this comment for the main idea I think we can use ByteBlockPool to store the byte[] slices, just appending a new byte[] slice when we store a new suffix. We never delete individual suffixes, but rather discard the entire \"secondary\" hash map in the double barrel cache, so we could just drop/recycle the ByteBlockPool at that point too.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Improvements", "change_id": "GITHUB#12847", "change_description": ": Test2BFST now reports the time it took to build the FST and the real FST size", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12183", "change_description": ": Make TermStates#build concurrent.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR now deals with making TermStates#build run concurrently using the IndexSearcher 's executor . Old idea/description : This change tries to make some queries with heavy rewrites make use of concurrency using the api added in #11840 . In #12160 it turns out have good gains in KnnVectorQuery. This is initial rough implementation to get some early thoughts/feedback on or ideas on if this could be useful and worth pursuing for some other queries as well which are doing some heavy lifting in rewrite. This change initially tries to achieve some benefit of concurrency for FieldExistsQuery , BlendedTermQuery and CompletionQuery .There are also others also I could find which are not covered in this but maybe could benefit with concurrency. Queries changed to have || rewrite  in this PR: Some possible candidate(s) with non-trivial rewrites ? Benchmarks : yet to be run PS : As some query rewrites calls rewrites on the child/sub queries. Can we achieve this in those queries as well? So I tried to make DisjunctionMaxQuery#rewrite concurrent but that seems to get stuck on my machine when running TestDisjunctionMaxQuery unit test. Maybe thats too much parallelism as the test is randomly generating a very large disjunctive query and making that rewrite concurrent is not correct or helpful ?", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12573", "change_description": ": Use radix sort to speed up the sorting of deleted terms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Recently, we captured a flame graph in a scene with frequent updates, which showed that sorting deleted terms occupied a high CPU ratio. In scenarios with many deleted terms, most terms could have the same field name. So a data structure like Map<String, Map<BytesRef, Integer>> instead of Map<Term, Integer> could be better here —— We can avoid field name compare and use MSBRadixSort to sort the bytes for each field. We can also take advantage of BytesRefHash to implement Map<BytesRef, Integer> to get a more efficient memory layout, and there's already a MSBRadixSort impl in BytesRefHash we can reuse. We benchmarked the sort logic for 1,000,000 terms, showing 66% took decreasing:  An E2E benchmark that delete document with term 1,000,000 times showing took decreased 43% (with default iw config). ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12382", "change_description": ": Faster top-level conjunctions on term queries when sorting by\ndescending score.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This implements a specialized BlockMaxConjunctionBulkScorer , which is similar to BlockMaxConjunctionScorer with the following differences:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12591", "change_description": ": Use stable radix sort to speed up the sorting of update terms.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Inspired by the #91 , This PR proposes to use a stable radix sorter to sort update terms instead of tie comparator to doc id. As terms are appended in order, the latest update of each term value should be the one to apply. This PR also refactor StringMSBRadixSorter to StringSorter for two purposes: A simple benchmark that update docvalues 1,000,000 times shows around ~27% E2E took decreasing.  PS: Glancing callers of BytesRefArray#sort it seems there's a couple of comparators could implement BytesRefComparator . It would require some more benchmark to optimize. I'd like to track them in following work.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12587", "change_description": ": Use radix sort to speed up the sorting of terms in TermInSetQuery.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Sort terms in TermInSetQuery with radix sort. This helps TermInSetQueries with a number of terms. I made a simple benchmark on sorting BytesRef[] with random bytes to verify the improvements. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12604", "change_description": ": Estimate the block size of FST BytesStore in BlockTreeTermsWriter\nto reduce GC load during indexing.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "https://blunders.io/jfr-demo/indexing-4kb-2023.09.25.18.03.36/allocations-drill-down Nightly benchmark shows that FSTCompiler#init allocated most of the memory during indexing. This is because FSTCompiler#init will always allocate 32k bytes as we param bytesPageBits default to 15. I counted the usage of BytesStore ( getPosition() when BytesStore#finish called) during the wikimediumall indexing, and the result shows that 99% FST won't even use more than 1k bytes. This PR proposes to reduce the block size of FST in Lucene90BlockTreeTermsWriter . closes #12598", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12623", "change_description": ": Use a MergeSorter taking advantage of extra storage for StableMSBRadixSorter.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As StableMSBRadixSorter always requires a O(n) extra memory. We can use a MergeSorter taking advantage of the extra memory instead of InPlaceMergeSorter . This reduces around 12% took of sorting ~10,000,000 timestamp values (LongPoint) during indexing. ", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12631", "change_description": ": Write MSB VLong for better outputs sharing in block tree index, decreasing ~14% size\nof .tip file.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "closes #12620", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12668", "change_description": ": ImpactsEnums now decode frequencies lazily like PostingsEnums.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The code was written as if frequencies should be lazily decoding, except that when refilling buffers freqs were getting eagerly decoded instead of lazily.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12651", "change_description": ": Use 2d array for OnHeapHnswGraph representation.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Make the OnHeapHnswGraph essentially a 2D array of NeighbourArray , there're several benefits of this change: The only regression on this approach is that when we get node for a non-zero level it need to traverse the whole graph. But since that is not a common operation and is only called when we serialize to disk, I made a cache for it such that the cost of first non-zero level call is O(whole_graph) and subsequent call will be trivial. I use writer buffer of 256MB and forcemerge at the end, and measured forceMerge time as well", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12653", "change_description": ": Optimize computing number of levels in MultiLevelSkipListWriter#bufferSkip.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "While going through MultiLevelSkipListWriter I happened to see we always calculate the numLevels (number of skip levels) here for a particular doc frequency df . Since we know the skipInterval and skipMultiplier (in the cx) upfront we could those to replace arithmetic operations of dividing df by skipInterval and then (df % skipMultiplier) == 0 ) with a single modulo operation for cases where numLevels is supposed to be 1 (which is more often?) i.e. the only df (doc frequency) which could have numLevels > 1 must suffice the check ( df % windowLength == 0 ) where windowLength is skipInterval * skipMultiplier i.e. Length of the window at which the skips are placed on skip level 1 .  Here windowLength = skipInterval * skipMultiplier = 3 x 3 = 9 (interval at which skips are placed on skip level 1). So for df=6,12,18,24,30... we would perform a single modulo operation to quickly evaluate numLevels which would be 1 in those cases. NOTE - This would be more helpful as we increase the skipMultiplier . Eg : for skipInterval=BLOCK_SIZE=128 and skipMultiplier=8 we would early evaluate numLevels to 1 for 6 doc frequencies out of 8 when buffering the skip data.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12589", "change_description": ": Disjunctions now sometimes run as conjunctions when the minimum\ncompetitive score requires multiple clauses to match.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The idea behind MAXSCORE is to run disjunctions as +(essentialClause1 ... essentialClauseM) nonEssentialClause1 ... nonEssentialClauseN , moving more and more clauses from the essential list to the non-essential list as the minimum competitive score increases. For instance, a query such as the book of life which I found in the Tantivy benchmark ends up running as +book the of life after some time, ie. with one required clause and other clauses optional. This is because matching the , of and life alone is not good enough for yielding a match. Here some statistics in that case: Actually if you look at these statistics, we could do better, because a match may only be competitive if it matches both book and life , so this query could actually execute as +book +life the of , which may help evaluate fewer documents compared to +book the of life . Especially if you enable recursive graph bisection. This is what this PR tries to achieve: in the event when there is a single essential clause and matching all clauses but the best non-essential clause cannot produce a competitive match, then the scorer will only evaluate documents that match the intersection of the essential clause and the best non-essential clause. It's worth noting that this optimization would kick in very frequently on 2-clauses disjunctions.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12710", "change_description": ": Use Arrays#mismatch for Outputs#common operations.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Make Outputs#common take advantage of Arrays#mismatch .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12712", "change_description": ": Speed up sorting postings file with an offline radix sorter in BPIndexReader.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Based on the idea mentioned here : Some thoughts / todos:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12702", "change_description": ": Disable suffix sharing for block tree index, making writing the terms dictionary index faster\nand less RAM hungry, while making the index a bit (~1.X% for the terms index file on wikipedia).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Today our block tree index will encode floordata as output. The floor data is guaranteed to be stored within single arc (never be prefix shared) in FST because fp is encoded before it. As a suffix, floor data can rarely be shared too. I wonder if we can avoid adding floor data outputs into NodeHash some way? Out of curiosity, i tried to completely disable suffix sharing in block tree index, result in only 1.47% total .tip size increased for wikimediumall . The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12726", "change_description": ": Return the same input vector if its a unit vector in VectorUtil#l2normalize.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "While going through VectorUtil class, I observed we don't have a check for unit vector in VectorUtil#l2normalize so passing a unit vector goes thorough the whole L2 normalization(which is totally not required and it should early exit?). I confirmed this by trying out a silly example of VectorUtil.l2normalize(VectorUtil.l2normalize(nonUnitVector)) and it performed every calculation twice. We could also argue that user should not call for this for a unit vector but I believe there would be cases where user simply want to perform the L2 normalization without checking the vector or if there are some overflowing values. TL;DR : We should early exit in VectorUtil#l2normalize , returning the same input vector if its a unit vector This is easily avoidable if we introduce a light check to see if the L1 norm or squared sum of input vector is equal to 1.0 (or) maybe just check Math.abs(l1norm - 1.0d) <= 1e-5 (as in this PR) because that unit vector dot product( v x v ) are not exactly 1.0 but like example : 0.9999999403953552 etc. With 1e-5 delta here we would be assuming a vector v having v x v >= 0.99999 is a unit vector or say already L2 normalized which seems fine as the delta is really small? and also the check is not heavy one?. I'm not sure this there existed some sort of similar check before or something(I tried to check but didn't find any history) so looking forward to thoughts if this makes sense to be added or not. Thanks!", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12719", "change_description": ": Top-level conjunctions that are not sorted by score now have a\nspecialized bulk scorer.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "PR #12382 added a bulk scorer for top-k hits on conjunctions that yielded a significant speedup (annotation FP ). This change proposes a similar change for exhaustive collection of conjunctive queries, e.g. for counting, faceting, etc.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12696", "change_description": ": Change Postings back to using FOR in Lucene99PostingsFormat. Freqs, positions and offset keep using PFOR.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Background: In https://github.com/Tony-X/search-benchmark-game we were comparing performance of Tantivy and Lucene. \"One difference between Lucene and Tantivy is Lucene uses the \"patch\" FOR, meaning the large values in a block are held out as exceptions so that the remaining values can use a smaller number of bits to encode, a tradeoff of CPU for lower storage space.\" In Tony-X/search-benchmark-game#46 , I disable the patching in Lucene, to match how Tantivy encodes and run the search-benchmark-game to test the change. Lucene modifications for testing: I cloned the pforUtil and removed all logic related to patching the exceptions. I modified the Lucene90PostingsReader + Writer to use the util with no patching logic, see sample code slow-J@ 83ec5a8 Hardware used: EC2 Graviton2 instance, m6g.4xlarge Results from the search-benchmark-game: Tony-X/search-benchmark-game#46 (comment) We saw Lucene's latency improve: -2% in COUNT, -2% in TOP_10_COUNT, -2.07% in TOP_100. I then ran a Lucene benchmark with luceneutil python3 src/python/localrun.py -source wikimediumall -r Hardware used: EC2 instance, m5.12xlarge Posting results below The tasks at the bottom of the table had the larger QPS improvement. AndHighLow has a QPS improvement of +7.6%! Size of test candidate index:   17.626 GiB\ttotal Size of test baseline index:   18.401 GiB\ttotal This change would bring a 4.39691% increase in index size. I propose adding an option to Lucene's codec that allows users to disable patching in the PFOR encoding, providing the option to leverage the performance benefits observed here at a cost of index size. I would appreciate all feedback and further evaluation of this idea by the Lucene community. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#1052", "change_description": ": Faster merging of terms enums.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This commit adds a new TermsEnumIndex abstraction in oal.index that wraps a TermsEnum and an index of the segment that it belongs to, and can be used to create priority queues that merge TermsEnum instances (either from the inverted index or from doc values). In either case, a long that holds the first 8 bytes of the term is computed in order to speed up comparisons. In the doc-values case, OrdinalMap also leverages seek-by-ord capabilities to reason about shared prefixes across entire windows of terms to not compare shared prefixes whenever re-ordering the queue, this should especially help with fields that may share long common prefixes like URLs. On luceneutil's OrdinalMap benchmark, construction time reduced by 30.5% for the id field and by 17.5% for the name field. JIRA: LUCENE-10560 #11596", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#11903", "change_description": ": Faster sort on high-cardinality string fields.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Since increasing the number of hits retrieved in nightly benchmarks from 10 to 100, the performance of sorting documents by title dropped back to the level it had before introducing dynamic pruning. This is not too surprising given that the title field is a unique field, so the optimization would only kick in when the current 100th hit would have an ordinal that is less than 128 - something that would only happen after collecting most hits. This change increases the threshold to 1024, so that the optimization would kick in when the current 100th hit has an ordinal that is less than 1024, something that happens a bit sooner. Title sort performance chart: http://people.apache.org/~mikemccand/lucenebench/TermTitleSort.html Results on wikimedium10m:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12381", "change_description": ": Skip docs with DocValues in NumericLeafComparator.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "like pr-399 , the DocsWithFieldSet#add() can avoid create instance for FixedBitSet in dense scene, so in NumericDocValuesWriter#sortDocValues() we can do the same thing, just the way to judge dense or sparse, I'm not sure if it's rigorous enough the benchmark for write ten SortedNumericDocValuesField, the optimization saves ~7% commit time", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12784", "change_description": ": Cache buckets to speed up BytesRefHash#sort.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Following #12775 , this PR tries another approach to speed up BytesRefHash#sort : The idea is that since we have extra ints in this map, we can cache the bucket when building the histograms, and reuse them when reorder .  I checked this approach on intel chip, showing ~30% speed up. I'll check M2 chip and wikimedium data tomorrow.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12806", "change_description": ": Utilize exact kNN search when gathering k >= numVectors in a segment", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "When requesting for k >= numVectors, it doesn't make sense to go through the HNSW graph. Even without a user supplied filter, we should not explore the HNSW graph if it contains fewer than k vectors. One scenario where we may still explore the graph if k >= numVectors is when not every document has a vector and there are deleted docs. But, this commit significantly improves things regardless.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12782", "change_description": ": Use group-varint encoding for the tail of postings.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As discussed in issue #12717 the read performance of group-varint  is 14-30% faster than vint, the size 16-248 is the number of ints will be read. feel free to close the PR if the performance improves is not enough :)", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12748", "change_description": ": Specialize arc store for continuous label in FST.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This PR resolves issue: #12701 . Thanks for the cool idea from @gf2121 It need some more benchmarking.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12825", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Optimizations", "change_id": "GITHUB#12834", "change_description": ",", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Changes in runtime behavior", "change_id": "GITHUB#12569", "change_description": ": Prevent concurrent tasks from parallelizing execution further which could cause deadlock", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Concurrent search is currently applied once per search call, either when search is called, or when concurrent query rewrite happens. They generally don't happen within one another. There are situations in which we are going to introduce parallelism in places where there could be multiple inner levels of parallelism requested as each task could try to parallelize further. In these cases, with certain executor implementations, like ThreadPoolExecutor, we may deadlock as we are waiting for all tasks to complete but they are waiting for threads to free up to complete their execution. This commit introduces a simple safeguard that makes sure that we only parallelize via the executor at the top-level invokeAll call. When each task tries to parallelize further, we just execute them directly instead of submitting them to the executor.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Changes in runtime behavior", "change_id": "GITHUB#12765", "change_description": ": Disable vectorization on VMs that are not Hotspot-based.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I did some tests after fixing the bug in VM option detection. With OpenJ9 (IBM Semeru) Java v20.0.2 there are no optimizations for Panama Vectors. If you enable the jdk.incubator.vector module it is like 1000 times slower, because there's no support at all. The module is there, but fallbacks to the default interpreted mode: Please note: The binary code is not slower, as the vectorization is not used at all: This PR enables vectorization only if Hotspot was detected by it's JVM flags, any other VM type is logged as not compatible.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Changes in runtime behavior", "change_id": "GITHUB#12552", "change_description": ": Make FSTPostingsFormat load FSTs off-heap.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "FSTs supports to load offheap for a while. As we were trying to use FSTPostingsFormat for some fields we realized heap usage bumped. Upon further investigation we realized the FSTPostingsFormat does not load FSTs offheap. This PR addresses that.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12654", "change_description": ": TestIndexWriterOnVMError.testUnknownError times out (fixes potential IndexWriter\ndeadlock with tragic exceptions).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "CI indicated the test suite timed out. So, I ran the reproduction line locally and had to kill the test running after 5 minutes. I seriously doubt this test should take longer than 5 minutes. I ran without the seed and it finished in seconds. The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12614", "change_description": ": Make LRUQueryCache respect Accountable queries on eviction and consistency check", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "…cy check Root cause onQueryCache increases ramBytesUsed for specified amount, that is being calculated with respect to query being Accountable or not. Unfortunately, onQueryEviction does not the same. If some heavy accountable query has ramBytesUsed() greater than QUERY_DEFAULT_RAM_BYTES_USED , the delta ghostBytes = ramBytesUsed() - QUERY_DEFAULT_RAM_BYTES_USED remains in total LRUQueryCache#ramBytesUsed forever. Hit rate drops to 0 since total sum of ghost bytes monotonously increases with each cached accountable query (>QUERY_DEFAULT_RAM_BYTES_USED), eventually it becomes greater than maxRamBytesUsed , and each newly cached query immediately evicts from the cache. Current behavior of LRUQueryCache for some real service in production [though not fully optimized] After fix", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#11556", "change_description": ": HTMLStripCharFilter fails on '>' or '<' characters in attribute values.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "If HTML input contains attributes with '<' or '>' characters in their values, HTMLStripCharFilter produces unexpected results. See the attached unit test for example. These characters are valid in attribute values, as by the HTML5 specification . The W3C validator does not have issues with the test HTML. Migrated from LUCENE-10520 by Alex Alishevskikh, updated Apr 19 2022 Attachments: HTMLStripCharFilterTest.java The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12698", "change_description": ": Fix IndexOutOfBoundsException when saving FSTStore-backed FST with different DataOutput for metadata", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Fix #12697 . This will move the writing of numBytes from OnHeapFSTStore.writeTo() to FST.save() . OnHeapFSTStore.size() will also be modified to return only the numBytes. As such, this might change the existing behavior. However searching through the Lucene code, size() is only referenced for OffHeapFSTStore , which already use a correct implementation. I think it could be backported to 9.9", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12642", "change_description": ": Ensure #finish only gets called once on the base collector during drill-sideways", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Small bug fix where #finish can be called multiple times on the base collector during drill-sideways", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12682", "change_description": ": Scorer should sum up scores into a double.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Addresses #12675 . Along with MultiSimilarity.MultiSimScorer found some others candidate scorer implementations for this fix.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12727", "change_description": ": Ensure negative scores are not returned by vector similarity functions", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We shouldn't ever return negative scores from vector similarity functions. Given vector panama and nearly antipodal float[] vectors, it is possible that cosine and (normalized) dot-product become slightly negative due to compounding floating point errors. Since we don't want to make panama vector incredibly slow, we stick to float32 operations for now, and just snap to 0 if the score is negative after our correction. closes : #12700", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12736", "change_description": ": Fix NullPointerException when Monitor.getQuery cannot find the requested queryId", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The javadoc for Monitor.getQuery suggests that it will return null when the query is not present but instead it throws a NullPointerException. This is because bytesHolder[0] will still be null after the call to search if it does not find any query matching the id in the monitor's index. Then when the serializer.deserialize call is made, any attempts to index the BytesRef will throw the NullPointerException. This will happen here when using the serializer/deserializer created with MonitorQuerySerializer.fromParser . This could instead be fixed just for the case of MonitorQuerySerializer.fromParser by making the serializer/deserializer returned from it null-safe (i.e. it checks if the input is null and returns null). But I went with this approach since it would be robust against custom implementations of the MonitorQuerySerializer interface that also do not check for null like the fromParser implementation. I introduced testGetQueryNotPresent which fails on master with the NullPointerException, but passes on this branch. I also added a corresponding testGetQueryPresent which passes on both master and this branch. Since I needed a monitor with persistence for these two new tests I refactored creating a monitor with persistence into a helper function newMonitorWithPersistence that gets reused throughout the TestMonitorPersistence file.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12770", "change_description": ": Stop exploring HNSW graph if scores are not getting better.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "I noticed while testing lower dimensionality and quantization, we would explore the HNSW graph way too much. I was stuck figuring out why until I noticed the searcher checks for distance equality (not just if the distance is better) when exploring neighbors-of-neighbors. This seems like a bad heurstic, but to double check I looked at what nmslib does. This pointed me back to this commit: nmslib/nmslib#106 Seems like this performance hitch was discovered awhile ago :). This commit adjusts HNSW to only explore the graph layer if the distance is actually better.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12640", "change_description": ": Ensure #finish is called on all drill-sideways collectors even if one throws a\nCollectionTerminatedException", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "As DrillSidewaysScorer is currently written, if any leaf collectors throw CollectionTerminatedException then LeafCollector#finish won't properly get called. This patch makes sure we always call #finish .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Bug Fixes", "change_id": "GITHUB#12626", "change_description": ": Fix segmentInfos replace to set userData", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "The replace method of SegmentInfos does not overwrite the userData field. This PR fixes that bug. Relevant issue is opened here. This closes #12637", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Build", "change_id": "GITHUB#12752", "change_description": ": tests.multiplier could be omitted in test failure reproduce lines (esp. in\nnightly mode).", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "LuceneTestCase computes its default value of tests.multiplier from TESTS_NIGHTLY. This could lead to subtle errors: nightly mode failures would not report tests.multipler=1 and when started from the IDE, the tests.multiplier would be set to 2 (leading to different randomness). LuceneTestCase now compares the actual value of the multiplier to the \"default\" value computed based off TESTS_NIGHTLY. I also opted to change gradle's test defaults to not pass tests.multiplier at all, unless specified explicitly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Build", "change_id": "GITHUB#12742", "change_description": ": JavaCompile tasks may be in up-to-date state when modular dependencies have changed\nleading to odd runtime errors", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Recent builds failed with: As Hoss noticed, there is a repro to get the above error: I'll try to figure out why this can happen, it shouldn't. No response The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Build", "change_id": "GITHUB#12612", "change_description": ": Upgrade forbiddenapis to version 3.6 and ASM for APIJAR extraction to 9.6.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "the usual maintenance. Allows us to process Java 22 APIs soon.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Build", "change_id": "GITHUB#12655", "change_description": ": Upgrade to Gradle 8.4", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "Upgrade Gradle from 7.6 to 8.4 - supports building directly with JDK 21 LTS. https://docs.gradle.org/8.4/release-notes.html The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Build", "change_id": "GITHUB#12845", "change_description": ": Only enable support for tests.profile if jdk.jfr module is available\nin Gradle runtime.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "If you use OpenJ9 JDKs to run Gradle and Tests, the compilation of buildSrc fails, due to Java Flight recorder not being available in the Gradle runtime. This PR fixes this by excluding the Profile code from compilation and also throwing an exception if you enable -Ptests.profile=true . I tested this with OpenJ9 and Hotspot. On Jenkins we do not see that problem, because we always run Gradle with a Hotspot JDK and only enable OpenJ9 as runtime. This issue also affects the OpenJ9 team when they try to reproduce bugs with OpenJ9 VM: eclipse-openj9/openj9#18400", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Other", "change_id": "GITHUB#12817", "change_description": ": Add demo for faceting with StringValueFacetCounts over KeywordField and SortedDocValuesField.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "We don't have a demo for faceting using KeywordField , SortedDocValuesField , or StringValueFacetCounts . This PR adds one, mostly inspired by SimpleSortedSetFacetsExample .", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Other", "change_id": "GITHUB#12657", "change_description": ": Internal refactor of HNSW graph merging", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "While working on the quantization codec & thinking about how merging will evolve, it became clearer that having merging attached directly to the vector writer is weird. I extracted it out to its own class and removed the \"initializedNodes\" logic from the base class builder. Also, there was on other refactoring around grabbing sorted nodes from the neighbor iterator, I just moved that static method so its not attached to the writer (as all bwc writers need it and all future HNSW writers will as well).", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Other", "change_id": "GITHUB#12625", "change_description": ": Refactor ByteBlockPool so it is just a \"shift/mask big array\".", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "While working in the code base I stumble with this TODO and this issue so I give it a try to simplify ByteBlockPool so it does not contain specific logic for terms. The result here is that I moved all the  hairy allocSlice stuff as static method in TermsHashPerField and I introduce a BytesRefBlockPool to encapsulate of the BytesRefHash write/read logic. I updated javadocs accordingly.", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Other", "change_id": "GITHUB#6675", "change_description": ": Various improvements related to ByteBlockPool. Slice functionality on top of ByteBlockPool moved to its\nown class, ByteSlicePool. ByteBlockPool's array of buffers is made private. There are new exceptions for buffer index\noverflows and slices that are too large. Some bits of code are simplified. Documentation is updated and expanded.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "This is a nice reusable component yet the documentation for it is completely useless. There are also API elements that seem to be remnants of an ancient past. Can we clean it up for 5.x? Migrated from LUCENE-5613 by Dawid Weiss ( @dweiss ) The text was updated successfully, but these errors were encountered:", "patch_link": "none", "patch_content": "none"}
{"library_version": "9.9.0", "change_type": "Other", "change_id": "GITHUB#12762", "change_description": ": Refactor BKD HeapPointWriter to hide the internal data structure.", "change_title": "none", "detail_type": "none", "detail_affect_versions": "none", "detail_fix_versions": "none", "detail_description": "HeapPointWriter uses a a byte array to hold points on heap. This array is access directly from BKD radix selector to sort points in place. In addition the HeapPointReader uses the array to read the points too. I wanted to check the performance impact of changing this data structure as this array can be very big which plays bad with garbage collector like G1 as the array will be considered humongous. The current structure of HeapPointWriter makes this very difficult. This PR refactor HeapPointWriter by adding methods for in place sorting and changes HeapPointReader to reads points using  a functional interface. It encapsulates the internal byte array inside HeapPointWriter. The nice side effect is it removes some code duplication too.", "patch_link": "none", "patch_content": "none"}

{"library_version": "7.7.2", "change_type": "Bug fixes", "change_id": "LUCENE-8726", "change_description": ": ValueSource.asDoubleValuesSource() could leak a reference to\nIndexSearcher", "change_title": "WrappedDoubleValuesSource can leak IndexSearcher references", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7.2,8.1", "detail_description": "Cause of SOLR-13315 Index-specific DoubleValuesSources can be created by DoubleValuesSource.rewrite(), and the various consumers of these sources are careful not to store these rewritten sources on long-lived objects, such as queries that may be re-used between searchers.  However, the bridge code between ValueSource and DoubleValuesSource does not return a new object from its rewrite method, instead caching the passed-in IndexSearcher, which means references to this searcher may leak.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962458/LUCENE-8726.patch", "patch_content": "none"}
{"library_version": "7.7.2", "change_type": "Bug fixes", "change_id": "LUCENE-8735", "change_description": ": FilterDirectory.getPendingDeletions now forwards to the delegate\neven the method is not abstract in the super class. This prevents issues\nwhere our best effort in carrying on generations in the IndexWriter since pending\ndeletions are swallowed by the FilterDirectory.", "change_title": "FileAlreadyExistsException after opening old commit", "detail_type": "Bug", "detail_affect_versions": "8.0", "detail_fix_versions": "7.7.1,7.7.2,8.0.1,8.1,9.0", "detail_description": "FilterDirectory.getPendingDeletes() does not delegate calls. This in turn means that IndexFileDeleter does not consider those as relevant files. When opening an IndexWriter for an older commit, excess files are attempted deleted. If an IndexReader exists using one of the newer commits, the excess files may fail to delete (at least on windows or when using the mocking WindowsFS). If then closing and opening the IndexWriter, the information on the pending deletes are gone if a FilterDirectory derivate is used. At the same time, the pending deletes are filtered out of listAll. This leads to a risk of hitting an existing file name, causing a FileAlreadyExistsException. This issue likely only exists on windows. Will create pull request with fix.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.2", "change_type": "Bug fixes", "change_id": "LUCENE-8688", "change_description": ": TieredMergePolicy#findForcedMerges now tries to create the\ncheapest merges that allow the index to go down to `maxSegmentCount` segments\nor less.", "change_title": "Forced merges merge more than necessary", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "A user reported some surprise after the upgrade to Lucene 7.5 due to changes to how forced merges are selected when maxSegmentCount is greater than 1. Before 7.5 forceMerge used to pick up the least amount of merging that would result in an index that has maxSegmentCount segments at most. Now that we share the same logic as regular merges, we are almost sure to pick a maxMergeAtOnceExplicit-segments merge (30 segments) given that merges that have more segments usually score better. This is due to the fact that natural merges assume that merges that run now save work for later, so the more segments get merged, the better. This assumption doesn't hold for forced merges that should run on read-only indices, so there won't be any future merging.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962381/LUCENE-8688.patch", "patch_content": "none"}
{"library_version": "7.7.2", "change_type": "Bug fixes", "change_id": "LUCENE-8785", "change_description": ": Ensure new threadstates are locked before retrieving the number of active threadstates.\nThis causes assertion errors and potentially broken field attributes in the IndexWriter when\nIndexWriter#deleteAll is called while actively indexing.", "change_title": "TestIndexWriterDelete.testDeleteAllNoDeadlock failure", "detail_type": "Bug", "detail_affect_versions": "7.6", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "I was running Lucene's core tests on an i3.16xlarge EC2 instance (64 cores), and hit this random yet spooky failure: It does not reproduce unfortunately ... but maybe there is some subtle thread safety issue in this code ... this is a hairy part of Lucene", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.7.2", "change_type": "Bug fixes", "change_id": "LUCENE-8720", "change_description": ": NameIntCacheLRU (in the facets module) had an int\noverflow bug that disabled cleaning of the cache", "change_title": "Integer overflow bug in NameIntCacheLRU.makeRoomLRU()", "detail_type": "Bug", "detail_affect_versions": "7.7.1", "detail_fix_versions": "7.7.2,8.1,9.0", "detail_description": "The NameIntCacheLRU.makeRoomLRU() method has an integer overflow bug because if maxCacheSize >= Integer.MAX_VALUE/2, 2*maxCacheSize will overflow to -(2^30) and the value of n will overflow to a negative integer as well, which will prevent any clearing of the cache whatsoever. Hence, performance will degrade once the cache becomes full because it will be impossible to remove any entries in order to add new entries to the cache. Moreover, comments in NameIntCacheLRU.java and LruTaxonomyWriterCache.java indicate that 2/3 of the cache will be cleared, whereas in fact only 1/3 of the cache is cleared. So as not to change the behavior of the NameIntCacheLRU.makeRoomLRU() method, I have not changed the code to clear 2/3 of the cache but instead I have changed the comments to indicate that 1/3 of the cache is cleared.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12962014/LUCENE-NNNN.patch", "patch_content": "none"}
{"library_version": "7.7.2", "change_type": "Bug fixes", "change_id": "LUCENE-8809", "change_description": ": Refresh and rollback concurrently can leave segment states unclosed", "change_title": "Refresh and rollback concurrently can leave segment states unclosed", "detail_type": "Bug", "detail_affect_versions": "7.7,8.1,8.2", "detail_fix_versions": "7.7.2,9.0,8.2,8.1.2", "detail_description": "A failed test from Elasticsearch shows that refresh and rollback concurrently can leave segment states unclosed leads to leaking refCount of some SegmentReaders.", "patch_link": "none", "patch_content": "none"}

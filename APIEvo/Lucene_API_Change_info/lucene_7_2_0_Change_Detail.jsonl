{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8039", "change_description": ": Introduce a \"delta distance\" method set to GeoDistance.  This\nallows distance calculations, especially for paths, to take into account an\n\"excursion\" to include the specified point.", "change_title": "Geo3d: Need a \"distance delta\" distance metric for paths and circles", "detail_type": "Improvement", "detail_affect_versions": "7.1", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "The current \"distance\" method for a path returns a distance computed along the path and then perpendicular to the path.  But, at least in the case of paths, it is often preferable to compute a \"delta\" distance, which would be the minimum straight-line distance assuming a diversion to reach the provided point. A similar \"distance delta\" for a circle would be defined as returning a number exactly the same as is currently returned, with the understanding that the point given would be the destination and not a new waypoint.  Similarly, the distance beyond the end of a path to the provided point would be counted only once, while the distance before the beginning of the path would be counted twice (one leg to the point, and the other leg back from that point to the start point (or nearest path point, if closer). This obviously must be implemented in a backwards-compatible fashion.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8017", "change_description": ",", "change_title": "FunctionRangeQuery and FunctionMatchQuery can pollute the QueryCache", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "The QueryCache assumes that queries will return the same set of documents when run over the same segment, independent of all other segments held by the parent IndexSearcher.  However, both FunctionRangeQuery and FunctionMatchQuery can select hits based on score, which depend on term statistics over the whole index, and could therefore theoretically return different result sets on a given segment.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12895604/LUCENE-8017.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8042", "change_description": ",", "change_title": "Add SegmentCachable interface", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "Following LUCENE-8017, I tried to add a getCacheHelper(LeafReaderContext) method to DoubleValuesSource so that Weights that use DVS can delegate on.  This ended up with the same method being added to LongValuesSource, and some of the similar objects in spatial-extras.  I think it makes sense to abstract this out into a separate SegmentCachable interface.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896675/LUCENE-8042.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8038", "change_description": ": Payload factors for scoring in PayloadScoreQuery are now\ncalculated by a PayloadDecoder, instead of delegating to the Similarity.", "change_title": "Decouple payload decoding from Similarity", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "PayloadScoreQuery is the only place that currently uses SimScorer.computePayloadFactor(), and as discussed on LUCENE-8014, this seems like the wrong place for it.  We should instead add a PayloadDecoder abstraction that is passed to PayloadScoreQuery.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896171/LUCENE-8038-master.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8014", "change_description": ": Similarity.computeSlopFactor() and\nSimilarity.computePayloadFactor() have been deprecated.", "change_title": "Remove SimScorer.computeSlopFactor and SimScorer.computePayloadFactor", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "This supersedes LUCENE-8013. We should hardcode computeSlopFactor to 1/(N+1) in SloppyPhraseScorer and move computePayloadFactor to PayloadFunction so that all the payload scoring logic is in a single place.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12897065/LUCENE-8014-tfidfsim.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-6278", "change_description": ": Scorer.freq() has been removed", "change_title": "Remove Scorer.freq()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "At the moment, Scorer.freq() does different things depending on the Scorer implementation: I think this is confusing.  I propose that we instead add a new coord() method to Scorer that by default returns 1, and that is overridden by the boolean scorers; and that we just remove freq() entirely.  PhraseWeight.explain() can call a package-private method on XPhraseScorer.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12897307/LUCENE-6278.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-7736", "change_description": ": DoubleValuesSource and LongValuesSource now expose a\nrewrite(IndexSearcher) function.", "change_title": "Expose some IndexReader stats via DoubleValuesSources", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "We have a number of ValueSource implementations that expose IndexReader stats (numDocs, termFreq, etc).  We should re-implement these as DoubleValuesSources, allowing them to be used in FunctionScoreQuery, etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12897342/LUCENE-7736.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-7998", "change_description": ": DoubleValuesSource.fromQuery() allows you to use the scores\nfrom a Query as a DoubleValuesSource.", "change_title": "Add a DoubleValuesSource that exposes a Query score", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "This will allow us to reproduce the behaviour of BoostingQuery with a FunctionScoreQuery, either by passing a simple wrapped Query as a single DVS or by combining several of them using the expressions module.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12893026/LUCENE-7998.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8049", "change_description": ": IndexWriter.getMergingSegments()'s return type was changed from\nCollection to Set to more accurately reflect it's nature.", "change_title": "IndexWriter.getMergingSegments should return a Set, not Collection", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "IndexWriter.getMergingSegments is called by the MergePolicy so that it can filter out segments to merge using mergingSegments.contains(seg).  The implementation is a HashSet, thankfully but the declared type of this method ought to be defined as a Set as well.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898519/LUCENE_8049.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "API Changes", "change_id": "LUCENE-8059", "change_description": ": TopFieldDocCollector can now early terminate collection when\nthe sort order is compatible with the index order. As a consequence,\nEarlyTerminatingSortingCollector is now deprecated.", "change_title": "Fold early termination support into TopFieldCollector", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "We should make early termination of requests easier to use.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12899018/LUCENE-8059.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "New Features", "change_id": "LUCENE-8061", "change_description": ": Add convenience factory methods to create BBoxes and XYZSolids\ndirectly from bounds objects.", "change_title": "Factory method to create Bbox from bounds object", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "Hi daddywri, I propose a new factory method in the bounding box factory from a LatLonBounds object. It is an utility method but I have the feeling to have to always write it when trying to build a bounding box so it may be worthy to add it. I will attach a patch.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898920/LUCENE_8061.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "New Features", "change_id": "LUCENE-7736", "change_description": ": IndexReaderFunctions expose various IndexReader statistics as\nDoubleValuesSources.", "change_title": "Expose some IndexReader stats via DoubleValuesSources", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "We have a number of ValueSource implementations that expose IndexReader stats (numDocs, termFreq, etc).  We should re-implement these as DoubleValuesSources, allowing them to be used in FunctionScoreQuery, etc.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12897342/LUCENE-7736.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "New Features", "change_id": "LUCENE-8068", "change_description": ": Allow IndexWriter to write a single DWPT to disk Adds a\nflushNextBuffer method to IndexWriter that allows the caller to\nsynchronously move the next pending or the biggest non-pending index buffer to\ndisk. This enables flushing selected buffer to disk without highjacking an\nindexing thread. This is for instance useful if more than one IW (shards) must\nbe maintained in a single JVM / system.", "change_title": "Allow IndexWriter to write a single DWPT to disk", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "Today we IW can only flush a DWPT to disk if an external resource calls flush()  or refreshes a NRT reader or if a DWPT is selected as flush pending. Yet, the latter has the problem that it always ties up an indexing thread and if flush / NRT refresh is called a whole bunch of indexing threads is tied up. If IW could offer a simple `flushNextBuffer()` method that synchronously flushes the next pending or biggest active buffer to disk memory could be controlled in a more fine granular fashion from outside of the IW. This is for instance useful if more than one IW (shards) must be maintained in a single JVM / system.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12899864/LUCENE-8068.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8076", "change_description": ": Normalize Vincenti distance calculation for planet models that aren't normalized.", "change_title": "Planet should take into account the scale of the ellipsoid", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "Hi Karl wright, PlanetModel should take into account the scale of the planet when calculation points on bearing. In addition surfaceDistance should take into account the scale if distance is returned in radians but that might not be the desire behavior.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12900368/LUCENE-8076.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8057", "change_description": ": Exact circle bounds computation was incorrect.", "change_title": "Bounds not properly computed for exact circles", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "Hi daddywri, I still get some errors in the tests but luckily it is not related with the last change. The errors always happen between complex polygons and exact circles. I look into it and the problem is that complex polygons require that shapes compute the correct bounds to compute intersections. This is not the case for exact circles. It seems a generic bug so I am not attaching a test case. But if you want I can try to work up something.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898869/LUCENE_8057.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8056", "change_description": ": Exact circle segment bounding suffered from precision errors.", "change_title": "Intersects for GeoExactCircle does not trigger in some cases", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "Hi daddywri, It seems there is another issue with exact circles that shows up in the tests. It seems that in some cases the method insertsects() do not provide the right results so the relationship is wrongly computed. I attach a test showing the issue in a moment. Cheers!", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898819/LUCENE_8056.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8054", "change_description": ": Fix the exact circle case where relationships fail when the\nplanet model has c <= ab, because the planes are constructed incorrectly.", "change_title": "Test failure, Geo3dRptTest", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "6.7,7.2,8.0", "detail_description": "Geo3dRptTest.testOperations fails with seed 39BCAE475BCFB043 CC kwright@metacarta.com ivera", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898662/LUCENE_8054_randomTest.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7991", "change_description": ": KNearestNeighborDocumentClassifier.knnSearch no longer applies\na previous boosted field's factor to subsequent unboosted fields.", "change_title": "KNearestNeighborDocumentClassifier.knnSearch applies previous boosted field's factor to subsequent unboosted fields", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "When reading code noticed that in KNearestNeighborClassifier a neutral boost factor is restored but in KNearestNeighborDocumentClassifier this currently does not happen. This seems unintended.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12891242/LUCENE-7991.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-7999", "change_description": ": Switch from int to long to track the name for the next\nsegment to write, so that very long lived indices with very frequent\nrefreshes or commits, and high indexing thread counts, do not\noverflow an int", "change_title": "Invalid segment file name", "detail_type": "Bug", "detail_affect_versions": "7.0", "detail_fix_versions": "7.2,8.0", "detail_description": "After really long and intensive index usage its possible to overflow counter that used to generate new segment name that will not satisfy validation criteria: Caused by: java.lang.IllegalArgumentException: invalid codec filename '-zik0zk_Lucene54_0.dvm', must match: _[a-z0-9]+(.)?\\..     at org.apache.lucene.index.SegmentInfo.checkFileNames(SegmentInfo.java:280)     at org.apache.lucene.index.SegmentInfo.addFiles(SegmentInfo.java:262)     at org.apache.lucene.index.SegmentInfo.setFiles(SegmentInfo.java:256)     at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4080)     at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3655)     at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)     at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12893619/LUCENE-7999.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8025", "change_description": ": Use sumTotalTermFreq=sumDocFreq when scoring DOCS_ONLY fields\nthat omit term frequency information, as it is equivalent in that case.\nPreviously bogus numbers were used, and many similarities would\ncompletely degrade.", "change_title": "compute avgdl correctly for DOCS_ONLY", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "Spinoff of LUCENE-8007: If you omit term frequencies, we should score as if all tf values were 1. This is the way it worked for e.g. ClassicSimilarity and you can understand how it degrades. However for sims such as BM25, we bail out on computing avg doclength (and just return a bogus value of 1) today, screwing up stuff related to length normalization too, which is separate. Instead of a bogus value, we should substitute sumDocFreq for sumTotalTermFreq (all postings have freq of 1, since you omitted them).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12894904/LUCENE-8025.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8045", "change_description": ": ParallelLeafReader did not correctly report FieldInfo.dvGen", "change_title": "ParallelReader does not propagate doc values generation numbers", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "Exposed by this test failure: https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/777/testReport/junit/org.apache.lucene.search/TestLRUQueryCache/testDocValuesUpdatesDontBreakCache/ A reader is randomly wrapped with a ParallelLeafReader, which does not then correctly propagate the dvGen into its own FieldInfo.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896849/LUCENE-8045.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8034", "change_description": ": Use subtraction instead of addition to sidestep int\noverflow in SpanNotQuery.", "change_title": "SpanNotWeight returns wrong results due to integer overflow", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "In SpanNotQuery, there is an acceptance condition: This overflows in case `candidate.endPosition() + post > Integer.MAX_VALUE`. I have a fix for this which I am working on. Basically I am flipping the add to a subtract.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896273/LUCENE-8034.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8078", "change_description": ": The query cache should not cache instances of\nMatchNoDocsQuery.", "change_title": "The query cache should not cache MatchNoDocsQuery", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "This was reported by Jon Harper at markmail.org/message/xfxvkeynil7olwmz.", "patch_link": "none", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Bug Fixes", "change_id": "LUCENE-8048", "change_description": ": Filesystems do not guarantee order of directories updates", "change_title": "Filesystems do not guarantee order of directories updates", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.2", "detail_description": "Currently when index is written to disk the following sequence of events is taking place: This sequence leads to potential window of opportunity for system to crash after 'rename list of segments' but before 'sync index directory' and depending on exact filesystem implementation this may potentially lead to 'list of segments' being visible in directory while some of the segments are not. Solution to this is to sync index directory after all segments have been written. This commit shows idea implemented. I'm fairly certain that I didn't find all the places this may be potentially happening.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12900708/LUCENE-8048.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8018", "change_description": ": Smaller FieldInfos memory footprint by not retaining unnecessary\nreferences to TreeMap entries.", "change_title": "FieldInfos retains garbage if non-sparse", "detail_type": "Bug", "detail_affect_versions": "6.5", "detail_fix_versions": "7.2,8.0", "detail_description": "A heap dump revealed a lot of TreeMap.Entry instances (millions of them) for a system with about ~1000 active searchers.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12894142/LUCENE-8018.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Optimizations", "change_id": "LUCENE-7994", "change_description": ": Use int/int scatter map to gather facet counts when the\nnumber of hits is small relative to the number of unique facet labels", "change_title": "Use int/int hash map for int taxonomy facet counts", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "Int taxonomy facets today always count into a dense int[], which is wasteful in cases where the number of unique facet labels is high and the size of the current result set is small. I factored the native hash map from LUCENE-7927 and use a simple heuristic (customizable by the user by subclassing) to decide up front whether to count sparse or dense.  I also made loading of the large children and siblings int[] lazy, so that they are only instantiated if you really need them.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12893566/LUCENE-7994.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8062", "change_description": ": GlobalOrdinalsQuery is no longer eligible for caching.", "change_title": "Never cache GlobalOrdinalQuery", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "GlobalOrdinalsQuery holds a possibly large bitset of global ordinals that can pollute the query cache because the size of the query is not accounted in the memory usage of the cache. Moreover two instances of this query must share the same top reader context to be considered equal so they are not the ideal candidate for segment level caching.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898951/LUCENE-8062.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8058", "change_description": ": Large instances of TermInSetQuery are no longer eligible for\ncaching as they could break memory accounting of the query cache.", "change_title": "Never cache large TermInSetQuery instances", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "I have seen several cases in which the query cache was highly underestimating its memory usage due to the fact that it had references to large queries that ended up using more memory than the associated doc id sets. We had a workaround for term-in-set queries by making TermInSetQuery implement Accountable, but this information is lost when it is wrapped in another query such as a BooleanQuery. So I would like to apply a safer fix that just disables caching on large TermInSetQuery instances. I know it's a pity given that large queries are probably more expensive and thus more cache-worthy, but I see such large queries as the result of a bad design or a workaround to the fact that Lucene is not the right tool for the job, so I think that disabling caching on large term-in-set queries is the right trade-off by making the query cache safer for the majority of our users.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12899637/LUCENE-8058.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8055", "change_description": ": MemoryIndex.MemoryDocValuesIterator returns 2 documents\ninstead of 1.", "change_title": "MemoryIndex.MemoryDocValuesIterator returns 2 document instead of 1", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "7.1.1,7.2,8.0", "detail_description": "If there is a DV field in the MemoryIndex the `MemoryIndex.MemoryDocValuesIterator` will return 2 documents instead of 1. Simple off by one error and no tests. I have a patch ready for it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12898647/LUCENE-8055.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Optimizations", "change_id": "LUCENE-8043", "change_description": ": Fix document accounting in IndexWriter to prevent writing too many\ndocuments. Once this happens, Lucene refuses to open the index and throws a\nCorruptIndexException.", "change_title": "Attempting to add documents past limit can corrupt index", "detail_type": "Bug", "detail_affect_versions": "4.10,7.0,8.0", "detail_fix_versions": "7.1.1,7.2,8.0", "detail_description": "The IndexWriter check for too many documents does not always work, resulting in going over the limit.  Once this happens, Lucene refuses to open the index and throws a CorruptIndexException: Too many documents. This appears to affect all versions of Lucene/Solr (the check was first implemented in LUCENE-5843 in v4.9.1/4.10 and we've seen this manifest in 4.10)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12900448/LUCENE-8043.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Tests", "change_id": "LUCENE-8035", "change_description": ": Run tests with JDK-specific options: --illegal-access=deny\non Java 9+.", "change_title": "Adopt JDK options for tests when running Java9+", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "7.2,8.0", "detail_description": "Currently, Policeman Jenkins uses --illegal-access=deny when running tests on Java 9 or later. We should do this by default, so we ensure that nothing uses private APIs of the JDK or tries to do setAccessible() on runtime classes.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12896082/LUCENE-8035.patch", "patch_content": "none"}
{"library_version": "7.2.0", "change_type": "Build", "change_id": "LUCENE-6144", "change_description": ": Upgrade Ivy to 2.4.0; 'ant ivy-bootstrap' now removes old Ivy\njars in ~/.ant/lib/.", "change_title": "Upgrade ivy to 2.4.0", "detail_type": "Task", "detail_affect_versions": "5.0", "detail_fix_versions": "5.5.6,6.6.3,7.0.2,7.1.1,7.2,8.0", "detail_description": "Ivy 2.4.0 is released.  IVY-1489 is likely to still be a problem. I'm not sure whether we have a minimum version check for ivy, or whether we are using any features that require a minimum version check.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12895412/LUCENE-6144.patch", "patch_content": "none"}

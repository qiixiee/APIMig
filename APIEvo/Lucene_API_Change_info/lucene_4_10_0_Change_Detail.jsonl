{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5778", "change_description": ": Support hunspell morphological description fields/aliases.", "change_title": "Support hunspell morphological description fields", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Currently hunspell stemmer doesn't support these (particularly the st:XYZ field which signifies a stemming \"exception\" basically). For example in english \"feet\" might have \"st:foot\". These can be encoded two ways, inline into the .dic or aliased via AM entries from the .aff. Unfortunately, our parsing was really lenient and in order to do this properly (e.g. handling words with spaces and morphological fields containing slashes and all that jazz), it had to be cleaned up a bit to follow the hunspell rules. For now, we dont waste space with part of speech and only concern ourselves with the \"st:\" field and the stemmer uses it transparently. Encoding these exceptions is a little complicated because these exceptions are rarely used, but when they are, they are typically common verbs and stuff (like english 'be'), so we dont want it to be slow.  They are also not \"per-word\" but \"per-form\", so you could have homonyms with different stems (at least theoretically).  On the other hand this is silly stuff particular to these silly languages, so we dont want it to blow up the datastructure for 99% of languages that dont use it. So the way we do it is to just store the exception ID alongside the form ID (this doubles the intsref, which is usually 1). So for e.g. english i think it typically boils down to an extra byte or so in the FST and doesn't blow up. For languages not using this stuff there is no impact.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651428/LUCENE-5778.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5801", "change_description": ": Added (back) OrdinalMappingAtomicReader for merging search\nindexes that contain category ordinals from separate taxonomy indexes.", "change_title": "Resurrect org.apache.lucene.facet.util.OrdinalMappingAtomicReader", "detail_type": "Bug", "detail_affect_versions": "4.7", "detail_fix_versions": "4.10,6.0", "detail_description": "from lucene > 4.6.1 the class: org.apache.lucene.facet.util.OrdinalMappingAtomicReader was removed; resurrect it because used merging indexes related to merged taxonomies.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12656792/LUCENE-5801.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-4175", "change_description": ",", "change_title": "Include BBox Spatial Strategy", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "This is an approach to indexing bounding boxes using 4 numeric fields (xmin,ymin,xmax,ymax) and a flag to say if it crosses the dateline. This is a modification from the Apache 2.0 code from the ESRI Geoportal: http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialClauseAdapter.java", "patch_link": "https://issues.apache.org/jira/secure/attachment/12533786/LUCENE-4175-bbox-strategy.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5714", "change_description": ",", "change_title": "Improve tests for BBoxStrategy then port to 4x.", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "BBoxStrategy needs better tests before I'm comfortable seeing it in 4x.  Specifically it should use random rectangles based validation (ones that may cross the dateline), akin to the other tests.  And I think I see an equals/hashcode bug to be fixed in there too. One particular thing I'd like to see added is how to handle a zero-area case for AreaSimilarity.  I think an additional feature in which you declare a minimum % area (relative to the query shape) would be good. It should be possible for the user to combine rectangle center-point to query shape center-point distance sorting as well.  I think it is but I need to make sure it's possible without having to index a separate center point field. Another possibility (probably not to be addressed here) is a minimum ratio between width/height, perhaps 10%.  A long but nearly no height line should not be massively disadvantaged relevancy-wise to an equivalently long diagonal road that has a square bbox.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651603/LUCENE-5714__Enhance_BBoxStrategy__more_tests%2C_fix_dateline_bugs%2C_new_AreaSimilarity_algor.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5779", "change_description": ",", "change_title": "Improve BBox AreaSimilarity algorithm to consider lines and points", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "GeoPortal's area overlap algorithm didn't consider lines and points; they end up turning the score 0.  I've thought about this for a bit and I've come up with an alternative scoring algorithm.  (already coded and tested and documented): New Javadocs:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651441/LUCENE-5779__Improved_bbox_AreaSimilarity_algorithm.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5806", "change_description": ": Extend expressions grammar to support array access in variables.\nAdded helper class VariableContext to parse complex variable into pieces.", "change_title": "Extend expression grammar to allow advanced \"variables\"", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "We currently allow dots in \"variable\" names in expressions, so that we can fake out object access.  We should extend this to allow array access as well (both integer and string keys).  This would allow faking out full object nesting through bindings.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12654384/LUCENE-5806.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5826", "change_description": ": Support proper hunspell case handling, LANG, KEEPCASE, NEEDAFFIX,\nand ONLYINCOMPOUND flags.", "change_title": "Support proper hunspell case handling and related options", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "When ignoreCase=false, we should accept title-cased/upper-cased forms just like hunspell -m. Furthermore there are some options around this: While we are here setting up the same logic anyway, add support for similar  options: This stuff is unrelated to the ignoreCase=true option. If you use that option though, it does use correct alternate casing for tr_TR/az_AZ now though. I didn't yet implement CHECKSHARPS because it seems more complicated, I have to figure out what the logic there should be first.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655983/LUCENE-5826.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5815", "change_description": ": Add TermAutomatonQuery, a proximity query allowing you\nto create an arbitrary automaton, using terms on the transitions,\nexpressing which sequence of sequential terms (including a special\n\"any\" term) are allowed.  This is a generalization of\nMultiPhraseQuery and span queries, and enables \"correct\" (including\nposition) length search-time graph synonyms.", "change_title": "Add TermAutomatonQuery, for proximity matching that generalizes MultiPhraseQuery/SpanNearQuery", "detail_type": "New Feature", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "I created a new query, called TermAutomatonQuery, that's a proximity query to generalize MultiPhraseQuery/SpanNearQuery: it lets you construct an arbitrary automaton whose transitions are whole terms, and then find all documents that the automaton matches.  This is different from a \"normal\" automaton whose transitions are usually bytes/characters within a term/s. So, if the automaton has just 1 transition, it's just an expensive TermQuery.  If you have two transitions in sequence, it's a phrase query of two terms.  You can express synonyms by using transitions that overlap one another but the automaton doesn't have to be a \"sausage\" (as MultiPhraseQuery requires) i.e. it \"respects\" posLength (at query time). It also allows \"any\" transitions, to match any term, so you can do sloppy matching and span-like queries, e.g. find \"lucene\" and \"python\" with up to 3 other terms in between. I also added a class to convert a TokenStream directly to the automaton for this query, preserving posLength.  (Of course, the index can't store posLength, so the matching won't be fully correct if any indexed tokens has posLength != 1).  But if you do query-time-only synonyms then the matching should finally be correct. I haven't tested performance but I suspect it's quite slowish ... its cost is O(sum-totalTF) of all terms \"used\" in the automaton.  There are some optimizations we could do, e.g. detecting that some terms in the automaton can be upgraded to MUST (right now they are all effectively SHOULD). I'm not sure how it should assign scores (punted on that for now), but the matching seems to be working.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655545/LUCENE-5815.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5819", "change_description": ": Add OrdsLucene41 block tree terms dict and postings\nformat, to include term ordinals in the index so the optional\nTermsEnum.ord() and TermsEnum.seekExact(long ord) APIs work.", "change_title": "Add block tree postings format that supports term ords", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "BlockTree is our default terms dictionary today, but it doesn't support term ords, which is an optional API in the postings format to retrieve the ordinal for the currently seek'd term, and also later seek by that ordinal e.g. to lookup the term. This can possibly be useful for e.g. faceting, and maybe at some point we can share the postings terms dict with the one used by sorted/set DV for cases when app wants to invert and facet on a given field. The older (3.x) block terms dict can easily support ords, and we have a Lucene41OrdsPF in test-framework, but it's not as fast / compact as block-tree, and doesn't (can't easily) implement an optimized intersect, but it could be for fields we'd want to facet on, these tradeoffs don't matter.  It's nice to have options...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655781/LUCENE-5819.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5835", "change_description": ": TermValComparator can sort missing values last.", "change_title": "Add sortMissingLast support to TermValComparator", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "It would be nice to allow to configure the behavior on missing values for this comparator, similarly to what TermOrdValComparator does.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12656721/LUCENE-5835.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5825", "change_description": ": Benchmark module can use custom postings format, e.g.:\n codec.postingsFormat=Memory", "change_title": "Allowing the benchmarking algorithm to choose PostingsFormat", "detail_type": "Improvement", "detail_affect_versions": "6.0", "detail_fix_versions": "4.10,6.0", "detail_description": "The algorithm file for benchmarking should allow PostingsFormat to be configurable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12657280/LUCENE-5825.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5842", "change_description": ": When opening large files (where its to expensive to compare\nchecksum against all the bytes), retrieve checksum to validate structure\nof footer, this can detect some forms of corruption such as truncation.", "change_title": "Validate checksum footers for postings lists, docvalues, storedfields, termvectors on init", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "For small files (e.g. where we read in all the bytes anyway), we currently validate the checksum on reader init. But for larger files like .doc/.frq/.pos/.dvd/.fdt/.tvd we currently do nothing at all on init, as it would be too expensive. We should at least do this:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12657350/LUCENE-5842.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "New Features", "change_id": "LUCENE-5739", "change_description": ": Added DataInput.readZ(Int|Long) and DataOutput.writeZ(Int|Long)\nto read and write small signed integers.", "change_title": "Add zig-zag encoding support to DataInput/DataOutput", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "It would be convenient to have support for zig-zag-encoded integers in DataInput/DataOutput. There are not many places that use that feature today but I think it's quite common to need to read/write small signed values.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12658887/LUCENE-5739.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5752", "change_description": ": Simplified Automaton API to be immutable.", "change_title": "Explore light weight Automaton replacement", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "This effort started with the patch on LUCENE-4556, to create a \"light weight\" replacement for the current object-heavy Automaton class (which creates separate State and Transition objects). I took that initial patch much further, and cutover most places in Lucene that use Automaton to LightAutomaton.  Tests pass. The core idea of LightAutomaton is all states are ints, and you build up the automaton under the restriction that you add all outgoing transitions one state at a time.  This worked well for most operations, but for some (e.g. UTF32ToUTF8!!) it was harder, so I also added a separate builder to add transitions in any order and then in the end they are sorted and added to the real automaton. If this is successful I think we should just replace the current Automaton with LightAutomaton; right now they both exist in my current patch... This is very much a work in progress, and I'm not sure the restrictions the API imposes are \"reasonable\" (some algos got uglier). But I think it's at least worth exploring/iterating... I'll make a branch and commit my current state.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651171/LUCENE-5752.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5793", "change_description": ": Add equals/hashCode to FieldType.", "change_title": "Add equals/hashCode to FieldType", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "would be nice to have equals and hashCode to FieldType, so one can easily check if they are the same, and for example, reuse existing default implementations of it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12653458/LUCENE-5793.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5692", "change_description": ": DisjointSpatialFilter is deprecated (used by RecursivePrefixTreeStrategy)", "change_title": "Deprecate spatial DisjointSpatialFilter", "detail_type": "Bug", "detail_affect_versions": "6.0", "detail_fix_versions": "4.10,6.0", "detail_description": "The spatial predicate \"IsDisjointTo\" is almost the same as the inverse of \"Intersects\", except that it shouldn't match documents without spatial data.  In another sense it's as if the query shape were inverted. DisjointSpatialFilter is a utility filter that works (or worked, rather) by using the FieldCache to see which documents have spatial data (getDocsWithField()). Calculating that was probably very slow but it was at least cacheable. Since LUCENE-5666 (v5/trunk only), Rob replaced this to use DocValues.  However for some SpatialStrategies (PrefixTree based) it wouldn't make any sense to use DocValues just so that at search time you could call getDocsWithField() when there's no other need for the un-inversion (e.g. no need to lookup terms by document). Perhaps an immediate fix is simply to revert the change made to DisjointSpatialFilter so that it uses the FieldCache again, if that works (though it's not public?).  But stepping back a bit, this DisjointSpatialFilter is really something unfortunate that doesn't work as well as it could because it's not at the level of Solr or ES – that is, there's no access to a filter-cache.  So I propose I simply remove it, and if a user wants to do this for real, they should index a boolean field marking wether there's spatial data and then combine that with a NOT and Intersects, in a straight-forward way. Alternatively, some sort of inverting query shape could be developed, although it wouldn't work with the SpatialPrefixTree technique because there is no edge distinction – the edge matches normally and notwithstanding changes to RPT algorithms it would also match the edge of an inverted shape.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12654167/LUCENE-5692_Deprecate_DisjointSpatialFilter__RPT_stops_using_it.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5771", "change_description": ": SpatialOperation's predicate names are now aliased to OGC standard names.\nThus you can use: Disjoint, Equals, Intersects, Overlaps, Within, Contains, Covers,\nCoveredBy. The area requirement on the predicates was removed, and Overlaps' definition\nwas fixed.", "change_title": "Review semantics of SpatialOperation predicates", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "SpatialOperation (which I wish was named SpatialPredicate) is a bunch of predicates – methods that return true/false based on a pair of shapes.  Some of them don't seem to be defined in a way consistent with their definitions on ESRI's site: http://edndoc.esri.com/arcsde/9.1/general_topics/understand_spatial_relations.htm  (which is linked as a reference, and is in turn equivalent to OGC spec definitions, I believe). Problems:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651674/LUCENE-5771_SpatialOperation_semantics.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5850", "change_description": ": Made Version handling more robust and extensible. Deprecated Constants.LUCENE_MAIN_VERSION, Constants.LUCENE_VERSION and current Version constants of the form LUCENE_X_Y. Added version constants that include bugfix number of form LUCENE_X_Y_Z.  Changed Version.LUCENE_CURRENT to Version.LATEST. CheckIndex now prints the Lucene version used to write\neach segment.", "change_title": "Constants#LUCENE_MAIN_VERSION can have broken values", "detail_type": "Bug", "detail_affect_versions": "4.3.1,4.5.1", "detail_fix_versions": "4.10,6.0", "detail_description": "Constants#LUCENE_MAIN_VERSION is set to the Lucene Main version and should not contain minor versions. Well this is at least what I thought and to my knowledge what the comments say too. Yet in for instance 4.3.1 and 4.5.1 we broke this such that the version from SegmentsInfo can not be parsed with Version#parseLeniently. IMO we should really add an assertion that this constant doesn't throw an error and / or make the smoketester catch this. to me this is actually a index BWC break. Note that 4.8.1 doesn't have this problem...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12658131/LUCENE-5850_smoketester.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5836", "change_description": ": BytesRef has been splitted into BytesRef, whose intended usage is\nto be just a reference to a section of a larger byte[] and BytesRefBuilder\nwhich is a StringBuilder-like class for BytesRef instances.", "change_title": "BytesRef.copyBytes and copyChars don't oversize", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "None", "detail_description": "When copying data from another BytesRef/CharSequence, these methods don't oversize. This is not an issue if this method is used only once per BytesRef instance but I just reviewed the usage of these methods and they are very frequently used in loops to do things like: Although unlikely, it might be possible to hit a worst-case and to resize the underlying byte[] on every call to copyBytes? Should we oversize the underlying array in these methods?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12656868/LUCENE-5836.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5883", "change_description": ": You can now change the MergePolicy instance on a live IndexWriter,\nwithout first closing and reopening the writer. This allows to e.g. run a special\nmerge with UpgradeIndexMergePolicy without reopening the writer. Also, MergePolicy\nno longer implements Closeable; if you need to release your custom MegePolicy's\nresources, you need to implement close() and call it explicitly.", "change_title": "Move MergePolicy to LiveIndexWriterConfig", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Since LUCENE-5711, MergePolicy is no longer wired to an IndexWriter instance. Therefore it can be moved to be a live setting on IndexWriter, which will allow apps to plug-in an MP on a live IW instance, without closing/reopening the writer. See for example LUCENE-5526 - instead of adding MP to forceMerge, apps could change the MP before calling forceMerge, with e.g. an UpgradeIndexMergePolicy. I think we can also make MergeScheduler a live setting, though I currently don't see the benefits of doing that, so I'd rather not touch it now.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12661652/LUCENE-5883.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "API Changes", "change_id": "LUCENE-5859", "change_description": ": Deprecate Analyzer constructors taking Version.  Use Analyzer.setVersion()\nto set the version an analyzer to replicate behavior from a specific release.", "change_title": "Remove Version from Analyzer constructors", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "This has always been a mess: analyzers are easy enough to make on your own, we don't need to \"take responsibility\" for the users analysis chain for 2 major releases. The code maintenance is horrible here. This creates a huge usability issue too, and as seen from numerous mailing list issues, users don't even understand how this versioning works anyway. I'm sure someone will whine if i try to remove these constants, but we can at least make no-arg ctors forwarding to VERSION_CURRENT so that people who don't care about back compat (e.g. just prototyping) don't have to deal with the horribly complex versioning system. If you want to make the argument that doing this is \"trappy\" (i heard this before), i think thats bogus, and ill counter by trying to remove them. Either way, I'm personally not going to add any of this kind of back compat logic myself ever again. Updated: description of the issue updated as expected. We should remove this API completely. No one else on the planet has APIs that require a mandatory version parameter.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12658752/LUCENE-5859_dead_code.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5780", "change_description": ": Make OrdinalMap more memory-efficient, especially in case the\nfirst segment has all values.", "change_title": "OrdinalMap's mapping from global ords to segment ords is sometimes wasteful", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Robert found a case when the ordinal map can be quite wasteful in terms of memory usage: in order to be able to resolve values given a global ordinals, it stores two things: The issue is that OrdinalMap currently picks any of the segments that contain the value but we can do better: we can pick the first segment that has the value. This will help for two reasons: I just tested on an index where all values are in the first segment and this helped reduce memory usage of the ordinal map by 4x (from 3.5MB to 800KB).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651644/LUCENE-5780.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5782", "change_description": ": OrdinalMap now sorts enums before being built in order to\nimprove compression.", "change_title": "Improve OrdinalMap compression by sorting the supplied terms enums", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "As mentionned in LUCENE-5780, OrdinalMaps might have much better compression when the terms enums are supplied sorted by descending cardinality. When it is not the case, we could sort the enums and re-map segment indices on top of it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12651819/LUCENE-5782.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5798", "change_description": ": Optimize MultiDocsEnum reuse.", "change_title": "minor optimizations to MultiDocs(AndPositions)Enum.reset()", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "This method is called by merging for each term, potentially many times, but only returning a few docs for each invocation (e.g. imagine high cardinality fields, unique id fields, normal zipf distribution on full text). Today we create a new EnumWithSlice[] array and new EnumWithSlice entry for each term, but this creates a fair amount of unnecessary garbage: instead we can just make this array up-front as size subReaderCount and reuse it.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12653223/LUCENE-5798.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5799", "change_description": ": Optimize numeric docvalues merging.", "change_title": "speed up DocValuesConsumer.mergeNumericField", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "This method (used for both numeric docvalues and norms) is a little slow:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12653244/LUCENE-5799.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5797", "change_description": ": Optimize norms merging", "change_title": "improve speed of norms merging", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10", "detail_description": "Today we use the following procedure: This results in each value being hashed twice... but the vast majority of the time people will just be using single-byte norms and a simple array is enough for that range.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12653202/LUCENE-5797.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5803", "change_description": ": Add DelegatingAnalyzerWrapper, an optimized variant\nof AnalyzerWrapper that doesn't allow to wrap components or readers.\nThis wrapper class is the base class of all analyzers that just delegate\nto another analyzer, e.g. per field name: PerFieldAnalyzerWrapper and\nSolr's schema support.", "change_title": "Add another AnalyzerWrapper class that does not have its own cache, so delegate-only wrappers don't create thread local resources several times", "detail_type": "Improvement", "detail_affect_versions": "4.9", "detail_fix_versions": "4.10,6.0", "detail_description": "This is a followup issue for the following Elasticsearch issue: https://github.com/elasticsearch/elasticsearch/pull/6714 Basically the problem is the following: PerFieldAnalyzerWrapper uses PER_FIELD_REUSE_STRATEGY. Because of this it caches the tokenstreams for every field. If there are many fields, this are a lot. In addition, the underlying analyzers may also cache tokenstreams and other PerFieldAnalyzerWrappers do the same, although the delegate Analyzer can always return the same components. We should add similar code to Elasticsearch's directly to Lucene: If the delegating Analyzer just delegates per Field or just wraps CharFilters around the Reader, there is no need to cache the TokenStreamComponents a second time in the delegating Analyzers. This is only needed, if the delegating Analyzers adds additional TokenFilters (like ShingleAnalyzerWrapper). We should name this new class DelegatingAnalyzerWrapper extends AnalyzerWrapper. The wrapComponents method must be final, because we are not allowed to add additional TokenFilters, but unlike ES, we don't need to disallow wrapping with CharFilters. Internally this class uses a private ReuseStrategy that just delegates to the underlying analyzer. It does not matter here if the strategy of the delegate is global or per field, this is private to the delegate.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12654117/LUCENE-5803.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5795", "change_description": ": MoreLikeThisQuery now only collects the top N terms instead\nof collecting all terms from the like text when building the query.", "change_title": "More Like This: ensures selection of best terms is indeed O(n)", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "none", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5681", "change_description": ": Fix RAMDirectory's IndexInput to not do double buffering\non slices (causes useless data copying, especially on random access slices).\nThis also improves slices of NRTCachingDirectory, because the cache\nis based on RAMDirectory. BufferedIndexInput.wrap() was marked with a\nwarning in javadocs. It is almost always a better idea to implement\nslicing on your own!", "change_title": "Fix RAMDirectory's IndexInput to not double-buffer on slice()", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "After LUCENE-4371, we still have a non-optimal implementation of IndexInput#slice() in RAMDirectory. We should fix that to use the cloning approach like other directories do", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655899/LUCENE-5681.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5834", "change_description": ": Empty sorted set and numeric doc values are now singletons.", "change_title": "Make empty doc values impls singletons", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Making these empty instances singletons would allow to use unwrapSingleton to check if they are single-valued.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12656720/LUCENE-5834.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5841", "change_description": ": Improve performance of block tree terms dictionary when\nassigning terms to blocks.", "change_title": "Remove FST.Builder.FreezeTail interface", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "The FST Builder has a crazy-hairy interface called FreezeTail, which is only used by BlockTreeTermsWriter to find appropriate prefixes (i.e. containing enough terms or sub-blocks) to write term blocks. But this is really a silly abuse ... it's cleaner and likely faster/less GC for BTTW to compute this itself just by tracking the term ordinal where each prefix started in the pending terms/blocks.  The code is also insanely hairy, and this is at least a baby step to try to make it a bit simpler. This also makes it very hard to experiment with different formats at write-time because you have to get your new formats working through this strange FreezeTail.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12657374/LUCENE-5841.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5856", "change_description": ": Optimize Fixed/Open/LongBitSet to remove unnecessary AND.", "change_title": "remove useless & 0x3f from *BitSet.get and company", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "java specification says: If the promoted type of the left-hand operand is long, then only the six lowest-order bits of the right-hand operand are used as the shift distance. It is as if the right-hand operand were subjected to a bitwise logical AND operator & (§15.22.1) with the mask value 0x3f (0b111111). The shift distance actually used is therefore always in the range 0 to 63, inclusive. and x86 works the same way. if we remove this, we just see less instructions with printassembly...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12658441/LUCENE-5856.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5884", "change_description": ": Optimize FST.ramBytesUsed.", "change_title": "Speed up FST.ramBytesUsed", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "It's a little heavy now, relying too much on reflection (RUE.shallowSizeOf)... and we do this up to 128 times per FST (= per indexed field, per segment, for the terms index).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12661581/LUCENE-5884.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5882", "change_description": ": Add Lucene410DocValuesFormat, with faster term lookups\nfor SORTED/SORTED_SET fields.", "change_title": "add 4.10 docvaluesformat", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "We can improve the current format in a few ways:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12661712/LUCENE-5882.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Optimizations", "change_id": "LUCENE-5887", "change_description": ": Remove WeakIdentityMap caching in AttributeFactory,\nAttributeSource, and VirtualMethod in favour of Java 7's ClassValue.\nAlways use MethodHandles to create AttributeImpl classes.", "change_title": "Remove horrible WeakIdentityMap caching in AttributeFactory, AttributeSource and VirtualMethod", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Especially the use case in AttributeFactory is horrible: Because of ClassLoader issues we cannot hold strong references (see LUCENE-5640 for explanation), we need WeakIdentityMap<Class, WeakReference<someVal>>. You could say: let's use a strong value for stuff like MethodHandles (used in AttributeFactory), but because those have a strong reference to the class, our reference to key would be strong, so garbage collector can no longer unload the class. This is why we use the WeakReference also on the value. The problem is if the value is something like a MethodHandle, which itsself has hard reference to (so it gets garbage collected). Then the cache is useless. In DefaultAttributeFactory I decided, to make methodhandles strong references, but then I needed to restrict it to our own classloader, otherwise we would have strong references to foreign classloaders. Since Java 7 there is java.lang.ClassValue, that fixes the following JVM bug: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6389107 See also: http://stackoverflow.com/questions/7444420/classvalue-in-java-7 In fact internally, there is a also a WeakReference/WeakHashMap used, but only as fallback - and its only one globally, used by many other JVM internals, too. By default it has a very fast path and the call to ClassValue.get() is incredibly fast. This should therefore also improve AttributeFactory alltogether. Next to AttributeFactory, I also improved the Interfaces cache of AttributeSource (this one assigns an array of Attribute interfaces to an AttributeImpl). The other one is VirtualMethod (assigns its own implementationDistance for every seen subclass). This also removes almost all uses of WeakIdentityMap, the remaining one is the ByteBuffer stuff in MMapDirectory. Unfortunately I have still no idea how to remove that one...", "patch_link": "https://issues.apache.org/jira/secure/attachment/12662040/LUCENE-5887.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5796", "change_description": ": Fixes the Scorer.getChildren() method for two combinations\nof BooleanQuery.", "change_title": "Scorer.getChildren() can throw or hide a subscorer for some boolean queries", "detail_type": "Bug", "detail_affect_versions": "4.9", "detail_fix_versions": "4.10,6.0", "detail_description": "I've isolated two example boolean queries that don't behave with release 4.9 of Lucene. Unit tests and patch based on branch_4x are available and will be attached as soon as this ticket has a number. They are immediately available on GitHub on branch shebiki/bqgetchildren as commit c64bb6f. I took the liberty of naming the relationship in BoostingScorer.getChildren() BOOSTING. Suspect someone will offer a better name for this. Here is a summary of the various relationships in play for all Scorer.getChildren() implementations on branch_4x to help choose. I also removed FilterScorer.getChildren() to prevent mistakes and force subclasses to provide a correct implementation.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12653190/LUCENE-5796.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5790", "change_description": ": Fix compareTo in MutableValueDouble and MutableValueBool, this caused\nincorrect results when grouping on fields with missing values.", "change_title": "broken compareTo in MutableValueDouble and MutableValueBool - affects grouping when exists==false", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "On the solr-user mailing list, Ebisawa & Alex both commented that they've noticed bugs in the grouping code when some documents don't have values in the grouping field. In Ebisawa's case, he tracked this down to what appears to be some bugs in the logic of the \"compareSameType\" method of some of the MutableValue implementations. Thread: https://mail-archives.apache.org/mod_mbox/lucene-solr-user/201406.mbox/%3C84f86fce4b8f42268048aecfb2806a8c@SIXPR04MB045.apcprd04.prod.outlook.com%3E", "patch_link": "https://issues.apache.org/jira/secure/attachment/12654420/LUCENE-5790.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5817", "change_description": ": Fix hunspell zero-affix handling: previously only zero-strips worked\ncorrectly.", "change_title": "hunspell buggy zero-affix handling", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "This only partially works today. But zero-affixes are used heavily by many dictionaries (e.g. i found a good number of bugs in czech and latvian just experimenting). The fix is easy: we just have to look for \"0\" in the affix portion as well as the strip portion, as indicated by the manual page: \"Zero stripping or affix are indicated by zero.\"", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655213/LUCENE-5817.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5818", "change_description": ",", "change_title": "Fix hunspell zero-string overgeneration", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Currently, its allowed to strip suffixes/prefixes all the way down to the empty string. But this is not really allowed, and creates overgeneration in some cases (especially where endings can be standalone ... typically these are stopwords so it causes a lot of damage). Example is czech 'už' which should just stem to itself, but today also stems to 'úžit' because it has a flag compatible with that.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655228/LUCENE-5818.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5823", "change_description": ",", "change_title": "recognize hunspell FULLSTRIP option in affix file", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "With LUCENE-5818 we fixed stripping to be correct (ensuring it doesnt strip the entire word before applying an affix). This is usually true, but there is an option in the affix file to allow this. Its used by several languages (french, latvian, swedish, etc)", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655739/LUCENE-5823.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5824", "change_description": ": Fix hunspell 'long' flag handling.", "change_title": "hunspell FLAG LONG implemented incorrectly", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "If you have more than 256 flags, you run out of 8-bit characters, so you have to use another flag type to get 64k: But our implementation for 'long' is wrong, it encodes as 'A+B', which means it cant distinguish between 'AB' and 'BA' and causes overgeneration.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655759/LUCENE-5824.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5827", "change_description": ": Make all Directory implementations correctly fail with\nIllegalArgumentException if slices are out of bounds.", "change_title": "Make all Directory implementations correctly fail with IllegalArgumentException if slices are out of bounds", "detail_type": "Bug", "detail_affect_versions": "4.8,4.9", "detail_fix_versions": "4.9.1,4.10,6.0", "detail_description": "After implementing LUCENE-5681, I noticed, that some directory implementations (like NIOFSDirectory) do not do bounds checks on slice creation. We should do this to early detect bugs, if file formats break because of index corrumption. This test in BaseDirectoryTestCase does not pass for all directory impls:", "patch_link": "https://issues.apache.org/jira/secure/attachment/12656008/LUCENE-5827.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5838", "change_description": ": Fix hunspell when the .aff file has over 64k affixes.", "change_title": "hunspell buggy with over 64k affixes", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "currently we build TreeMap<String,List<Character>> in ram, to sort before adding to the FST (which encodes the list as IntsRef). char overflows here if there are more than 64k affixes (e.g. basque).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12656861/LUCENE-5838.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5844", "change_description": ": ArrayUtil.grow/oversize now returns a maximum of\nInteger.MAX_VALUE - 8 for the maximum array size.", "change_title": "ArrayUtil.grow should not pretend you can actually allocate array[Integer.MAX_VALUE]", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9.1,4.10,6.0", "detail_description": "Today if the growth it wants would exceed Integer.MAX_VALUE, it returns Integer.MAX_VALUE, but you can't actually allocate arrays this large; the actual limit is JVM dependent and varies across JVMs ... It would be nice if we could somehow \"introspect\" the JVM to find out what its  actual limit is and use that.  http://stackoverflow.com/questions/3038392/do-java-arrays-have-a-maximum-size seems to imply that using Integer.MAX_VALUE - 8 may be \"safe\" (it's what ArrayList.java apparently uses).", "patch_link": "https://issues.apache.org/jira/secure/attachment/12657419/LUCENE-5844.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5843", "change_description": ": Added IndexWriter.MAX_DOCS which is the maximum number\nof documents allowed in a single index, and any operations that add\ndocuments will now throw IllegalStateException if the max count\nwould be exceeded, instead of silently creating an unusable\nindex.", "change_title": "IndexWriter should refuse to create an index with more than INT_MAX docs", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9.1,4.10,6.0", "detail_description": "It's more and more common for users these days to create very large indices, e.g.  indexing lines from log files, or packets on a network, etc., and it's not hard to accidentally exceed the maximum number of documents in one index. I think the limit is actually Integer.MAX_VALUE-1 docs, because we use that value as a sentinel during searching. I'm not sure what IW does today if you create a too-big index but it's probably horrible; it may succeed and then at search time you hit nasty exceptions when we overflow int. I think it should throw an IndexFullException instead.  It'd be nice if we could do this on the very doc that when added would go over the limit, but I would also settle for just throwing at flush as well ... i.e. I think what's really important is that the index does not become unusable.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12658412/LUCENE-5843.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5869", "change_description": ": Added restriction to positive values for maxExpansions in\nFuzzyQuery.", "change_title": "FuzzyQuery should require positive maxExpansions", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "FuzzyQuery currently only disallows negative values of maxExpansions.  However, passing 0 causes an NPE when the underlying TopTermsRewrite does a peek() on an empty queue, which returns null, and then goes on using it unknowingly.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12659816/LUCENE-5869.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5672", "change_description": ": IndexWriter.addIndexes() calls maybeMerge(), to ensure the index stays\nhealthy. If you don't want merging use NoMergePolicy instead.", "change_title": "Addindexes does not call maybeMerge", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "I don't know why this was removed, but this is buggy and just asking for trouble.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12655322/LUCENE-5672.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5897", "change_description": ",", "change_title": "performance bug (\"adversary\") in StandardTokenizer", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9.1,4.10,6.0", "detail_description": "There seem to be some conditions (I don't know how rare or what conditions) that cause StandardTokenizer to essentially hang on input: I havent looked hard yet, but as its essentially a DFA I think something wierd might be going on. An easy way to reproduce is with 1MB of underscores, it will just hang forever.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12663628/LUCENE-5897.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5400", "change_description": ",", "change_title": "Long text matching email local-part rule in UAX29URLEmailTokenizer causes extremely slow tokenization", "detail_type": "Bug", "detail_affect_versions": "4.5", "detail_fix_versions": "4.9.1,4.10,6.0", "detail_description": "This is a pretty nasty bug, and causes the cluster to stop accepting updates. I'm not sure how to consistently reproduce it but I have done so numerous times. Switching to a whitespace tokenizer improved indexing speed, and I never got the issue again. I'm running a 4.6 Snapshot - I had issues with deadlocks with numerous versions of Solr, and have finally narrowed down the problem to this code, which affects many/all versions of Solr. When the thread hits this issue it uses 100% CPU, restarting the node which has the error allows indexing to continue until hit again. Here is thread dump: http-bio-8080-exec-45 (201) org.apache.lucene.analysis.standard.UAX29URLEmailTokenizerImpl.getNextToken​(UAX29URLEmailTokenizerImpl.java:4343)     org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer.incrementToken​(UAX29URLEmailTokenizer.java:147)     org.apache.lucene.analysis.util.FilteringTokenFilter.incrementToken​(FilteringTokenFilter.java:82)     org.apache.lucene.analysis.core.LowerCaseFilter.incrementToken​(LowerCaseFilter.java:54)     org.apache.lucene.index.DocInverterPerField.processFields​(DocInverterPerField.java:174)     org.apache.lucene.index.DocFieldProcessor.processDocument​(DocFieldProcessor.java:248)     org.apache.lucene.index.DocumentsWriterPerThread.updateDocument​(DocumentsWriterPerThread.java:253)     org.apache.lucene.index.DocumentsWriter.updateDocument​(DocumentsWriter.java:453)     org.apache.lucene.index.IndexWriter.updateDocument​(IndexWriter.java:1517)     org.apache.solr.update.DirectUpdateHandler2.addDoc​(DirectUpdateHandler2.java:217)     org.apache.solr.update.processor.RunUpdateProcessor.processAdd​(RunUpdateProcessorFactory.java:69)     org.apache.solr.update.processor.UpdateRequestProcessor.processAdd​(UpdateRequestProcessor.java:51)     org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd​(DistributedUpdateProcessor.java:583)     org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd​(DistributedUpdateProcessor.java:719)     org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd​(DistributedUpdateProcessor.java:449)     org.apache.solr.handler.loader.JavabinLoader$1.update​(JavabinLoader.java:89)     org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator​(JavaBinUpdateRequestCodec.java:151)     org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator​(JavaBinUpdateRequestCodec.java:131)     org.apache.solr.common.util.JavaBinCodec.readVal​(JavaBinCodec.java:221)     org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList​(JavaBinUpdateRequestCodec.java:116)     org.apache.solr.common.util.JavaBinCodec.readVal​(JavaBinCodec.java:186)     org.apache.solr.common.util.JavaBinCodec.unmarshal​(JavaBinCodec.java:112)     org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal​(JavaBinUpdateRequestCodec.java:158)     org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs​(JavabinLoader.java:99)     org.apache.solr.handler.loader.JavabinLoader.load​(JavabinLoader.java:58)     org.apache.solr.handler.UpdateRequestHandler$1.load​(UpdateRequestHandler.java:92)     org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody​(ContentStreamHandlerBase.java:74)     org.apache.solr.handler.RequestHandlerBase.handleRequest​(RequestHandlerBase.java:135)     org.apache.solr.core.SolrCore.execute​(SolrCore.java:1859)     org.apache.solr.servlet.SolrDispatchFilter.execute​(SolrDispatchFilter.java:703)     org.apache.solr.servlet.SolrDispatchFilter.doFilter​(SolrDispatchFilter.java:406)     org.apache.solr.servlet.SolrDispatchFilter.doFilter​(SolrDispatchFilter.java:195)     org.apache.catalina.core.ApplicationFilterChain.internalDoFilter​(ApplicationFilterChain.java:243)     org.apache.catalina.core.ApplicationFilterChain.doFilter​(ApplicationFilterChain.java:210)     org.apache.catalina.core.StandardWrapperValve.invoke​(StandardWrapperValve.java:222)     org.apache.catalina.core.StandardContextValve.invoke​(StandardContextValve.java:123)     org.apache.catalina.core.StandardHostValve.invoke​(StandardHostValve.java:171)     org.apache.catalina.valves.ErrorReportValve.invoke​(ErrorReportValve.java:99)     org.apache.catalina.valves.AccessLogValve.invoke​(AccessLogValve.java:953)     org.apache.catalina.core.StandardEngineValve.invoke​(StandardEngineValve.java:118)     org.apache.catalina.connector.CoyoteAdapter.service​(CoyoteAdapter.java:408)     org.apache.coyote.http11.AbstractHttp11Processor.process​(AbstractHttp11Processor.java:1023)     org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process​(AbstractProtocol.java:589)     org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run​(JIoEndpoint.java:312)     java.util.concurrent.ThreadPoolExecutor.runWorker​(Unknown Source)     java.util.concurrent.ThreadPoolExecutor$Worker.run​(Unknown Source)     java.lang.Thread.run​(Unknown Source)", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5907", "change_description": ": Fix corruption case when opening a pre-4.x index with\nIndexWriter, then opening an NRT reader from that writer, then\ncalling commit from the writer, then closing the NRT reader.  This\ncase would remove the wrong files from the index leading to a\ncorrupt index.", "change_title": "closing NRT reader after upgrading from 3.x index can cause index corruption", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.9.1,4.10,6.0", "detail_description": "I have a small test case showing the issue.... I think we should fix this for 4.10?", "patch_link": "https://issues.apache.org/jira/secure/attachment/12664417/LUCENE-5907.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Bug Fixes", "change_id": "LUCENE-5908", "change_description": ": Fix Lucene43NGramTokenizer to be final", "change_title": "Assertion fails for Lucene43NGramTokenizer", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "As a side effect from LUCENE-5859, Lucene43NGramTokenizer was made not final.  This can trip an assert we have that Tokenizer need to either be final, or have their incrementToken() function be final.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12664455/LUCENE-5908.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Test Framework", "change_id": "LUCENE-5786", "change_description": ": Unflushed/ truncated events file (hung testing subprocess).", "change_title": "Unflushed/ truncated events file (hung testing subprocess)", "detail_type": "Bug", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "This has happened several times on Jenkins, typically on SSLMigrationTest.testDistribSearch, but probably on other tests as well. The symptom is: the test framework never terminates, it also reports an incorrect  hung test. The problem is that the actual forked JVM is hung on reading stdin, waiting for the next test suite (no test thread is present); the master process is hung on receiving data from the forked jvm (both the events file and stdout spill is truncated in the middle of a test). The last output is: Overall, it looks insane – there are flushes after each test completes (normally or not), there are tests following the one that last reported output and before dynamic suites on stdin. I have no idea. The best explanation is insane – looks like the test thread just died in the middle of executing Java code...", "patch_link": "none", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Test Framework", "change_id": "LUCENE-5881", "change_description": ": Add \"beasting\" of tests: repeats the whole \"test\" Ant target\nN times with \"ant beast -Dbeast.iters=N\".", "change_title": "Add test beaster", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "On dev@lao we discussed about integrating a test beaster directly into google. This extra target in common-build.xml does the same like Mike's Python script using Groovy.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12660852/LUCENE-5881.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Build", "change_id": "LUCENE-5770", "change_description": ": Upgrade to JFlex 1.6, which has direct support for\nsupplementary code points - as a result, ICU4J is no longer used\nto generate surrogate pairs to augment JFlex scanner specifications.", "change_title": "Upgrade JFlex to 1.6.0", "detail_type": "Task", "detail_affect_versions": "None", "detail_fix_versions": "4.10,5.0", "detail_description": "JFlex 1.6, to be released shortly, will have direct support for supplementary code points - JFlex 1.5 and earlier only support code points in the BMP. We can drop the use of ICU4J to generate surrogate pairs to extend our JFlex scanner specifications to handle supplementary code points.", "patch_link": "https://issues.apache.org/jira/secure/attachment/12654183/LUCENE-5770.patch", "patch_content": "none"}
{"library_version": "4.10.0", "change_type": "Build", "change_id": "SOLR-6358", "change_description": ": Remove VcsDirectoryMappings from idea configuration\nvcs.xml", "change_title": "Remove VcsDirectoryMappings from idea configuration vcs.xml", "detail_type": "Improvement", "detail_affect_versions": "None", "detail_fix_versions": "4.10,6.0", "detail_description": "Idea detects and sets this correctly anyway. Currently ant idea copies a file which specifies the VCS mapping to be SVN, which ends up not that working well when cloning using the Git mirror. Will attach a patch which removes the section.", "patch_link": "none", "patch_content": "none"}
